Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.619s
user	0m0.845s
sys	0m1.309s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Built target test-chat-template
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-gguf
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-barrier
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Built target test-quantize-fns
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 71%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Built target llama-imatrix
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-parallel
[ 81%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-quantize
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Built target llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-run
[ 90%] Built target llama-speculative
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-tts
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-llava-cli
[ 97%] Built target llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.138s
user	0m6.669s
sys	0m10.087s

main: quantize time =  4980.16 ms
main:    total time =  4980.16 ms

main: quantize time =  2480.17 ms
main:    total time =  2480.17 ms

main: quantize time =  2552.39 ms
main:    total time =  2552.39 ms

main: quantize time =  2498.05 ms
main:    total time =  2498.05 ms

main: quantize time =  2841.16 ms
main:    total time =  2841.16 ms

main: quantize time =  5625.79 ms
main:    total time =  5625.79 ms

main: quantize time =  6185.56 ms
main:    total time =  6185.56 ms

main: quantize time =  6961.18 ms
main:    total time =  6961.18 ms

main: quantize time =  6250.01 ms
main:    total time =  6250.01 ms

main: quantize time =  4451.48 ms
main:    total time =  4451.48 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.144 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.325 I main: llama backend init
0.00.000.332 I main: load the model and apply lora adapter, if any
0.00.066.462 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.079.543 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.598 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.606 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.089.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.098.993 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.098.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.098.999 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.098.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.099.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.099.002 I llama_model_loader: - type  f32:  194 tensors
0.00.099.002 I llama_model_loader: - type  f16:   98 tensors
0.00.099.004 I print_info: file format = GGUF V3 (latest)
0.00.099.005 I print_info: file type   = all F32 (guessed)
0.00.099.007 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.114.886 I load: special tokens cache size = 25
0.00.124.053 I load: token to piece cache size = 0.2984 MB
0.00.124.057 I print_info: arch             = gptneox
0.00.124.057 I print_info: vocab_only       = 0
0.00.124.057 I print_info: n_ctx_train      = 2048
0.00.124.057 I print_info: n_embd           = 2048
0.00.124.057 I print_info: n_layer          = 24
0.00.124.061 I print_info: n_head           = 16
0.00.124.062 I print_info: n_head_kv        = 16
0.00.124.062 I print_info: n_rot            = 32
0.00.124.062 I print_info: n_swa            = 0
0.00.124.062 I print_info: n_embd_head_k    = 128
0.00.124.063 I print_info: n_embd_head_v    = 128
0.00.124.063 I print_info: n_gqa            = 1
0.00.124.064 I print_info: n_embd_k_gqa     = 2048
0.00.124.067 I print_info: n_embd_v_gqa     = 2048
0.00.124.068 I print_info: f_norm_eps       = 1.0e-05
0.00.124.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.124.069 I print_info: f_clamp_kqv      = 0.0e+00
0.00.124.069 I print_info: f_max_alibi_bias = 0.0e+00
0.00.124.069 I print_info: f_logit_scale    = 0.0e+00
0.00.124.070 I print_info: n_ff             = 8192
0.00.124.070 I print_info: n_expert         = 0
0.00.124.071 I print_info: n_expert_used    = 0
0.00.124.071 I print_info: causal attn      = 1
0.00.124.071 I print_info: pooling type     = 0
0.00.124.071 I print_info: rope type        = 2
0.00.124.071 I print_info: rope scaling     = linear
0.00.124.072 I print_info: freq_base_train  = 10000.0
0.00.124.072 I print_info: freq_scale_train = 1
0.00.124.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.124.073 I print_info: rope_finetuned   = unknown
0.00.124.073 I print_info: ssm_d_conv       = 0
0.00.124.073 I print_info: ssm_d_inner      = 0
0.00.124.073 I print_info: ssm_d_state      = 0
0.00.124.073 I print_info: ssm_dt_rank      = 0
0.00.124.075 I print_info: ssm_dt_b_c_rms   = 0
0.00.124.075 I print_info: model type       = 1.4B
0.00.124.076 I print_info: model params     = 1.41 B
0.00.124.076 I print_info: general.name     = 1.4B
0.00.124.077 I print_info: vocab type       = BPE
0.00.124.077 I print_info: n_vocab          = 50304
0.00.124.077 I print_info: n_merges         = 50009
0.00.124.077 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.124.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.124.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.124.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.124.078 I print_info: LF token         = 187 'Ċ'
0.00.124.079 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.124.079 I print_info: max token length = 1024
0.00.124.079 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.170.837 I load_tensors: offloading 24 repeating layers to GPU
0.00.170.841 I load_tensors: offloading output layer to GPU
0.00.170.842 I load_tensors: offloaded 25/25 layers to GPU
0.00.170.870 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.170.872 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.171.350 I llama_init_from_model: n_seq_max     = 1
0.00.171.351 I llama_init_from_model: n_ctx         = 2048
0.00.171.351 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.171.351 I llama_init_from_model: n_batch       = 2048
0.00.171.351 I llama_init_from_model: n_ubatch      = 512
0.00.171.351 I llama_init_from_model: flash_attn    = 0
0.00.171.352 I llama_init_from_model: freq_base     = 10000.0
0.00.171.352 I llama_init_from_model: freq_scale    = 1
0.00.171.354 I ggml_metal_init: allocating
0.00.171.415 I ggml_metal_init: found device: Apple M4
0.00.171.422 I ggml_metal_init: picking default device: Apple M4
0.00.172.132 I ggml_metal_init: using embedded metal library
0.00.211.228 I ggml_metal_init: GPU name:   Apple M4
0.00.211.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.211.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.211.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.211.235 I ggml_metal_init: simdgroup reduction   = true
0.00.211.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.211.235 I ggml_metal_init: has residency sets    = true
0.00.211.235 I ggml_metal_init: has bfloat            = true
0.00.211.235 I ggml_metal_init: use bfloat            = true
0.00.211.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.211.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.750 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.387 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.429.393 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.436 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.020 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.434.023 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.434.023 I llama_init_from_model: graph nodes  = 967
0.00.434.023 I llama_init_from_model: graph splits = 2
0.00.434.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.077 I main: llama threadpool init, n_threads = 4
0.00.502.116 I 
0.00.502.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.131 I 
0.00.502.316 I sampler seed: 1234
0.00.502.320 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.502.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.502.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.502.347 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.333.803 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.02.333.804 I llama_perf_context_print:        load time =     434.76 ms
0.02.333.804 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.09 tokens per second)
0.02.333.805 I llama_perf_context_print:        eval time =    1784.94 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.333.805 I llama_perf_context_print:       total time =    1832.55 ms /    70 tokens
0.02.334.048 I ggml_metal_free: deallocating

real	0m2.674s
user	0m0.143s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.008 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.509 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.526 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.530 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.531 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.532 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.533 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.534 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.534 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.535 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.535 I llama_model_loader: - type  f32:  194 tensors
0.00.036.536 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.537 I print_info: file format = GGUF V3 (latest)
0.00.036.537 I print_info: file type   = Q8_0
0.00.036.539 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.369 I load: special tokens cache size = 25
0.00.052.508 I load: token to piece cache size = 0.2984 MB
0.00.052.512 I print_info: arch             = gptneox
0.00.052.513 I print_info: vocab_only       = 0
0.00.052.514 I print_info: n_ctx_train      = 2048
0.00.052.515 I print_info: n_embd           = 2048
0.00.052.515 I print_info: n_layer          = 24
0.00.052.520 I print_info: n_head           = 16
0.00.052.521 I print_info: n_head_kv        = 16
0.00.052.521 I print_info: n_rot            = 32
0.00.052.522 I print_info: n_swa            = 0
0.00.052.522 I print_info: n_embd_head_k    = 128
0.00.052.522 I print_info: n_embd_head_v    = 128
0.00.052.523 I print_info: n_gqa            = 1
0.00.052.524 I print_info: n_embd_k_gqa     = 2048
0.00.052.524 I print_info: n_embd_v_gqa     = 2048
0.00.052.525 I print_info: f_norm_eps       = 1.0e-05
0.00.052.525 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.525 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.526 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.526 I print_info: f_logit_scale    = 0.0e+00
0.00.052.527 I print_info: n_ff             = 8192
0.00.052.527 I print_info: n_expert         = 0
0.00.052.527 I print_info: n_expert_used    = 0
0.00.052.528 I print_info: causal attn      = 1
0.00.052.528 I print_info: pooling type     = 0
0.00.052.528 I print_info: rope type        = 2
0.00.052.530 I print_info: rope scaling     = linear
0.00.052.530 I print_info: freq_base_train  = 10000.0
0.00.052.531 I print_info: freq_scale_train = 1
0.00.052.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.531 I print_info: rope_finetuned   = unknown
0.00.052.531 I print_info: ssm_d_conv       = 0
0.00.052.531 I print_info: ssm_d_inner      = 0
0.00.052.532 I print_info: ssm_d_state      = 0
0.00.052.532 I print_info: ssm_dt_rank      = 0
0.00.052.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.532 I print_info: model type       = 1.4B
0.00.052.532 I print_info: model params     = 1.41 B
0.00.052.532 I print_info: general.name     = 1.4B
0.00.052.533 I print_info: vocab type       = BPE
0.00.052.533 I print_info: n_vocab          = 50304
0.00.052.533 I print_info: n_merges         = 50009
0.00.052.534 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.534 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.534 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.534 I print_info: LF token         = 187 'Ċ'
0.00.052.535 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.535 I print_info: max token length = 1024
0.00.052.539 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.306.776 I load_tensors: offloading 24 repeating layers to GPU
0.01.306.780 I load_tensors: offloading output layer to GPU
0.01.306.782 I load_tensors: offloaded 25/25 layers to GPU
0.01.306.805 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.306.806 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.307.867 I llama_init_from_model: n_seq_max     = 1
0.01.307.869 I llama_init_from_model: n_ctx         = 2048
0.01.307.869 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.307.870 I llama_init_from_model: n_batch       = 2048
0.01.307.870 I llama_init_from_model: n_ubatch      = 512
0.01.307.871 I llama_init_from_model: flash_attn    = 0
0.01.307.871 I llama_init_from_model: freq_base     = 10000.0
0.01.307.872 I llama_init_from_model: freq_scale    = 1
0.01.307.873 I ggml_metal_init: allocating
0.01.307.889 I ggml_metal_init: found device: Apple M4
0.01.307.898 I ggml_metal_init: picking default device: Apple M4
0.01.309.246 I ggml_metal_init: using embedded metal library
0.01.314.460 I ggml_metal_init: GPU name:   Apple M4
0.01.314.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.314.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.314.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.314.465 I ggml_metal_init: simdgroup reduction   = true
0.01.314.465 I ggml_metal_init: simdgroup matrix mul. = true
0.01.314.465 I ggml_metal_init: has residency sets    = true
0.01.314.466 I ggml_metal_init: has bfloat            = true
0.01.314.466 I ggml_metal_init: use bfloat            = true
0.01.314.467 I ggml_metal_init: hasUnifiedMemory      = true
0.01.314.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.329.967 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.375.723 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.375.734 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.375.768 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.380.435 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.380.437 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.380.437 I llama_init_from_model: graph nodes  = 967
0.01.380.437 I llama_init_from_model: graph splits = 2
0.01.380.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.380.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.380.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.434.636 I main: llama threadpool init, n_threads = 4
0.01.434.680 I 
0.01.434.695 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.434.695 I 
0.01.434.870 I sampler seed: 1234
0.01.434.875 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.434.886 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.434.886 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.434.886 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.526.757 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.02.526.758 I llama_perf_context_print:        load time =    1423.92 ms
0.02.526.759 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.41 tokens per second)
0.02.526.759 I llama_perf_context_print:        eval time =    1040.15 ms /    63 runs   (   16.51 ms per token,    60.57 tokens per second)
0.02.526.760 I llama_perf_context_print:       total time =    1092.82 ms /    70 tokens
0.02.526.977 I ggml_metal_free: deallocating

real	0m2.548s
user	0m0.109s
sys	0m0.253s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.864 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.522 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.528 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.532 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.535 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.538 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.539 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.271 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.272 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.272 I llama_model_loader: - type  f32:  194 tensors
0.00.028.273 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.273 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.274 I print_info: file format = GGUF V3 (latest)
0.00.028.275 I print_info: file type   = Q4_0
0.00.028.276 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.247 I load: special tokens cache size = 25
0.00.042.357 I load: token to piece cache size = 0.2984 MB
0.00.042.360 I print_info: arch             = gptneox
0.00.042.360 I print_info: vocab_only       = 0
0.00.042.361 I print_info: n_ctx_train      = 2048
0.00.042.361 I print_info: n_embd           = 2048
0.00.042.361 I print_info: n_layer          = 24
0.00.042.366 I print_info: n_head           = 16
0.00.042.367 I print_info: n_head_kv        = 16
0.00.042.367 I print_info: n_rot            = 32
0.00.042.367 I print_info: n_swa            = 0
0.00.042.368 I print_info: n_embd_head_k    = 128
0.00.042.368 I print_info: n_embd_head_v    = 128
0.00.042.369 I print_info: n_gqa            = 1
0.00.042.369 I print_info: n_embd_k_gqa     = 2048
0.00.042.370 I print_info: n_embd_v_gqa     = 2048
0.00.042.371 I print_info: f_norm_eps       = 1.0e-05
0.00.042.371 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.372 I print_info: f_logit_scale    = 0.0e+00
0.00.042.372 I print_info: n_ff             = 8192
0.00.042.373 I print_info: n_expert         = 0
0.00.042.373 I print_info: n_expert_used    = 0
0.00.042.373 I print_info: causal attn      = 1
0.00.042.373 I print_info: pooling type     = 0
0.00.042.373 I print_info: rope type        = 2
0.00.042.373 I print_info: rope scaling     = linear
0.00.042.374 I print_info: freq_base_train  = 10000.0
0.00.042.374 I print_info: freq_scale_train = 1
0.00.042.374 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.375 I print_info: rope_finetuned   = unknown
0.00.042.375 I print_info: ssm_d_conv       = 0
0.00.042.379 I print_info: ssm_d_inner      = 0
0.00.042.379 I print_info: ssm_d_state      = 0
0.00.042.379 I print_info: ssm_dt_rank      = 0
0.00.042.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.379 I print_info: model type       = 1.4B
0.00.042.380 I print_info: model params     = 1.41 B
0.00.042.380 I print_info: general.name     = 1.4B
0.00.042.381 I print_info: vocab type       = BPE
0.00.042.382 I print_info: n_vocab          = 50304
0.00.042.382 I print_info: n_merges         = 50009
0.00.042.382 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.383 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.383 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.383 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.383 I print_info: LF token         = 187 'Ċ'
0.00.042.384 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.385 I print_info: max token length = 1024
0.00.042.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.346 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.363 I load_tensors: offloading output layer to GPU
0.00.608.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.397 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.608.398 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.610.036 I llama_init_from_model: n_seq_max     = 1
0.00.610.038 I llama_init_from_model: n_ctx         = 2048
0.00.610.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.040 I llama_init_from_model: n_batch       = 2048
0.00.610.040 I llama_init_from_model: n_ubatch      = 512
0.00.610.041 I llama_init_from_model: flash_attn    = 0
0.00.610.043 I llama_init_from_model: freq_base     = 10000.0
0.00.610.043 I llama_init_from_model: freq_scale    = 1
0.00.610.045 I ggml_metal_init: allocating
0.00.610.121 I ggml_metal_init: found device: Apple M4
0.00.610.134 I ggml_metal_init: picking default device: Apple M4
0.00.611.965 I ggml_metal_init: using embedded metal library
0.00.617.494 I ggml_metal_init: GPU name:   Apple M4
0.00.617.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.500 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.502 I ggml_metal_init: simdgroup reduction   = true
0.00.617.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.502 I ggml_metal_init: has residency sets    = true
0.00.617.503 I ggml_metal_init: has bfloat            = true
0.00.617.503 I ggml_metal_init: use bfloat            = true
0.00.617.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.930 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.936 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.969 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.863 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.865 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.866 I llama_init_from_model: graph nodes  = 967
0.00.698.866 I llama_init_from_model: graph splits = 2
0.00.698.870 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.699.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.699.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.038 I main: llama threadpool init, n_threads = 4
0.00.753.082 I 
0.00.753.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.096 I 
0.00.753.264 I sampler seed: 1234
0.00.753.269 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.280 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.280 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.280 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.442.992 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50070.52 tokens per second)
0.01.442.993 I llama_perf_context_print:        load time =     740.48 ms
0.01.442.993 I llama_perf_context_print: prompt eval time =      48.86 ms /     7 tokens (    6.98 ms per token,   143.28 tokens per second)
0.01.442.994 I llama_perf_context_print:        eval time =     637.90 ms /    63 runs   (   10.13 ms per token,    98.76 tokens per second)
0.01.442.995 I llama_perf_context_print:       total time =     690.64 ms /    70 tokens
0.01.443.194 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.443 I llama_model_loader: - type  f32:  194 tensors
0.00.025.443 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.443 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.444 I print_info: file format = GGUF V3 (latest)
0.00.025.444 I print_info: file type   = Q4_1
0.00.025.445 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.167 I load: special tokens cache size = 25
0.00.039.075 I load: token to piece cache size = 0.2984 MB
0.00.039.078 I print_info: arch             = gptneox
0.00.039.078 I print_info: vocab_only       = 0
0.00.039.078 I print_info: n_ctx_train      = 2048
0.00.039.078 I print_info: n_embd           = 2048
0.00.039.079 I print_info: n_layer          = 24
0.00.039.082 I print_info: n_head           = 16
0.00.039.083 I print_info: n_head_kv        = 16
0.00.039.083 I print_info: n_rot            = 32
0.00.039.083 I print_info: n_swa            = 0
0.00.039.083 I print_info: n_embd_head_k    = 128
0.00.039.083 I print_info: n_embd_head_v    = 128
0.00.039.084 I print_info: n_gqa            = 1
0.00.039.085 I print_info: n_embd_k_gqa     = 2048
0.00.039.086 I print_info: n_embd_v_gqa     = 2048
0.00.039.087 I print_info: f_norm_eps       = 1.0e-05
0.00.039.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.088 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.088 I print_info: f_logit_scale    = 0.0e+00
0.00.039.089 I print_info: n_ff             = 8192
0.00.039.089 I print_info: n_expert         = 0
0.00.039.089 I print_info: n_expert_used    = 0
0.00.039.089 I print_info: causal attn      = 1
0.00.039.089 I print_info: pooling type     = 0
0.00.039.089 I print_info: rope type        = 2
0.00.039.090 I print_info: rope scaling     = linear
0.00.039.090 I print_info: freq_base_train  = 10000.0
0.00.039.091 I print_info: freq_scale_train = 1
0.00.039.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.091 I print_info: rope_finetuned   = unknown
0.00.039.091 I print_info: ssm_d_conv       = 0
0.00.039.091 I print_info: ssm_d_inner      = 0
0.00.039.091 I print_info: ssm_d_state      = 0
0.00.039.092 I print_info: ssm_dt_rank      = 0
0.00.039.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.092 I print_info: model type       = 1.4B
0.00.039.092 I print_info: model params     = 1.41 B
0.00.039.093 I print_info: general.name     = 1.4B
0.00.039.095 I print_info: vocab type       = BPE
0.00.039.095 I print_info: n_vocab          = 50304
0.00.039.095 I print_info: n_merges         = 50009
0.00.039.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: LF token         = 187 'Ċ'
0.00.039.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: max token length = 1024
0.00.039.097 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.323 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.337 I load_tensors: offloading output layer to GPU
0.00.652.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.376 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.652.378 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.654.134 I llama_init_from_model: n_seq_max     = 1
0.00.654.136 I llama_init_from_model: n_ctx         = 2048
0.00.654.137 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.137 I llama_init_from_model: n_batch       = 2048
0.00.654.138 I llama_init_from_model: n_ubatch      = 512
0.00.654.138 I llama_init_from_model: flash_attn    = 0
0.00.654.140 I llama_init_from_model: freq_base     = 10000.0
0.00.654.141 I llama_init_from_model: freq_scale    = 1
0.00.654.144 I ggml_metal_init: allocating
0.00.654.249 I ggml_metal_init: found device: Apple M4
0.00.654.262 I ggml_metal_init: picking default device: Apple M4
0.00.656.214 I ggml_metal_init: using embedded metal library
0.00.662.840 I ggml_metal_init: GPU name:   Apple M4
0.00.662.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.662.846 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.662.847 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.662.848 I ggml_metal_init: simdgroup reduction   = true
0.00.662.848 I ggml_metal_init: simdgroup matrix mul. = true
0.00.662.848 I ggml_metal_init: has residency sets    = true
0.00.662.849 I ggml_metal_init: has bfloat            = true
0.00.662.849 I ggml_metal_init: use bfloat            = true
0.00.662.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.662.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.134 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.913 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.740.921 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.968 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.745.823 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.745.825 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.745.825 I llama_init_from_model: graph nodes  = 967
0.00.745.825 I llama_init_from_model: graph splits = 2
0.00.745.831 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.745.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.915 I main: llama threadpool init, n_threads = 4
0.00.802.959 I 
0.00.802.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.975 I 
0.00.803.132 I sampler seed: 1234
0.00.803.137 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.160 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.162 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.162 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.534.076 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.534.077 I llama_perf_context_print:        load time =     793.31 ms
0.01.534.077 I llama_perf_context_print: prompt eval time =      47.99 ms /     7 tokens (    6.86 ms per token,   145.87 tokens per second)
0.01.534.079 I llama_perf_context_print:        eval time =     680.16 ms /    63 runs   (   10.80 ms per token,    92.63 tokens per second)
0.01.534.080 I llama_perf_context_print:       total time =     731.87 ms /    70 tokens
0.01.534.355 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.439 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.441 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.442 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.451 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.984 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.985 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.985 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.985 I llama_model_loader: - type  f32:  194 tensors
0.00.024.986 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.987 I print_info: file format = GGUF V3 (latest)
0.00.024.987 I print_info: file type   = Q5_0
0.00.024.988 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.676 I load: special tokens cache size = 25
0.00.038.669 I load: token to piece cache size = 0.2984 MB
0.00.038.672 I print_info: arch             = gptneox
0.00.038.672 I print_info: vocab_only       = 0
0.00.038.672 I print_info: n_ctx_train      = 2048
0.00.038.672 I print_info: n_embd           = 2048
0.00.038.672 I print_info: n_layer          = 24
0.00.038.675 I print_info: n_head           = 16
0.00.038.676 I print_info: n_head_kv        = 16
0.00.038.678 I print_info: n_rot            = 32
0.00.038.678 I print_info: n_swa            = 0
0.00.038.679 I print_info: n_embd_head_k    = 128
0.00.038.679 I print_info: n_embd_head_v    = 128
0.00.038.679 I print_info: n_gqa            = 1
0.00.038.680 I print_info: n_embd_k_gqa     = 2048
0.00.038.681 I print_info: n_embd_v_gqa     = 2048
0.00.038.681 I print_info: f_norm_eps       = 1.0e-05
0.00.038.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.682 I print_info: f_logit_scale    = 0.0e+00
0.00.038.683 I print_info: n_ff             = 8192
0.00.038.683 I print_info: n_expert         = 0
0.00.038.683 I print_info: n_expert_used    = 0
0.00.038.683 I print_info: causal attn      = 1
0.00.038.684 I print_info: pooling type     = 0
0.00.038.690 I print_info: rope type        = 2
0.00.038.693 I print_info: rope scaling     = linear
0.00.038.693 I print_info: freq_base_train  = 10000.0
0.00.038.694 I print_info: freq_scale_train = 1
0.00.038.694 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.694 I print_info: rope_finetuned   = unknown
0.00.038.694 I print_info: ssm_d_conv       = 0
0.00.038.695 I print_info: ssm_d_inner      = 0
0.00.038.695 I print_info: ssm_d_state      = 0
0.00.038.695 I print_info: ssm_dt_rank      = 0
0.00.038.695 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.695 I print_info: model type       = 1.4B
0.00.038.696 I print_info: model params     = 1.41 B
0.00.038.696 I print_info: general.name     = 1.4B
0.00.038.696 I print_info: vocab type       = BPE
0.00.038.697 I print_info: n_vocab          = 50304
0.00.038.697 I print_info: n_merges         = 50009
0.00.038.700 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: LF token         = 187 'Ċ'
0.00.038.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: max token length = 1024
0.00.038.702 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.313 I load_tensors: offloading output layer to GPU
0.00.660.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.347 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.660.348 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.662.052 I llama_init_from_model: n_seq_max     = 1
0.00.662.054 I llama_init_from_model: n_ctx         = 2048
0.00.662.054 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.662.055 I llama_init_from_model: n_batch       = 2048
0.00.662.055 I llama_init_from_model: n_ubatch      = 512
0.00.662.056 I llama_init_from_model: flash_attn    = 0
0.00.662.057 I llama_init_from_model: freq_base     = 10000.0
0.00.662.057 I llama_init_from_model: freq_scale    = 1
0.00.662.058 I ggml_metal_init: allocating
0.00.662.068 I ggml_metal_init: found device: Apple M4
0.00.662.077 I ggml_metal_init: picking default device: Apple M4
0.00.663.547 I ggml_metal_init: using embedded metal library
0.00.669.668 I ggml_metal_init: GPU name:   Apple M4
0.00.669.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.674 I ggml_metal_init: simdgroup reduction   = true
0.00.669.674 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.674 I ggml_metal_init: has residency sets    = true
0.00.669.675 I ggml_metal_init: has bfloat            = true
0.00.669.675 I ggml_metal_init: use bfloat            = true
0.00.669.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.140 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.644 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.740.650 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.685 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.744.753 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.744.756 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.744.756 I llama_init_from_model: graph nodes  = 967
0.00.744.757 I llama_init_from_model: graph splits = 2
0.00.744.761 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.699 I main: llama threadpool init, n_threads = 4
0.00.804.742 I 
0.00.804.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.761 I 
0.00.804.928 I sampler seed: 1234
0.00.804.933 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.954 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.954 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.597.746 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.597.746 I llama_perf_context_print:        load time =     795.09 ms
0.01.597.747 I llama_perf_context_print: prompt eval time =      53.04 ms /     7 tokens (    7.58 ms per token,   131.98 tokens per second)
0.01.597.748 I llama_perf_context_print:        eval time =     736.87 ms /    63 runs   (   11.70 ms per token,    85.50 tokens per second)
0.01.597.748 I llama_perf_context_print:       total time =     793.78 ms /    70 tokens
0.01.597.979 I ggml_metal_free: deallocating

real	0m1.616s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.269 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.977 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.978 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.979 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.984 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.985 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.985 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.987 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.634 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.637 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.637 I llama_model_loader: - type  f32:  194 tensors
0.00.026.638 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.638 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.638 I print_info: file format = GGUF V3 (latest)
0.00.026.639 I print_info: file type   = Q5_1
0.00.026.641 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.475 I load: special tokens cache size = 25
0.00.040.257 I load: token to piece cache size = 0.2984 MB
0.00.040.260 I print_info: arch             = gptneox
0.00.040.261 I print_info: vocab_only       = 0
0.00.040.261 I print_info: n_ctx_train      = 2048
0.00.040.261 I print_info: n_embd           = 2048
0.00.040.261 I print_info: n_layer          = 24
0.00.040.264 I print_info: n_head           = 16
0.00.040.265 I print_info: n_head_kv        = 16
0.00.040.265 I print_info: n_rot            = 32
0.00.040.265 I print_info: n_swa            = 0
0.00.040.265 I print_info: n_embd_head_k    = 128
0.00.040.265 I print_info: n_embd_head_v    = 128
0.00.040.267 I print_info: n_gqa            = 1
0.00.040.268 I print_info: n_embd_k_gqa     = 2048
0.00.040.270 I print_info: n_embd_v_gqa     = 2048
0.00.040.271 I print_info: f_norm_eps       = 1.0e-05
0.00.040.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.272 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.272 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.272 I print_info: f_logit_scale    = 0.0e+00
0.00.040.273 I print_info: n_ff             = 8192
0.00.040.273 I print_info: n_expert         = 0
0.00.040.274 I print_info: n_expert_used    = 0
0.00.040.274 I print_info: causal attn      = 1
0.00.040.274 I print_info: pooling type     = 0
0.00.040.276 I print_info: rope type        = 2
0.00.040.277 I print_info: rope scaling     = linear
0.00.040.277 I print_info: freq_base_train  = 10000.0
0.00.040.278 I print_info: freq_scale_train = 1
0.00.040.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.278 I print_info: rope_finetuned   = unknown
0.00.040.278 I print_info: ssm_d_conv       = 0
0.00.040.278 I print_info: ssm_d_inner      = 0
0.00.040.278 I print_info: ssm_d_state      = 0
0.00.040.278 I print_info: ssm_dt_rank      = 0
0.00.040.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.282 I print_info: model type       = 1.4B
0.00.040.283 I print_info: model params     = 1.41 B
0.00.040.283 I print_info: general.name     = 1.4B
0.00.040.284 I print_info: vocab type       = BPE
0.00.040.284 I print_info: n_vocab          = 50304
0.00.040.284 I print_info: n_merges         = 50009
0.00.040.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.285 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.285 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.285 I print_info: LF token         = 187 'Ċ'
0.00.040.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.285 I print_info: max token length = 1024
0.00.040.286 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.992 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.007 I load_tensors: offloading output layer to GPU
0.00.614.008 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.043 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.614.044 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.615.560 I llama_init_from_model: n_seq_max     = 1
0.00.615.563 I llama_init_from_model: n_ctx         = 2048
0.00.615.564 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.564 I llama_init_from_model: n_batch       = 2048
0.00.615.565 I llama_init_from_model: n_ubatch      = 512
0.00.615.566 I llama_init_from_model: flash_attn    = 0
0.00.615.568 I llama_init_from_model: freq_base     = 10000.0
0.00.615.569 I llama_init_from_model: freq_scale    = 1
0.00.615.572 I ggml_metal_init: allocating
0.00.615.647 I ggml_metal_init: found device: Apple M4
0.00.615.659 I ggml_metal_init: picking default device: Apple M4
0.00.617.166 I ggml_metal_init: using embedded metal library
0.00.623.499 I ggml_metal_init: GPU name:   Apple M4
0.00.623.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.504 I ggml_metal_init: simdgroup reduction   = true
0.00.623.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.505 I ggml_metal_init: has residency sets    = true
0.00.623.505 I ggml_metal_init: has bfloat            = true
0.00.623.506 I ggml_metal_init: use bfloat            = true
0.00.623.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.508 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.380 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.451 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.696.456 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.696.491 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.755 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.757 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.757 I llama_init_from_model: graph nodes  = 967
0.00.700.757 I llama_init_from_model: graph splits = 2
0.00.700.764 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.909 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.434 I main: llama threadpool init, n_threads = 4
0.00.758.477 I 
0.00.758.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.516 I 
0.00.758.732 I sampler seed: 1234
0.00.758.741 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.756 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.758 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.758 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.597.551 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.01.597.552 I llama_perf_context_print:        load time =     747.46 ms
0.01.597.554 I llama_perf_context_print: prompt eval time =      52.71 ms /     7 tokens (    7.53 ms per token,   132.80 tokens per second)
0.01.597.554 I llama_perf_context_print:        eval time =     783.19 ms /    63 runs   (   12.43 ms per token,    80.44 tokens per second)
0.01.597.555 I llama_perf_context_print:       total time =     839.81 ms /    70 tokens
0.01.597.798 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.107s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.575 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.577 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.577 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.581 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.198 I llama_model_loader: - type  f32:  194 tensors
0.00.024.198 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.199 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.199 I print_info: file format = GGUF V3 (latest)
0.00.024.200 I print_info: file type   = Q2_K - Medium
0.00.024.201 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.259 I load: special tokens cache size = 25
0.00.038.395 I load: token to piece cache size = 0.2984 MB
0.00.038.398 I print_info: arch             = gptneox
0.00.038.398 I print_info: vocab_only       = 0
0.00.038.398 I print_info: n_ctx_train      = 2048
0.00.038.398 I print_info: n_embd           = 2048
0.00.038.399 I print_info: n_layer          = 24
0.00.038.401 I print_info: n_head           = 16
0.00.038.402 I print_info: n_head_kv        = 16
0.00.038.402 I print_info: n_rot            = 32
0.00.038.402 I print_info: n_swa            = 0
0.00.038.402 I print_info: n_embd_head_k    = 128
0.00.038.403 I print_info: n_embd_head_v    = 128
0.00.038.404 I print_info: n_gqa            = 1
0.00.038.405 I print_info: n_embd_k_gqa     = 2048
0.00.038.405 I print_info: n_embd_v_gqa     = 2048
0.00.038.406 I print_info: f_norm_eps       = 1.0e-05
0.00.038.406 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.407 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.407 I print_info: f_logit_scale    = 0.0e+00
0.00.038.408 I print_info: n_ff             = 8192
0.00.038.408 I print_info: n_expert         = 0
0.00.038.408 I print_info: n_expert_used    = 0
0.00.038.408 I print_info: causal attn      = 1
0.00.038.408 I print_info: pooling type     = 0
0.00.038.408 I print_info: rope type        = 2
0.00.038.409 I print_info: rope scaling     = linear
0.00.038.409 I print_info: freq_base_train  = 10000.0
0.00.038.409 I print_info: freq_scale_train = 1
0.00.038.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.410 I print_info: rope_finetuned   = unknown
0.00.038.410 I print_info: ssm_d_conv       = 0
0.00.038.411 I print_info: ssm_d_inner      = 0
0.00.038.411 I print_info: ssm_d_state      = 0
0.00.038.412 I print_info: ssm_dt_rank      = 0
0.00.038.412 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.412 I print_info: model type       = 1.4B
0.00.038.412 I print_info: model params     = 1.41 B
0.00.038.412 I print_info: general.name     = 1.4B
0.00.038.413 I print_info: vocab type       = BPE
0.00.038.413 I print_info: n_vocab          = 50304
0.00.038.413 I print_info: n_merges         = 50009
0.00.038.414 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.414 I print_info: LF token         = 187 'Ċ'
0.00.038.415 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.415 I print_info: max token length = 1024
0.00.038.415 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.360.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.360.309 I load_tensors: offloading output layer to GPU
0.00.360.310 I load_tensors: offloaded 25/25 layers to GPU
0.00.360.341 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.360.342 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.362.006 I llama_init_from_model: n_seq_max     = 1
0.00.362.012 I llama_init_from_model: n_ctx         = 2048
0.00.362.013 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.362.013 I llama_init_from_model: n_batch       = 2048
0.00.362.014 I llama_init_from_model: n_ubatch      = 512
0.00.362.014 I llama_init_from_model: flash_attn    = 0
0.00.362.015 I llama_init_from_model: freq_base     = 10000.0
0.00.362.016 I llama_init_from_model: freq_scale    = 1
0.00.362.019 I ggml_metal_init: allocating
0.00.362.104 I ggml_metal_init: found device: Apple M4
0.00.362.125 I ggml_metal_init: picking default device: Apple M4
0.00.363.922 I ggml_metal_init: using embedded metal library
0.00.369.651 I ggml_metal_init: GPU name:   Apple M4
0.00.369.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.369.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.369.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.369.667 I ggml_metal_init: simdgroup reduction   = true
0.00.369.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.369.668 I ggml_metal_init: has residency sets    = true
0.00.369.668 I ggml_metal_init: has bfloat            = true
0.00.369.668 I ggml_metal_init: use bfloat            = true
0.00.369.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.369.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.392.006 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.077 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.092 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.454.646 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.454.648 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.454.648 I llama_init_from_model: graph nodes  = 967
0.00.454.648 I llama_init_from_model: graph splits = 2
0.00.454.651 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.454.776 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.454.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.100 I main: llama threadpool init, n_threads = 4
0.00.513.143 I 
0.00.513.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.159 I 
0.00.513.300 I sampler seed: 1234
0.00.513.305 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.316 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.317 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.317 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.183.601 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.183.602 I llama_perf_context_print:        load time =     503.50 ms
0.01.183.603 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.45 tokens per second)
0.01.183.603 I llama_perf_context_print:        eval time =     631.93 ms /    63 runs   (   10.03 ms per token,    99.69 tokens per second)
0.01.183.604 I llama_perf_context_print:       total time =     671.18 ms /    70 tokens
0.01.183.831 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.112s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.679 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.049 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.050 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.059 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.060 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.670 I llama_model_loader: - type  f32:  194 tensors
0.00.024.670 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.670 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.670 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.671 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.671 I print_info: file format = GGUF V3 (latest)
0.00.024.672 I print_info: file type   = Q3_K - Medium
0.00.024.672 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.453 I load: special tokens cache size = 25
0.00.038.530 I load: token to piece cache size = 0.2984 MB
0.00.038.533 I print_info: arch             = gptneox
0.00.038.533 I print_info: vocab_only       = 0
0.00.038.533 I print_info: n_ctx_train      = 2048
0.00.038.533 I print_info: n_embd           = 2048
0.00.038.533 I print_info: n_layer          = 24
0.00.038.537 I print_info: n_head           = 16
0.00.038.537 I print_info: n_head_kv        = 16
0.00.038.537 I print_info: n_rot            = 32
0.00.038.539 I print_info: n_swa            = 0
0.00.038.539 I print_info: n_embd_head_k    = 128
0.00.038.539 I print_info: n_embd_head_v    = 128
0.00.038.540 I print_info: n_gqa            = 1
0.00.038.541 I print_info: n_embd_k_gqa     = 2048
0.00.038.541 I print_info: n_embd_v_gqa     = 2048
0.00.038.542 I print_info: f_norm_eps       = 1.0e-05
0.00.038.544 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.544 I print_info: f_logit_scale    = 0.0e+00
0.00.038.545 I print_info: n_ff             = 8192
0.00.038.545 I print_info: n_expert         = 0
0.00.038.545 I print_info: n_expert_used    = 0
0.00.038.547 I print_info: causal attn      = 1
0.00.038.548 I print_info: pooling type     = 0
0.00.038.548 I print_info: rope type        = 2
0.00.038.548 I print_info: rope scaling     = linear
0.00.038.549 I print_info: freq_base_train  = 10000.0
0.00.038.549 I print_info: freq_scale_train = 1
0.00.038.549 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.550 I print_info: rope_finetuned   = unknown
0.00.038.550 I print_info: ssm_d_conv       = 0
0.00.038.550 I print_info: ssm_d_inner      = 0
0.00.038.550 I print_info: ssm_d_state      = 0
0.00.038.550 I print_info: ssm_dt_rank      = 0
0.00.038.550 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.550 I print_info: model type       = 1.4B
0.00.038.551 I print_info: model params     = 1.41 B
0.00.038.552 I print_info: general.name     = 1.4B
0.00.038.553 I print_info: vocab type       = BPE
0.00.038.553 I print_info: n_vocab          = 50304
0.00.038.553 I print_info: n_merges         = 50009
0.00.038.553 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.554 I print_info: LF token         = 187 'Ċ'
0.00.038.554 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.555 I print_info: max token length = 1024
0.00.038.555 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.433.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.544 I load_tensors: offloading output layer to GPU
0.00.433.545 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.578 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.580 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.435.348 I llama_init_from_model: n_seq_max     = 1
0.00.435.351 I llama_init_from_model: n_ctx         = 2048
0.00.435.351 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.435.352 I llama_init_from_model: n_batch       = 2048
0.00.435.352 I llama_init_from_model: n_ubatch      = 512
0.00.435.352 I llama_init_from_model: flash_attn    = 0
0.00.435.355 I llama_init_from_model: freq_base     = 10000.0
0.00.435.355 I llama_init_from_model: freq_scale    = 1
0.00.435.357 I ggml_metal_init: allocating
0.00.435.406 I ggml_metal_init: found device: Apple M4
0.00.435.419 I ggml_metal_init: picking default device: Apple M4
0.00.437.181 I ggml_metal_init: using embedded metal library
0.00.443.361 I ggml_metal_init: GPU name:   Apple M4
0.00.443.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.369 I ggml_metal_init: simdgroup reduction   = true
0.00.443.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.370 I ggml_metal_init: has residency sets    = true
0.00.443.370 I ggml_metal_init: has bfloat            = true
0.00.443.370 I ggml_metal_init: use bfloat            = true
0.00.443.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.260 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.518.097 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.518.107 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.518.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.522.331 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.522.334 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.522.335 I llama_init_from_model: graph nodes  = 967
0.00.522.335 I llama_init_from_model: graph splits = 2
0.00.522.338 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.522.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.522.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.569.938 I main: llama threadpool init, n_threads = 4
0.00.569.981 I 
0.00.569.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.569.995 I 
0.00.570.103 I sampler seed: 1234
0.00.570.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.570.125 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.570.125 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.570.125 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.310.854 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47051.03 tokens per second)
0.01.310.856 I llama_perf_context_print:        load time =     560.56 ms
0.01.310.856 I llama_perf_context_print: prompt eval time =      40.27 ms /     7 tokens (    5.75 ms per token,   173.81 tokens per second)
0.01.310.858 I llama_perf_context_print:        eval time =     697.79 ms /    63 runs   (   11.08 ms per token,    90.28 tokens per second)
0.01.310.858 I llama_perf_context_print:       total time =     741.61 ms /    70 tokens
0.01.311.092 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.109s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.098 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.214 I main: llama backend init
0.00.000.220 I main: load the model and apply lora adapter, if any
0.00.086.047 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.127.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.127.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.127.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.127.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.127.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.127.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.127.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.127.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.127.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.127.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.127.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.127.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.127.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.127.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.127.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.127.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.127.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.134.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.136.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.144.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.144.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.144.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.144.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.144.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.144.095 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.144.097 I llama_model_loader: - type  f32:  194 tensors
0.00.144.098 I llama_model_loader: - type q4_K:   61 tensors
0.00.144.099 I llama_model_loader: - type q5_K:   24 tensors
0.00.144.099 I llama_model_loader: - type q6_K:   13 tensors
0.00.144.102 I print_info: file format = GGUF V3 (latest)
0.00.144.103 I print_info: file type   = Q4_K - Medium
0.00.144.107 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.157.702 I load: special tokens cache size = 25
0.00.163.913 I load: token to piece cache size = 0.2984 MB
0.00.163.917 I print_info: arch             = gptneox
0.00.163.917 I print_info: vocab_only       = 0
0.00.163.917 I print_info: n_ctx_train      = 2048
0.00.163.917 I print_info: n_embd           = 2048
0.00.163.918 I print_info: n_layer          = 24
0.00.163.922 I print_info: n_head           = 16
0.00.163.923 I print_info: n_head_kv        = 16
0.00.163.923 I print_info: n_rot            = 32
0.00.163.923 I print_info: n_swa            = 0
0.00.163.923 I print_info: n_embd_head_k    = 128
0.00.163.923 I print_info: n_embd_head_v    = 128
0.00.163.924 I print_info: n_gqa            = 1
0.00.163.925 I print_info: n_embd_k_gqa     = 2048
0.00.163.926 I print_info: n_embd_v_gqa     = 2048
0.00.163.926 I print_info: f_norm_eps       = 1.0e-05
0.00.163.928 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.163.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.163.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.163.928 I print_info: f_logit_scale    = 0.0e+00
0.00.163.929 I print_info: n_ff             = 8192
0.00.163.929 I print_info: n_expert         = 0
0.00.163.930 I print_info: n_expert_used    = 0
0.00.163.930 I print_info: causal attn      = 1
0.00.163.930 I print_info: pooling type     = 0
0.00.163.931 I print_info: rope type        = 2
0.00.163.933 I print_info: rope scaling     = linear
0.00.163.934 I print_info: freq_base_train  = 10000.0
0.00.163.934 I print_info: freq_scale_train = 1
0.00.163.934 I print_info: n_ctx_orig_yarn  = 2048
0.00.163.934 I print_info: rope_finetuned   = unknown
0.00.163.934 I print_info: ssm_d_conv       = 0
0.00.163.935 I print_info: ssm_d_inner      = 0
0.00.163.935 I print_info: ssm_d_state      = 0
0.00.163.936 I print_info: ssm_dt_rank      = 0
0.00.163.936 I print_info: ssm_dt_b_c_rms   = 0
0.00.163.936 I print_info: model type       = 1.4B
0.00.163.943 I print_info: model params     = 1.41 B
0.00.163.945 I print_info: general.name     = 1.4B
0.00.163.946 I print_info: vocab type       = BPE
0.00.163.947 I print_info: n_vocab          = 50304
0.00.163.947 I print_info: n_merges         = 50009
0.00.163.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.163.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.163.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.163.948 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.163.948 I print_info: LF token         = 187 'Ċ'
0.00.163.950 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.163.950 I print_info: max token length = 1024
0.00.163.951 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.781.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.781.616 I load_tensors: offloading output layer to GPU
0.00.781.616 I load_tensors: offloaded 25/25 layers to GPU
0.00.781.636 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.781.637 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.782.534 I llama_init_from_model: n_seq_max     = 1
0.00.782.540 I llama_init_from_model: n_ctx         = 2048
0.00.782.540 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.782.541 I llama_init_from_model: n_batch       = 2048
0.00.782.541 I llama_init_from_model: n_ubatch      = 512
0.00.782.541 I llama_init_from_model: flash_attn    = 0
0.00.782.542 I llama_init_from_model: freq_base     = 10000.0
0.00.782.543 I llama_init_from_model: freq_scale    = 1
0.00.782.544 I ggml_metal_init: allocating
0.00.782.574 I ggml_metal_init: found device: Apple M4
0.00.782.583 I ggml_metal_init: picking default device: Apple M4
0.00.783.677 I ggml_metal_init: using embedded metal library
0.00.787.969 I ggml_metal_init: GPU name:   Apple M4
0.00.787.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.787.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.787.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.787.977 I ggml_metal_init: simdgroup reduction   = true
0.00.787.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.787.978 I ggml_metal_init: has residency sets    = true
0.00.787.978 I ggml_metal_init: has bfloat            = true
0.00.787.978 I ggml_metal_init: use bfloat            = true
0.00.787.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.787.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.803.858 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.837.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.837.249 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.837.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.842.533 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.842.534 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.842.535 I llama_init_from_model: graph nodes  = 967
0.00.842.535 I llama_init_from_model: graph splits = 2
0.00.842.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.842.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.842.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.901.190 I main: llama threadpool init, n_threads = 4
0.00.901.234 I 
0.00.901.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.901.248 I 
0.00.901.429 I sampler seed: 1234
0.00.901.433 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.901.444 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.901.445 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.901.446 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.674.048 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 46044.10 tokens per second)
0.01.674.048 I llama_perf_context_print:        load time =     814.44 ms
0.01.674.049 I llama_perf_context_print: prompt eval time =      58.37 ms /     7 tokens (    8.34 ms per token,   119.93 tokens per second)
0.01.674.054 I llama_perf_context_print:        eval time =     711.60 ms /    63 runs   (   11.30 ms per token,    88.53 tokens per second)
0.01.674.055 I llama_perf_context_print:       total time =     773.55 ms /    70 tokens
0.01.674.321 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.127s
sys	0m0.173s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.738 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.025.122 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.124 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.133 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.133 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.128 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.007 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.034.009 I llama_model_loader: - type  f32:  194 tensors
0.00.034.009 I llama_model_loader: - type q5_K:   61 tensors
0.00.034.009 I llama_model_loader: - type q6_K:   37 tensors
0.00.034.010 I print_info: file format = GGUF V3 (latest)
0.00.034.011 I print_info: file type   = Q5_K - Medium
0.00.034.012 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.852 I load: special tokens cache size = 25
0.00.049.039 I load: token to piece cache size = 0.2984 MB
0.00.049.043 I print_info: arch             = gptneox
0.00.049.044 I print_info: vocab_only       = 0
0.00.049.044 I print_info: n_ctx_train      = 2048
0.00.049.044 I print_info: n_embd           = 2048
0.00.049.044 I print_info: n_layer          = 24
0.00.049.048 I print_info: n_head           = 16
0.00.049.049 I print_info: n_head_kv        = 16
0.00.049.049 I print_info: n_rot            = 32
0.00.049.049 I print_info: n_swa            = 0
0.00.049.049 I print_info: n_embd_head_k    = 128
0.00.049.051 I print_info: n_embd_head_v    = 128
0.00.049.052 I print_info: n_gqa            = 1
0.00.049.054 I print_info: n_embd_k_gqa     = 2048
0.00.049.054 I print_info: n_embd_v_gqa     = 2048
0.00.049.054 I print_info: f_norm_eps       = 1.0e-05
0.00.049.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.055 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.055 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.055 I print_info: f_logit_scale    = 0.0e+00
0.00.049.057 I print_info: n_ff             = 8192
0.00.049.058 I print_info: n_expert         = 0
0.00.049.058 I print_info: n_expert_used    = 0
0.00.049.058 I print_info: causal attn      = 1
0.00.049.058 I print_info: pooling type     = 0
0.00.049.058 I print_info: rope type        = 2
0.00.049.059 I print_info: rope scaling     = linear
0.00.049.059 I print_info: freq_base_train  = 10000.0
0.00.049.060 I print_info: freq_scale_train = 1
0.00.049.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.060 I print_info: rope_finetuned   = unknown
0.00.049.060 I print_info: ssm_d_conv       = 0
0.00.049.060 I print_info: ssm_d_inner      = 0
0.00.049.060 I print_info: ssm_d_state      = 0
0.00.049.060 I print_info: ssm_dt_rank      = 0
0.00.049.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.061 I print_info: model type       = 1.4B
0.00.049.061 I print_info: model params     = 1.41 B
0.00.049.061 I print_info: general.name     = 1.4B
0.00.049.062 I print_info: vocab type       = BPE
0.00.049.062 I print_info: n_vocab          = 50304
0.00.049.063 I print_info: n_merges         = 50009
0.00.049.063 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.064 I print_info: LF token         = 187 'Ċ'
0.00.049.064 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.064 I print_info: max token length = 1024
0.00.049.065 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.575 I load_tensors: offloading output layer to GPU
0.00.653.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.607 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.653.608 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.654.963 I llama_init_from_model: n_seq_max     = 1
0.00.654.967 I llama_init_from_model: n_ctx         = 2048
0.00.654.967 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.968 I llama_init_from_model: n_batch       = 2048
0.00.654.968 I llama_init_from_model: n_ubatch      = 512
0.00.654.968 I llama_init_from_model: flash_attn    = 0
0.00.654.971 I llama_init_from_model: freq_base     = 10000.0
0.00.654.972 I llama_init_from_model: freq_scale    = 1
0.00.654.975 I ggml_metal_init: allocating
0.00.655.037 I ggml_metal_init: found device: Apple M4
0.00.655.051 I ggml_metal_init: picking default device: Apple M4
0.00.657.008 I ggml_metal_init: using embedded metal library
0.00.663.716 I ggml_metal_init: GPU name:   Apple M4
0.00.663.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.723 I ggml_metal_init: simdgroup reduction   = true
0.00.663.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.723 I ggml_metal_init: has residency sets    = true
0.00.663.723 I ggml_metal_init: has bfloat            = true
0.00.663.724 I ggml_metal_init: use bfloat            = true
0.00.663.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.260 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.259 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.740.266 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.301 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.745.137 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.745.140 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.745.140 I llama_init_from_model: graph nodes  = 967
0.00.745.140 I llama_init_from_model: graph splits = 2
0.00.745.145 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.745.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.276 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.952 I main: llama threadpool init, n_threads = 4
0.00.809.992 I 
0.00.810.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.007 I 
0.00.810.171 I sampler seed: 1234
0.00.810.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.186 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.187 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.187 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.659.049 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.659.049 I llama_perf_context_print:        load time =     800.40 ms
0.01.659.050 I llama_perf_context_print: prompt eval time =      52.76 ms /     7 tokens (    7.54 ms per token,   132.67 tokens per second)
0.01.659.051 I llama_perf_context_print:        eval time =     793.20 ms /    63 runs   (   12.59 ms per token,    79.42 tokens per second)
0.01.659.051 I llama_perf_context_print:       total time =     849.90 ms /    70 tokens
0.01.659.268 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.111s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.982 I llama_model_loader: - type  f32:  194 tensors
0.00.023.982 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.983 I print_info: file format = GGUF V3 (latest)
0.00.023.983 I print_info: file type   = Q6_K
0.00.023.984 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.858 I load: special tokens cache size = 25
0.00.037.854 I load: token to piece cache size = 0.2984 MB
0.00.037.856 I print_info: arch             = gptneox
0.00.037.857 I print_info: vocab_only       = 0
0.00.037.857 I print_info: n_ctx_train      = 2048
0.00.037.857 I print_info: n_embd           = 2048
0.00.037.857 I print_info: n_layer          = 24
0.00.037.860 I print_info: n_head           = 16
0.00.037.861 I print_info: n_head_kv        = 16
0.00.037.861 I print_info: n_rot            = 32
0.00.037.862 I print_info: n_swa            = 0
0.00.037.862 I print_info: n_embd_head_k    = 128
0.00.037.862 I print_info: n_embd_head_v    = 128
0.00.037.863 I print_info: n_gqa            = 1
0.00.037.863 I print_info: n_embd_k_gqa     = 2048
0.00.037.864 I print_info: n_embd_v_gqa     = 2048
0.00.037.865 I print_info: f_norm_eps       = 1.0e-05
0.00.037.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.868 I print_info: f_logit_scale    = 0.0e+00
0.00.037.869 I print_info: n_ff             = 8192
0.00.037.869 I print_info: n_expert         = 0
0.00.037.869 I print_info: n_expert_used    = 0
0.00.037.869 I print_info: causal attn      = 1
0.00.037.869 I print_info: pooling type     = 0
0.00.037.869 I print_info: rope type        = 2
0.00.037.870 I print_info: rope scaling     = linear
0.00.037.870 I print_info: freq_base_train  = 10000.0
0.00.037.870 I print_info: freq_scale_train = 1
0.00.037.871 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.871 I print_info: rope_finetuned   = unknown
0.00.037.871 I print_info: ssm_d_conv       = 0
0.00.037.871 I print_info: ssm_d_inner      = 0
0.00.037.871 I print_info: ssm_d_state      = 0
0.00.037.871 I print_info: ssm_dt_rank      = 0
0.00.037.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.872 I print_info: model type       = 1.4B
0.00.037.872 I print_info: model params     = 1.41 B
0.00.037.872 I print_info: general.name     = 1.4B
0.00.037.873 I print_info: vocab type       = BPE
0.00.037.873 I print_info: n_vocab          = 50304
0.00.037.873 I print_info: n_merges         = 50009
0.00.037.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.874 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.874 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.874 I print_info: LF token         = 187 'Ċ'
0.00.037.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.875 I print_info: max token length = 1024
0.00.037.875 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.724 I load_tensors: offloading output layer to GPU
0.00.650.725 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.748 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.650.751 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.652.191 I llama_init_from_model: n_seq_max     = 1
0.00.652.193 I llama_init_from_model: n_ctx         = 2048
0.00.652.193 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.652.194 I llama_init_from_model: n_batch       = 2048
0.00.652.194 I llama_init_from_model: n_ubatch      = 512
0.00.652.195 I llama_init_from_model: flash_attn    = 0
0.00.652.196 I llama_init_from_model: freq_base     = 10000.0
0.00.652.197 I llama_init_from_model: freq_scale    = 1
0.00.652.198 I ggml_metal_init: allocating
0.00.652.231 I ggml_metal_init: found device: Apple M4
0.00.652.242 I ggml_metal_init: picking default device: Apple M4
0.00.653.641 I ggml_metal_init: using embedded metal library
0.00.659.244 I ggml_metal_init: GPU name:   Apple M4
0.00.659.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.249 I ggml_metal_init: simdgroup reduction   = true
0.00.659.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.249 I ggml_metal_init: has residency sets    = true
0.00.659.250 I ggml_metal_init: has bfloat            = true
0.00.659.250 I ggml_metal_init: use bfloat            = true
0.00.659.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.328 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.991 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.998 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.726.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.710 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.711 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.712 I llama_init_from_model: graph nodes  = 967
0.00.730.712 I llama_init_from_model: graph splits = 2
0.00.730.717 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.231 I main: llama threadpool init, n_threads = 4
0.00.789.270 I 
0.00.789.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.287 I 
0.00.789.423 I sampler seed: 1234
0.00.789.427 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.438 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.673.686 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.673.687 I llama_perf_context_print:        load time =     779.71 ms
0.01.673.688 I llama_perf_context_print: prompt eval time =      57.45 ms /     7 tokens (    8.21 ms per token,   121.84 tokens per second)
0.01.673.688 I llama_perf_context_print:        eval time =     823.83 ms /    63 runs   (   13.08 ms per token,    76.47 tokens per second)
0.01.673.689 I llama_perf_context_print:       total time =     885.13 ms /    70 tokens
0.01.673.962 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.106s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.774 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.305 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.814 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.822 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.824 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.826 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.831 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.831 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.407 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.411 I llama_model_loader: - type  f32:  194 tensors
0.00.056.411 I llama_model_loader: - type  f16:   98 tensors
0.00.056.412 I print_info: file format = GGUF V3 (latest)
0.00.056.414 I print_info: file type   = all F32 (guessed)
0.00.056.415 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.995 I load: special tokens cache size = 25
0.00.077.378 I load: token to piece cache size = 0.2984 MB
0.00.077.381 I print_info: arch             = gptneox
0.00.077.381 I print_info: vocab_only       = 0
0.00.077.382 I print_info: n_ctx_train      = 2048
0.00.077.382 I print_info: n_embd           = 2048
0.00.077.382 I print_info: n_layer          = 24
0.00.077.384 I print_info: n_head           = 16
0.00.077.385 I print_info: n_head_kv        = 16
0.00.077.385 I print_info: n_rot            = 32
0.00.077.386 I print_info: n_swa            = 0
0.00.077.386 I print_info: n_embd_head_k    = 128
0.00.077.386 I print_info: n_embd_head_v    = 128
0.00.077.387 I print_info: n_gqa            = 1
0.00.077.388 I print_info: n_embd_k_gqa     = 2048
0.00.077.388 I print_info: n_embd_v_gqa     = 2048
0.00.077.389 I print_info: f_norm_eps       = 1.0e-05
0.00.077.390 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.390 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.390 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.390 I print_info: f_logit_scale    = 0.0e+00
0.00.077.391 I print_info: n_ff             = 8192
0.00.077.391 I print_info: n_expert         = 0
0.00.077.391 I print_info: n_expert_used    = 0
0.00.077.391 I print_info: causal attn      = 1
0.00.077.391 I print_info: pooling type     = 0
0.00.077.392 I print_info: rope type        = 2
0.00.077.392 I print_info: rope scaling     = linear
0.00.077.392 I print_info: freq_base_train  = 10000.0
0.00.077.393 I print_info: freq_scale_train = 1
0.00.077.393 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.393 I print_info: rope_finetuned   = unknown
0.00.077.395 I print_info: ssm_d_conv       = 0
0.00.077.395 I print_info: ssm_d_inner      = 0
0.00.077.396 I print_info: ssm_d_state      = 0
0.00.077.396 I print_info: ssm_dt_rank      = 0
0.00.077.396 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.396 I print_info: model type       = 1.4B
0.00.077.397 I print_info: model params     = 1.41 B
0.00.077.397 I print_info: general.name     = 1.4B
0.00.077.397 I print_info: vocab type       = BPE
0.00.077.397 I print_info: n_vocab          = 50304
0.00.077.398 I print_info: n_merges         = 50009
0.00.077.398 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.398 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.398 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.398 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.399 I print_info: LF token         = 187 'Ċ'
0.00.077.404 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.404 I print_info: max token length = 1024
0.00.077.405 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.480.996 I load_tensors: offloading 24 repeating layers to GPU
0.01.481.001 I load_tensors: offloading output layer to GPU
0.01.481.001 I load_tensors: offloaded 25/25 layers to GPU
0.01.481.026 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.481.027 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.481.695 I llama_init_from_model: n_seq_max     = 1
0.01.481.696 I llama_init_from_model: n_ctx         = 128
0.01.481.696 I llama_init_from_model: n_ctx_per_seq = 128
0.01.481.697 I llama_init_from_model: n_batch       = 128
0.01.481.697 I llama_init_from_model: n_ubatch      = 128
0.01.481.697 I llama_init_from_model: flash_attn    = 0
0.01.481.698 I llama_init_from_model: freq_base     = 10000.0
0.01.481.698 I llama_init_from_model: freq_scale    = 1
0.01.481.698 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.481.699 I ggml_metal_init: allocating
0.01.481.794 I ggml_metal_init: found device: Apple M4
0.01.481.800 I ggml_metal_init: picking default device: Apple M4
0.01.482.931 I ggml_metal_init: using embedded metal library
0.01.486.688 I ggml_metal_init: GPU name:   Apple M4
0.01.486.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.486.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.486.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.486.692 I ggml_metal_init: simdgroup reduction   = true
0.01.486.692 I ggml_metal_init: simdgroup matrix mul. = true
0.01.486.692 I ggml_metal_init: has residency sets    = true
0.01.486.692 I ggml_metal_init: has bfloat            = true
0.01.486.692 I ggml_metal_init: use bfloat            = true
0.01.486.693 I ggml_metal_init: hasUnifiedMemory      = true
0.01.486.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.496.966 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.498.728 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.498.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.498.756 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.500.408 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.500.409 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.500.410 I llama_init_from_model: graph nodes  = 967
0.01.500.410 I llama_init_from_model: graph splits = 2
0.01.500.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.500.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.535.904 I 
0.01.535.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.535.937 I perplexity: tokenizing the input ..
0.01.541.092 I perplexity: tokenization took 5.153 ms
0.01.541.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.659.997 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.661.350 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.661.383 I llama_perf_context_print:        load time =    1511.57 ms
0.01.661.384 I llama_perf_context_print: prompt eval time =     118.59 ms /   128 tokens (    0.93 ms per token,  1079.34 tokens per second)
0.01.661.385 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.661.385 I llama_perf_context_print:       total time =     125.48 ms /   129 tokens
0.01.661.752 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.098s
sys	0m0.282s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.483 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.497 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.498 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.502 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.504 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.065 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.067 I llama_model_loader: - type  f32:  194 tensors
0.00.025.067 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.068 I print_info: file format = GGUF V3 (latest)
0.00.025.068 I print_info: file type   = Q8_0
0.00.025.070 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.390 I load: special tokens cache size = 25
0.00.039.479 I load: token to piece cache size = 0.2984 MB
0.00.039.483 I print_info: arch             = gptneox
0.00.039.484 I print_info: vocab_only       = 0
0.00.039.484 I print_info: n_ctx_train      = 2048
0.00.039.484 I print_info: n_embd           = 2048
0.00.039.484 I print_info: n_layer          = 24
0.00.039.489 I print_info: n_head           = 16
0.00.039.490 I print_info: n_head_kv        = 16
0.00.039.490 I print_info: n_rot            = 32
0.00.039.490 I print_info: n_swa            = 0
0.00.039.490 I print_info: n_embd_head_k    = 128
0.00.039.490 I print_info: n_embd_head_v    = 128
0.00.039.495 I print_info: n_gqa            = 1
0.00.039.495 I print_info: n_embd_k_gqa     = 2048
0.00.039.496 I print_info: n_embd_v_gqa     = 2048
0.00.039.497 I print_info: f_norm_eps       = 1.0e-05
0.00.039.497 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.497 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.497 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.498 I print_info: f_logit_scale    = 0.0e+00
0.00.039.499 I print_info: n_ff             = 8192
0.00.039.499 I print_info: n_expert         = 0
0.00.039.500 I print_info: n_expert_used    = 0
0.00.039.500 I print_info: causal attn      = 1
0.00.039.500 I print_info: pooling type     = 0
0.00.039.500 I print_info: rope type        = 2
0.00.039.500 I print_info: rope scaling     = linear
0.00.039.501 I print_info: freq_base_train  = 10000.0
0.00.039.501 I print_info: freq_scale_train = 1
0.00.039.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.501 I print_info: rope_finetuned   = unknown
0.00.039.501 I print_info: ssm_d_conv       = 0
0.00.039.502 I print_info: ssm_d_inner      = 0
0.00.039.502 I print_info: ssm_d_state      = 0
0.00.039.502 I print_info: ssm_dt_rank      = 0
0.00.039.503 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.503 I print_info: model type       = 1.4B
0.00.039.504 I print_info: model params     = 1.41 B
0.00.039.504 I print_info: general.name     = 1.4B
0.00.039.505 I print_info: vocab type       = BPE
0.00.039.505 I print_info: n_vocab          = 50304
0.00.039.505 I print_info: n_merges         = 50009
0.00.039.505 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.505 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.505 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: LF token         = 187 'Ċ'
0.00.039.506 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.506 I print_info: max token length = 1024
0.00.039.511 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.881.238 I load_tensors: offloading 24 repeating layers to GPU
0.00.881.245 I load_tensors: offloading output layer to GPU
0.00.881.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.881.277 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.881.279 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.882.488 I llama_init_from_model: n_seq_max     = 1
0.00.882.490 I llama_init_from_model: n_ctx         = 128
0.00.882.490 I llama_init_from_model: n_ctx_per_seq = 128
0.00.882.491 I llama_init_from_model: n_batch       = 128
0.00.882.491 I llama_init_from_model: n_ubatch      = 128
0.00.882.491 I llama_init_from_model: flash_attn    = 0
0.00.882.492 I llama_init_from_model: freq_base     = 10000.0
0.00.882.493 I llama_init_from_model: freq_scale    = 1
0.00.882.493 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.882.494 I ggml_metal_init: allocating
0.00.882.594 I ggml_metal_init: found device: Apple M4
0.00.882.605 I ggml_metal_init: picking default device: Apple M4
0.00.884.007 I ggml_metal_init: using embedded metal library
0.00.889.205 I ggml_metal_init: GPU name:   Apple M4
0.00.889.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.889.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.889.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.889.210 I ggml_metal_init: simdgroup reduction   = true
0.00.889.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.889.210 I ggml_metal_init: has residency sets    = true
0.00.889.211 I ggml_metal_init: has bfloat            = true
0.00.889.211 I ggml_metal_init: use bfloat            = true
0.00.889.212 I ggml_metal_init: hasUnifiedMemory      = true
0.00.889.214 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.903.440 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.906.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.906.781 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.906.827 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.909.784 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.909.786 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.909.786 I llama_init_from_model: graph nodes  = 967
0.00.909.786 I llama_init_from_model: graph splits = 2
0.00.909.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.909.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.993 I 
0.00.936.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.063 I perplexity: tokenizing the input ..
0.00.943.200 I perplexity: tokenization took 7.134 ms
0.00.943.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.080.702 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.082.036 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.082.060 I llama_perf_context_print:        load time =     926.72 ms
0.01.082.061 I llama_perf_context_print: prompt eval time =     136.59 ms /   128 tokens (    1.07 ms per token,   937.08 tokens per second)
0.01.082.062 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.082.062 I llama_perf_context_print:       total time =     146.07 ms /   129 tokens
0.01.082.408 I ggml_metal_free: deallocating

real	0m1.098s
user	0m0.075s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.588 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.600 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.605 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.607 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.283 I llama_model_loader: - type  f32:  194 tensors
0.00.026.284 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.284 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.285 I print_info: file format = GGUF V3 (latest)
0.00.026.286 I print_info: file type   = Q4_0
0.00.026.287 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.588 I load: special tokens cache size = 25
0.00.040.713 I load: token to piece cache size = 0.2984 MB
0.00.040.719 I print_info: arch             = gptneox
0.00.040.719 I print_info: vocab_only       = 0
0.00.040.719 I print_info: n_ctx_train      = 2048
0.00.040.719 I print_info: n_embd           = 2048
0.00.040.720 I print_info: n_layer          = 24
0.00.040.724 I print_info: n_head           = 16
0.00.040.725 I print_info: n_head_kv        = 16
0.00.040.725 I print_info: n_rot            = 32
0.00.040.728 I print_info: n_swa            = 0
0.00.040.728 I print_info: n_embd_head_k    = 128
0.00.040.728 I print_info: n_embd_head_v    = 128
0.00.040.729 I print_info: n_gqa            = 1
0.00.040.730 I print_info: n_embd_k_gqa     = 2048
0.00.040.730 I print_info: n_embd_v_gqa     = 2048
0.00.040.731 I print_info: f_norm_eps       = 1.0e-05
0.00.040.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.732 I print_info: f_logit_scale    = 0.0e+00
0.00.040.732 I print_info: n_ff             = 8192
0.00.040.732 I print_info: n_expert         = 0
0.00.040.733 I print_info: n_expert_used    = 0
0.00.040.733 I print_info: causal attn      = 1
0.00.040.733 I print_info: pooling type     = 0
0.00.040.733 I print_info: rope type        = 2
0.00.040.733 I print_info: rope scaling     = linear
0.00.040.734 I print_info: freq_base_train  = 10000.0
0.00.040.734 I print_info: freq_scale_train = 1
0.00.040.734 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.734 I print_info: rope_finetuned   = unknown
0.00.040.734 I print_info: ssm_d_conv       = 0
0.00.040.734 I print_info: ssm_d_inner      = 0
0.00.040.735 I print_info: ssm_d_state      = 0
0.00.040.735 I print_info: ssm_dt_rank      = 0
0.00.040.735 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.735 I print_info: model type       = 1.4B
0.00.040.736 I print_info: model params     = 1.41 B
0.00.040.736 I print_info: general.name     = 1.4B
0.00.040.736 I print_info: vocab type       = BPE
0.00.040.736 I print_info: n_vocab          = 50304
0.00.040.737 I print_info: n_merges         = 50009
0.00.040.737 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.737 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.737 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.737 I print_info: LF token         = 187 'Ċ'
0.00.040.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.738 I print_info: max token length = 1024
0.00.040.738 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.324 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.342 I load_tensors: offloading output layer to GPU
0.00.601.343 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.381 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.601.382 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.603.123 I llama_init_from_model: n_seq_max     = 1
0.00.603.126 I llama_init_from_model: n_ctx         = 128
0.00.603.127 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.127 I llama_init_from_model: n_batch       = 128
0.00.603.128 I llama_init_from_model: n_ubatch      = 128
0.00.603.128 I llama_init_from_model: flash_attn    = 0
0.00.603.130 I llama_init_from_model: freq_base     = 10000.0
0.00.603.131 I llama_init_from_model: freq_scale    = 1
0.00.603.131 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.133 I ggml_metal_init: allocating
0.00.603.254 I ggml_metal_init: found device: Apple M4
0.00.603.268 I ggml_metal_init: picking default device: Apple M4
0.00.605.185 I ggml_metal_init: using embedded metal library
0.00.611.917 I ggml_metal_init: GPU name:   Apple M4
0.00.611.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.929 I ggml_metal_init: simdgroup reduction   = true
0.00.611.930 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.930 I ggml_metal_init: has residency sets    = true
0.00.611.930 I ggml_metal_init: has bfloat            = true
0.00.611.931 I ggml_metal_init: use bfloat            = true
0.00.611.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.025 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.521 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.565 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.691 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.693 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.693 I llama_init_from_model: graph nodes  = 967
0.00.636.694 I llama_init_from_model: graph splits = 2
0.00.636.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.312 I 
0.00.664.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.367 I perplexity: tokenizing the input ..
0.00.670.663 I perplexity: tokenization took 6.293 ms
0.00.670.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.210 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.804.809 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.804.833 I llama_perf_context_print:        load time =     654.20 ms
0.00.804.834 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.67 tokens per second)
0.00.804.834 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.834 I llama_perf_context_print:       total time =     140.52 ms /   129 tokens
0.00.805.177 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.440 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.442 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.443 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.443 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.445 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.445 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.445 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.209 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.210 I llama_model_loader: - type  f32:  194 tensors
0.00.025.210 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.210 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.211 I print_info: file format = GGUF V3 (latest)
0.00.025.211 I print_info: file type   = Q4_1
0.00.025.212 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.654 I load: special tokens cache size = 25
0.00.039.837 I load: token to piece cache size = 0.2984 MB
0.00.039.841 I print_info: arch             = gptneox
0.00.039.842 I print_info: vocab_only       = 0
0.00.039.842 I print_info: n_ctx_train      = 2048
0.00.039.842 I print_info: n_embd           = 2048
0.00.039.842 I print_info: n_layer          = 24
0.00.039.847 I print_info: n_head           = 16
0.00.039.848 I print_info: n_head_kv        = 16
0.00.039.848 I print_info: n_rot            = 32
0.00.039.848 I print_info: n_swa            = 0
0.00.039.848 I print_info: n_embd_head_k    = 128
0.00.039.848 I print_info: n_embd_head_v    = 128
0.00.039.849 I print_info: n_gqa            = 1
0.00.039.850 I print_info: n_embd_k_gqa     = 2048
0.00.039.851 I print_info: n_embd_v_gqa     = 2048
0.00.039.851 I print_info: f_norm_eps       = 1.0e-05
0.00.039.851 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.851 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.852 I print_info: f_logit_scale    = 0.0e+00
0.00.039.852 I print_info: n_ff             = 8192
0.00.039.853 I print_info: n_expert         = 0
0.00.039.853 I print_info: n_expert_used    = 0
0.00.039.853 I print_info: causal attn      = 1
0.00.039.853 I print_info: pooling type     = 0
0.00.039.853 I print_info: rope type        = 2
0.00.039.855 I print_info: rope scaling     = linear
0.00.039.855 I print_info: freq_base_train  = 10000.0
0.00.039.857 I print_info: freq_scale_train = 1
0.00.039.857 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.857 I print_info: rope_finetuned   = unknown
0.00.039.857 I print_info: ssm_d_conv       = 0
0.00.039.858 I print_info: ssm_d_inner      = 0
0.00.039.858 I print_info: ssm_d_state      = 0
0.00.039.858 I print_info: ssm_dt_rank      = 0
0.00.039.858 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.858 I print_info: model type       = 1.4B
0.00.039.858 I print_info: model params     = 1.41 B
0.00.039.859 I print_info: general.name     = 1.4B
0.00.039.859 I print_info: vocab type       = BPE
0.00.039.859 I print_info: n_vocab          = 50304
0.00.039.859 I print_info: n_merges         = 50009
0.00.039.861 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: LF token         = 187 'Ċ'
0.00.039.862 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.862 I print_info: max token length = 1024
0.00.039.862 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.858 I load_tensors: offloading output layer to GPU
0.00.659.858 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.894 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.659.896 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.661.387 I llama_init_from_model: n_seq_max     = 1
0.00.661.390 I llama_init_from_model: n_ctx         = 128
0.00.661.391 I llama_init_from_model: n_ctx_per_seq = 128
0.00.661.391 I llama_init_from_model: n_batch       = 128
0.00.661.392 I llama_init_from_model: n_ubatch      = 128
0.00.661.392 I llama_init_from_model: flash_attn    = 0
0.00.661.394 I llama_init_from_model: freq_base     = 10000.0
0.00.661.395 I llama_init_from_model: freq_scale    = 1
0.00.661.395 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.661.398 I ggml_metal_init: allocating
0.00.661.478 I ggml_metal_init: found device: Apple M4
0.00.661.492 I ggml_metal_init: picking default device: Apple M4
0.00.663.297 I ggml_metal_init: using embedded metal library
0.00.669.777 I ggml_metal_init: GPU name:   Apple M4
0.00.669.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.788 I ggml_metal_init: simdgroup reduction   = true
0.00.669.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.788 I ggml_metal_init: has residency sets    = true
0.00.669.788 I ggml_metal_init: has bfloat            = true
0.00.669.789 I ggml_metal_init: use bfloat            = true
0.00.669.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.330 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.824 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.691.831 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.691.880 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.695.112 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.695.114 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.695.115 I llama_init_from_model: graph nodes  = 967
0.00.695.115 I llama_init_from_model: graph splits = 2
0.00.695.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.695.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.509 I 
0.00.726.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.581 I perplexity: tokenizing the input ..
0.00.733.516 I perplexity: tokenization took 6.931 ms
0.00.733.526 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.813 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.865.151 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.865.178 I llama_perf_context_print:        load time =     717.34 ms
0.00.865.179 I llama_perf_context_print: prompt eval time =     129.40 ms /   128 tokens (    1.01 ms per token,   989.18 tokens per second)
0.00.865.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.865.180 I llama_perf_context_print:       total time =     138.67 ms /   129 tokens
0.00.865.539 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.080s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.095 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.760 I llama_model_loader: - type  f32:  194 tensors
0.00.024.760 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.761 I print_info: file format = GGUF V3 (latest)
0.00.024.762 I print_info: file type   = Q5_0
0.00.024.764 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.988 I load: special tokens cache size = 25
0.00.038.986 I load: token to piece cache size = 0.2984 MB
0.00.038.992 I print_info: arch             = gptneox
0.00.038.992 I print_info: vocab_only       = 0
0.00.038.992 I print_info: n_ctx_train      = 2048
0.00.038.994 I print_info: n_embd           = 2048
0.00.038.994 I print_info: n_layer          = 24
0.00.039.001 I print_info: n_head           = 16
0.00.039.002 I print_info: n_head_kv        = 16
0.00.039.002 I print_info: n_rot            = 32
0.00.039.002 I print_info: n_swa            = 0
0.00.039.002 I print_info: n_embd_head_k    = 128
0.00.039.002 I print_info: n_embd_head_v    = 128
0.00.039.003 I print_info: n_gqa            = 1
0.00.039.003 I print_info: n_embd_k_gqa     = 2048
0.00.039.004 I print_info: n_embd_v_gqa     = 2048
0.00.039.004 I print_info: f_norm_eps       = 1.0e-05
0.00.039.006 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.006 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.006 I print_info: f_logit_scale    = 0.0e+00
0.00.039.007 I print_info: n_ff             = 8192
0.00.039.007 I print_info: n_expert         = 0
0.00.039.007 I print_info: n_expert_used    = 0
0.00.039.007 I print_info: causal attn      = 1
0.00.039.007 I print_info: pooling type     = 0
0.00.039.008 I print_info: rope type        = 2
0.00.039.008 I print_info: rope scaling     = linear
0.00.039.008 I print_info: freq_base_train  = 10000.0
0.00.039.008 I print_info: freq_scale_train = 1
0.00.039.009 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.009 I print_info: rope_finetuned   = unknown
0.00.039.009 I print_info: ssm_d_conv       = 0
0.00.039.009 I print_info: ssm_d_inner      = 0
0.00.039.009 I print_info: ssm_d_state      = 0
0.00.039.009 I print_info: ssm_dt_rank      = 0
0.00.039.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.010 I print_info: model type       = 1.4B
0.00.039.010 I print_info: model params     = 1.41 B
0.00.039.010 I print_info: general.name     = 1.4B
0.00.039.010 I print_info: vocab type       = BPE
0.00.039.011 I print_info: n_vocab          = 50304
0.00.039.011 I print_info: n_merges         = 50009
0.00.039.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.013 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.013 I print_info: LF token         = 187 'Ċ'
0.00.039.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.013 I print_info: max token length = 1024
0.00.039.014 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.399 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.413 I load_tensors: offloading output layer to GPU
0.00.685.414 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.447 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.685.448 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.687.270 I llama_init_from_model: n_seq_max     = 1
0.00.687.272 I llama_init_from_model: n_ctx         = 128
0.00.687.273 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.273 I llama_init_from_model: n_batch       = 128
0.00.687.274 I llama_init_from_model: n_ubatch      = 128
0.00.687.274 I llama_init_from_model: flash_attn    = 0
0.00.687.276 I llama_init_from_model: freq_base     = 10000.0
0.00.687.277 I llama_init_from_model: freq_scale    = 1
0.00.687.278 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.280 I ggml_metal_init: allocating
0.00.687.372 I ggml_metal_init: found device: Apple M4
0.00.687.385 I ggml_metal_init: picking default device: Apple M4
0.00.689.074 I ggml_metal_init: using embedded metal library
0.00.695.387 I ggml_metal_init: GPU name:   Apple M4
0.00.695.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.395 I ggml_metal_init: simdgroup reduction   = true
0.00.695.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.396 I ggml_metal_init: has residency sets    = true
0.00.695.396 I ggml_metal_init: has bfloat            = true
0.00.695.396 I ggml_metal_init: use bfloat            = true
0.00.695.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.341 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.895 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.716.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.716.952 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.144 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.720.146 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.720.147 I llama_init_from_model: graph nodes  = 967
0.00.720.147 I llama_init_from_model: graph splits = 2
0.00.720.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.720.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.636 I 
0.00.751.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.707 I perplexity: tokenizing the input ..
0.00.758.893 I perplexity: tokenization took 7.182 ms
0.00.758.907 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.999 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.908.348 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.908.370 I llama_perf_context_print:        load time =     742.79 ms
0.00.908.371 I llama_perf_context_print: prompt eval time =     147.23 ms /   128 tokens (    1.15 ms per token,   869.39 tokens per second)
0.00.908.371 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.908.372 I llama_perf_context_print:       total time =     156.74 ms /   129 tokens
0.00.908.724 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.079s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.887 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.897 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.908 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.909 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.909 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.621 I llama_model_loader: - type  f32:  194 tensors
0.00.025.621 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.622 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.622 I print_info: file format = GGUF V3 (latest)
0.00.025.623 I print_info: file type   = Q5_1
0.00.025.624 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.642 I load: special tokens cache size = 25
0.00.039.693 I load: token to piece cache size = 0.2984 MB
0.00.039.696 I print_info: arch             = gptneox
0.00.039.696 I print_info: vocab_only       = 0
0.00.039.696 I print_info: n_ctx_train      = 2048
0.00.039.697 I print_info: n_embd           = 2048
0.00.039.697 I print_info: n_layer          = 24
0.00.039.701 I print_info: n_head           = 16
0.00.039.702 I print_info: n_head_kv        = 16
0.00.039.702 I print_info: n_rot            = 32
0.00.039.702 I print_info: n_swa            = 0
0.00.039.702 I print_info: n_embd_head_k    = 128
0.00.039.702 I print_info: n_embd_head_v    = 128
0.00.039.704 I print_info: n_gqa            = 1
0.00.039.705 I print_info: n_embd_k_gqa     = 2048
0.00.039.705 I print_info: n_embd_v_gqa     = 2048
0.00.039.706 I print_info: f_norm_eps       = 1.0e-05
0.00.039.706 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.709 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.709 I print_info: f_logit_scale    = 0.0e+00
0.00.039.709 I print_info: n_ff             = 8192
0.00.039.710 I print_info: n_expert         = 0
0.00.039.710 I print_info: n_expert_used    = 0
0.00.039.710 I print_info: causal attn      = 1
0.00.039.710 I print_info: pooling type     = 0
0.00.039.710 I print_info: rope type        = 2
0.00.039.710 I print_info: rope scaling     = linear
0.00.039.711 I print_info: freq_base_train  = 10000.0
0.00.039.711 I print_info: freq_scale_train = 1
0.00.039.711 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.711 I print_info: rope_finetuned   = unknown
0.00.039.711 I print_info: ssm_d_conv       = 0
0.00.039.715 I print_info: ssm_d_inner      = 0
0.00.039.715 I print_info: ssm_d_state      = 0
0.00.039.715 I print_info: ssm_dt_rank      = 0
0.00.039.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.715 I print_info: model type       = 1.4B
0.00.039.716 I print_info: model params     = 1.41 B
0.00.039.716 I print_info: general.name     = 1.4B
0.00.039.716 I print_info: vocab type       = BPE
0.00.039.717 I print_info: n_vocab          = 50304
0.00.039.717 I print_info: n_merges         = 50009
0.00.039.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.718 I print_info: LF token         = 187 'Ċ'
0.00.039.718 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.718 I print_info: max token length = 1024
0.00.039.718 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.180 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.192 I load_tensors: offloading output layer to GPU
0.00.634.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.223 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.634.225 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.635.330 I llama_init_from_model: n_seq_max     = 1
0.00.635.333 I llama_init_from_model: n_ctx         = 128
0.00.635.334 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.335 I llama_init_from_model: n_batch       = 128
0.00.635.335 I llama_init_from_model: n_ubatch      = 128
0.00.635.335 I llama_init_from_model: flash_attn    = 0
0.00.635.337 I llama_init_from_model: freq_base     = 10000.0
0.00.635.337 I llama_init_from_model: freq_scale    = 1
0.00.635.338 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.340 I ggml_metal_init: allocating
0.00.635.403 I ggml_metal_init: found device: Apple M4
0.00.635.417 I ggml_metal_init: picking default device: Apple M4
0.00.637.227 I ggml_metal_init: using embedded metal library
0.00.643.822 I ggml_metal_init: GPU name:   Apple M4
0.00.643.825 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.827 I ggml_metal_init: simdgroup reduction   = true
0.00.643.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.828 I ggml_metal_init: has residency sets    = true
0.00.643.828 I ggml_metal_init: has bfloat            = true
0.00.643.828 I ggml_metal_init: use bfloat            = true
0.00.643.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.526 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.075 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.078 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.573 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.575 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.575 I llama_init_from_model: graph nodes  = 967
0.00.668.576 I llama_init_from_model: graph splits = 2
0.00.668.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.887 I 
0.00.698.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.961 I perplexity: tokenizing the input ..
0.00.705.645 I perplexity: tokenization took 6.682 ms
0.00.705.651 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.662 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.842.023 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.842.046 I llama_perf_context_print:        load time =     688.90 ms
0.00.842.047 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   955.00 tokens per second)
0.00.842.047 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.047 I llama_perf_context_print:       total time =     143.16 ms /   129 tokens
0.00.842.476 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.079s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.971 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.261 I llama_model_loader: - type  f32:  194 tensors
0.00.025.262 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.262 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.263 I print_info: file format = GGUF V3 (latest)
0.00.025.263 I print_info: file type   = Q2_K - Medium
0.00.025.267 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.595 I load: special tokens cache size = 25
0.00.039.722 I load: token to piece cache size = 0.2984 MB
0.00.039.727 I print_info: arch             = gptneox
0.00.039.727 I print_info: vocab_only       = 0
0.00.039.728 I print_info: n_ctx_train      = 2048
0.00.039.728 I print_info: n_embd           = 2048
0.00.039.728 I print_info: n_layer          = 24
0.00.039.733 I print_info: n_head           = 16
0.00.039.733 I print_info: n_head_kv        = 16
0.00.039.734 I print_info: n_rot            = 32
0.00.039.736 I print_info: n_swa            = 0
0.00.039.736 I print_info: n_embd_head_k    = 128
0.00.039.736 I print_info: n_embd_head_v    = 128
0.00.039.737 I print_info: n_gqa            = 1
0.00.039.738 I print_info: n_embd_k_gqa     = 2048
0.00.039.738 I print_info: n_embd_v_gqa     = 2048
0.00.039.739 I print_info: f_norm_eps       = 1.0e-05
0.00.039.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.740 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.740 I print_info: f_logit_scale    = 0.0e+00
0.00.039.740 I print_info: n_ff             = 8192
0.00.039.741 I print_info: n_expert         = 0
0.00.039.741 I print_info: n_expert_used    = 0
0.00.039.741 I print_info: causal attn      = 1
0.00.039.741 I print_info: pooling type     = 0
0.00.039.745 I print_info: rope type        = 2
0.00.039.745 I print_info: rope scaling     = linear
0.00.039.745 I print_info: freq_base_train  = 10000.0
0.00.039.746 I print_info: freq_scale_train = 1
0.00.039.746 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.746 I print_info: rope_finetuned   = unknown
0.00.039.746 I print_info: ssm_d_conv       = 0
0.00.039.746 I print_info: ssm_d_inner      = 0
0.00.039.747 I print_info: ssm_d_state      = 0
0.00.039.747 I print_info: ssm_dt_rank      = 0
0.00.039.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.747 I print_info: model type       = 1.4B
0.00.039.747 I print_info: model params     = 1.41 B
0.00.039.747 I print_info: general.name     = 1.4B
0.00.039.748 I print_info: vocab type       = BPE
0.00.039.748 I print_info: n_vocab          = 50304
0.00.039.748 I print_info: n_merges         = 50009
0.00.039.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: LF token         = 187 'Ċ'
0.00.039.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.750 I print_info: max token length = 1024
0.00.039.750 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.129 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.143 I load_tensors: offloading output layer to GPU
0.00.348.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.175 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.177 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.970 I llama_init_from_model: n_seq_max     = 1
0.00.349.977 I llama_init_from_model: n_ctx         = 128
0.00.349.978 I llama_init_from_model: n_ctx_per_seq = 128
0.00.349.978 I llama_init_from_model: n_batch       = 128
0.00.349.979 I llama_init_from_model: n_ubatch      = 128
0.00.349.979 I llama_init_from_model: flash_attn    = 0
0.00.349.981 I llama_init_from_model: freq_base     = 10000.0
0.00.349.982 I llama_init_from_model: freq_scale    = 1
0.00.349.982 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.349.984 I ggml_metal_init: allocating
0.00.350.084 I ggml_metal_init: found device: Apple M4
0.00.350.098 I ggml_metal_init: picking default device: Apple M4
0.00.352.000 I ggml_metal_init: using embedded metal library
0.00.357.454 I ggml_metal_init: GPU name:   Apple M4
0.00.357.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.357.469 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.357.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.357.471 I ggml_metal_init: simdgroup reduction   = true
0.00.357.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.357.471 I ggml_metal_init: has residency sets    = true
0.00.357.472 I ggml_metal_init: has bfloat            = true
0.00.357.472 I ggml_metal_init: use bfloat            = true
0.00.357.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.357.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.378.676 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.382.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.382.299 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.382.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.385.677 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.385.679 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.385.680 I llama_init_from_model: graph nodes  = 967
0.00.385.680 I llama_init_from_model: graph splits = 2
0.00.385.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.385.683 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.531 I 
0.00.418.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.606 I perplexity: tokenizing the input ..
0.00.425.722 I perplexity: tokenization took 7.113 ms
0.00.425.729 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.565.340 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.566.655 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.566.679 I llama_perf_context_print:        load time =     409.55 ms
0.00.566.679 I llama_perf_context_print: prompt eval time =     138.67 ms /   128 tokens (    1.08 ms per token,   923.06 tokens per second)
0.00.566.680 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.566.680 I llama_perf_context_print:       total time =     148.15 ms /   129 tokens
0.00.567.052 I ggml_metal_free: deallocating

real	0m0.582s
user	0m0.082s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.081 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.101 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.102 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.102 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.021 I llama_model_loader: - type  f32:  194 tensors
0.00.025.021 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.021 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.022 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.022 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.023 I print_info: file format = GGUF V3 (latest)
0.00.025.024 I print_info: file type   = Q3_K - Medium
0.00.025.025 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.614 I load: special tokens cache size = 25
0.00.039.673 I load: token to piece cache size = 0.2984 MB
0.00.039.678 I print_info: arch             = gptneox
0.00.039.678 I print_info: vocab_only       = 0
0.00.039.678 I print_info: n_ctx_train      = 2048
0.00.039.678 I print_info: n_embd           = 2048
0.00.039.679 I print_info: n_layer          = 24
0.00.039.683 I print_info: n_head           = 16
0.00.039.684 I print_info: n_head_kv        = 16
0.00.039.684 I print_info: n_rot            = 32
0.00.039.684 I print_info: n_swa            = 0
0.00.039.685 I print_info: n_embd_head_k    = 128
0.00.039.685 I print_info: n_embd_head_v    = 128
0.00.039.685 I print_info: n_gqa            = 1
0.00.039.686 I print_info: n_embd_k_gqa     = 2048
0.00.039.687 I print_info: n_embd_v_gqa     = 2048
0.00.039.687 I print_info: f_norm_eps       = 1.0e-05
0.00.039.688 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.688 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.688 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.688 I print_info: f_logit_scale    = 0.0e+00
0.00.039.689 I print_info: n_ff             = 8192
0.00.039.689 I print_info: n_expert         = 0
0.00.039.689 I print_info: n_expert_used    = 0
0.00.039.690 I print_info: causal attn      = 1
0.00.039.690 I print_info: pooling type     = 0
0.00.039.690 I print_info: rope type        = 2
0.00.039.690 I print_info: rope scaling     = linear
0.00.039.690 I print_info: freq_base_train  = 10000.0
0.00.039.691 I print_info: freq_scale_train = 1
0.00.039.691 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.691 I print_info: rope_finetuned   = unknown
0.00.039.691 I print_info: ssm_d_conv       = 0
0.00.039.692 I print_info: ssm_d_inner      = 0
0.00.039.692 I print_info: ssm_d_state      = 0
0.00.039.692 I print_info: ssm_dt_rank      = 0
0.00.039.692 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.692 I print_info: model type       = 1.4B
0.00.039.693 I print_info: model params     = 1.41 B
0.00.039.693 I print_info: general.name     = 1.4B
0.00.039.693 I print_info: vocab type       = BPE
0.00.039.694 I print_info: n_vocab          = 50304
0.00.039.694 I print_info: n_merges         = 50009
0.00.039.694 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.694 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.695 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.695 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.695 I print_info: LF token         = 187 'Ċ'
0.00.039.695 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.696 I print_info: max token length = 1024
0.00.039.696 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.524 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.541 I load_tensors: offloading output layer to GPU
0.00.435.542 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.575 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.577 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.437.109 I llama_init_from_model: n_seq_max     = 1
0.00.437.112 I llama_init_from_model: n_ctx         = 128
0.00.437.112 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.113 I llama_init_from_model: n_batch       = 128
0.00.437.113 I llama_init_from_model: n_ubatch      = 128
0.00.437.113 I llama_init_from_model: flash_attn    = 0
0.00.437.116 I llama_init_from_model: freq_base     = 10000.0
0.00.437.117 I llama_init_from_model: freq_scale    = 1
0.00.437.117 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.120 I ggml_metal_init: allocating
0.00.437.177 I ggml_metal_init: found device: Apple M4
0.00.437.192 I ggml_metal_init: picking default device: Apple M4
0.00.438.959 I ggml_metal_init: using embedded metal library
0.00.444.512 I ggml_metal_init: GPU name:   Apple M4
0.00.444.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.532 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.532 I ggml_metal_init: simdgroup reduction   = true
0.00.444.533 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.533 I ggml_metal_init: has residency sets    = true
0.00.444.533 I ggml_metal_init: has bfloat            = true
0.00.444.533 I ggml_metal_init: use bfloat            = true
0.00.444.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.624 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.468.213 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.468.220 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.468.275 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.471.426 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.471.428 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.471.428 I llama_init_from_model: graph nodes  = 967
0.00.471.429 I llama_init_from_model: graph splits = 2
0.00.471.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.471.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.999 I 
0.00.502.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.082 I perplexity: tokenizing the input ..
0.00.507.919 I perplexity: tokenization took 5.835 ms
0.00.507.924 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.640.344 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.641.678 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.641.703 I llama_perf_context_print:        load time =     493.15 ms
0.00.641.704 I llama_perf_context_print: prompt eval time =     132.17 ms /   128 tokens (    1.03 ms per token,   968.44 tokens per second)
0.00.641.705 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.641.705 I llama_perf_context_print:       total time =     139.71 ms /   129 tokens
0.00.642.077 I ggml_metal_free: deallocating

real	0m0.656s
user	0m0.079s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.053 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.068 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.074 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.069 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.019 I llama_model_loader: - type  f32:  194 tensors
0.00.026.020 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.020 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.020 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.021 I print_info: file format = GGUF V3 (latest)
0.00.026.022 I print_info: file type   = Q4_K - Medium
0.00.026.023 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.305 I load: special tokens cache size = 25
0.00.040.428 I load: token to piece cache size = 0.2984 MB
0.00.040.432 I print_info: arch             = gptneox
0.00.040.432 I print_info: vocab_only       = 0
0.00.040.432 I print_info: n_ctx_train      = 2048
0.00.040.433 I print_info: n_embd           = 2048
0.00.040.433 I print_info: n_layer          = 24
0.00.040.437 I print_info: n_head           = 16
0.00.040.438 I print_info: n_head_kv        = 16
0.00.040.438 I print_info: n_rot            = 32
0.00.040.438 I print_info: n_swa            = 0
0.00.040.438 I print_info: n_embd_head_k    = 128
0.00.040.438 I print_info: n_embd_head_v    = 128
0.00.040.439 I print_info: n_gqa            = 1
0.00.040.440 I print_info: n_embd_k_gqa     = 2048
0.00.040.441 I print_info: n_embd_v_gqa     = 2048
0.00.040.441 I print_info: f_norm_eps       = 1.0e-05
0.00.040.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.442 I print_info: f_logit_scale    = 0.0e+00
0.00.040.443 I print_info: n_ff             = 8192
0.00.040.443 I print_info: n_expert         = 0
0.00.040.443 I print_info: n_expert_used    = 0
0.00.040.443 I print_info: causal attn      = 1
0.00.040.443 I print_info: pooling type     = 0
0.00.040.443 I print_info: rope type        = 2
0.00.040.444 I print_info: rope scaling     = linear
0.00.040.444 I print_info: freq_base_train  = 10000.0
0.00.040.444 I print_info: freq_scale_train = 1
0.00.040.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.445 I print_info: rope_finetuned   = unknown
0.00.040.445 I print_info: ssm_d_conv       = 0
0.00.040.445 I print_info: ssm_d_inner      = 0
0.00.040.445 I print_info: ssm_d_state      = 0
0.00.040.445 I print_info: ssm_dt_rank      = 0
0.00.040.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.446 I print_info: model type       = 1.4B
0.00.040.446 I print_info: model params     = 1.41 B
0.00.040.446 I print_info: general.name     = 1.4B
0.00.040.447 I print_info: vocab type       = BPE
0.00.040.447 I print_info: n_vocab          = 50304
0.00.040.448 I print_info: n_merges         = 50009
0.00.040.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: LF token         = 187 'Ċ'
0.00.040.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: max token length = 1024
0.00.040.450 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.370 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.384 I load_tensors: offloading output layer to GPU
0.00.528.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.415 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.416 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.914 I llama_init_from_model: n_seq_max     = 1
0.00.529.921 I llama_init_from_model: n_ctx         = 128
0.00.529.921 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.922 I llama_init_from_model: n_batch       = 128
0.00.529.922 I llama_init_from_model: n_ubatch      = 128
0.00.529.922 I llama_init_from_model: flash_attn    = 0
0.00.529.923 I llama_init_from_model: freq_base     = 10000.0
0.00.529.924 I llama_init_from_model: freq_scale    = 1
0.00.529.925 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.930 I ggml_metal_init: allocating
0.00.529.978 I ggml_metal_init: found device: Apple M4
0.00.529.992 I ggml_metal_init: picking default device: Apple M4
0.00.531.650 I ggml_metal_init: using embedded metal library
0.00.538.554 I ggml_metal_init: GPU name:   Apple M4
0.00.538.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.562 I ggml_metal_init: simdgroup reduction   = true
0.00.538.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.563 I ggml_metal_init: has residency sets    = true
0.00.538.563 I ggml_metal_init: has bfloat            = true
0.00.538.564 I ggml_metal_init: use bfloat            = true
0.00.538.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.339 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.559.976 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.559.984 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.034 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.249 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.251 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.251 I llama_init_from_model: graph nodes  = 967
0.00.563.251 I llama_init_from_model: graph splits = 2
0.00.563.254 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.595.589 I 
0.00.595.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.662 I perplexity: tokenizing the input ..
0.00.602.634 I perplexity: tokenization took 6.969 ms
0.00.602.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.743 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.744.077 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.744.103 I llama_perf_context_print:        load time =     585.56 ms
0.00.744.104 I llama_perf_context_print: prompt eval time =     139.16 ms /   128 tokens (    1.09 ms per token,   919.82 tokens per second)
0.00.744.105 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.105 I llama_perf_context_print:       total time =     148.51 ms /   129 tokens
0.00.744.464 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.080s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.984 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.124 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.124 I llama_model_loader: - type  f32:  194 tensors
0.00.026.125 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.125 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.126 I print_info: file format = GGUF V3 (latest)
0.00.026.126 I print_info: file type   = Q5_K - Medium
0.00.026.128 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.409 I load: special tokens cache size = 25
0.00.040.486 I load: token to piece cache size = 0.2984 MB
0.00.040.491 I print_info: arch             = gptneox
0.00.040.491 I print_info: vocab_only       = 0
0.00.040.491 I print_info: n_ctx_train      = 2048
0.00.040.491 I print_info: n_embd           = 2048
0.00.040.492 I print_info: n_layer          = 24
0.00.040.496 I print_info: n_head           = 16
0.00.040.496 I print_info: n_head_kv        = 16
0.00.040.497 I print_info: n_rot            = 32
0.00.040.497 I print_info: n_swa            = 0
0.00.040.497 I print_info: n_embd_head_k    = 128
0.00.040.498 I print_info: n_embd_head_v    = 128
0.00.040.499 I print_info: n_gqa            = 1
0.00.040.500 I print_info: n_embd_k_gqa     = 2048
0.00.040.500 I print_info: n_embd_v_gqa     = 2048
0.00.040.501 I print_info: f_norm_eps       = 1.0e-05
0.00.040.501 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.502 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.502 I print_info: f_logit_scale    = 0.0e+00
0.00.040.502 I print_info: n_ff             = 8192
0.00.040.503 I print_info: n_expert         = 0
0.00.040.503 I print_info: n_expert_used    = 0
0.00.040.503 I print_info: causal attn      = 1
0.00.040.503 I print_info: pooling type     = 0
0.00.040.503 I print_info: rope type        = 2
0.00.040.503 I print_info: rope scaling     = linear
0.00.040.504 I print_info: freq_base_train  = 10000.0
0.00.040.504 I print_info: freq_scale_train = 1
0.00.040.504 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.504 I print_info: rope_finetuned   = unknown
0.00.040.505 I print_info: ssm_d_conv       = 0
0.00.040.505 I print_info: ssm_d_inner      = 0
0.00.040.505 I print_info: ssm_d_state      = 0
0.00.040.505 I print_info: ssm_dt_rank      = 0
0.00.040.505 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.506 I print_info: model type       = 1.4B
0.00.040.506 I print_info: model params     = 1.41 B
0.00.040.506 I print_info: general.name     = 1.4B
0.00.040.517 I print_info: vocab type       = BPE
0.00.040.521 I print_info: n_vocab          = 50304
0.00.040.521 I print_info: n_merges         = 50009
0.00.040.521 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.521 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.522 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.522 I print_info: LF token         = 187 'Ċ'
0.00.040.522 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.522 I print_info: max token length = 1024
0.00.040.523 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.935 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.939 I load_tensors: offloading output layer to GPU
0.00.603.940 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.957 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.603.958 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.604.824 I llama_init_from_model: n_seq_max     = 1
0.00.604.827 I llama_init_from_model: n_ctx         = 128
0.00.604.827 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.827 I llama_init_from_model: n_batch       = 128
0.00.604.828 I llama_init_from_model: n_ubatch      = 128
0.00.604.828 I llama_init_from_model: flash_attn    = 0
0.00.604.829 I llama_init_from_model: freq_base     = 10000.0
0.00.604.830 I llama_init_from_model: freq_scale    = 1
0.00.604.831 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.832 I ggml_metal_init: allocating
0.00.604.874 I ggml_metal_init: found device: Apple M4
0.00.604.886 I ggml_metal_init: picking default device: Apple M4
0.00.605.895 I ggml_metal_init: using embedded metal library
0.00.610.022 I ggml_metal_init: GPU name:   Apple M4
0.00.610.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.031 I ggml_metal_init: simdgroup reduction   = true
0.00.610.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.031 I ggml_metal_init: has residency sets    = true
0.00.610.031 I ggml_metal_init: has bfloat            = true
0.00.610.032 I ggml_metal_init: use bfloat            = true
0.00.610.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.380 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.061 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.626.065 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.094 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.770 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.771 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.771 I llama_init_from_model: graph nodes  = 967
0.00.627.772 I llama_init_from_model: graph splits = 2
0.00.627.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.919 I 
0.00.657.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.947 I perplexity: tokenizing the input ..
0.00.661.932 I perplexity: tokenization took 3.983 ms
0.00.661.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.483 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.769 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.791 I llama_perf_context_print:        load time =     649.11 ms
0.00.798.792 I llama_perf_context_print: prompt eval time =     135.32 ms /   128 tokens (    1.06 ms per token,   945.93 tokens per second)
0.00.798.793 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.793 I llama_perf_context_print:       total time =     140.87 ms /   129 tokens
0.00.799.133 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.069s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.754 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.592 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.599 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.599 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.601 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.602 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.605 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.130 I llama_model_loader: - type  f32:  194 tensors
0.00.025.131 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.131 I print_info: file format = GGUF V3 (latest)
0.00.025.132 I print_info: file type   = Q6_K
0.00.025.133 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.928 I load: special tokens cache size = 25
0.00.039.068 I load: token to piece cache size = 0.2984 MB
0.00.039.072 I print_info: arch             = gptneox
0.00.039.073 I print_info: vocab_only       = 0
0.00.039.073 I print_info: n_ctx_train      = 2048
0.00.039.073 I print_info: n_embd           = 2048
0.00.039.073 I print_info: n_layer          = 24
0.00.039.078 I print_info: n_head           = 16
0.00.039.078 I print_info: n_head_kv        = 16
0.00.039.078 I print_info: n_rot            = 32
0.00.039.078 I print_info: n_swa            = 0
0.00.039.079 I print_info: n_embd_head_k    = 128
0.00.039.079 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.081 I print_info: n_embd_v_gqa     = 2048
0.00.039.081 I print_info: f_norm_eps       = 1.0e-05
0.00.039.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.084 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.085 I print_info: f_logit_scale    = 0.0e+00
0.00.039.085 I print_info: n_ff             = 8192
0.00.039.085 I print_info: n_expert         = 0
0.00.039.085 I print_info: n_expert_used    = 0
0.00.039.086 I print_info: causal attn      = 1
0.00.039.086 I print_info: pooling type     = 0
0.00.039.086 I print_info: rope type        = 2
0.00.039.086 I print_info: rope scaling     = linear
0.00.039.087 I print_info: freq_base_train  = 10000.0
0.00.039.087 I print_info: freq_scale_train = 1
0.00.039.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.087 I print_info: rope_finetuned   = unknown
0.00.039.088 I print_info: ssm_d_conv       = 0
0.00.039.088 I print_info: ssm_d_inner      = 0
0.00.039.088 I print_info: ssm_d_state      = 0
0.00.039.088 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.089 I print_info: model params     = 1.41 B
0.00.039.089 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.090 I print_info: n_vocab          = 50304
0.00.039.090 I print_info: n_merges         = 50009
0.00.039.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: LF token         = 187 'Ċ'
0.00.039.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: max token length = 1024
0.00.039.093 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.366 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.377 I load_tensors: offloading output layer to GPU
0.00.590.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.419 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.590.420 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.591.860 I llama_init_from_model: n_seq_max     = 1
0.00.591.863 I llama_init_from_model: n_ctx         = 128
0.00.591.863 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.864 I llama_init_from_model: n_batch       = 128
0.00.591.864 I llama_init_from_model: n_ubatch      = 128
0.00.591.864 I llama_init_from_model: flash_attn    = 0
0.00.591.866 I llama_init_from_model: freq_base     = 10000.0
0.00.591.867 I llama_init_from_model: freq_scale    = 1
0.00.591.868 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.878 I ggml_metal_init: allocating
0.00.591.937 I ggml_metal_init: found device: Apple M4
0.00.591.951 I ggml_metal_init: picking default device: Apple M4
0.00.593.419 I ggml_metal_init: using embedded metal library
0.00.599.675 I ggml_metal_init: GPU name:   Apple M4
0.00.599.679 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.680 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.681 I ggml_metal_init: simdgroup reduction   = true
0.00.599.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.682 I ggml_metal_init: has residency sets    = true
0.00.599.682 I ggml_metal_init: has bfloat            = true
0.00.599.682 I ggml_metal_init: use bfloat            = true
0.00.599.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.367 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.954 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.958 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.369 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.371 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.371 I llama_init_from_model: graph nodes  = 967
0.00.623.372 I llama_init_from_model: graph splits = 2
0.00.623.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.104 I 
0.00.661.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.179 I perplexity: tokenizing the input ..
0.00.667.518 I perplexity: tokenization took 6.338 ms
0.00.667.523 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.974 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.800.385 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.800.407 I llama_perf_context_print:        load time =     652.34 ms
0.00.800.408 I llama_perf_context_print: prompt eval time =     131.16 ms /   128 tokens (    1.02 ms per token,   975.90 tokens per second)
0.00.800.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.409 I llama_perf_context_print:       total time =     139.31 ms /   129 tokens
0.00.800.769 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.077s
sys	0m0.150s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.197 I build: 4731 (0f2bbe65) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.147 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.646 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.656 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.649 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.651 I llama_model_loader: - type  f32:  194 tensors
0.00.048.651 I llama_model_loader: - type  f16:   98 tensors
0.00.048.657 I print_info: file format = GGUF V3 (latest)
0.00.048.658 I print_info: file type   = all F32 (guessed)
0.00.048.661 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.056.541 I load: special tokens cache size = 25
0.00.062.758 I load: token to piece cache size = 0.2984 MB
0.00.062.767 I print_info: arch             = gptneox
0.00.062.767 I print_info: vocab_only       = 0
0.00.062.768 I print_info: n_ctx_train      = 2048
0.00.062.768 I print_info: n_embd           = 2048
0.00.062.768 I print_info: n_layer          = 24
0.00.062.773 I print_info: n_head           = 16
0.00.062.773 I print_info: n_head_kv        = 16
0.00.062.773 I print_info: n_rot            = 32
0.00.062.774 I print_info: n_swa            = 0
0.00.062.774 I print_info: n_embd_head_k    = 128
0.00.062.774 I print_info: n_embd_head_v    = 128
0.00.062.775 I print_info: n_gqa            = 1
0.00.062.775 I print_info: n_embd_k_gqa     = 2048
0.00.062.777 I print_info: n_embd_v_gqa     = 2048
0.00.062.779 I print_info: f_norm_eps       = 1.0e-05
0.00.062.779 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.779 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.780 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.780 I print_info: f_logit_scale    = 0.0e+00
0.00.062.780 I print_info: n_ff             = 8192
0.00.062.781 I print_info: n_expert         = 0
0.00.062.782 I print_info: n_expert_used    = 0
0.00.062.783 I print_info: causal attn      = 1
0.00.062.783 I print_info: pooling type     = 0
0.00.062.783 I print_info: rope type        = 2
0.00.062.783 I print_info: rope scaling     = linear
0.00.062.784 I print_info: freq_base_train  = 10000.0
0.00.062.784 I print_info: freq_scale_train = 1
0.00.062.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.784 I print_info: rope_finetuned   = unknown
0.00.062.784 I print_info: ssm_d_conv       = 0
0.00.062.784 I print_info: ssm_d_inner      = 0
0.00.062.784 I print_info: ssm_d_state      = 0
0.00.062.785 I print_info: ssm_dt_rank      = 0
0.00.062.785 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.785 I print_info: model type       = 1.4B
0.00.062.785 I print_info: model params     = 1.41 B
0.00.062.785 I print_info: general.name     = 1.4B
0.00.062.786 I print_info: vocab type       = BPE
0.00.062.786 I print_info: n_vocab          = 50304
0.00.062.786 I print_info: n_merges         = 50009
0.00.062.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.787 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.787 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.787 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.787 I print_info: LF token         = 187 'Ċ'
0.00.062.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.787 I print_info: max token length = 1024
0.00.062.788 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.434.588 I load_tensors: offloading 24 repeating layers to GPU
0.01.434.600 I load_tensors: offloading output layer to GPU
0.01.434.601 I load_tensors: offloaded 25/25 layers to GPU
0.01.434.630 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.434.631 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.435.263 I llama_init_from_model: n_seq_max     = 1
0.01.435.265 I llama_init_from_model: n_ctx         = 128
0.01.435.265 I llama_init_from_model: n_ctx_per_seq = 128
0.01.435.265 I llama_init_from_model: n_batch       = 128
0.01.435.265 I llama_init_from_model: n_ubatch      = 128
0.01.435.266 I llama_init_from_model: flash_attn    = 0
0.01.435.266 I llama_init_from_model: freq_base     = 10000.0
0.01.435.266 I llama_init_from_model: freq_scale    = 1
0.01.435.267 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.435.268 I ggml_metal_init: allocating
0.01.435.310 I ggml_metal_init: found device: Apple M4
0.01.435.317 I ggml_metal_init: picking default device: Apple M4
0.01.436.416 I ggml_metal_init: using embedded metal library
0.01.440.168 I ggml_metal_init: GPU name:   Apple M4
0.01.440.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.440.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.440.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.440.172 I ggml_metal_init: simdgroup reduction   = true
0.01.440.172 I ggml_metal_init: simdgroup matrix mul. = true
0.01.440.172 I ggml_metal_init: has residency sets    = true
0.01.440.172 I ggml_metal_init: has bfloat            = true
0.01.440.172 I ggml_metal_init: use bfloat            = true
0.01.440.173 I ggml_metal_init: hasUnifiedMemory      = true
0.01.440.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.450.364 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.452.171 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.452.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.452.197 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.453.812 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.453.813 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.453.813 I llama_init_from_model: graph nodes  = 967
0.01.453.813 I llama_init_from_model: graph splits = 2
0.01.453.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.453.815 I 
0.01.453.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.453.846 I compute_imatrix: tokenizing the input ..
0.01.457.798 I compute_imatrix: tokenization took 3.951 ms
0.01.457.799 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.723.380 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.726.696 I llama_perf_context_print:        load time =    1705.22 ms
0.01.726.698 I llama_perf_context_print: prompt eval time =     263.86 ms /   128 tokens (    2.06 ms per token,   485.11 tokens per second)
0.01.726.698 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.726.699 I llama_perf_context_print:       total time =    1708.53 ms /   129 tokens
0.01.727.204 I ggml_metal_free: deallocating

real	0m1.910s
user	0m0.116s
sys	0m0.252s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4731 (0f2bbe65)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb0e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb10c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb16160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb1d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb1fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bb21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bb224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bb22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bb22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bb234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bb23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bb23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bb244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bb24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bb24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bb254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bb25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bb25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bb264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bb26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bb26f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bb274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bb279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bb27f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bb28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bb289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bb28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bb29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bb299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bb196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bb29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bb2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bb2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bb2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bb2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bb2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bb2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bb2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bb2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bb2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bb2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bb2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bb2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bb2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bb2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ae04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ae046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ae04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ae04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ae053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ae05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ae05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ae069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ae06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ae06f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ae07390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ae07800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ae07c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ae080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ae08550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ae089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ae08e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ae092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ae09710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ae09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ae09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ae0a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ae0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ae0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ae0b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ae0b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ae0ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ae0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ae0c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ae0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ae0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ae0d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ae0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ae0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ae0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ae0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ae0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ae0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ae0efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ae0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ae0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ae0fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ae10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ae10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ae10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ae10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ae11350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ae117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ae11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ae120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ae12510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ae12980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ae12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ae13260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ae136d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ae13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ae13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ae14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ae14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ae14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ae15170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ae155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ae15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ae15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ae16330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ae167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ae16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ae17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ae174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ae17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ae18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ae186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ae18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ae18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ae19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ae19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ae19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ae1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ae1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ae1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ae1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ae1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ae1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ae1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ae1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ae1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ae1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ae1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ae1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ae1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ae1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ae1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ae1e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ae1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ae1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ae1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ae1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ae20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ae20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ae210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ae216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ae21c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ae22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ae227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ae22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ae23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ae23930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ae23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ae244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ae24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ae25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ae255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ae25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ae26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ae26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ae26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ae272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ae27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ae27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ae283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ae289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ae28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ae29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ae29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ae2a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ae2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ae2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ae2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ae2b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ae2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ae2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ae2c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ae2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ae2d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ae2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ae2dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ae2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ae2eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ae2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ae2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ae2fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ae30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ae30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ae30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ae313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ae31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ae31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ae324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ae32ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ae33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ae33630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ae33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ae34030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ae34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ae34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ae34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ae35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ae35930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ae35e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ae36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ae36830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ae36d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ae37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ae37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ae37c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ae38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ae38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ae39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ae39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ae3a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ae3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ae3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ae3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ae3b420 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.736.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105304dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105305240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1053056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105305b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105305f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105306400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105306870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105306ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105307150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1053075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105307a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105308120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105308c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1053093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105309c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10530a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10530aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10530b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10530b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10530bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10530c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10530cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10530d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10530dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10530e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10530e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10530e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10530ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10530f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10530f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10530fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10530ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105310430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1053106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105310b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105310fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1053118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105312190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105312600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105312a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105312ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105313350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1053137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105313c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1053140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105314510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105314980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105314df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105315260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1053156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105315b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105315fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105316420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105316890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105316e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105317300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105317770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105317be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105318050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1053184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105318930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105318da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105319210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105319680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105319af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105319f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10531a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10531a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10531acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10531b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10531b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10531ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10531be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10531c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10531c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10531cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10531d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10531d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10531d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10531dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10531e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10531e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10531ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10531ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10531f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10531f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10531fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105320100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105320570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1053209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105320e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1053212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105321730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105321ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105322010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105322480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1053228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105322d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1053231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105323640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105323ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105323f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105324390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105324800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105324c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1053250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105325550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1053259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105325e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1053262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105326710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105326b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105326ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105327460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1053278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105327d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1053281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105328620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105328a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105329370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1053297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105329c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10532a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10532a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10532a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10532ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10532b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10532b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10532bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10532bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10532c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10532c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10532cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10532d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10532d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10532da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10532dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10532e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10532e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10532ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10532f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10532f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10532f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10532fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105330260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1053306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105330b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105330fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105331420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105331890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105332170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1053325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105332a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105332ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105333330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1053337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1053344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105334960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105334dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105335240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105335e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105336130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1053363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105336860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105336cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105337140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1053375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105337a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105337e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105338300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105338770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105338be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105339050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1053394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105339930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105339da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10533a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10533a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10533aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10533af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10533b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10533b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10533bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10533c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10533c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10533ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10533ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10533d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10533d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10533dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10533e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10533e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10533e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10533ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10533f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10533f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10533fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1053400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105340540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1053409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1053417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105342830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105342af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1053430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105343670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105343c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1053441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1053447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105344d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105345330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1053458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105345eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105346470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105346a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1053475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105347b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105348130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1053486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105348cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105349270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105349df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10534a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10534a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10534af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10534b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10534bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10534c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10534c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10534cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10534d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10534d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10534dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10534e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10534e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10534ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10534f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10534f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10534ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105350570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105350b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1053510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1053516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105351c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105352230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1053527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105352db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105353370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105353930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105353ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1053544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105354a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105355030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1053555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105355bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105356170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105356730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105356cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1053571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1053576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105357bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1053580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1053585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105358af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105358ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1053594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1053599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105359ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10535a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10535a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10535adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10535b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10535b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10535c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10535c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10535d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10535d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10535da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10535e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10535e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10535eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb08a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb09040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb1a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb13400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb32fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb34b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bb38aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bb38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bb39020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bb392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bb395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bb39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bb39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bb3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bb3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bb3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bb3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bb3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bb3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bb3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bb3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bb3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bb3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bb3d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bb3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bb3e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bb3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bb3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bb3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bb3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bb3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bb402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bb407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bb40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bb41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bb417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bb41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bb42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bb427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bb42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bb43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bb437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bb43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bb44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bb447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bb44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bb45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bb46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bb468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bb46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bb47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bb476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bb47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bb47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bb48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bb48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bb48dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bb49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bb49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bb49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bb4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bb4a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bb4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bb4ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bb4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bb4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bb4bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bb4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bb4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bb4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bb4ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bb4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bb4d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bb4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bb4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bb4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bb4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bb4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bb4f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bb4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bb4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bb50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bb50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bb50aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bb50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bb513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bb51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bb51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bb521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bb52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bb52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bb52fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bb53440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bb538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bb53d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bb54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bb546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bb54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bb55000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bb554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bb55940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bb55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bb56280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bb56720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bb56bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bb57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bb57500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bb579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bb57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bb582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bb58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bb58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bb590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bb59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bb59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bb59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bb5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bb5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bb5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bb5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bb5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bb5ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bb5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bb5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bb5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bb5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bb5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bb5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bb5dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bb5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bb5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bb5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bb5f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bb5f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bb5fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bb604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bb60c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bb61130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bb615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bb61a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bb62220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bb62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bb62cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bb63210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bb63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bb63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bb64200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bb64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bb64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bb651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bb65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bb65c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bb661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bb66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bb66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bb671d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bb67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bb67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bb681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bb68710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bb68c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bb691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bb69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bb69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bb6a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bb6a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bb6ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bb6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bb6b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bb6bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bb6c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bb6c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bb6cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bb6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bb6d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bb6dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bb6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bb6e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bb6ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bb6f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bb6f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bb6fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bb70140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bb70690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bb70be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bb71130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bb71680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bb71bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bb72120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bb72670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bb72bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bb73110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bb73660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bb73bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bb74100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bb74650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bb74ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bb75040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bb754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bb75980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bb75e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bb762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bb76760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bb76c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bb770a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bb77540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bb779e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bb77e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bb78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bb787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bb78c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bb79100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bb79650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bb79d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bb7a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bb7abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bb7b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bb7b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bb7bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bb7c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bb7c650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.280s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4731 (0f2bbe65)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f0e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f0f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f10a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f12710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f13640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f15a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f23d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f24af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f25d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f26210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f26b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f28a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f28fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f2bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f2c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f2d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f30b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f34e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f36a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f36ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f37380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f38aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f38f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f39880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f3a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f3bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f3c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f3c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f3d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f3d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f3dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f3e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f3f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f3f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f3f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f41a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f43120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f43f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f45620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f46400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f48900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f4dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f51b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f53af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f54040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f55ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f56570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f56ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f57010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f57560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f57ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f58000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f58aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f59540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f5a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f5aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f5bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f5c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f5ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f5da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f5dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f5e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f5ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f5ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f5ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f60f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f61a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f61f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f624b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f62a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f62f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f634a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f64880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f64d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f651c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f65b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f65fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f66d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f67b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f68000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f69110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f69f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f6a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f6a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f6b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f6b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f6b9f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f6b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f4f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f1dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f23080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f6abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f1a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f6c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f6c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f6c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f6c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f6cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f6ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f6d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f6d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f6d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f6dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f6df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f6e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f6e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f6e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f6ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f6ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f6efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f6f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f6f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f6f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f6fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f6fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f70050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f70310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f705d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f70890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f70b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f70e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f710d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f71390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f71650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f71910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f71bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f71e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f72150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f72410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f726d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f72990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f72c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f72f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f73490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f73750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f73a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f73cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f73f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f74250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f74510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f747d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f74a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f74d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f75010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f752d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f75590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f75850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f75b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f75dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f76090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f76350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f76610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f76b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f76e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f77110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f77690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f77950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f77c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f77ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f78190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f78450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f78710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f789d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f78f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f79210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f794d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f79790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f79a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f79d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f79fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f7a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f7a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f7a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f7aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f7ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f7b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f7b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f7b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f7b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f7bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f7be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f7c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f7c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f7c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f7c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f7cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f7ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f7d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f7d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f7d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f7d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f7dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f7df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f7e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f7e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f7e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f7ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f7ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f7ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f7f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f7fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f7fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f80010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f802d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f80590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f80850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f80b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f80dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f81090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f81350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f81610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f818d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f81b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f81e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f82110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f823d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f82690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f82950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f82c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f82ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f83190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f83450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f83710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f839d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f83c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f83f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f84210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f844d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f84790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f84a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f84d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f84fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f85290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f85550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f85810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f85ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f85d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f86050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f86310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f86890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f86b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f86e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f870d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f87390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f87650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f87910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f87bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f87e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f88150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f88410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f886d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f88990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f88c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f88f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f891d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f89490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f89750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f89a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f89cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f89f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f8a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f8a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f8a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f8aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f8afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f8b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f8b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f8bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f8c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f8c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f8cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f8cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f8d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f8d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f8daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f8df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f8e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f8e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f8ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f8f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f8f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f8fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f8fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f902e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f90750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f90bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f91030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f91910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f91d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f92660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f92ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f92f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f93820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f93c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f94100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f94570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f94e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f95730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f95ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f96010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f96480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f96d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f971d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f97640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f97ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f97f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f98390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f98800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f98c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f990e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f99550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f999c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f99e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f9a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f9a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f9ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f9aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f9b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f9b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f9bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f9c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f9c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f9ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f9cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f9d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f9d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f9dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f9e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f9e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f9e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f9ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f9f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f9f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f9fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f9ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146fa0440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146fa0eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146fa15d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146fa1cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146fa2410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146fa26d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146fa2ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146fa3180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146fa3790 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148105e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148106270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1481066e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148106b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148106fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148107430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1481078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148107d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148108180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1481085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148108a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148109100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148109c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14810a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14810abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14810b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14810ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14810c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14810c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14810d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14810d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14810de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14810e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14810ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14810f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14810f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14810f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14810fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148110230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1481106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148110b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148111040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1481114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148111770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148111be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148112050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1481124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148112930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148112da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148113210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148113680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148113af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148113f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1481143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148114840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148114cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148115120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148115590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148115a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148115e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1481162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148116750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148116bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148117030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1481174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148117910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148117e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148118380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1481187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148118c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1481190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148119540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1481199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148119e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14811a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14811a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14811ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14811afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14811b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14811b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14811bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14811c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14811c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14811ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14811cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14811d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14811d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14811dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14811e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14811e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14811e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14811ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14811f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14811f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14811fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14811ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148120430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1481208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148120d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148121180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1481215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148121a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148121ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148122340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1481227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148122c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148123090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148123500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148123970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148123de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148124250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1481246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148124b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148124fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148125910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148125bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148126040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1481264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148126920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148126d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148127200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148127670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148127ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148127f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1481283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148128830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148128ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148129110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148129580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1481299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148129e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14812a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14812a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14812abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14812b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14812b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14812b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14812bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14812c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14812c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14812cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14812cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14812d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14812d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14812dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14812e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14812e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14812e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14812ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14812f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14812f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14812fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148130000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148130470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1481308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148130d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1481311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148131630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148131aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148131f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148132380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1481327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148132c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1481330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148133540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1481339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148133e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148134290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148134700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148134b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148134fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148135450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1481358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148135d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1481361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148136610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148136a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148136ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148137360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1481377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148137c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1481380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148138520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148138990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148138e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148139270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1481396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148139b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148139fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14813a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14813a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14813ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14813b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14813b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14813ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14813bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14813c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14813c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14813cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14813d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14813d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14813d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14813dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14813e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14813e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14813eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14813efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14813f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14813f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14813fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148140160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1481405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148140a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148140eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148141320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148141790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148141c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148142190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148142600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148142a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1481435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148143880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148143b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148143fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148144420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148144890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148144d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148145170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1481455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148145a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148145ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148146330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1481467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148146c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148147080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1481474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148147960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148147dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148148240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1481486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148148b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148148f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148149400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148149870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148149ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14814a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14814a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14814aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14814aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14814b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14814b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14814bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14814c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14814c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14814c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14814cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14814d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14814d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14814db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14814df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14814e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14814e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14814ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14814f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14814f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14814fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14814fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1481502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148150760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148150bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148151040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1481514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148151920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148151d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148152200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148152670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148152ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148152f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1481533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148153830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148153ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148154110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148154580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1481549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148154e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1481552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148155740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148155bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148156020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148156490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148156900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148156d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1481571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148157c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148158370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148158a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1481591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148159470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1481598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148159ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14815a4f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.955s
user	0m0.230s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
