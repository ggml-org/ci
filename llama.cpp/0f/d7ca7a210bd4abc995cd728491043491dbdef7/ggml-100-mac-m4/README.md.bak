### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.13 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.31 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.82 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.33 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.95 sec*proc (29 tests)

Total Test time (real) = 165.96 sec

real	2m45.959s
user	4m38.816s
sys	0m5.865s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.26 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.79 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.50 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.45 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.51 sec*proc (29 tests)

Total Test time (real) =  48.52 sec

real	0m48.533s
user	0m54.348s
sys	0m5.350s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.123 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.593 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.037 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.014.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.043 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.049 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.050 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.050 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.053 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.053 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.053 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.054 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.054 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.056 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.057 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.014.057 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.014.058 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.014.058 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.058 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.014.059 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.656 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.657 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.657 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.658 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.658 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.658 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.016.659 I llama_model_loader: - type  f32:  124 tensors
0.00.016.659 I llama_model_loader: - type  f16:   73 tensors
0.00.016.659 I print_info: file format = GGUF V3 (latest)
0.00.016.660 I print_info: file type   = F16
0.00.016.661 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.018.899 I load: special tokens cache size = 5
0.00.020.111 I load: token to piece cache size = 0.2032 MB
0.00.020.133 I print_info: arch             = bert
0.00.020.134 I print_info: vocab_only       = 0
0.00.020.134 I print_info: n_ctx_train      = 512
0.00.020.134 I print_info: n_embd           = 384
0.00.020.135 I print_info: n_layer          = 12
0.00.020.138 I print_info: n_head           = 12
0.00.020.138 I print_info: n_head_kv        = 12
0.00.020.139 I print_info: n_rot            = 32
0.00.020.139 I print_info: n_swa            = 0
0.00.020.139 I print_info: n_embd_head_k    = 32
0.00.020.139 I print_info: n_embd_head_v    = 32
0.00.020.140 I print_info: n_gqa            = 1
0.00.020.140 I print_info: n_embd_k_gqa     = 384
0.00.020.141 I print_info: n_embd_v_gqa     = 384
0.00.020.141 I print_info: f_norm_eps       = 1.0e-12
0.00.020.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.142 I print_info: f_logit_scale    = 0.0e+00
0.00.020.143 I print_info: n_ff             = 1536
0.00.020.143 I print_info: n_expert         = 0
0.00.020.143 I print_info: n_expert_used    = 0
0.00.020.144 I print_info: causal attn      = 0
0.00.020.144 I print_info: pooling type     = 2
0.00.020.144 I print_info: rope type        = 2
0.00.020.144 I print_info: rope scaling     = linear
0.00.020.144 I print_info: freq_base_train  = 10000.0
0.00.020.145 I print_info: freq_scale_train = 1
0.00.020.145 I print_info: n_ctx_orig_yarn  = 512
0.00.020.145 I print_info: rope_finetuned   = unknown
0.00.020.145 I print_info: ssm_d_conv       = 0
0.00.020.145 I print_info: ssm_d_inner      = 0
0.00.020.145 I print_info: ssm_d_state      = 0
0.00.020.146 I print_info: ssm_dt_rank      = 0
0.00.020.146 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.146 I print_info: model type       = 33M
0.00.020.146 I print_info: model params     = 33.21 M
0.00.020.146 I print_info: general.name     = Bge Small
0.00.020.147 I print_info: vocab type       = WPM
0.00.020.147 I print_info: n_vocab          = 30522
0.00.020.147 I print_info: n_merges         = 0
0.00.020.148 I print_info: BOS token        = 101 '[CLS]'
0.00.020.148 I print_info: UNK token        = 100 '[UNK]'
0.00.020.148 I print_info: SEP token        = 102 '[SEP]'
0.00.020.148 I print_info: PAD token        = 0 '[PAD]'
0.00.020.148 I print_info: MASK token       = 103 '[MASK]'
0.00.020.148 I print_info: LF token         = 0 '[PAD]'
0.00.020.149 I print_info: max token length = 21
0.00.020.149 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.173 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.174 I load_tensors: offloading output layer to GPU
0.00.022.174 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.194 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.022.196 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.022.423 I llama_init_from_model: n_seq_max     = 1
0.00.022.424 I llama_init_from_model: n_ctx         = 512
0.00.022.424 I llama_init_from_model: n_ctx_per_seq = 512
0.00.022.425 I llama_init_from_model: n_batch       = 2048
0.00.022.425 I llama_init_from_model: n_ubatch      = 2048
0.00.022.425 I llama_init_from_model: flash_attn    = 0
0.00.022.425 I llama_init_from_model: freq_base     = 10000.0
0.00.022.426 I llama_init_from_model: freq_scale    = 1
0.00.022.426 I ggml_metal_init: allocating
0.00.022.435 I ggml_metal_init: found device: Apple M4
0.00.022.440 I ggml_metal_init: picking default device: Apple M4
0.00.022.931 I ggml_metal_init: using embedded metal library
0.00.025.481 I ggml_metal_init: GPU name:   Apple M4
0.00.025.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.484 I ggml_metal_init: simdgroup reduction   = true
0.00.025.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.485 I ggml_metal_init: has residency sets    = true
0.00.025.485 I ggml_metal_init: has bfloat            = true
0.00.025.485 I ggml_metal_init: use bfloat            = true
0.00.025.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.487 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.235 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.036.823 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.036.825 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.827 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.037.895 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.037.896 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.037.896 I llama_init_from_model: graph nodes  = 429
0.00.037.897 I llama_init_from_model: graph splits = 2
0.00.037.898 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.266 I 
0.00.042.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.830 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.047.149 I llama_perf_context_print:        load time =      30.67 ms
0.00.047.150 I llama_perf_context_print: prompt eval time =       4.19 ms /     9 tokens (    0.47 ms per token,  2149.00 tokens per second)
0.00.047.151 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.151 I llama_perf_context_print:       total time =       4.88 ms /    10 tokens
0.00.047.330 I ggml_metal_free: deallocating

real	0m0.244s
user	0m0.033s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.290 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.295 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.298 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.298 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.298 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.299 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.300 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.300 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.301 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.301 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.303 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.304 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.304 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.304 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.305 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.305 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.432 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.060 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.062 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.062 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.062 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.063 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.063 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.063 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.064 I llama_model_loader: - type  f32:  124 tensors
0.00.014.064 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.065 I print_info: file format = GGUF V3 (latest)
0.00.014.065 I print_info: file type   = Q8_0
0.00.014.066 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.389 I load: special tokens cache size = 5
0.00.017.627 I load: token to piece cache size = 0.2032 MB
0.00.017.637 I print_info: arch             = bert
0.00.017.638 I print_info: vocab_only       = 0
0.00.017.638 I print_info: n_ctx_train      = 512
0.00.017.638 I print_info: n_embd           = 384
0.00.017.639 I print_info: n_layer          = 12
0.00.017.642 I print_info: n_head           = 12
0.00.017.643 I print_info: n_head_kv        = 12
0.00.017.643 I print_info: n_rot            = 32
0.00.017.643 I print_info: n_swa            = 0
0.00.017.643 I print_info: n_embd_head_k    = 32
0.00.017.643 I print_info: n_embd_head_v    = 32
0.00.017.644 I print_info: n_gqa            = 1
0.00.017.645 I print_info: n_embd_k_gqa     = 384
0.00.017.645 I print_info: n_embd_v_gqa     = 384
0.00.017.646 I print_info: f_norm_eps       = 1.0e-12
0.00.017.646 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.647 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.647 I print_info: f_logit_scale    = 0.0e+00
0.00.017.648 I print_info: n_ff             = 1536
0.00.017.648 I print_info: n_expert         = 0
0.00.017.648 I print_info: n_expert_used    = 0
0.00.017.648 I print_info: causal attn      = 0
0.00.017.648 I print_info: pooling type     = 2
0.00.017.648 I print_info: rope type        = 2
0.00.017.648 I print_info: rope scaling     = linear
0.00.017.649 I print_info: freq_base_train  = 10000.0
0.00.017.649 I print_info: freq_scale_train = 1
0.00.017.649 I print_info: n_ctx_orig_yarn  = 512
0.00.017.650 I print_info: rope_finetuned   = unknown
0.00.017.650 I print_info: ssm_d_conv       = 0
0.00.017.650 I print_info: ssm_d_inner      = 0
0.00.017.650 I print_info: ssm_d_state      = 0
0.00.017.650 I print_info: ssm_dt_rank      = 0
0.00.017.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.651 I print_info: model type       = 33M
0.00.017.651 I print_info: model params     = 33.21 M
0.00.017.651 I print_info: general.name     = Bge Small
0.00.017.652 I print_info: vocab type       = WPM
0.00.017.652 I print_info: n_vocab          = 30522
0.00.017.652 I print_info: n_merges         = 0
0.00.017.653 I print_info: BOS token        = 101 '[CLS]'
0.00.017.653 I print_info: UNK token        = 100 '[UNK]'
0.00.017.653 I print_info: SEP token        = 102 '[SEP]'
0.00.017.653 I print_info: PAD token        = 0 '[PAD]'
0.00.017.653 I print_info: MASK token       = 103 '[MASK]'
0.00.017.655 I print_info: LF token         = 0 '[PAD]'
0.00.017.655 I print_info: max token length = 21
0.00.017.655 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.311 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.312 I load_tensors: offloading output layer to GPU
0.00.019.312 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.318 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.319 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.496 I llama_init_from_model: n_seq_max     = 1
0.00.019.497 I llama_init_from_model: n_ctx         = 512
0.00.019.497 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.497 I llama_init_from_model: n_batch       = 2048
0.00.019.497 I llama_init_from_model: n_ubatch      = 2048
0.00.019.498 I llama_init_from_model: flash_attn    = 0
0.00.019.498 I llama_init_from_model: freq_base     = 10000.0
0.00.019.498 I llama_init_from_model: freq_scale    = 1
0.00.019.499 I ggml_metal_init: allocating
0.00.019.502 I ggml_metal_init: found device: Apple M4
0.00.019.506 I ggml_metal_init: picking default device: Apple M4
0.00.019.934 I ggml_metal_init: using embedded metal library
0.00.022.326 I ggml_metal_init: GPU name:   Apple M4
0.00.022.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.328 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.329 I ggml_metal_init: simdgroup reduction   = true
0.00.022.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.329 I ggml_metal_init: has residency sets    = true
0.00.022.329 I ggml_metal_init: has bfloat            = true
0.00.022.329 I ggml_metal_init: use bfloat            = true
0.00.022.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.984 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.569 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.571 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.574 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.522 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.523 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.523 I llama_init_from_model: graph nodes  = 429
0.00.034.523 I llama_init_from_model: graph splits = 2
0.00.034.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.579 I 
0.00.038.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.108 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.563 I llama_perf_context_print:        load time =      29.67 ms
0.00.043.564 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.043.565 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.566 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.043.761 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.247 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.384 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.538 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.546 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.547 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.548 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.555 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.557 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.557 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.558 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.559 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.559 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.564 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.565 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.565 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.242 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.023 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.023 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.024 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.024 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.024 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.025 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.025 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.046.026 I llama_model_loader: - type  f32:   40 tensors
0.00.046.026 I llama_model_loader: - type  f16:   30 tensors
0.00.046.026 I print_info: file format = GGUF V3 (latest)
0.00.046.027 I print_info: file type   = F16
0.00.046.028 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.050.100 W load: empty token at index 5
0.00.055.237 W load: model vocab missing newline token, using special_pad_id instead
0.00.056.758 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.056.793 I load: special tokens cache size = 5
0.00.318.478 I load: token to piece cache size = 1.5060 MB
0.00.318.508 I print_info: arch             = jina-bert-v2
0.00.318.509 I print_info: vocab_only       = 0
0.00.318.509 I print_info: n_ctx_train      = 8192
0.00.318.510 I print_info: n_embd           = 384
0.00.318.510 I print_info: n_layer          = 4
0.00.318.517 I print_info: n_head           = 12
0.00.318.520 I print_info: n_head_kv        = 12
0.00.318.521 I print_info: n_rot            = 32
0.00.318.521 I print_info: n_swa            = 0
0.00.318.522 I print_info: n_embd_head_k    = 32
0.00.318.522 I print_info: n_embd_head_v    = 32
0.00.318.522 I print_info: n_gqa            = 1
0.00.318.523 I print_info: n_embd_k_gqa     = 384
0.00.318.524 I print_info: n_embd_v_gqa     = 384
0.00.318.525 I print_info: f_norm_eps       = 1.0e-12
0.00.318.525 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.318.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.318.530 I print_info: f_max_alibi_bias = 8.0e+00
0.00.318.530 I print_info: f_logit_scale    = 0.0e+00
0.00.318.531 I print_info: n_ff             = 1536
0.00.318.532 I print_info: n_expert         = 0
0.00.318.532 I print_info: n_expert_used    = 0
0.00.318.532 I print_info: causal attn      = 0
0.00.318.532 I print_info: pooling type     = -1
0.00.318.533 I print_info: rope type        = -1
0.00.318.533 I print_info: rope scaling     = linear
0.00.318.533 I print_info: freq_base_train  = 10000.0
0.00.318.540 I print_info: freq_scale_train = 1
0.00.318.540 I print_info: n_ctx_orig_yarn  = 8192
0.00.318.541 I print_info: rope_finetuned   = unknown
0.00.318.541 I print_info: ssm_d_conv       = 0
0.00.318.541 I print_info: ssm_d_inner      = 0
0.00.318.541 I print_info: ssm_d_state      = 0
0.00.318.544 I print_info: ssm_dt_rank      = 0
0.00.318.544 I print_info: ssm_dt_b_c_rms   = 0
0.00.318.544 I print_info: model type       = 33M
0.00.318.545 I print_info: model params     = 32.90 M
0.00.318.545 I print_info: general.name     = Jina Bert Implementation
0.00.318.547 I print_info: vocab type       = BPE
0.00.318.547 I print_info: n_vocab          = 61056
0.00.318.547 I print_info: n_merges         = 39382
0.00.318.547 I print_info: BOS token        = 0 '<s>'
0.00.318.548 I print_info: EOS token        = 2 '</s>'
0.00.318.548 I print_info: UNK token        = 3 '<unk>'
0.00.318.548 I print_info: SEP token        = 2 '</s>'
0.00.318.548 I print_info: PAD token        = 1 '<pad>'
0.00.318.548 I print_info: MASK token       = 4 '<mask>'
0.00.318.549 I print_info: EOG token        = 2 '</s>'
0.00.318.549 I print_info: max token length = 45
0.00.318.549 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.320.761 I load_tensors: offloading 4 repeating layers to GPU
0.00.320.762 I load_tensors: offloading output layer to GPU
0.00.320.763 I load_tensors: offloaded 5/5 layers to GPU
0.00.320.786 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.787 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.306 I llama_init_from_model: n_seq_max     = 1
0.00.321.307 I llama_init_from_model: n_ctx         = 8192
0.00.321.307 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.321.307 I llama_init_from_model: n_batch       = 2048
0.00.321.307 I llama_init_from_model: n_ubatch      = 2048
0.00.321.307 I llama_init_from_model: flash_attn    = 0
0.00.321.308 I llama_init_from_model: freq_base     = 10000.0
0.00.321.308 I llama_init_from_model: freq_scale    = 1
0.00.321.308 I ggml_metal_init: allocating
0.00.321.313 I ggml_metal_init: found device: Apple M4
0.00.321.317 I ggml_metal_init: picking default device: Apple M4
0.00.321.928 I ggml_metal_init: using embedded metal library
0.00.324.898 I ggml_metal_init: GPU name:   Apple M4
0.00.324.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.324.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.324.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.324.900 I ggml_metal_init: simdgroup reduction   = true
0.00.324.900 I ggml_metal_init: simdgroup matrix mul. = true
0.00.324.900 I ggml_metal_init: has residency sets    = true
0.00.324.900 I ggml_metal_init: has bfloat            = true
0.00.324.901 I ggml_metal_init: use bfloat            = true
0.00.324.901 I ggml_metal_init: hasUnifiedMemory      = true
0.00.324.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.334.681 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.337.758 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.337.761 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.337.762 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.344.584 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.344.585 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.344.586 I llama_init_from_model: graph nodes  = 154
0.00.344.586 I llama_init_from_model: graph splits = 2
0.00.344.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.351.969 I 
0.00.351.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.352.378 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.352.379 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.352.392 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.352.392 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.352.397 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.352.397 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.352.907 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.356.558 I llama_perf_context_print:        load time =     330.58 ms
0.00.356.559 I llama_perf_context_print: prompt eval time =       3.64 ms /    62 tokens (    0.06 ms per token, 17014.27 tokens per second)
0.00.356.559 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.356.560 I llama_perf_context_print:       total time =       4.59 ms /    63 tokens
0.00.356.810 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.326s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.187 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.388 I main: llama backend init
0.00.000.396 I main: load the model and apply lora adapter, if any
0.00.050.119 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.463 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.495 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.081.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.081.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.081.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.081.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.081.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.081.061 I llama_model_loader: - type  f32:  194 tensors
0.00.081.062 I llama_model_loader: - type  f16:   98 tensors
0.00.081.063 I print_info: file format = GGUF V3 (latest)
0.00.081.069 I print_info: file type   = all F32 (guessed)
0.00.081.071 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.704 I load: special tokens cache size = 25
0.00.108.293 I load: token to piece cache size = 0.2984 MB
0.00.108.321 I print_info: arch             = gptneox
0.00.108.322 I print_info: vocab_only       = 0
0.00.108.323 I print_info: n_ctx_train      = 2048
0.00.108.323 I print_info: n_embd           = 2048
0.00.108.323 I print_info: n_layer          = 24
0.00.108.329 I print_info: n_head           = 16
0.00.108.330 I print_info: n_head_kv        = 16
0.00.108.331 I print_info: n_rot            = 32
0.00.108.331 I print_info: n_swa            = 0
0.00.108.331 I print_info: n_embd_head_k    = 128
0.00.108.334 I print_info: n_embd_head_v    = 128
0.00.108.335 I print_info: n_gqa            = 1
0.00.108.336 I print_info: n_embd_k_gqa     = 2048
0.00.108.337 I print_info: n_embd_v_gqa     = 2048
0.00.108.339 I print_info: f_norm_eps       = 1.0e-05
0.00.108.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.341 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.342 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.342 I print_info: f_logit_scale    = 0.0e+00
0.00.108.343 I print_info: n_ff             = 8192
0.00.108.343 I print_info: n_expert         = 0
0.00.108.343 I print_info: n_expert_used    = 0
0.00.108.344 I print_info: causal attn      = 1
0.00.108.344 I print_info: pooling type     = 0
0.00.108.344 I print_info: rope type        = 2
0.00.108.344 I print_info: rope scaling     = linear
0.00.108.345 I print_info: freq_base_train  = 10000.0
0.00.108.345 I print_info: freq_scale_train = 1
0.00.108.345 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.346 I print_info: rope_finetuned   = unknown
0.00.108.346 I print_info: ssm_d_conv       = 0
0.00.108.346 I print_info: ssm_d_inner      = 0
0.00.108.346 I print_info: ssm_d_state      = 0
0.00.108.346 I print_info: ssm_dt_rank      = 0
0.00.108.346 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.351 I print_info: model type       = 1.4B
0.00.108.352 I print_info: model params     = 1.41 B
0.00.108.352 I print_info: general.name     = 1.4B
0.00.108.353 I print_info: vocab type       = BPE
0.00.108.353 I print_info: n_vocab          = 50304
0.00.108.354 I print_info: n_merges         = 50009
0.00.108.354 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.354 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.354 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.356 I print_info: LF token         = 187 ''
0.00.108.357 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.357 I print_info: max token length = 1024
0.00.108.358 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.169.264 I load_tensors: offloading 24 repeating layers to GPU
0.00.169.267 I load_tensors: offloading output layer to GPU
0.00.169.267 I load_tensors: offloaded 25/25 layers to GPU
0.00.169.295 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.169.296 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.169.976 I llama_init_from_model: n_seq_max     = 1
0.00.169.977 I llama_init_from_model: n_ctx         = 2048
0.00.169.977 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.169.977 I llama_init_from_model: n_batch       = 2048
0.00.169.977 I llama_init_from_model: n_ubatch      = 512
0.00.169.977 I llama_init_from_model: flash_attn    = 0
0.00.169.978 I llama_init_from_model: freq_base     = 10000.0
0.00.169.978 I llama_init_from_model: freq_scale    = 1
0.00.169.979 I ggml_metal_init: allocating
0.00.170.034 I ggml_metal_init: found device: Apple M4
0.00.170.042 I ggml_metal_init: picking default device: Apple M4
0.00.170.613 I ggml_metal_init: using embedded metal library
0.00.201.648 I ggml_metal_init: GPU name:   Apple M4
0.00.201.650 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.201.650 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.201.651 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.201.651 I ggml_metal_init: simdgroup reduction   = true
0.00.201.651 I ggml_metal_init: simdgroup matrix mul. = true
0.00.201.651 I ggml_metal_init: has residency sets    = true
0.00.201.651 I ggml_metal_init: has bfloat            = true
0.00.201.651 I ggml_metal_init: use bfloat            = true
0.00.201.652 I ggml_metal_init: hasUnifiedMemory      = true
0.00.201.653 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.358.168 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.386.838 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.386.845 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.386.919 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.390.631 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.390.634 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.390.634 I llama_init_from_model: graph nodes  = 967
0.00.390.634 I llama_init_from_model: graph splits = 2
0.00.390.640 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.390.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.390.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.457.361 I main: llama threadpool init, n_threads = 4
0.00.457.404 I 
0.00.457.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.457.437 I 
0.00.457.634 I sampler seed: 1234
0.00.457.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.457.673 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.457.674 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.457.674 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.288.859 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.02.288.860 I llama_perf_context_print:        load time =     406.36 ms
0.02.288.861 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.21 tokens per second)
0.02.288.861 I llama_perf_context_print:        eval time =    1784.54 ms /    63 runs   (   28.33 ms per token,    35.30 tokens per second)
0.02.288.862 I llama_perf_context_print:       total time =    1832.37 ms /    70 tokens
0.02.289.157 I ggml_metal_free: deallocating

real	0m2.640s
user	0m0.137s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.671 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.504 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.620 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.628 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.633 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.830 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.832 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.833 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.833 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.834 I llama_model_loader: - type  f32:  194 tensors
0.00.056.834 I llama_model_loader: - type  f16:   98 tensors
0.00.056.835 I print_info: file format = GGUF V3 (latest)
0.00.056.836 I print_info: file type   = all F32 (guessed)
0.00.056.837 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.081 I load: special tokens cache size = 25
0.00.075.892 I load: token to piece cache size = 0.2984 MB
0.00.075.906 I print_info: arch             = gptneox
0.00.075.907 I print_info: vocab_only       = 0
0.00.075.908 I print_info: n_ctx_train      = 2048
0.00.075.908 I print_info: n_embd           = 2048
0.00.075.908 I print_info: n_layer          = 24
0.00.075.912 I print_info: n_head           = 16
0.00.075.912 I print_info: n_head_kv        = 16
0.00.075.912 I print_info: n_rot            = 32
0.00.075.913 I print_info: n_swa            = 0
0.00.075.915 I print_info: n_embd_head_k    = 128
0.00.075.915 I print_info: n_embd_head_v    = 128
0.00.075.916 I print_info: n_gqa            = 1
0.00.075.917 I print_info: n_embd_k_gqa     = 2048
0.00.075.918 I print_info: n_embd_v_gqa     = 2048
0.00.075.918 I print_info: f_norm_eps       = 1.0e-05
0.00.075.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.920 I print_info: f_logit_scale    = 0.0e+00
0.00.075.921 I print_info: n_ff             = 8192
0.00.075.921 I print_info: n_expert         = 0
0.00.075.921 I print_info: n_expert_used    = 0
0.00.075.921 I print_info: causal attn      = 1
0.00.075.921 I print_info: pooling type     = 0
0.00.075.921 I print_info: rope type        = 2
0.00.075.922 I print_info: rope scaling     = linear
0.00.075.922 I print_info: freq_base_train  = 10000.0
0.00.075.922 I print_info: freq_scale_train = 1
0.00.075.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.923 I print_info: rope_finetuned   = unknown
0.00.075.923 I print_info: ssm_d_conv       = 0
0.00.075.923 I print_info: ssm_d_inner      = 0
0.00.075.923 I print_info: ssm_d_state      = 0
0.00.075.927 I print_info: ssm_dt_rank      = 0
0.00.075.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.927 I print_info: model type       = 1.4B
0.00.075.928 I print_info: model params     = 1.41 B
0.00.075.928 I print_info: general.name     = 1.4B
0.00.075.928 I print_info: vocab type       = BPE
0.00.075.928 I print_info: n_vocab          = 50304
0.00.075.929 I print_info: n_merges         = 50009
0.00.075.929 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.929 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.929 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.930 I print_info: LF token         = 187 ''
0.00.075.930 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.930 I print_info: max token length = 1024
0.00.075.931 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.485.195 I load_tensors: offloading 24 repeating layers to GPU
0.01.485.203 I load_tensors: offloading output layer to GPU
0.01.485.203 I load_tensors: offloaded 25/25 layers to GPU
0.01.485.227 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.485.228 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.486.170 I llama_init_from_model: n_seq_max     = 1
0.01.486.171 I llama_init_from_model: n_ctx         = 128
0.01.486.171 I llama_init_from_model: n_ctx_per_seq = 128
0.01.486.171 I llama_init_from_model: n_batch       = 128
0.01.486.172 I llama_init_from_model: n_ubatch      = 128
0.01.486.172 I llama_init_from_model: flash_attn    = 0
0.01.486.173 I llama_init_from_model: freq_base     = 10000.0
0.01.486.173 I llama_init_from_model: freq_scale    = 1
0.01.486.173 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.486.175 I ggml_metal_init: allocating
0.01.486.235 I ggml_metal_init: found device: Apple M4
0.01.486.242 I ggml_metal_init: picking default device: Apple M4
0.01.487.224 I ggml_metal_init: using embedded metal library
0.01.491.171 I ggml_metal_init: GPU name:   Apple M4
0.01.491.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.491.175 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.491.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.491.176 I ggml_metal_init: simdgroup reduction   = true
0.01.491.176 I ggml_metal_init: simdgroup matrix mul. = true
0.01.491.176 I ggml_metal_init: has residency sets    = true
0.01.491.176 I ggml_metal_init: has bfloat            = true
0.01.491.176 I ggml_metal_init: use bfloat            = true
0.01.491.177 I ggml_metal_init: hasUnifiedMemory      = true
0.01.491.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.502.203 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.503.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.503.940 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.503.958 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.505.610 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.505.611 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.505.612 I llama_init_from_model: graph nodes  = 967
0.01.505.612 I llama_init_from_model: graph splits = 2
0.01.505.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.505.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.539.901 I 
0.01.539.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.539.946 I perplexity: tokenizing the input ..
0.01.545.000 I perplexity: tokenization took 5.052 ms
0.01.545.004 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.675.330 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.676.742 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.676.778 I llama_perf_context_print:        load time =    1516.38 ms
0.01.676.779 I llama_perf_context_print: prompt eval time =     129.99 ms /   128 tokens (    1.02 ms per token,   984.68 tokens per second)
0.01.676.780 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.676.781 I llama_perf_context_print:       total time =     136.88 ms /   129 tokens
0.01.677.185 I ggml_metal_free: deallocating

real	0m1.889s
user	0m0.098s
sys	0m0.254s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.452 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.909 I llama_model_loader: - type  f32:  194 tensors
0.00.041.909 I llama_model_loader: - type q8_0:   98 tensors
0.00.041.911 I print_info: file format = GGUF V3 (latest)
0.00.041.912 I print_info: file type   = Q8_0
0.00.041.913 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.753 I load: special tokens cache size = 25
0.00.059.366 I load: token to piece cache size = 0.2984 MB
0.00.059.384 I print_info: arch             = gptneox
0.00.059.385 I print_info: vocab_only       = 0
0.00.059.386 I print_info: n_ctx_train      = 2048
0.00.059.386 I print_info: n_embd           = 2048
0.00.059.386 I print_info: n_layer          = 24
0.00.059.392 I print_info: n_head           = 16
0.00.059.393 I print_info: n_head_kv        = 16
0.00.059.393 I print_info: n_rot            = 32
0.00.059.393 I print_info: n_swa            = 0
0.00.059.397 I print_info: n_embd_head_k    = 128
0.00.059.397 I print_info: n_embd_head_v    = 128
0.00.059.398 I print_info: n_gqa            = 1
0.00.059.399 I print_info: n_embd_k_gqa     = 2048
0.00.059.400 I print_info: n_embd_v_gqa     = 2048
0.00.059.400 I print_info: f_norm_eps       = 1.0e-05
0.00.059.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.401 I print_info: f_logit_scale    = 0.0e+00
0.00.059.402 I print_info: n_ff             = 8192
0.00.059.402 I print_info: n_expert         = 0
0.00.059.403 I print_info: n_expert_used    = 0
0.00.059.403 I print_info: causal attn      = 1
0.00.059.403 I print_info: pooling type     = 0
0.00.059.403 I print_info: rope type        = 2
0.00.059.403 I print_info: rope scaling     = linear
0.00.059.404 I print_info: freq_base_train  = 10000.0
0.00.059.404 I print_info: freq_scale_train = 1
0.00.059.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.405 I print_info: rope_finetuned   = unknown
0.00.059.405 I print_info: ssm_d_conv       = 0
0.00.059.405 I print_info: ssm_d_inner      = 0
0.00.059.407 I print_info: ssm_d_state      = 0
0.00.059.407 I print_info: ssm_dt_rank      = 0
0.00.059.407 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.407 I print_info: model type       = 1.4B
0.00.059.408 I print_info: model params     = 1.41 B
0.00.059.408 I print_info: general.name     = 1.4B
0.00.059.410 I print_info: vocab type       = BPE
0.00.059.410 I print_info: n_vocab          = 50304
0.00.059.411 I print_info: n_merges         = 50009
0.00.059.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.412 I print_info: LF token         = 187 ''
0.00.059.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.419 I print_info: max token length = 1024
0.00.059.420 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.172.140 I load_tensors: offloading 24 repeating layers to GPU
0.01.172.143 I load_tensors: offloading output layer to GPU
0.01.172.144 I load_tensors: offloaded 25/25 layers to GPU
0.01.172.161 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.172.164 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.173.346 I llama_init_from_model: n_seq_max     = 1
0.01.173.348 I llama_init_from_model: n_ctx         = 2048
0.01.173.348 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.173.349 I llama_init_from_model: n_batch       = 2048
0.01.173.349 I llama_init_from_model: n_ubatch      = 512
0.01.173.350 I llama_init_from_model: flash_attn    = 0
0.01.173.350 I llama_init_from_model: freq_base     = 10000.0
0.01.173.351 I llama_init_from_model: freq_scale    = 1
0.01.173.352 I ggml_metal_init: allocating
0.01.173.408 I ggml_metal_init: found device: Apple M4
0.01.173.421 I ggml_metal_init: picking default device: Apple M4
0.01.174.623 I ggml_metal_init: using embedded metal library
0.01.180.428 I ggml_metal_init: GPU name:   Apple M4
0.01.180.431 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.180.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.180.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.180.433 I ggml_metal_init: simdgroup reduction   = true
0.01.180.434 I ggml_metal_init: simdgroup matrix mul. = true
0.01.180.434 I ggml_metal_init: has residency sets    = true
0.01.180.434 I ggml_metal_init: has bfloat            = true
0.01.180.434 I ggml_metal_init: use bfloat            = true
0.01.180.435 I ggml_metal_init: hasUnifiedMemory      = true
0.01.180.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.196.386 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.249.759 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.249.765 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.249.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.254.937 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.254.940 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.254.941 I llama_init_from_model: graph nodes  = 967
0.01.254.941 I llama_init_from_model: graph splits = 2
0.01.254.947 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.255.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.255.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.310.843 I main: llama threadpool init, n_threads = 4
0.01.310.884 I 
0.01.310.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.310.907 I 
0.01.311.088 I sampler seed: 1234
0.01.311.094 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.311.132 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.311.133 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.311.133 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.392.670 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.02.392.672 I llama_perf_context_print:        load time =    1299.66 ms
0.02.392.672 I llama_perf_context_print: prompt eval time =      39.50 ms /     7 tokens (    5.64 ms per token,   177.22 tokens per second)
0.02.392.673 I llama_perf_context_print:        eval time =    1039.07 ms /    63 runs   (   16.49 ms per token,    60.63 tokens per second)
0.02.392.675 I llama_perf_context_print:       total time =    1082.55 ms /    70 tokens
0.02.392.879 I ggml_metal_free: deallocating

real	0m2.413s
user	0m0.113s
sys	0m0.284s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.332 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.777 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.406 I llama_model_loader: - type  f32:  194 tensors
0.00.025.406 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.407 I print_info: file format = GGUF V3 (latest)
0.00.025.408 I print_info: file type   = Q8_0
0.00.025.411 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.747 I load: special tokens cache size = 25
0.00.040.247 I load: token to piece cache size = 0.2984 MB
0.00.040.265 I print_info: arch             = gptneox
0.00.040.266 I print_info: vocab_only       = 0
0.00.040.266 I print_info: n_ctx_train      = 2048
0.00.040.266 I print_info: n_embd           = 2048
0.00.040.267 I print_info: n_layer          = 24
0.00.040.270 I print_info: n_head           = 16
0.00.040.271 I print_info: n_head_kv        = 16
0.00.040.271 I print_info: n_rot            = 32
0.00.040.271 I print_info: n_swa            = 0
0.00.040.271 I print_info: n_embd_head_k    = 128
0.00.040.272 I print_info: n_embd_head_v    = 128
0.00.040.272 I print_info: n_gqa            = 1
0.00.040.273 I print_info: n_embd_k_gqa     = 2048
0.00.040.273 I print_info: n_embd_v_gqa     = 2048
0.00.040.274 I print_info: f_norm_eps       = 1.0e-05
0.00.040.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.275 I print_info: f_logit_scale    = 0.0e+00
0.00.040.276 I print_info: n_ff             = 8192
0.00.040.276 I print_info: n_expert         = 0
0.00.040.276 I print_info: n_expert_used    = 0
0.00.040.276 I print_info: causal attn      = 1
0.00.040.276 I print_info: pooling type     = 0
0.00.040.279 I print_info: rope type        = 2
0.00.040.279 I print_info: rope scaling     = linear
0.00.040.279 I print_info: freq_base_train  = 10000.0
0.00.040.279 I print_info: freq_scale_train = 1
0.00.040.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.280 I print_info: rope_finetuned   = unknown
0.00.040.280 I print_info: ssm_d_conv       = 0
0.00.040.280 I print_info: ssm_d_inner      = 0
0.00.040.280 I print_info: ssm_d_state      = 0
0.00.040.280 I print_info: ssm_dt_rank      = 0
0.00.040.282 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.282 I print_info: model type       = 1.4B
0.00.040.282 I print_info: model params     = 1.41 B
0.00.040.282 I print_info: general.name     = 1.4B
0.00.040.282 I print_info: vocab type       = BPE
0.00.040.283 I print_info: n_vocab          = 50304
0.00.040.283 I print_info: n_merges         = 50009
0.00.040.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.283 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.284 I print_info: LF token         = 187 ''
0.00.040.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.284 I print_info: max token length = 1024
0.00.040.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.876.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.876.042 I load_tensors: offloading output layer to GPU
0.00.876.043 I load_tensors: offloaded 25/25 layers to GPU
0.00.876.070 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.876.073 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.877.224 I llama_init_from_model: n_seq_max     = 1
0.00.877.226 I llama_init_from_model: n_ctx         = 128
0.00.877.226 I llama_init_from_model: n_ctx_per_seq = 128
0.00.877.227 I llama_init_from_model: n_batch       = 128
0.00.877.227 I llama_init_from_model: n_ubatch      = 128
0.00.877.227 I llama_init_from_model: flash_attn    = 0
0.00.877.228 I llama_init_from_model: freq_base     = 10000.0
0.00.877.228 I llama_init_from_model: freq_scale    = 1
0.00.877.229 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.877.230 I ggml_metal_init: allocating
0.00.877.336 I ggml_metal_init: found device: Apple M4
0.00.877.347 I ggml_metal_init: picking default device: Apple M4
0.00.878.545 I ggml_metal_init: using embedded metal library
0.00.883.987 I ggml_metal_init: GPU name:   Apple M4
0.00.883.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.883.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.883.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.883.993 I ggml_metal_init: simdgroup reduction   = true
0.00.883.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.883.994 I ggml_metal_init: has residency sets    = true
0.00.883.994 I ggml_metal_init: has bfloat            = true
0.00.883.994 I ggml_metal_init: use bfloat            = true
0.00.883.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.883.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.899.305 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.902.586 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.902.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.902.627 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.905.730 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.905.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.905.732 I llama_init_from_model: graph nodes  = 967
0.00.905.733 I llama_init_from_model: graph splits = 2
0.00.905.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.905.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.932.510 I 
0.00.932.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.932.609 I perplexity: tokenizing the input ..
0.00.939.744 I perplexity: tokenization took 7.131 ms
0.00.939.755 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.075.967 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.077.312 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.077.331 I llama_perf_context_print:        load time =     923.17 ms
0.01.077.332 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.72 tokens per second)
0.01.077.333 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.077.335 I llama_perf_context_print:       total time =     144.83 ms /   129 tokens
0.01.077.686 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.077s
sys	0m0.179s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.121 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.161 I main: llama backend init
0.00.000.163 I main: load the model and apply lora adapter, if any
0.00.014.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.131 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.131 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.132 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.132 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.133 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.135 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.009 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.010 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.010 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.011 I llama_model_loader: - type  f32:  194 tensors
0.00.037.011 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.012 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.012 I print_info: file format = GGUF V3 (latest)
0.00.037.013 I print_info: file type   = Q4_0
0.00.037.014 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.725 I load: special tokens cache size = 25
0.00.063.418 I load: token to piece cache size = 0.2984 MB
0.00.063.429 I print_info: arch             = gptneox
0.00.063.431 I print_info: vocab_only       = 0
0.00.063.431 I print_info: n_ctx_train      = 2048
0.00.063.431 I print_info: n_embd           = 2048
0.00.063.432 I print_info: n_layer          = 24
0.00.063.435 I print_info: n_head           = 16
0.00.063.438 I print_info: n_head_kv        = 16
0.00.063.439 I print_info: n_rot            = 32
0.00.063.439 I print_info: n_swa            = 0
0.00.063.439 I print_info: n_embd_head_k    = 128
0.00.063.439 I print_info: n_embd_head_v    = 128
0.00.063.441 I print_info: n_gqa            = 1
0.00.063.442 I print_info: n_embd_k_gqa     = 2048
0.00.063.443 I print_info: n_embd_v_gqa     = 2048
0.00.063.444 I print_info: f_norm_eps       = 1.0e-05
0.00.063.444 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.444 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.447 I print_info: f_logit_scale    = 0.0e+00
0.00.063.448 I print_info: n_ff             = 8192
0.00.063.449 I print_info: n_expert         = 0
0.00.063.449 I print_info: n_expert_used    = 0
0.00.063.449 I print_info: causal attn      = 1
0.00.063.449 I print_info: pooling type     = 0
0.00.063.449 I print_info: rope type        = 2
0.00.063.450 I print_info: rope scaling     = linear
0.00.063.451 I print_info: freq_base_train  = 10000.0
0.00.063.451 I print_info: freq_scale_train = 1
0.00.063.451 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.452 I print_info: rope_finetuned   = unknown
0.00.063.452 I print_info: ssm_d_conv       = 0
0.00.063.452 I print_info: ssm_d_inner      = 0
0.00.063.453 I print_info: ssm_d_state      = 0
0.00.063.453 I print_info: ssm_dt_rank      = 0
0.00.063.453 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.453 I print_info: model type       = 1.4B
0.00.063.454 I print_info: model params     = 1.41 B
0.00.063.462 I print_info: general.name     = 1.4B
0.00.063.464 I print_info: vocab type       = BPE
0.00.063.465 I print_info: n_vocab          = 50304
0.00.063.465 I print_info: n_merges         = 50009
0.00.063.466 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.466 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.466 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.466 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.467 I print_info: LF token         = 187 ''
0.00.063.467 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.468 I print_info: max token length = 1024
0.00.063.468 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.900 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.914 I load_tensors: offloading output layer to GPU
0.00.696.915 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.953 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.696.954 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.698.434 I llama_init_from_model: n_seq_max     = 1
0.00.698.437 I llama_init_from_model: n_ctx         = 2048
0.00.698.437 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.698.438 I llama_init_from_model: n_batch       = 2048
0.00.698.439 I llama_init_from_model: n_ubatch      = 512
0.00.698.439 I llama_init_from_model: flash_attn    = 0
0.00.698.441 I llama_init_from_model: freq_base     = 10000.0
0.00.698.441 I llama_init_from_model: freq_scale    = 1
0.00.698.447 I ggml_metal_init: allocating
0.00.698.570 I ggml_metal_init: found device: Apple M4
0.00.698.585 I ggml_metal_init: picking default device: Apple M4
0.00.700.261 I ggml_metal_init: using embedded metal library
0.00.706.411 I ggml_metal_init: GPU name:   Apple M4
0.00.706.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.706.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.706.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.706.419 I ggml_metal_init: simdgroup reduction   = true
0.00.706.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.706.420 I ggml_metal_init: has residency sets    = true
0.00.706.420 I ggml_metal_init: has bfloat            = true
0.00.706.420 I ggml_metal_init: use bfloat            = true
0.00.706.421 I ggml_metal_init: hasUnifiedMemory      = true
0.00.706.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.726.264 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.485 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.785.492 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.785.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.790.000 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.790.003 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.790.003 I llama_init_from_model: graph nodes  = 967
0.00.790.003 I llama_init_from_model: graph splits = 2
0.00.790.010 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.790.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.790.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.002 I main: llama threadpool init, n_threads = 4
0.00.847.051 I 
0.00.847.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.072 I 
0.00.847.232 I sampler seed: 1234
0.00.847.237 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.847.252 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.847.252 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.847.254 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.531.564 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.531.565 I llama_perf_context_print:        load time =     831.46 ms
0.01.531.566 I llama_perf_context_print: prompt eval time =      49.05 ms /     7 tokens (    7.01 ms per token,   142.71 tokens per second)
0.01.531.566 I llama_perf_context_print:        eval time =     632.37 ms /    63 runs   (   10.04 ms per token,    99.63 tokens per second)
0.01.531.567 I llama_perf_context_print:       total time =     685.30 ms /    70 tokens
0.01.531.857 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.129s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.011 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.345 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.988 I llama_model_loader: - type  f32:  194 tensors
0.00.025.988 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.989 I print_info: file format = GGUF V3 (latest)
0.00.025.990 I print_info: file type   = Q4_0
0.00.025.991 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.971 I load: special tokens cache size = 25
0.00.040.373 I load: token to piece cache size = 0.2984 MB
0.00.040.389 I print_info: arch             = gptneox
0.00.040.390 I print_info: vocab_only       = 0
0.00.040.390 I print_info: n_ctx_train      = 2048
0.00.040.390 I print_info: n_embd           = 2048
0.00.040.391 I print_info: n_layer          = 24
0.00.040.394 I print_info: n_head           = 16
0.00.040.395 I print_info: n_head_kv        = 16
0.00.040.395 I print_info: n_rot            = 32
0.00.040.398 I print_info: n_swa            = 0
0.00.040.398 I print_info: n_embd_head_k    = 128
0.00.040.398 I print_info: n_embd_head_v    = 128
0.00.040.399 I print_info: n_gqa            = 1
0.00.040.399 I print_info: n_embd_k_gqa     = 2048
0.00.040.400 I print_info: n_embd_v_gqa     = 2048
0.00.040.401 I print_info: f_norm_eps       = 1.0e-05
0.00.040.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.401 I print_info: f_logit_scale    = 0.0e+00
0.00.040.402 I print_info: n_ff             = 8192
0.00.040.402 I print_info: n_expert         = 0
0.00.040.402 I print_info: n_expert_used    = 0
0.00.040.403 I print_info: causal attn      = 1
0.00.040.403 I print_info: pooling type     = 0
0.00.040.403 I print_info: rope type        = 2
0.00.040.403 I print_info: rope scaling     = linear
0.00.040.403 I print_info: freq_base_train  = 10000.0
0.00.040.404 I print_info: freq_scale_train = 1
0.00.040.404 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.404 I print_info: rope_finetuned   = unknown
0.00.040.404 I print_info: ssm_d_conv       = 0
0.00.040.404 I print_info: ssm_d_inner      = 0
0.00.040.404 I print_info: ssm_d_state      = 0
0.00.040.405 I print_info: ssm_dt_rank      = 0
0.00.040.405 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.405 I print_info: model type       = 1.4B
0.00.040.407 I print_info: model params     = 1.41 B
0.00.040.407 I print_info: general.name     = 1.4B
0.00.040.407 I print_info: vocab type       = BPE
0.00.040.407 I print_info: n_vocab          = 50304
0.00.040.407 I print_info: n_merges         = 50009
0.00.040.408 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.408 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.408 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.408 I print_info: LF token         = 187 ''
0.00.040.409 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.409 I print_info: max token length = 1024
0.00.040.409 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.173 I load_tensors: offloading output layer to GPU
0.00.585.173 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.210 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.585.211 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.979 I llama_init_from_model: n_seq_max     = 1
0.00.586.982 I llama_init_from_model: n_ctx         = 128
0.00.586.983 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.984 I llama_init_from_model: n_batch       = 128
0.00.586.984 I llama_init_from_model: n_ubatch      = 128
0.00.586.984 I llama_init_from_model: flash_attn    = 0
0.00.586.986 I llama_init_from_model: freq_base     = 10000.0
0.00.586.987 I llama_init_from_model: freq_scale    = 1
0.00.586.988 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.991 I ggml_metal_init: allocating
0.00.587.094 I ggml_metal_init: found device: Apple M4
0.00.587.108 I ggml_metal_init: picking default device: Apple M4
0.00.588.729 I ggml_metal_init: using embedded metal library
0.00.595.104 I ggml_metal_init: GPU name:   Apple M4
0.00.595.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.114 I ggml_metal_init: simdgroup reduction   = true
0.00.595.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.115 I ggml_metal_init: has residency sets    = true
0.00.595.115 I ggml_metal_init: has bfloat            = true
0.00.595.115 I ggml_metal_init: use bfloat            = true
0.00.595.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.200 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.758 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.788 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.991 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.992 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.993 I llama_init_from_model: graph nodes  = 967
0.00.620.993 I llama_init_from_model: graph splits = 2
0.00.620.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.996 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.428 I 
0.00.648.523 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.531 I perplexity: tokenizing the input ..
0.00.655.652 I perplexity: tokenization took 7.118 ms
0.00.655.659 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.011 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.792.328 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.792.350 I llama_perf_context_print:        load time =     638.41 ms
0.00.792.353 I llama_perf_context_print: prompt eval time =     134.46 ms /   128 tokens (    1.05 ms per token,   951.95 tokens per second)
0.00.792.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.354 I llama_perf_context_print:       total time =     143.93 ms /   129 tokens
0.00.792.794 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.080s
sys	0m0.122s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.665 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.154 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.171 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.171 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.418 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.253 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.254 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.254 I llama_model_loader: - type  f32:  194 tensors
0.00.036.255 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.255 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.255 I print_info: file format = GGUF V3 (latest)
0.00.036.256 I print_info: file type   = Q4_1
0.00.036.257 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.507 I load: special tokens cache size = 25
0.00.055.306 I load: token to piece cache size = 0.2984 MB
0.00.055.321 I print_info: arch             = gptneox
0.00.055.323 I print_info: vocab_only       = 0
0.00.055.323 I print_info: n_ctx_train      = 2048
0.00.055.323 I print_info: n_embd           = 2048
0.00.055.323 I print_info: n_layer          = 24
0.00.055.326 I print_info: n_head           = 16
0.00.055.327 I print_info: n_head_kv        = 16
0.00.055.327 I print_info: n_rot            = 32
0.00.055.328 I print_info: n_swa            = 0
0.00.055.328 I print_info: n_embd_head_k    = 128
0.00.055.328 I print_info: n_embd_head_v    = 128
0.00.055.329 I print_info: n_gqa            = 1
0.00.055.329 I print_info: n_embd_k_gqa     = 2048
0.00.055.330 I print_info: n_embd_v_gqa     = 2048
0.00.055.331 I print_info: f_norm_eps       = 1.0e-05
0.00.055.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.333 I print_info: f_logit_scale    = 0.0e+00
0.00.055.334 I print_info: n_ff             = 8192
0.00.055.334 I print_info: n_expert         = 0
0.00.055.334 I print_info: n_expert_used    = 0
0.00.055.334 I print_info: causal attn      = 1
0.00.055.334 I print_info: pooling type     = 0
0.00.055.334 I print_info: rope type        = 2
0.00.055.337 I print_info: rope scaling     = linear
0.00.055.337 I print_info: freq_base_train  = 10000.0
0.00.055.337 I print_info: freq_scale_train = 1
0.00.055.338 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.338 I print_info: rope_finetuned   = unknown
0.00.055.338 I print_info: ssm_d_conv       = 0
0.00.055.338 I print_info: ssm_d_inner      = 0
0.00.055.338 I print_info: ssm_d_state      = 0
0.00.055.338 I print_info: ssm_dt_rank      = 0
0.00.055.339 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.339 I print_info: model type       = 1.4B
0.00.055.339 I print_info: model params     = 1.41 B
0.00.055.339 I print_info: general.name     = 1.4B
0.00.055.344 I print_info: vocab type       = BPE
0.00.055.344 I print_info: n_vocab          = 50304
0.00.055.344 I print_info: n_merges         = 50009
0.00.055.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.346 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.346 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.346 I print_info: LF token         = 187 ''
0.00.055.347 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.347 I print_info: max token length = 1024
0.00.055.347 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.673.501 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.519 I load_tensors: offloading output layer to GPU
0.00.673.520 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.551 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.673.552 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.675.044 I llama_init_from_model: n_seq_max     = 1
0.00.675.047 I llama_init_from_model: n_ctx         = 2048
0.00.675.047 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.048 I llama_init_from_model: n_batch       = 2048
0.00.675.049 I llama_init_from_model: n_ubatch      = 512
0.00.675.049 I llama_init_from_model: flash_attn    = 0
0.00.675.051 I llama_init_from_model: freq_base     = 10000.0
0.00.675.052 I llama_init_from_model: freq_scale    = 1
0.00.675.055 I ggml_metal_init: allocating
0.00.675.116 I ggml_metal_init: found device: Apple M4
0.00.675.128 I ggml_metal_init: picking default device: Apple M4
0.00.676.688 I ggml_metal_init: using embedded metal library
0.00.682.217 I ggml_metal_init: GPU name:   Apple M4
0.00.682.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.229 I ggml_metal_init: simdgroup reduction   = true
0.00.682.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.230 I ggml_metal_init: has residency sets    = true
0.00.682.230 I ggml_metal_init: has bfloat            = true
0.00.682.230 I ggml_metal_init: use bfloat            = true
0.00.682.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.878 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.082 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.763.089 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.763.113 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.289 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.767.291 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.767.292 I llama_init_from_model: graph nodes  = 967
0.00.767.292 I llama_init_from_model: graph splits = 2
0.00.767.298 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.767.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.767.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.962 I main: llama threadpool init, n_threads = 4
0.00.822.016 I 
0.00.822.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.037 I 
0.00.822.220 I sampler seed: 1234
0.00.822.224 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.246 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.247 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.247 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.549.779 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.01.549.780 I llama_perf_context_print:        load time =     812.59 ms
0.01.549.780 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.34 tokens per second)
0.01.549.781 I llama_perf_context_print:        eval time =     676.02 ms /    63 runs   (   10.73 ms per token,    93.19 tokens per second)
0.01.549.781 I llama_perf_context_print:       total time =     728.53 ms /    70 tokens
0.01.550.022 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.117s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.332 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.338 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.339 I print_info: file format = GGUF V3 (latest)
0.00.024.340 I print_info: file type   = Q4_1
0.00.024.341 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.700 I load: special tokens cache size = 25
0.00.039.251 I load: token to piece cache size = 0.2984 MB
0.00.039.268 I print_info: arch             = gptneox
0.00.039.269 I print_info: vocab_only       = 0
0.00.039.269 I print_info: n_ctx_train      = 2048
0.00.039.270 I print_info: n_embd           = 2048
0.00.039.270 I print_info: n_layer          = 24
0.00.039.274 I print_info: n_head           = 16
0.00.039.275 I print_info: n_head_kv        = 16
0.00.039.275 I print_info: n_rot            = 32
0.00.039.275 I print_info: n_swa            = 0
0.00.039.275 I print_info: n_embd_head_k    = 128
0.00.039.275 I print_info: n_embd_head_v    = 128
0.00.039.276 I print_info: n_gqa            = 1
0.00.039.276 I print_info: n_embd_k_gqa     = 2048
0.00.039.277 I print_info: n_embd_v_gqa     = 2048
0.00.039.278 I print_info: f_norm_eps       = 1.0e-05
0.00.039.278 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.278 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.278 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.278 I print_info: f_logit_scale    = 0.0e+00
0.00.039.279 I print_info: n_ff             = 8192
0.00.039.279 I print_info: n_expert         = 0
0.00.039.279 I print_info: n_expert_used    = 0
0.00.039.279 I print_info: causal attn      = 1
0.00.039.282 I print_info: pooling type     = 0
0.00.039.283 I print_info: rope type        = 2
0.00.039.283 I print_info: rope scaling     = linear
0.00.039.283 I print_info: freq_base_train  = 10000.0
0.00.039.283 I print_info: freq_scale_train = 1
0.00.039.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.284 I print_info: rope_finetuned   = unknown
0.00.039.284 I print_info: ssm_d_conv       = 0
0.00.039.284 I print_info: ssm_d_inner      = 0
0.00.039.284 I print_info: ssm_d_state      = 0
0.00.039.286 I print_info: ssm_dt_rank      = 0
0.00.039.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.286 I print_info: model type       = 1.4B
0.00.039.286 I print_info: model params     = 1.41 B
0.00.039.286 I print_info: general.name     = 1.4B
0.00.039.287 I print_info: vocab type       = BPE
0.00.039.287 I print_info: n_vocab          = 50304
0.00.039.287 I print_info: n_merges         = 50009
0.00.039.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: LF token         = 187 ''
0.00.039.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: max token length = 1024
0.00.039.289 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.514 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.528 I load_tensors: offloading output layer to GPU
0.00.626.529 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.567 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.626.569 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.628.269 I llama_init_from_model: n_seq_max     = 1
0.00.628.272 I llama_init_from_model: n_ctx         = 128
0.00.628.272 I llama_init_from_model: n_ctx_per_seq = 128
0.00.628.273 I llama_init_from_model: n_batch       = 128
0.00.628.273 I llama_init_from_model: n_ubatch      = 128
0.00.628.274 I llama_init_from_model: flash_attn    = 0
0.00.628.275 I llama_init_from_model: freq_base     = 10000.0
0.00.628.275 I llama_init_from_model: freq_scale    = 1
0.00.628.276 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.628.282 I ggml_metal_init: allocating
0.00.628.379 I ggml_metal_init: found device: Apple M4
0.00.628.394 I ggml_metal_init: picking default device: Apple M4
0.00.630.351 I ggml_metal_init: using embedded metal library
0.00.637.590 I ggml_metal_init: GPU name:   Apple M4
0.00.637.599 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.601 I ggml_metal_init: simdgroup reduction   = true
0.00.637.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.602 I ggml_metal_init: has residency sets    = true
0.00.637.602 I ggml_metal_init: has bfloat            = true
0.00.637.602 I ggml_metal_init: use bfloat            = true
0.00.637.603 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.150 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.777 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.806 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.039 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.041 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.041 I llama_init_from_model: graph nodes  = 967
0.00.663.042 I llama_init_from_model: graph splits = 2
0.00.663.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.180 I 
0.00.692.265 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.272 I perplexity: tokenizing the input ..
0.00.699.349 I perplexity: tokenization took 7.075 ms
0.00.699.356 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.813 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.837.326 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.837.354 I llama_perf_context_print:        load time =     683.37 ms
0.00.837.354 I llama_perf_context_print: prompt eval time =     135.53 ms /   128 tokens (    1.06 ms per token,   944.43 tokens per second)
0.00.837.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.356 I llama_perf_context_print:       total time =     145.18 ms /   129 tokens
0.00.837.751 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.080s
sys	0m0.122s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.626 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.633 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.635 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.636 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.160 I llama_model_loader: - type  f32:  194 tensors
0.00.034.160 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.160 I print_info: file format = GGUF V3 (latest)
0.00.034.161 I print_info: file type   = Q5_0
0.00.034.162 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.042.531 I load: special tokens cache size = 25
0.00.049.569 I load: token to piece cache size = 0.2984 MB
0.00.049.583 I print_info: arch             = gptneox
0.00.049.584 I print_info: vocab_only       = 0
0.00.049.584 I print_info: n_ctx_train      = 2048
0.00.049.584 I print_info: n_embd           = 2048
0.00.049.584 I print_info: n_layer          = 24
0.00.049.587 I print_info: n_head           = 16
0.00.049.588 I print_info: n_head_kv        = 16
0.00.049.588 I print_info: n_rot            = 32
0.00.049.588 I print_info: n_swa            = 0
0.00.049.588 I print_info: n_embd_head_k    = 128
0.00.049.588 I print_info: n_embd_head_v    = 128
0.00.049.590 I print_info: n_gqa            = 1
0.00.049.590 I print_info: n_embd_k_gqa     = 2048
0.00.049.591 I print_info: n_embd_v_gqa     = 2048
0.00.049.592 I print_info: f_norm_eps       = 1.0e-05
0.00.049.592 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.594 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.594 I print_info: f_logit_scale    = 0.0e+00
0.00.049.595 I print_info: n_ff             = 8192
0.00.049.595 I print_info: n_expert         = 0
0.00.049.595 I print_info: n_expert_used    = 0
0.00.049.595 I print_info: causal attn      = 1
0.00.049.595 I print_info: pooling type     = 0
0.00.049.595 I print_info: rope type        = 2
0.00.049.595 I print_info: rope scaling     = linear
0.00.049.596 I print_info: freq_base_train  = 10000.0
0.00.049.596 I print_info: freq_scale_train = 1
0.00.049.596 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.600 I print_info: rope_finetuned   = unknown
0.00.049.600 I print_info: ssm_d_conv       = 0
0.00.049.600 I print_info: ssm_d_inner      = 0
0.00.049.600 I print_info: ssm_d_state      = 0
0.00.049.601 I print_info: ssm_dt_rank      = 0
0.00.049.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.601 I print_info: model type       = 1.4B
0.00.049.602 I print_info: model params     = 1.41 B
0.00.049.602 I print_info: general.name     = 1.4B
0.00.049.602 I print_info: vocab type       = BPE
0.00.049.603 I print_info: n_vocab          = 50304
0.00.049.603 I print_info: n_merges         = 50009
0.00.049.604 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.604 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.605 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.605 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.605 I print_info: LF token         = 187 ''
0.00.049.605 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.605 I print_info: max token length = 1024
0.00.049.606 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.775.294 I load_tensors: offloading 24 repeating layers to GPU
0.00.775.311 I load_tensors: offloading output layer to GPU
0.00.775.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.775.345 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.775.346 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.777.022 I llama_init_from_model: n_seq_max     = 1
0.00.777.025 I llama_init_from_model: n_ctx         = 2048
0.00.777.026 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.777.026 I llama_init_from_model: n_batch       = 2048
0.00.777.026 I llama_init_from_model: n_ubatch      = 512
0.00.777.027 I llama_init_from_model: flash_attn    = 0
0.00.777.029 I llama_init_from_model: freq_base     = 10000.0
0.00.777.029 I llama_init_from_model: freq_scale    = 1
0.00.777.032 I ggml_metal_init: allocating
0.00.777.108 I ggml_metal_init: found device: Apple M4
0.00.777.122 I ggml_metal_init: picking default device: Apple M4
0.00.778.832 I ggml_metal_init: using embedded metal library
0.00.785.376 I ggml_metal_init: GPU name:   Apple M4
0.00.785.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.785.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.785.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.785.382 I ggml_metal_init: simdgroup reduction   = true
0.00.785.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.785.383 I ggml_metal_init: has residency sets    = true
0.00.785.383 I ggml_metal_init: has bfloat            = true
0.00.785.383 I ggml_metal_init: use bfloat            = true
0.00.785.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.785.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.803.535 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.860.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.860.987 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.861.023 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.865.089 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.865.091 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.865.091 I llama_init_from_model: graph nodes  = 967
0.00.865.091 I llama_init_from_model: graph splits = 2
0.00.865.097 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.865.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.865.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.923.070 I main: llama threadpool init, n_threads = 4
0.00.923.132 I 
0.00.923.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.923.155 I 
0.00.923.329 I sampler seed: 1234
0.00.923.333 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.923.349 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.923.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.923.349 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.725.529 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.725.530 I llama_perf_context_print:        load time =     913.47 ms
0.01.725.531 I llama_perf_context_print: prompt eval time =      52.93 ms /     7 tokens (    7.56 ms per token,   132.24 tokens per second)
0.01.725.531 I llama_perf_context_print:        eval time =     746.46 ms /    63 runs   (   11.85 ms per token,    84.40 tokens per second)
0.01.725.532 I llama_perf_context_print:       total time =     803.18 ms /    70 tokens
0.01.725.782 I ggml_metal_free: deallocating

real	0m1.743s
user	0m0.111s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.262 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.264 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.266 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.267 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.668 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.668 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.669 I print_info: file format = GGUF V3 (latest)
0.00.024.674 I print_info: file type   = Q5_0
0.00.024.675 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.657 I load: special tokens cache size = 25
0.00.039.010 I load: token to piece cache size = 0.2984 MB
0.00.039.027 I print_info: arch             = gptneox
0.00.039.028 I print_info: vocab_only       = 0
0.00.039.028 I print_info: n_ctx_train      = 2048
0.00.039.028 I print_info: n_embd           = 2048
0.00.039.028 I print_info: n_layer          = 24
0.00.039.032 I print_info: n_head           = 16
0.00.039.033 I print_info: n_head_kv        = 16
0.00.039.033 I print_info: n_rot            = 32
0.00.039.033 I print_info: n_swa            = 0
0.00.039.033 I print_info: n_embd_head_k    = 128
0.00.039.033 I print_info: n_embd_head_v    = 128
0.00.039.034 I print_info: n_gqa            = 1
0.00.039.034 I print_info: n_embd_k_gqa     = 2048
0.00.039.035 I print_info: n_embd_v_gqa     = 2048
0.00.039.035 I print_info: f_norm_eps       = 1.0e-05
0.00.039.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.036 I print_info: f_logit_scale    = 0.0e+00
0.00.039.037 I print_info: n_ff             = 8192
0.00.039.037 I print_info: n_expert         = 0
0.00.039.037 I print_info: n_expert_used    = 0
0.00.039.037 I print_info: causal attn      = 1
0.00.039.037 I print_info: pooling type     = 0
0.00.039.038 I print_info: rope type        = 2
0.00.039.038 I print_info: rope scaling     = linear
0.00.039.038 I print_info: freq_base_train  = 10000.0
0.00.039.038 I print_info: freq_scale_train = 1
0.00.039.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.039 I print_info: rope_finetuned   = unknown
0.00.039.039 I print_info: ssm_d_conv       = 0
0.00.039.039 I print_info: ssm_d_inner      = 0
0.00.039.039 I print_info: ssm_d_state      = 0
0.00.039.039 I print_info: ssm_dt_rank      = 0
0.00.039.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.040 I print_info: model type       = 1.4B
0.00.039.041 I print_info: model params     = 1.41 B
0.00.039.041 I print_info: general.name     = 1.4B
0.00.039.042 I print_info: vocab type       = BPE
0.00.039.042 I print_info: n_vocab          = 50304
0.00.039.042 I print_info: n_merges         = 50009
0.00.039.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: LF token         = 187 ''
0.00.039.044 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.044 I print_info: max token length = 1024
0.00.039.044 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.715.043 I load_tensors: offloading 24 repeating layers to GPU
0.00.715.062 I load_tensors: offloading output layer to GPU
0.00.715.063 I load_tensors: offloaded 25/25 layers to GPU
0.00.715.099 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.715.105 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.716.890 I llama_init_from_model: n_seq_max     = 1
0.00.716.893 I llama_init_from_model: n_ctx         = 128
0.00.716.893 I llama_init_from_model: n_ctx_per_seq = 128
0.00.716.894 I llama_init_from_model: n_batch       = 128
0.00.716.894 I llama_init_from_model: n_ubatch      = 128
0.00.716.895 I llama_init_from_model: flash_attn    = 0
0.00.716.897 I llama_init_from_model: freq_base     = 10000.0
0.00.716.897 I llama_init_from_model: freq_scale    = 1
0.00.716.898 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.716.901 I ggml_metal_init: allocating
0.00.716.980 I ggml_metal_init: found device: Apple M4
0.00.716.995 I ggml_metal_init: picking default device: Apple M4
0.00.718.584 I ggml_metal_init: using embedded metal library
0.00.725.050 I ggml_metal_init: GPU name:   Apple M4
0.00.725.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.725.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.725.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.725.057 I ggml_metal_init: simdgroup reduction   = true
0.00.725.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.725.057 I ggml_metal_init: has residency sets    = true
0.00.725.058 I ggml_metal_init: has bfloat            = true
0.00.725.058 I ggml_metal_init: use bfloat            = true
0.00.725.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.725.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.568 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.171 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.747.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.747.202 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.530 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.532 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.532 I llama_init_from_model: graph nodes  = 967
0.00.750.533 I llama_init_from_model: graph splits = 2
0.00.750.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.932 I 
0.00.779.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.038 I perplexity: tokenizing the input ..
0.00.786.282 I perplexity: tokenization took 7.239 ms
0.00.786.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.389 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.923.722 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.923.748 I llama_perf_context_print:        load time =     769.87 ms
0.00.923.749 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.19 tokens per second)
0.00.923.749 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.750 I llama_perf_context_print:       total time =     144.82 ms /   129 tokens
0.00.924.121 I ggml_metal_free: deallocating

real	0m0.938s
user	0m0.080s
sys	0m0.133s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.113 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.445 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.457 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.882 I llama_model_loader: - type  f32:  194 tensors
0.00.025.882 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.882 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.883 I print_info: file format = GGUF V3 (latest)
0.00.025.883 I print_info: file type   = Q5_1
0.00.025.884 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.952 I load: special tokens cache size = 25
0.00.040.277 I load: token to piece cache size = 0.2984 MB
0.00.040.291 I print_info: arch             = gptneox
0.00.040.292 I print_info: vocab_only       = 0
0.00.040.292 I print_info: n_ctx_train      = 2048
0.00.040.292 I print_info: n_embd           = 2048
0.00.040.293 I print_info: n_layer          = 24
0.00.040.295 I print_info: n_head           = 16
0.00.040.296 I print_info: n_head_kv        = 16
0.00.040.296 I print_info: n_rot            = 32
0.00.040.297 I print_info: n_swa            = 0
0.00.040.299 I print_info: n_embd_head_k    = 128
0.00.040.299 I print_info: n_embd_head_v    = 128
0.00.040.300 I print_info: n_gqa            = 1
0.00.040.300 I print_info: n_embd_k_gqa     = 2048
0.00.040.301 I print_info: n_embd_v_gqa     = 2048
0.00.040.302 I print_info: f_norm_eps       = 1.0e-05
0.00.040.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.303 I print_info: f_logit_scale    = 0.0e+00
0.00.040.304 I print_info: n_ff             = 8192
0.00.040.304 I print_info: n_expert         = 0
0.00.040.304 I print_info: n_expert_used    = 0
0.00.040.304 I print_info: causal attn      = 1
0.00.040.304 I print_info: pooling type     = 0
0.00.040.304 I print_info: rope type        = 2
0.00.040.305 I print_info: rope scaling     = linear
0.00.040.305 I print_info: freq_base_train  = 10000.0
0.00.040.309 I print_info: freq_scale_train = 1
0.00.040.309 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.309 I print_info: rope_finetuned   = unknown
0.00.040.309 I print_info: ssm_d_conv       = 0
0.00.040.309 I print_info: ssm_d_inner      = 0
0.00.040.309 I print_info: ssm_d_state      = 0
0.00.040.310 I print_info: ssm_dt_rank      = 0
0.00.040.310 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.310 I print_info: model type       = 1.4B
0.00.040.310 I print_info: model params     = 1.41 B
0.00.040.314 I print_info: general.name     = 1.4B
0.00.040.315 I print_info: vocab type       = BPE
0.00.040.315 I print_info: n_vocab          = 50304
0.00.040.316 I print_info: n_merges         = 50009
0.00.040.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.316 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.316 I print_info: LF token         = 187 ''
0.00.040.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.317 I print_info: max token length = 1024
0.00.040.317 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.735 I load_tensors: offloading output layer to GPU
0.00.616.735 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.762 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.765 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.618.411 I llama_init_from_model: n_seq_max     = 1
0.00.618.413 I llama_init_from_model: n_ctx         = 2048
0.00.618.414 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.618.414 I llama_init_from_model: n_batch       = 2048
0.00.618.415 I llama_init_from_model: n_ubatch      = 512
0.00.618.415 I llama_init_from_model: flash_attn    = 0
0.00.618.416 I llama_init_from_model: freq_base     = 10000.0
0.00.618.416 I llama_init_from_model: freq_scale    = 1
0.00.618.418 I ggml_metal_init: allocating
0.00.618.486 I ggml_metal_init: found device: Apple M4
0.00.618.498 I ggml_metal_init: picking default device: Apple M4
0.00.619.802 I ggml_metal_init: using embedded metal library
0.00.625.925 I ggml_metal_init: GPU name:   Apple M4
0.00.625.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.929 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.930 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.931 I ggml_metal_init: simdgroup reduction   = true
0.00.625.931 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.931 I ggml_metal_init: has residency sets    = true
0.00.625.931 I ggml_metal_init: has bfloat            = true
0.00.625.932 I ggml_metal_init: use bfloat            = true
0.00.625.932 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.934 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.446 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.725 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.696.733 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.696.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.325 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.701.328 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.701.328 I llama_init_from_model: graph nodes  = 967
0.00.701.328 I llama_init_from_model: graph splits = 2
0.00.701.333 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.701.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.714 I main: llama threadpool init, n_threads = 4
0.00.758.761 I 
0.00.758.783 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.784 I 
0.00.758.953 I sampler seed: 1234
0.00.758.958 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.002 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.003 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.598.890 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.598.890 I llama_perf_context_print:        load time =     747.86 ms
0.01.598.891 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.68 tokens per second)
0.01.598.892 I llama_perf_context_print:        eval time =     794.68 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.598.893 I llama_perf_context_print:       total time =     840.91 ms /    70 tokens
0.01.599.093 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.518 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.358 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.374 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.374 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.375 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.375 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.837 I llama_model_loader: - type  f32:  194 tensors
0.00.025.837 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.838 I print_info: file format = GGUF V3 (latest)
0.00.025.839 I print_info: file type   = Q5_1
0.00.025.840 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.875 I load: special tokens cache size = 25
0.00.040.305 I load: token to piece cache size = 0.2984 MB
0.00.040.322 I print_info: arch             = gptneox
0.00.040.322 I print_info: vocab_only       = 0
0.00.040.323 I print_info: n_ctx_train      = 2048
0.00.040.323 I print_info: n_embd           = 2048
0.00.040.323 I print_info: n_layer          = 24
0.00.040.333 I print_info: n_head           = 16
0.00.040.333 I print_info: n_head_kv        = 16
0.00.040.333 I print_info: n_rot            = 32
0.00.040.334 I print_info: n_swa            = 0
0.00.040.334 I print_info: n_embd_head_k    = 128
0.00.040.334 I print_info: n_embd_head_v    = 128
0.00.040.335 I print_info: n_gqa            = 1
0.00.040.335 I print_info: n_embd_k_gqa     = 2048
0.00.040.336 I print_info: n_embd_v_gqa     = 2048
0.00.040.336 I print_info: f_norm_eps       = 1.0e-05
0.00.040.338 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.339 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.339 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.339 I print_info: f_logit_scale    = 0.0e+00
0.00.040.339 I print_info: n_ff             = 8192
0.00.040.340 I print_info: n_expert         = 0
0.00.040.340 I print_info: n_expert_used    = 0
0.00.040.340 I print_info: causal attn      = 1
0.00.040.341 I print_info: pooling type     = 0
0.00.040.341 I print_info: rope type        = 2
0.00.040.342 I print_info: rope scaling     = linear
0.00.040.342 I print_info: freq_base_train  = 10000.0
0.00.040.342 I print_info: freq_scale_train = 1
0.00.040.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.343 I print_info: rope_finetuned   = unknown
0.00.040.343 I print_info: ssm_d_conv       = 0
0.00.040.343 I print_info: ssm_d_inner      = 0
0.00.040.343 I print_info: ssm_d_state      = 0
0.00.040.343 I print_info: ssm_dt_rank      = 0
0.00.040.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.343 I print_info: model type       = 1.4B
0.00.040.344 I print_info: model params     = 1.41 B
0.00.040.344 I print_info: general.name     = 1.4B
0.00.040.344 I print_info: vocab type       = BPE
0.00.040.344 I print_info: n_vocab          = 50304
0.00.040.345 I print_info: n_merges         = 50009
0.00.040.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: LF token         = 187 ''
0.00.040.348 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: max token length = 1024
0.00.040.349 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.585 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.604 I load_tensors: offloading output layer to GPU
0.00.610.605 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.642 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.610.644 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.612.327 I llama_init_from_model: n_seq_max     = 1
0.00.612.330 I llama_init_from_model: n_ctx         = 128
0.00.612.331 I llama_init_from_model: n_ctx_per_seq = 128
0.00.612.332 I llama_init_from_model: n_batch       = 128
0.00.612.332 I llama_init_from_model: n_ubatch      = 128
0.00.612.333 I llama_init_from_model: flash_attn    = 0
0.00.612.335 I llama_init_from_model: freq_base     = 10000.0
0.00.612.336 I llama_init_from_model: freq_scale    = 1
0.00.612.339 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.612.341 I ggml_metal_init: allocating
0.00.612.453 I ggml_metal_init: found device: Apple M4
0.00.612.467 I ggml_metal_init: picking default device: Apple M4
0.00.614.157 I ggml_metal_init: using embedded metal library
0.00.620.904 I ggml_metal_init: GPU name:   Apple M4
0.00.620.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.912 I ggml_metal_init: simdgroup reduction   = true
0.00.620.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.912 I ggml_metal_init: has residency sets    = true
0.00.620.912 I ggml_metal_init: has bfloat            = true
0.00.620.913 I ggml_metal_init: use bfloat            = true
0.00.620.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.341 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.783 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.641.787 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.813 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.104 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.106 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.106 I llama_init_from_model: graph nodes  = 967
0.00.645.107 I llama_init_from_model: graph splits = 2
0.00.645.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.537 I 
0.00.672.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.634 I perplexity: tokenizing the input ..
0.00.680.176 I perplexity: tokenization took 7.539 ms
0.00.680.192 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.864 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.817.202 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.817.228 I llama_perf_context_print:        load time =     662.01 ms
0.00.817.229 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.72 tokens per second)
0.00.817.230 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.230 I llama_perf_context_print:       total time =     144.70 ms /   129 tokens
0.00.817.581 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.080s
sys	0m0.141s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.554 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.027 I llama_model_loader: - type  f32:  194 tensors
0.00.025.027 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.027 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.028 I print_info: file format = GGUF V3 (latest)
0.00.025.029 I print_info: file type   = Q2_K - Medium
0.00.025.029 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.785 I load: special tokens cache size = 25
0.00.039.122 I load: token to piece cache size = 0.2984 MB
0.00.039.136 I print_info: arch             = gptneox
0.00.039.137 I print_info: vocab_only       = 0
0.00.039.137 I print_info: n_ctx_train      = 2048
0.00.039.137 I print_info: n_embd           = 2048
0.00.039.137 I print_info: n_layer          = 24
0.00.039.140 I print_info: n_head           = 16
0.00.039.141 I print_info: n_head_kv        = 16
0.00.039.141 I print_info: n_rot            = 32
0.00.039.141 I print_info: n_swa            = 0
0.00.039.141 I print_info: n_embd_head_k    = 128
0.00.039.141 I print_info: n_embd_head_v    = 128
0.00.039.142 I print_info: n_gqa            = 1
0.00.039.143 I print_info: n_embd_k_gqa     = 2048
0.00.039.144 I print_info: n_embd_v_gqa     = 2048
0.00.039.144 I print_info: f_norm_eps       = 1.0e-05
0.00.039.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.145 I print_info: f_logit_scale    = 0.0e+00
0.00.039.146 I print_info: n_ff             = 8192
0.00.039.146 I print_info: n_expert         = 0
0.00.039.146 I print_info: n_expert_used    = 0
0.00.039.146 I print_info: causal attn      = 1
0.00.039.146 I print_info: pooling type     = 0
0.00.039.147 I print_info: rope type        = 2
0.00.039.148 I print_info: rope scaling     = linear
0.00.039.148 I print_info: freq_base_train  = 10000.0
0.00.039.149 I print_info: freq_scale_train = 1
0.00.039.149 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.149 I print_info: rope_finetuned   = unknown
0.00.039.149 I print_info: ssm_d_conv       = 0
0.00.039.149 I print_info: ssm_d_inner      = 0
0.00.039.149 I print_info: ssm_d_state      = 0
0.00.039.150 I print_info: ssm_dt_rank      = 0
0.00.039.150 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.150 I print_info: model type       = 1.4B
0.00.039.150 I print_info: model params     = 1.41 B
0.00.039.150 I print_info: general.name     = 1.4B
0.00.039.151 I print_info: vocab type       = BPE
0.00.039.151 I print_info: n_vocab          = 50304
0.00.039.151 I print_info: n_merges         = 50009
0.00.039.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: LF token         = 187 ''
0.00.039.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.153 I print_info: max token length = 1024
0.00.039.153 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.335.392 I load_tensors: offloading 24 repeating layers to GPU
0.00.335.407 I load_tensors: offloading output layer to GPU
0.00.335.407 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.447 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.448 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.174 I llama_init_from_model: n_seq_max     = 1
0.00.337.178 I llama_init_from_model: n_ctx         = 2048
0.00.337.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.179 I llama_init_from_model: n_batch       = 2048
0.00.337.179 I llama_init_from_model: n_ubatch      = 512
0.00.337.180 I llama_init_from_model: flash_attn    = 0
0.00.337.182 I llama_init_from_model: freq_base     = 10000.0
0.00.337.182 I llama_init_from_model: freq_scale    = 1
0.00.337.184 I ggml_metal_init: allocating
0.00.337.254 I ggml_metal_init: found device: Apple M4
0.00.337.267 I ggml_metal_init: picking default device: Apple M4
0.00.338.887 I ggml_metal_init: using embedded metal library
0.00.344.587 I ggml_metal_init: GPU name:   Apple M4
0.00.344.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.603 I ggml_metal_init: simdgroup reduction   = true
0.00.344.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.603 I ggml_metal_init: has residency sets    = true
0.00.344.604 I ggml_metal_init: has bfloat            = true
0.00.344.604 I ggml_metal_init: use bfloat            = true
0.00.344.608 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.420.720 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.420.730 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.420.763 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.425.252 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.425.254 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.425.254 I llama_init_from_model: graph nodes  = 967
0.00.425.255 I llama_init_from_model: graph splits = 2
0.00.425.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.425.420 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.425.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.497 I main: llama threadpool init, n_threads = 4
0.00.486.548 I 
0.00.486.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.571 I 
0.00.486.739 I sampler seed: 1234
0.00.486.743 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.486.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.486.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.486.759 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.165.184 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.165.185 I llama_perf_context_print:        load time =     476.91 ms
0.01.165.185 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.15 tokens per second)
0.01.165.186 I llama_perf_context_print:        eval time =     632.01 ms /    63 runs   (   10.03 ms per token,    99.68 tokens per second)
0.01.165.186 I llama_perf_context_print:       total time =     679.41 ms /    70 tokens
0.01.165.418 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.111s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.015 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.024 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.025 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.026 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.027 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.027 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.028 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.028 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.029 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.031 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.032 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.032 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.752 I llama_model_loader: - type  f32:  194 tensors
0.00.024.752 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.752 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.753 I print_info: file format = GGUF V3 (latest)
0.00.024.754 I print_info: file type   = Q2_K - Medium
0.00.024.755 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.981 I load: special tokens cache size = 25
0.00.039.542 I load: token to piece cache size = 0.2984 MB
0.00.039.560 I print_info: arch             = gptneox
0.00.039.560 I print_info: vocab_only       = 0
0.00.039.561 I print_info: n_ctx_train      = 2048
0.00.039.561 I print_info: n_embd           = 2048
0.00.039.561 I print_info: n_layer          = 24
0.00.039.565 I print_info: n_head           = 16
0.00.039.565 I print_info: n_head_kv        = 16
0.00.039.566 I print_info: n_rot            = 32
0.00.039.566 I print_info: n_swa            = 0
0.00.039.566 I print_info: n_embd_head_k    = 128
0.00.039.566 I print_info: n_embd_head_v    = 128
0.00.039.567 I print_info: n_gqa            = 1
0.00.039.567 I print_info: n_embd_k_gqa     = 2048
0.00.039.568 I print_info: n_embd_v_gqa     = 2048
0.00.039.569 I print_info: f_norm_eps       = 1.0e-05
0.00.039.569 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.569 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.569 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.569 I print_info: f_logit_scale    = 0.0e+00
0.00.039.570 I print_info: n_ff             = 8192
0.00.039.570 I print_info: n_expert         = 0
0.00.039.570 I print_info: n_expert_used    = 0
0.00.039.570 I print_info: causal attn      = 1
0.00.039.570 I print_info: pooling type     = 0
0.00.039.570 I print_info: rope type        = 2
0.00.039.570 I print_info: rope scaling     = linear
0.00.039.571 I print_info: freq_base_train  = 10000.0
0.00.039.571 I print_info: freq_scale_train = 1
0.00.039.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.571 I print_info: rope_finetuned   = unknown
0.00.039.571 I print_info: ssm_d_conv       = 0
0.00.039.572 I print_info: ssm_d_inner      = 0
0.00.039.572 I print_info: ssm_d_state      = 0
0.00.039.572 I print_info: ssm_dt_rank      = 0
0.00.039.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.572 I print_info: model type       = 1.4B
0.00.039.572 I print_info: model params     = 1.41 B
0.00.039.572 I print_info: general.name     = 1.4B
0.00.039.573 I print_info: vocab type       = BPE
0.00.039.577 I print_info: n_vocab          = 50304
0.00.039.577 I print_info: n_merges         = 50009
0.00.039.577 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.577 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.578 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.578 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.578 I print_info: LF token         = 187 ''
0.00.039.578 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.579 I print_info: max token length = 1024
0.00.039.579 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.570 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.582 I load_tensors: offloading output layer to GPU
0.00.345.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.613 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.614 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.981 I llama_init_from_model: n_seq_max     = 1
0.00.346.989 I llama_init_from_model: n_ctx         = 128
0.00.346.990 I llama_init_from_model: n_ctx_per_seq = 128
0.00.346.990 I llama_init_from_model: n_batch       = 128
0.00.346.990 I llama_init_from_model: n_ubatch      = 128
0.00.346.991 I llama_init_from_model: flash_attn    = 0
0.00.346.991 I llama_init_from_model: freq_base     = 10000.0
0.00.346.992 I llama_init_from_model: freq_scale    = 1
0.00.346.993 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.346.995 I ggml_metal_init: allocating
0.00.347.081 I ggml_metal_init: found device: Apple M4
0.00.347.096 I ggml_metal_init: picking default device: Apple M4
0.00.348.931 I ggml_metal_init: using embedded metal library
0.00.354.641 I ggml_metal_init: GPU name:   Apple M4
0.00.354.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.660 I ggml_metal_init: simdgroup reduction   = true
0.00.354.661 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.661 I ggml_metal_init: has residency sets    = true
0.00.354.661 I ggml_metal_init: has bfloat            = true
0.00.354.661 I ggml_metal_init: use bfloat            = true
0.00.354.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.108 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.380.950 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.380.960 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.381.007 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.384.397 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.384.399 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.384.399 I llama_init_from_model: graph nodes  = 967
0.00.384.400 I llama_init_from_model: graph splits = 2
0.00.384.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.384.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.869 I 
0.00.417.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.417.967 I perplexity: tokenizing the input ..
0.00.424.668 I perplexity: tokenization took 6.7 ms
0.00.424.671 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.809 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.570.143 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.570.167 I llama_perf_context_print:        load time =     408.95 ms
0.00.570.168 I llama_perf_context_print: prompt eval time =     143.90 ms /   128 tokens (    1.12 ms per token,   889.49 tokens per second)
0.00.570.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.570.170 I llama_perf_context_print:       total time =     152.30 ms /   129 tokens
0.00.570.582 I ggml_metal_free: deallocating

real	0m0.584s
user	0m0.081s
sys	0m0.093s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.000 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.001 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.001 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.002 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.002 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.003 I llama_model_loader: - type  f32:  194 tensors
0.00.024.003 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.003 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.003 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.004 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.004 I print_info: file format = GGUF V3 (latest)
0.00.024.005 I print_info: file type   = Q3_K - Medium
0.00.024.006 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.135 I load: special tokens cache size = 25
0.00.038.146 I load: token to piece cache size = 0.2984 MB
0.00.038.159 I print_info: arch             = gptneox
0.00.038.161 I print_info: vocab_only       = 0
0.00.038.161 I print_info: n_ctx_train      = 2048
0.00.038.161 I print_info: n_embd           = 2048
0.00.038.161 I print_info: n_layer          = 24
0.00.038.164 I print_info: n_head           = 16
0.00.038.165 I print_info: n_head_kv        = 16
0.00.038.165 I print_info: n_rot            = 32
0.00.038.165 I print_info: n_swa            = 0
0.00.038.165 I print_info: n_embd_head_k    = 128
0.00.038.166 I print_info: n_embd_head_v    = 128
0.00.038.166 I print_info: n_gqa            = 1
0.00.038.167 I print_info: n_embd_k_gqa     = 2048
0.00.038.168 I print_info: n_embd_v_gqa     = 2048
0.00.038.169 I print_info: f_norm_eps       = 1.0e-05
0.00.038.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.169 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.169 I print_info: f_logit_scale    = 0.0e+00
0.00.038.170 I print_info: n_ff             = 8192
0.00.038.172 I print_info: n_expert         = 0
0.00.038.172 I print_info: n_expert_used    = 0
0.00.038.172 I print_info: causal attn      = 1
0.00.038.172 I print_info: pooling type     = 0
0.00.038.172 I print_info: rope type        = 2
0.00.038.173 I print_info: rope scaling     = linear
0.00.038.173 I print_info: freq_base_train  = 10000.0
0.00.038.173 I print_info: freq_scale_train = 1
0.00.038.175 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.175 I print_info: rope_finetuned   = unknown
0.00.038.175 I print_info: ssm_d_conv       = 0
0.00.038.175 I print_info: ssm_d_inner      = 0
0.00.038.175 I print_info: ssm_d_state      = 0
0.00.038.175 I print_info: ssm_dt_rank      = 0
0.00.038.175 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.177 I print_info: model type       = 1.4B
0.00.038.177 I print_info: model params     = 1.41 B
0.00.038.177 I print_info: general.name     = 1.4B
0.00.038.178 I print_info: vocab type       = BPE
0.00.038.178 I print_info: n_vocab          = 50304
0.00.038.178 I print_info: n_merges         = 50009
0.00.038.178 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.178 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.178 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.178 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.179 I print_info: LF token         = 187 ''
0.00.038.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.180 I print_info: max token length = 1024
0.00.038.180 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.465.080 I load_tensors: offloading 24 repeating layers to GPU
0.00.465.091 I load_tensors: offloading output layer to GPU
0.00.465.092 I load_tensors: offloaded 25/25 layers to GPU
0.00.465.136 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.465.137 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.466.761 I llama_init_from_model: n_seq_max     = 1
0.00.466.765 I llama_init_from_model: n_ctx         = 2048
0.00.466.766 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.466.766 I llama_init_from_model: n_batch       = 2048
0.00.466.767 I llama_init_from_model: n_ubatch      = 512
0.00.466.767 I llama_init_from_model: flash_attn    = 0
0.00.466.770 I llama_init_from_model: freq_base     = 10000.0
0.00.466.770 I llama_init_from_model: freq_scale    = 1
0.00.466.779 I ggml_metal_init: allocating
0.00.466.849 I ggml_metal_init: found device: Apple M4
0.00.466.862 I ggml_metal_init: picking default device: Apple M4
0.00.468.379 I ggml_metal_init: using embedded metal library
0.00.474.068 I ggml_metal_init: GPU name:   Apple M4
0.00.474.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.474.083 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.474.084 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.474.085 I ggml_metal_init: simdgroup reduction   = true
0.00.474.085 I ggml_metal_init: simdgroup matrix mul. = true
0.00.474.085 I ggml_metal_init: has residency sets    = true
0.00.474.086 I ggml_metal_init: has bfloat            = true
0.00.474.086 I ggml_metal_init: use bfloat            = true
0.00.474.090 I ggml_metal_init: hasUnifiedMemory      = true
0.00.474.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.496.059 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.551.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.551.687 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.551.712 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.557.889 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.557.891 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.557.892 I llama_init_from_model: graph nodes  = 967
0.00.557.892 I llama_init_from_model: graph splits = 2
0.00.557.903 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.558.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.558.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.525 I main: llama threadpool init, n_threads = 4
0.00.617.570 I 
0.00.617.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.590 I 
0.00.617.767 I sampler seed: 1234
0.00.617.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.788 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.788 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.362.067 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.362.067 I llama_perf_context_print:        load time =     607.92 ms
0.01.362.068 I llama_perf_context_print: prompt eval time =      49.89 ms /     7 tokens (    7.13 ms per token,   140.31 tokens per second)
0.01.362.070 I llama_perf_context_print:        eval time =     691.53 ms /    63 runs   (   10.98 ms per token,    91.10 tokens per second)
0.01.362.070 I llama_perf_context_print:       total time =     745.27 ms /    70 tokens
0.01.362.268 I ggml_metal_free: deallocating

real	0m1.379s
user	0m0.112s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.101 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.963 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.963 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.727 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.536 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.536 I llama_model_loader: - type  f32:  194 tensors
0.00.024.537 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.537 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.537 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.537 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.538 I print_info: file format = GGUF V3 (latest)
0.00.024.539 I print_info: file type   = Q3_K - Medium
0.00.024.540 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.860 I load: special tokens cache size = 25
0.00.039.192 I load: token to piece cache size = 0.2984 MB
0.00.039.210 I print_info: arch             = gptneox
0.00.039.210 I print_info: vocab_only       = 0
0.00.039.211 I print_info: n_ctx_train      = 2048
0.00.039.211 I print_info: n_embd           = 2048
0.00.039.211 I print_info: n_layer          = 24
0.00.039.215 I print_info: n_head           = 16
0.00.039.216 I print_info: n_head_kv        = 16
0.00.039.216 I print_info: n_rot            = 32
0.00.039.216 I print_info: n_swa            = 0
0.00.039.216 I print_info: n_embd_head_k    = 128
0.00.039.217 I print_info: n_embd_head_v    = 128
0.00.039.217 I print_info: n_gqa            = 1
0.00.039.218 I print_info: n_embd_k_gqa     = 2048
0.00.039.220 I print_info: n_embd_v_gqa     = 2048
0.00.039.221 I print_info: f_norm_eps       = 1.0e-05
0.00.039.221 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.221 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.221 I print_info: f_logit_scale    = 0.0e+00
0.00.039.222 I print_info: n_ff             = 8192
0.00.039.222 I print_info: n_expert         = 0
0.00.039.222 I print_info: n_expert_used    = 0
0.00.039.222 I print_info: causal attn      = 1
0.00.039.222 I print_info: pooling type     = 0
0.00.039.222 I print_info: rope type        = 2
0.00.039.223 I print_info: rope scaling     = linear
0.00.039.223 I print_info: freq_base_train  = 10000.0
0.00.039.223 I print_info: freq_scale_train = 1
0.00.039.223 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.224 I print_info: rope_finetuned   = unknown
0.00.039.224 I print_info: ssm_d_conv       = 0
0.00.039.224 I print_info: ssm_d_inner      = 0
0.00.039.224 I print_info: ssm_d_state      = 0
0.00.039.224 I print_info: ssm_dt_rank      = 0
0.00.039.224 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.224 I print_info: model type       = 1.4B
0.00.039.225 I print_info: model params     = 1.41 B
0.00.039.225 I print_info: general.name     = 1.4B
0.00.039.225 I print_info: vocab type       = BPE
0.00.039.226 I print_info: n_vocab          = 50304
0.00.039.226 I print_info: n_merges         = 50009
0.00.039.226 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.226 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.226 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.227 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.227 I print_info: LF token         = 187 ''
0.00.039.227 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.228 I print_info: max token length = 1024
0.00.039.228 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.702 I load_tensors: offloading output layer to GPU
0.00.439.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.745 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.746 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.441.571 I llama_init_from_model: n_seq_max     = 1
0.00.441.574 I llama_init_from_model: n_ctx         = 128
0.00.441.575 I llama_init_from_model: n_ctx_per_seq = 128
0.00.441.576 I llama_init_from_model: n_batch       = 128
0.00.441.576 I llama_init_from_model: n_ubatch      = 128
0.00.441.576 I llama_init_from_model: flash_attn    = 0
0.00.441.578 I llama_init_from_model: freq_base     = 10000.0
0.00.441.579 I llama_init_from_model: freq_scale    = 1
0.00.441.579 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.441.585 I ggml_metal_init: allocating
0.00.441.706 I ggml_metal_init: found device: Apple M4
0.00.441.720 I ggml_metal_init: picking default device: Apple M4
0.00.443.355 I ggml_metal_init: using embedded metal library
0.00.449.008 I ggml_metal_init: GPU name:   Apple M4
0.00.449.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.023 I ggml_metal_init: simdgroup reduction   = true
0.00.449.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.024 I ggml_metal_init: has residency sets    = true
0.00.449.024 I ggml_metal_init: has bfloat            = true
0.00.449.025 I ggml_metal_init: use bfloat            = true
0.00.449.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.212 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.473.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.473.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.477.189 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.477.191 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.477.192 I llama_init_from_model: graph nodes  = 967
0.00.477.192 I llama_init_from_model: graph splits = 2
0.00.477.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.477.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.596 I 
0.00.503.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.747 I perplexity: tokenizing the input ..
0.00.510.887 I perplexity: tokenization took 7.136 ms
0.00.510.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.642.656 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.643.992 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.016 I llama_perf_context_print:        load time =     494.49 ms
0.00.644.017 I llama_perf_context_print: prompt eval time =     131.18 ms /   128 tokens (    1.02 ms per token,   975.73 tokens per second)
0.00.644.018 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.019 I llama_perf_context_print:       total time =     140.42 ms /   129 tokens
0.00.644.394 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.081s
sys	0m0.114s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.260 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.538 I llama_model_loader: - type  f32:  194 tensors
0.00.024.538 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.538 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.538 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.539 I print_info: file format = GGUF V3 (latest)
0.00.024.539 I print_info: file type   = Q4_K - Medium
0.00.024.540 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.365 I load: special tokens cache size = 25
0.00.038.578 I load: token to piece cache size = 0.2984 MB
0.00.038.592 I print_info: arch             = gptneox
0.00.038.593 I print_info: vocab_only       = 0
0.00.038.593 I print_info: n_ctx_train      = 2048
0.00.038.593 I print_info: n_embd           = 2048
0.00.038.594 I print_info: n_layer          = 24
0.00.038.597 I print_info: n_head           = 16
0.00.038.597 I print_info: n_head_kv        = 16
0.00.038.598 I print_info: n_rot            = 32
0.00.038.598 I print_info: n_swa            = 0
0.00.038.598 I print_info: n_embd_head_k    = 128
0.00.038.598 I print_info: n_embd_head_v    = 128
0.00.038.599 I print_info: n_gqa            = 1
0.00.038.601 I print_info: n_embd_k_gqa     = 2048
0.00.038.601 I print_info: n_embd_v_gqa     = 2048
0.00.038.602 I print_info: f_norm_eps       = 1.0e-05
0.00.038.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.603 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.603 I print_info: f_logit_scale    = 0.0e+00
0.00.038.603 I print_info: n_ff             = 8192
0.00.038.603 I print_info: n_expert         = 0
0.00.038.604 I print_info: n_expert_used    = 0
0.00.038.604 I print_info: causal attn      = 1
0.00.038.604 I print_info: pooling type     = 0
0.00.038.604 I print_info: rope type        = 2
0.00.038.604 I print_info: rope scaling     = linear
0.00.038.604 I print_info: freq_base_train  = 10000.0
0.00.038.605 I print_info: freq_scale_train = 1
0.00.038.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.605 I print_info: rope_finetuned   = unknown
0.00.038.605 I print_info: ssm_d_conv       = 0
0.00.038.605 I print_info: ssm_d_inner      = 0
0.00.038.605 I print_info: ssm_d_state      = 0
0.00.038.605 I print_info: ssm_dt_rank      = 0
0.00.038.605 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.606 I print_info: model type       = 1.4B
0.00.038.606 I print_info: model params     = 1.41 B
0.00.038.606 I print_info: general.name     = 1.4B
0.00.038.607 I print_info: vocab type       = BPE
0.00.038.607 I print_info: n_vocab          = 50304
0.00.038.607 I print_info: n_merges         = 50009
0.00.038.607 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.608 I print_info: LF token         = 187 ''
0.00.038.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.608 I print_info: max token length = 1024
0.00.038.608 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.514.559 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.573 I load_tensors: offloading output layer to GPU
0.00.514.574 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.606 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.608 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.516.393 I llama_init_from_model: n_seq_max     = 1
0.00.516.395 I llama_init_from_model: n_ctx         = 2048
0.00.516.396 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.516.396 I llama_init_from_model: n_batch       = 2048
0.00.516.397 I llama_init_from_model: n_ubatch      = 512
0.00.516.397 I llama_init_from_model: flash_attn    = 0
0.00.516.400 I llama_init_from_model: freq_base     = 10000.0
0.00.516.401 I llama_init_from_model: freq_scale    = 1
0.00.516.403 I ggml_metal_init: allocating
0.00.516.507 I ggml_metal_init: found device: Apple M4
0.00.516.521 I ggml_metal_init: picking default device: Apple M4
0.00.518.112 I ggml_metal_init: using embedded metal library
0.00.524.839 I ggml_metal_init: GPU name:   Apple M4
0.00.524.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.846 I ggml_metal_init: simdgroup reduction   = true
0.00.524.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.846 I ggml_metal_init: has residency sets    = true
0.00.524.847 I ggml_metal_init: has bfloat            = true
0.00.524.847 I ggml_metal_init: use bfloat            = true
0.00.524.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.622 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.594.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.594.552 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.594.574 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.914 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.598.916 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.598.916 I llama_init_from_model: graph nodes  = 967
0.00.598.917 I llama_init_from_model: graph splits = 2
0.00.598.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.599.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.599.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.975 I main: llama threadpool init, n_threads = 4
0.00.657.023 I 
0.00.657.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.044 I 
0.00.657.218 I sampler seed: 1234
0.00.657.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.237 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.238 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.427.398 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47207.45 tokens per second)
0.01.427.399 I llama_perf_context_print:        load time =     646.44 ms
0.01.427.400 I llama_perf_context_print: prompt eval time =      57.61 ms /     7 tokens (    8.23 ms per token,   121.52 tokens per second)
0.01.427.401 I llama_perf_context_print:        eval time =     709.51 ms /    63 runs   (   11.26 ms per token,    88.79 tokens per second)
0.01.427.401 I llama_perf_context_print:       total time =     771.21 ms /    70 tokens
0.01.427.674 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.832 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.636 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.381 I llama_model_loader: - type  f32:  194 tensors
0.00.024.381 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.381 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.381 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.382 I print_info: file format = GGUF V3 (latest)
0.00.024.383 I print_info: file type   = Q4_K - Medium
0.00.024.384 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.710 I load: special tokens cache size = 25
0.00.038.946 I load: token to piece cache size = 0.2984 MB
0.00.038.963 I print_info: arch             = gptneox
0.00.038.964 I print_info: vocab_only       = 0
0.00.038.964 I print_info: n_ctx_train      = 2048
0.00.038.964 I print_info: n_embd           = 2048
0.00.038.964 I print_info: n_layer          = 24
0.00.038.969 I print_info: n_head           = 16
0.00.038.970 I print_info: n_head_kv        = 16
0.00.038.970 I print_info: n_rot            = 32
0.00.038.970 I print_info: n_swa            = 0
0.00.038.970 I print_info: n_embd_head_k    = 128
0.00.038.970 I print_info: n_embd_head_v    = 128
0.00.038.971 I print_info: n_gqa            = 1
0.00.038.972 I print_info: n_embd_k_gqa     = 2048
0.00.038.972 I print_info: n_embd_v_gqa     = 2048
0.00.038.973 I print_info: f_norm_eps       = 1.0e-05
0.00.038.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.974 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.974 I print_info: f_logit_scale    = 0.0e+00
0.00.038.975 I print_info: n_ff             = 8192
0.00.038.975 I print_info: n_expert         = 0
0.00.038.975 I print_info: n_expert_used    = 0
0.00.038.975 I print_info: causal attn      = 1
0.00.038.975 I print_info: pooling type     = 0
0.00.038.977 I print_info: rope type        = 2
0.00.038.977 I print_info: rope scaling     = linear
0.00.038.978 I print_info: freq_base_train  = 10000.0
0.00.038.978 I print_info: freq_scale_train = 1
0.00.038.978 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.978 I print_info: rope_finetuned   = unknown
0.00.038.979 I print_info: ssm_d_conv       = 0
0.00.038.979 I print_info: ssm_d_inner      = 0
0.00.038.979 I print_info: ssm_d_state      = 0
0.00.038.979 I print_info: ssm_dt_rank      = 0
0.00.038.979 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.979 I print_info: model type       = 1.4B
0.00.038.980 I print_info: model params     = 1.41 B
0.00.038.980 I print_info: general.name     = 1.4B
0.00.038.981 I print_info: vocab type       = BPE
0.00.038.981 I print_info: n_vocab          = 50304
0.00.038.981 I print_info: n_merges         = 50009
0.00.038.981 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: LF token         = 187 ''
0.00.038.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: max token length = 1024
0.00.038.983 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.507.569 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.587 I load_tensors: offloading output layer to GPU
0.00.507.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.625 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.507.626 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.509.339 I llama_init_from_model: n_seq_max     = 1
0.00.509.341 I llama_init_from_model: n_ctx         = 128
0.00.509.342 I llama_init_from_model: n_ctx_per_seq = 128
0.00.509.342 I llama_init_from_model: n_batch       = 128
0.00.509.343 I llama_init_from_model: n_ubatch      = 128
0.00.509.343 I llama_init_from_model: flash_attn    = 0
0.00.509.345 I llama_init_from_model: freq_base     = 10000.0
0.00.509.346 I llama_init_from_model: freq_scale    = 1
0.00.509.346 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.509.349 I ggml_metal_init: allocating
0.00.509.466 I ggml_metal_init: found device: Apple M4
0.00.509.481 I ggml_metal_init: picking default device: Apple M4
0.00.511.279 I ggml_metal_init: using embedded metal library
0.00.517.861 I ggml_metal_init: GPU name:   Apple M4
0.00.517.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.870 I ggml_metal_init: simdgroup reduction   = true
0.00.517.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.871 I ggml_metal_init: has residency sets    = true
0.00.517.871 I ggml_metal_init: has bfloat            = true
0.00.517.871 I ggml_metal_init: use bfloat            = true
0.00.517.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.876 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.421 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.883 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.538.887 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.538.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.541.961 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.541.963 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.541.964 I llama_init_from_model: graph nodes  = 967
0.00.541.964 I llama_init_from_model: graph splits = 2
0.00.541.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.541.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.400 I 
0.00.571.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.513 I perplexity: tokenizing the input ..
0.00.578.765 I perplexity: tokenization took 7.247 ms
0.00.578.773 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.499 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.725.848 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.725.867 I llama_perf_context_print:        load time =     562.51 ms
0.00.725.868 I llama_perf_context_print: prompt eval time =     144.78 ms /   128 tokens (    1.13 ms per token,   884.10 tokens per second)
0.00.725.869 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.725.869 I llama_perf_context_print:       total time =     154.48 ms /   129 tokens
0.00.726.248 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.080s
sys	0m0.118s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.366 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.805 I llama_model_loader: - type  f32:  194 tensors
0.00.023.805 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.805 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.806 I print_info: file format = GGUF V3 (latest)
0.00.023.807 I print_info: file type   = Q5_K - Medium
0.00.023.808 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.981 I load: special tokens cache size = 25
0.00.038.240 I load: token to piece cache size = 0.2984 MB
0.00.038.255 I print_info: arch             = gptneox
0.00.038.256 I print_info: vocab_only       = 0
0.00.038.256 I print_info: n_ctx_train      = 2048
0.00.038.256 I print_info: n_embd           = 2048
0.00.038.256 I print_info: n_layer          = 24
0.00.038.259 I print_info: n_head           = 16
0.00.038.260 I print_info: n_head_kv        = 16
0.00.038.260 I print_info: n_rot            = 32
0.00.038.263 I print_info: n_swa            = 0
0.00.038.263 I print_info: n_embd_head_k    = 128
0.00.038.263 I print_info: n_embd_head_v    = 128
0.00.038.264 I print_info: n_gqa            = 1
0.00.038.265 I print_info: n_embd_k_gqa     = 2048
0.00.038.265 I print_info: n_embd_v_gqa     = 2048
0.00.038.266 I print_info: f_norm_eps       = 1.0e-05
0.00.038.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.268 I print_info: f_logit_scale    = 0.0e+00
0.00.038.268 I print_info: n_ff             = 8192
0.00.038.268 I print_info: n_expert         = 0
0.00.038.268 I print_info: n_expert_used    = 0
0.00.038.269 I print_info: causal attn      = 1
0.00.038.269 I print_info: pooling type     = 0
0.00.038.269 I print_info: rope type        = 2
0.00.038.269 I print_info: rope scaling     = linear
0.00.038.271 I print_info: freq_base_train  = 10000.0
0.00.038.271 I print_info: freq_scale_train = 1
0.00.038.271 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.271 I print_info: rope_finetuned   = unknown
0.00.038.272 I print_info: ssm_d_conv       = 0
0.00.038.272 I print_info: ssm_d_inner      = 0
0.00.038.272 I print_info: ssm_d_state      = 0
0.00.038.273 I print_info: ssm_dt_rank      = 0
0.00.038.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.273 I print_info: model type       = 1.4B
0.00.038.273 I print_info: model params     = 1.41 B
0.00.038.273 I print_info: general.name     = 1.4B
0.00.038.275 I print_info: vocab type       = BPE
0.00.038.275 I print_info: n_vocab          = 50304
0.00.038.275 I print_info: n_merges         = 50009
0.00.038.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.275 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.276 I print_info: LF token         = 187 ''
0.00.038.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.276 I print_info: max token length = 1024
0.00.038.277 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.486 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.500 I load_tensors: offloading output layer to GPU
0.00.591.500 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.536 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.538 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.226 I llama_init_from_model: n_seq_max     = 1
0.00.593.228 I llama_init_from_model: n_ctx         = 2048
0.00.593.229 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.229 I llama_init_from_model: n_batch       = 2048
0.00.593.230 I llama_init_from_model: n_ubatch      = 512
0.00.593.231 I llama_init_from_model: flash_attn    = 0
0.00.593.232 I llama_init_from_model: freq_base     = 10000.0
0.00.593.232 I llama_init_from_model: freq_scale    = 1
0.00.593.234 I ggml_metal_init: allocating
0.00.593.278 I ggml_metal_init: found device: Apple M4
0.00.593.291 I ggml_metal_init: picking default device: Apple M4
0.00.594.643 I ggml_metal_init: using embedded metal library
0.00.601.157 I ggml_metal_init: GPU name:   Apple M4
0.00.601.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.162 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.163 I ggml_metal_init: simdgroup reduction   = true
0.00.601.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.163 I ggml_metal_init: has residency sets    = true
0.00.601.164 I ggml_metal_init: has bfloat            = true
0.00.601.164 I ggml_metal_init: use bfloat            = true
0.00.601.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.170 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.177 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.199 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.223 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.226 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.226 I llama_init_from_model: graph nodes  = 967
0.00.676.227 I llama_init_from_model: graph splits = 2
0.00.676.234 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.948 I main: llama threadpool init, n_threads = 4
0.00.737.997 I 
0.00.738.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.016 I 
0.00.738.199 I sampler seed: 1234
0.00.738.203 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.218 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.220 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.220 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.589.320 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.589.321 I llama_perf_context_print:        load time =     728.52 ms
0.01.589.321 I llama_perf_context_print: prompt eval time =      52.67 ms /     7 tokens (    7.52 ms per token,   132.89 tokens per second)
0.01.589.322 I llama_perf_context_print:        eval time =     795.67 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.589.322 I llama_perf_context_print:       total time =     852.09 ms /    70 tokens
0.01.589.556 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.628 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.631 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.402 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.180 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.180 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.181 I print_info: file format = GGUF V3 (latest)
0.00.025.181 I print_info: file type   = Q5_K - Medium
0.00.025.182 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.198 I load: special tokens cache size = 25
0.00.039.646 I load: token to piece cache size = 0.2984 MB
0.00.039.664 I print_info: arch             = gptneox
0.00.039.664 I print_info: vocab_only       = 0
0.00.039.665 I print_info: n_ctx_train      = 2048
0.00.039.665 I print_info: n_embd           = 2048
0.00.039.665 I print_info: n_layer          = 24
0.00.039.669 I print_info: n_head           = 16
0.00.039.669 I print_info: n_head_kv        = 16
0.00.039.670 I print_info: n_rot            = 32
0.00.039.670 I print_info: n_swa            = 0
0.00.039.670 I print_info: n_embd_head_k    = 128
0.00.039.670 I print_info: n_embd_head_v    = 128
0.00.039.670 I print_info: n_gqa            = 1
0.00.039.671 I print_info: n_embd_k_gqa     = 2048
0.00.039.672 I print_info: n_embd_v_gqa     = 2048
0.00.039.672 I print_info: f_norm_eps       = 1.0e-05
0.00.039.673 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.673 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.673 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.673 I print_info: f_logit_scale    = 0.0e+00
0.00.039.674 I print_info: n_ff             = 8192
0.00.039.675 I print_info: n_expert         = 0
0.00.039.678 I print_info: n_expert_used    = 0
0.00.039.678 I print_info: causal attn      = 1
0.00.039.679 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.679 I print_info: freq_base_train  = 10000.0
0.00.039.680 I print_info: freq_scale_train = 1
0.00.039.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.680 I print_info: rope_finetuned   = unknown
0.00.039.680 I print_info: ssm_d_conv       = 0
0.00.039.680 I print_info: ssm_d_inner      = 0
0.00.039.680 I print_info: ssm_d_state      = 0
0.00.039.680 I print_info: ssm_dt_rank      = 0
0.00.039.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.682 I print_info: model type       = 1.4B
0.00.039.682 I print_info: model params     = 1.41 B
0.00.039.682 I print_info: general.name     = 1.4B
0.00.039.683 I print_info: vocab type       = BPE
0.00.039.683 I print_info: n_vocab          = 50304
0.00.039.683 I print_info: n_merges         = 50009
0.00.039.685 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: LF token         = 187 ''
0.00.039.686 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: max token length = 1024
0.00.039.687 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.264 I load_tensors: offloading output layer to GPU
0.00.593.264 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.298 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.593.299 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.111 I llama_init_from_model: n_seq_max     = 1
0.00.595.114 I llama_init_from_model: n_ctx         = 128
0.00.595.114 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.115 I llama_init_from_model: n_batch       = 128
0.00.595.116 I llama_init_from_model: n_ubatch      = 128
0.00.595.116 I llama_init_from_model: flash_attn    = 0
0.00.595.118 I llama_init_from_model: freq_base     = 10000.0
0.00.595.119 I llama_init_from_model: freq_scale    = 1
0.00.595.119 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.122 I ggml_metal_init: allocating
0.00.595.252 I ggml_metal_init: found device: Apple M4
0.00.595.276 I ggml_metal_init: picking default device: Apple M4
0.00.596.540 I ggml_metal_init: using embedded metal library
0.00.603.082 I ggml_metal_init: GPU name:   Apple M4
0.00.603.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.089 I ggml_metal_init: simdgroup reduction   = true
0.00.603.089 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.089 I ggml_metal_init: has residency sets    = true
0.00.603.089 I ggml_metal_init: has bfloat            = true
0.00.603.090 I ggml_metal_init: use bfloat            = true
0.00.603.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.291 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.695 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.703 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.735 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.769 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.771 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.772 I llama_init_from_model: graph nodes  = 967
0.00.626.772 I llama_init_from_model: graph splits = 2
0.00.626.776 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.334 I 
0.00.665.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.441 I perplexity: tokenizing the input ..
0.00.671.615 I perplexity: tokenization took 6.173 ms
0.00.671.618 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.100 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.815.513 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.815.539 I llama_perf_context_print:        load time =     655.49 ms
0.00.815.540 I llama_perf_context_print: prompt eval time =     142.25 ms /   128 tokens (    1.11 ms per token,   899.82 tokens per second)
0.00.815.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.540 I llama_perf_context_print:       total time =     150.21 ms /   129 tokens
0.00.815.938 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.077s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.505 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.203 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.853 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.853 I llama_model_loader: - type  f32:  194 tensors
0.00.024.854 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.854 I print_info: file format = GGUF V3 (latest)
0.00.024.855 I print_info: file type   = Q6_K
0.00.024.856 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.140 I load: special tokens cache size = 25
0.00.039.860 I load: token to piece cache size = 0.2984 MB
0.00.039.877 I print_info: arch             = gptneox
0.00.039.879 I print_info: vocab_only       = 0
0.00.039.879 I print_info: n_ctx_train      = 2048
0.00.039.879 I print_info: n_embd           = 2048
0.00.039.879 I print_info: n_layer          = 24
0.00.039.883 I print_info: n_head           = 16
0.00.039.883 I print_info: n_head_kv        = 16
0.00.039.884 I print_info: n_rot            = 32
0.00.039.884 I print_info: n_swa            = 0
0.00.039.884 I print_info: n_embd_head_k    = 128
0.00.039.884 I print_info: n_embd_head_v    = 128
0.00.039.885 I print_info: n_gqa            = 1
0.00.039.885 I print_info: n_embd_k_gqa     = 2048
0.00.039.886 I print_info: n_embd_v_gqa     = 2048
0.00.039.886 I print_info: f_norm_eps       = 1.0e-05
0.00.039.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.890 I print_info: f_logit_scale    = 0.0e+00
0.00.039.890 I print_info: n_ff             = 8192
0.00.039.890 I print_info: n_expert         = 0
0.00.039.891 I print_info: n_expert_used    = 0
0.00.039.891 I print_info: causal attn      = 1
0.00.039.891 I print_info: pooling type     = 0
0.00.039.891 I print_info: rope type        = 2
0.00.039.891 I print_info: rope scaling     = linear
0.00.039.891 I print_info: freq_base_train  = 10000.0
0.00.039.892 I print_info: freq_scale_train = 1
0.00.039.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.892 I print_info: rope_finetuned   = unknown
0.00.039.892 I print_info: ssm_d_conv       = 0
0.00.039.892 I print_info: ssm_d_inner      = 0
0.00.039.894 I print_info: ssm_d_state      = 0
0.00.039.894 I print_info: ssm_dt_rank      = 0
0.00.039.894 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.894 I print_info: model type       = 1.4B
0.00.039.895 I print_info: model params     = 1.41 B
0.00.039.895 I print_info: general.name     = 1.4B
0.00.039.895 I print_info: vocab type       = BPE
0.00.039.896 I print_info: n_vocab          = 50304
0.00.039.896 I print_info: n_merges         = 50009
0.00.039.896 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: LF token         = 187 ''
0.00.039.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: max token length = 1024
0.00.039.897 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.773 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.780 I load_tensors: offloading output layer to GPU
0.00.642.780 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.799 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.642.800 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.643.655 I llama_init_from_model: n_seq_max     = 1
0.00.643.659 I llama_init_from_model: n_ctx         = 2048
0.00.643.659 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.660 I llama_init_from_model: n_batch       = 2048
0.00.643.660 I llama_init_from_model: n_ubatch      = 512
0.00.643.660 I llama_init_from_model: flash_attn    = 0
0.00.643.662 I llama_init_from_model: freq_base     = 10000.0
0.00.643.662 I llama_init_from_model: freq_scale    = 1
0.00.643.664 I ggml_metal_init: allocating
0.00.643.705 I ggml_metal_init: found device: Apple M4
0.00.643.716 I ggml_metal_init: picking default device: Apple M4
0.00.644.699 I ggml_metal_init: using embedded metal library
0.00.649.066 I ggml_metal_init: GPU name:   Apple M4
0.00.649.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.073 I ggml_metal_init: simdgroup reduction   = true
0.00.649.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.074 I ggml_metal_init: has residency sets    = true
0.00.649.074 I ggml_metal_init: has bfloat            = true
0.00.649.075 I ggml_metal_init: use bfloat            = true
0.00.649.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.001 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.693.515 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.693.544 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.588 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.590 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.590 I llama_init_from_model: graph nodes  = 967
0.00.697.590 I llama_init_from_model: graph splits = 2
0.00.697.599 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.731 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.732 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.634 I main: llama threadpool init, n_threads = 4
0.00.764.678 I 
0.00.764.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.699 I 
0.00.764.859 I sampler seed: 1234
0.00.764.864 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.909 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.912 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.654.667 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.654.668 I llama_perf_context_print:        load time =     754.40 ms
0.01.654.668 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.11 tokens per second)
0.01.654.669 I llama_perf_context_print:        eval time =     829.22 ms /    63 runs   (   13.16 ms per token,    75.98 tokens per second)
0.01.654.669 I llama_perf_context_print:       total time =     890.76 ms /    70 tokens
0.01.654.877 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.105s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4857 (0fd7ca7a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.594 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.498 I llama_model_loader: - type  f32:  194 tensors
0.00.024.499 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.499 I print_info: file format = GGUF V3 (latest)
0.00.024.500 I print_info: file type   = Q6_K
0.00.024.501 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.918 I load: special tokens cache size = 25
0.00.039.348 I load: token to piece cache size = 0.2984 MB
0.00.039.365 I print_info: arch             = gptneox
0.00.039.367 I print_info: vocab_only       = 0
0.00.039.367 I print_info: n_ctx_train      = 2048
0.00.039.367 I print_info: n_embd           = 2048
0.00.039.367 I print_info: n_layer          = 24
0.00.039.371 I print_info: n_head           = 16
0.00.039.371 I print_info: n_head_kv        = 16
0.00.039.372 I print_info: n_rot            = 32
0.00.039.372 I print_info: n_swa            = 0
0.00.039.372 I print_info: n_embd_head_k    = 128
0.00.039.372 I print_info: n_embd_head_v    = 128
0.00.039.372 I print_info: n_gqa            = 1
0.00.039.373 I print_info: n_embd_k_gqa     = 2048
0.00.039.374 I print_info: n_embd_v_gqa     = 2048
0.00.039.374 I print_info: f_norm_eps       = 1.0e-05
0.00.039.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.376 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.376 I print_info: f_logit_scale    = 0.0e+00
0.00.039.377 I print_info: n_ff             = 8192
0.00.039.379 I print_info: n_expert         = 0
0.00.039.379 I print_info: n_expert_used    = 0
0.00.039.380 I print_info: causal attn      = 1
0.00.039.380 I print_info: pooling type     = 0
0.00.039.380 I print_info: rope type        = 2
0.00.039.380 I print_info: rope scaling     = linear
0.00.039.380 I print_info: freq_base_train  = 10000.0
0.00.039.381 I print_info: freq_scale_train = 1
0.00.039.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.381 I print_info: rope_finetuned   = unknown
0.00.039.381 I print_info: ssm_d_conv       = 0
0.00.039.381 I print_info: ssm_d_inner      = 0
0.00.039.381 I print_info: ssm_d_state      = 0
0.00.039.381 I print_info: ssm_dt_rank      = 0
0.00.039.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.382 I print_info: model type       = 1.4B
0.00.039.382 I print_info: model params     = 1.41 B
0.00.039.382 I print_info: general.name     = 1.4B
0.00.039.383 I print_info: vocab type       = BPE
0.00.039.383 I print_info: n_vocab          = 50304
0.00.039.383 I print_info: n_merges         = 50009
0.00.039.383 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.384 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.384 I print_info: LF token         = 187 ''
0.00.039.384 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.384 I print_info: max token length = 1024
0.00.039.385 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.602.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.900 I load_tensors: offloading output layer to GPU
0.00.602.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.926 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.602.928 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.604.425 I llama_init_from_model: n_seq_max     = 1
0.00.604.427 I llama_init_from_model: n_ctx         = 128
0.00.604.428 I llama_init_from_model: n_ctx_per_seq = 128
0.00.604.428 I llama_init_from_model: n_batch       = 128
0.00.604.428 I llama_init_from_model: n_ubatch      = 128
0.00.604.429 I llama_init_from_model: flash_attn    = 0
0.00.604.430 I llama_init_from_model: freq_base     = 10000.0
0.00.604.431 I llama_init_from_model: freq_scale    = 1
0.00.604.431 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.433 I ggml_metal_init: allocating
0.00.604.470 I ggml_metal_init: found device: Apple M4
0.00.604.480 I ggml_metal_init: picking default device: Apple M4
0.00.605.674 I ggml_metal_init: using embedded metal library
0.00.611.534 I ggml_metal_init: GPU name:   Apple M4
0.00.611.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.539 I ggml_metal_init: simdgroup reduction   = true
0.00.611.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.539 I ggml_metal_init: has residency sets    = true
0.00.611.540 I ggml_metal_init: has bfloat            = true
0.00.611.540 I ggml_metal_init: use bfloat            = true
0.00.611.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.038 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.505 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.509 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.548 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.593 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.595 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.596 I llama_init_from_model: graph nodes  = 967
0.00.634.596 I llama_init_from_model: graph splits = 2
0.00.634.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.783 I 
0.00.670.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.878 I perplexity: tokenizing the input ..
0.00.677.275 I perplexity: tokenization took 6.394 ms
0.00.677.279 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.874 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.810.207 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.810.237 I llama_perf_context_print:        load time =     661.91 ms
0.00.810.239 I llama_perf_context_print: prompt eval time =     131.19 ms /   128 tokens (    1.02 ms per token,   975.68 tokens per second)
0.00.810.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.242 I llama_perf_context_print:       total time =     139.46 ms /   129 tokens
0.00.810.624 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.077s
sys	0m0.133s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4857 (0fd7ca7a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14cf08090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14cf087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14cf08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14cf09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14cf098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14cf09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14cf0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14cf0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14cf0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14cf0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14cf0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14cf0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14cf0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14cf0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14cf0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14cf0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14cf0e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14cf0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14cf0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14cf0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14cf104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14cf10c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14cf11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14cf11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14cf122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14cf12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14cf12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14cf132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14cf13770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14cf13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14cf13ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14cf145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14cf14880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14cf14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14cf151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14cf15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14cf15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14cf15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14cf16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14cf168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14cf16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14cf17220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14cf176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14cf17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14cf17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14cf18330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14cf18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14cf19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14cf196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14cf19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14cf1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14cf1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14cf1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14cf1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14cf1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14cf1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14cf1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14cf1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14cf1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14cf1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14cf1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14cf1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14cf1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14cf1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14cf1dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14cf1e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14cf1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14cf1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14cf1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14cf1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14cf1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14cf20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14cf204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14cf20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14cf20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14cf214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14cf219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14cf21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14cf22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14cf229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14cf22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14cf23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14cf239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14cf23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14cf24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14cf249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14cf24f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14cf25460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14cf259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14cf25f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14cf26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14cf269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14cf26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14cf27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14cf27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14cf27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14cf28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14cf18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14cf288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14cf29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14cf295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14cf29af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14cf2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14cf2a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14cf2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14cf2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14cf2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14cf2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14cf2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14cf2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14cf2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14cf2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14cf2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14cf2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14cf2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14cf2e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14cf2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14cf2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14cf2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14cf2f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14cf2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14cf2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14cf303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14cf30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14cf30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14cf31180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14cf31620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14cf31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14cf31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14cf32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14cf328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14cf32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14cf331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14cf33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14cf33b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14cf33fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14cf34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14cf34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14cf34da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14cf35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14cf356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14cf35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14cf36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14cf364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14cf36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14cf36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14cf372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14cf37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14cf37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14cf38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14cf38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14cf389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14cf38e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14cf39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14cf397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14cf39c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14cf3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14cf3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14cf3aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14cf3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14cf3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14cf3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14cf3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14cf3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14cf3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ce04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ce04580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ce049f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ce04e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ce052d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ce05740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ce05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ce06020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ce06490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ce06900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ce06d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ce071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ce07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ce07ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ce07f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ce083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ce08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ce08c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ce090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ce09560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ce099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ce09e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ce0a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ce0a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ce0ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ce0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ce0b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ce0b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ce0bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ce0c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ce0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ce0caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ce0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ce0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ce0d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ce0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ce0e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ce0e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ce0ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ce0f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ce0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ce0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ce10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ce10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ce10bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ce11420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ce116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ce11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ce12240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ce127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ce12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ce13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ce13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ce13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ce14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ce14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ce14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ce15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ce15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ce160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ce16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ce16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ce171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ce17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ce17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ce182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ce188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ce18e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ce19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ce199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ce19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ce1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ce1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ce1b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ce1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ce1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ce1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ce1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ce1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ce1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ce1d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ce1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ce1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ce1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ce1ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ce1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ce1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ce20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ce205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ce20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ce21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ce216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ce21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ce22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ce227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ce22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ce23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ce238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ce23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ce24450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ce24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ce24fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ce25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ce25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ce25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ce26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ce26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ce26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ce27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ce27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ce27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ce28260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ce28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ce28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ce29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ce29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ce29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11ce2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11ce2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11ce2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11ce2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11ce2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11ce2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11ce2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11ce2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11ce2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11ce2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ce2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ce2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ce2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ce2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ce2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ce2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ce2fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ce300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ce30560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.659.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.659.584 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c904d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c9051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c905630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c905aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c905f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c906380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c9067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c906c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c9070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c907540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c9079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c9080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c908bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c909370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c909b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c90a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c90a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c90b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c90b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c90bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c90c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c90cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c90d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c90dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c90e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c90e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c90e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c90ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c90f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c90f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c90fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c910070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c910750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c910bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c911090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c911530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c9119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c911e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c912310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c9127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c912c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c9130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c913590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c913a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c913ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c914370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c914810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c914cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c915150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c9155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c915a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c915f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c9163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c916870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c916d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c9171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c917650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c917910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c917bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c918040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c9184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c918920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c918d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c919200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c919670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c919ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c919f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c91a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c91a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c91aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c91b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c91b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c91b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c91be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c91c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c91c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c91cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c91d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c91d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c91d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c91dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c91e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c91e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c91eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c91ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c91f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c91f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c91fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c9200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c920560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c9209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c920e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c9212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c921720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c921b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c922000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c922470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c9228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c922d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c9231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c923630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c923aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c923f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c924380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c9247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c924c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c9250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c925540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c9259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c925e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c926290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c926700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c926b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c926fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c927450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c9278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c927d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c9281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c928610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c928a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c928ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c929360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c9297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c929c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c92a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c92a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c92a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c92ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c92b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c92b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c92bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c92bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c92c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c92c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c92cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c92d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c92d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c92da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c92ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c92e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c92e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c92ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c92f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c92f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c92f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c92fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c930250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c9306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c930b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c930fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c931410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c931880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c931cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c932160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c9325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c932a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c932eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c933320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c933790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c933c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c934070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c9344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c934950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c934dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c935230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c9356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c935e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c9360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c936550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c9369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c936e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c9372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c937710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c937b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c937ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c938460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c9388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c938d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c9391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c939620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c939a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c939f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c93a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c93a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c93ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c93b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c93b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c93b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c93be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c93c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c93c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c93cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c93cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c93d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c93d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c93dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c93e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c93e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c93ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c93eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c93f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c93f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c93ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c940250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c940800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c940d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c9413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c941880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c941d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c9421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c942a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c942cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c943280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c943830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c943de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c944390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c944940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c944ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c9454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c945a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c946000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c9465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c946b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c947110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c9476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c947c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c948220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c9487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c948d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c949330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c9498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c949e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c94a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c94a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c94afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c94b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c94bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c94c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c94c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c94cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c94d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c94d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c94dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c94e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c94e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c94ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c94f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c94f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c94ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c9504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c950aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c951050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c951600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c951bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c952160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c952710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c952cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c953270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c953820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c953dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c954380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c954930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c954ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c955490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c955a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c955ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c9565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c956b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c957050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c957550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c957a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c957f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c958450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c958950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c958e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c959350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c959850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c959d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c95a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c95a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c95ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c95b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11c95b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11c95bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11c95c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11c95c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11c95ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11c95cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11c95d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11c95d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11c95de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11c95e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c95e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c95f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c95f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c9600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c9607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c960a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c961210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c9616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c961b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ce16ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ce14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ce23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ce213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ce1f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ce1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ce15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ce12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ce17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ce18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ce1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ce1ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ce22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ce15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ce16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ce1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ce196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ce1b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ce16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ce21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ce1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ce12500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ce24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ce2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ce18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ce1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ce1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ce15de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ce202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ce14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ce23050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ce20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ce174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ce1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ce25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ce13bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ce24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ce13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ce23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ce1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ce20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ce30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ce31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ce31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ce31e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ce32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ce32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ce32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ce33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ce33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ce33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ce34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ce34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ce34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ce35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ce35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ce35a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ce35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ce363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ce36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ce36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ce37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ce375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ce37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ce37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ce38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ce38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ce38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ce39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ce394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ce39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ce39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ce3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ce3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ce3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ce3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ce3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ce3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ce3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ce3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ce3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ce3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ce3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ce3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ce3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ce3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ce3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ce3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ce3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ce3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ce3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ce3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ce3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ce3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ce403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ce40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ce40c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ce41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ce41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ce419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ce41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ce422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ce42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ce42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ce43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ce43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ce438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ce43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ce441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ce44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ce44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ce44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ce45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ce45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ce45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ce460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ce46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ce469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ce46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ce472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ce47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ce47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ce47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ce48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ce488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ce48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ce491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ce49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ce49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ce49f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ce4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ce4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ce4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ce4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ce4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ce4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ce4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ce4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ce4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ce4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ce4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ce4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ce4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ce4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ce4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ce4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ce4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ce4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ce4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ce4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ce4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ce500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ce50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ce50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ce50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ce51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ce516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ce51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ce51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ce52420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ce52890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ce52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ce53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ce535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ce53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ce53ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ce54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ce547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ce54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ce55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ce554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ce55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ce55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ce56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ce566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ce56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ce56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ce57400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ce57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ce57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ce58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ce585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ce58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ce58ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ce59310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ce59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ce59bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ce5a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ce5a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ce5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ce5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ce5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ce5b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ce5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ce5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ce5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ce5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ce5ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ce5d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ce5d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ce5db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ce5df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ce5e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ce5ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ce5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ce5f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ce5fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ce60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ce604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ce60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ce611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ce614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ce61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ce62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ce625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ce62b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ce63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ce636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ce63c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ce64220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ce647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ce64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ce65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ce658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ce65e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ce66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ce669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ce66fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ce67550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ce67b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ce680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ce68660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ce68c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ce691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ce69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ce69d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ce6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ce6a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ce6ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ce6b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ce6b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ce6bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ce6c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ce6caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ce6d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ce6d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ce6dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ce6e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ce6e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ce6ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ce6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ce6f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ce6fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ce70380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ce70930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ce70ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ce71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ce71a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ce71ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ce725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ce72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ce73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ce736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ce73c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ce74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ce747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ce74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ce75320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ce75820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ce75d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ce76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ce76720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ce76c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ce77120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ce77620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ce77b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ce78020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ce78520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ce78a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ce78f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ce79420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ce79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11ce79e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11ce7a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11ce7a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11ce7ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11ce7b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11ce7b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11ce7bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11ce7c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11ce7c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11ce7cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ce7d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ce7da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ce7e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ce7e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ce7ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ce7f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ce7f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ce7fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ce80320 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.711s
user	0m0.265s
sys	0m0.252s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4857 (0fd7ca7a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a008fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a0096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a009c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a00a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a00a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a00ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a00b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a00b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a00beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a00c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a00c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a00cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a00d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a00e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a00e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a00efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a00f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a00fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a010510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a010ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a011400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a012240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a013200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a0136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a013b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a0141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a014680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a014b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a014de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a0154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a015790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a015c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a0160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a016a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a016eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a017350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a0177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a017c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a018130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a0185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a018a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a018d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a019240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a019750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a01a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a01a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a01aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a01af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a01b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a01b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a01bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a01c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a01c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a01caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a01cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a01d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a01dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a01e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a01e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a01ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a01eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a01f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a01f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a01fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a020140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a020a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a020f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a0213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a021910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a021e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a0223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a022900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a022e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a0233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a0238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a023e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a024390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a0248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a024e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a025380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a0258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a025e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a026370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a0268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a026e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a027360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a0278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a027e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a028350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a0288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a028df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a029340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a019c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a0297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a029f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a02a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a02aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a02af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a02b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a02b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a02bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a02c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a02c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a02cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a02d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a02d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a02df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a02e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a02e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a02edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a02f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a02f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a02fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a030030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a0304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a030970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a030e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a0312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a031750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a031bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a032090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a032530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a0329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a032e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a033310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a0337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a033c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a0340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a034590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a034a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a034ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a035370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a035810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a035cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a036150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a0365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a036a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a036f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a0373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a037870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a0381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a038650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a038af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a038f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a039430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a0398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a039d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a03a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a03a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a03ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a03aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a03b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a03b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a03bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a03c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a03c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a03cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a03d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a03d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a03d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a03de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a03e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a03e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a03ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a03f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a03f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a03f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a03fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a040330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a0407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a040c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a041110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a0415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a041a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a041ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a042390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a042830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a042cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a043170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a043610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a043ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a043f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a0443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a044890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a044d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a0451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a045670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a045bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a046110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a046660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a046bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a047050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a0474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a047990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a047e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a0482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a048770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a048cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a049160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a049600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a049aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a049f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a04a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a04a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a04b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a04b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a04b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a04be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a04c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a04c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a04cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a04d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a04dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a04e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a04e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a04ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a04f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a04f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a04fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a0502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a050850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a050e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a0513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a051960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a051f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a0524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a052a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a053020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a0535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a053b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a054130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a0546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a054c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a055240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a0557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a055da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a056350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a056900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a056eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a057460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a057a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a057fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a058570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a058b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a0590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a059680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a059c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a05a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a05a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a05ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a05b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a05b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a05be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a05c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a05c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a05cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a05d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a05dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a05e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a05e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a05ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a05f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a05f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a05fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a060080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a060580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a060a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a060f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a061480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a061980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a061e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a062380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a062880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a062d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a063280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a063780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15a063c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15a064180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15a064680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15a064b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15a065080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15a065580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15a065a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15a065f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15a066480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15a066980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a066e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a067890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a067fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a0686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a068df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a0690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a069840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a069ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a06a180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a04bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a054f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a053e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a050b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a04e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a05d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a05b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a058de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a056bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a04eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a04c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a051670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a052780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a057cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a0549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a05c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a04f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a050560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a057720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a059940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a0521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a0532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a058830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a055500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a055ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a04ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a0510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a05dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a05b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a04d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a056610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a04c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a04dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a05e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a053890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a067140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a05bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a051c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a0543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a058280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a04fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a059ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a04e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a05cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a05a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a056060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a05ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a04d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a05e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a04cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a05d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a057170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a059390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a05c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a05aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a052d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a0150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a06a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a06a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a06a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a06ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a06af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a06b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a06b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a06b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a06ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a06bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a06bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a06c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a06c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a06c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a06cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a06cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a06d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a06d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a06d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a06d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a06db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a06de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a06e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a06e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a06e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a06e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a06ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a06ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a06f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a06f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a06f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a06f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a06fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a06ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a0701c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a070480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a070740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a070a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a070cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a070f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a071240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a071500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a0717c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a071a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a071d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a072000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a0722c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a072580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a072840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a072b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a072dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a073080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a073340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a073600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a0738c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a073b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a073e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a074100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a0743c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a074680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a074940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a074c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a074ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a075180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a075440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a075700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a0759c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a075c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a075f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a076200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a0764c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a076780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a076a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a076d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a076fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a077280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a077540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a077800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a077ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a077d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a078040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a078300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a0785c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a078880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a078b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a078e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a0790c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a079380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a079640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a079900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a079bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a079e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a07a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a07a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a07a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a07a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a07ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a07af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a07b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a07b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a07b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a07ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a07bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a07bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a07c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a07c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a07c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a07ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a07cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a07d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a07d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a07d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a07d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a07db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a07ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a07e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a07e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a07e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a07e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a07eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a07ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a07f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a07f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a07f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a07f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a07fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a0800a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a080540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a0809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a080e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a081320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a0817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a081c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a082100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a0825a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a082a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a082f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a0834e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a083a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a083f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a084240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a084500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a084a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a084f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a085400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a085910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a085e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a0863d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a0868d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a086de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a0874d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a087790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a087ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a088740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a088a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a088fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a089580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a089b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a08a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a08a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a08ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a08b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a08b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a08bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a08c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a08c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a08cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a08d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a08da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a08e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a08e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a08ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a08f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a08f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a08fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a0902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a090880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a090e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a091400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a0919c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a091f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a092540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a092b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a0930c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a093680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a093c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a094200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a0947c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a094d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a095340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a095900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a095ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a096480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a096a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a097000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a0975c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a097b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a098140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a098700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a098cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a099280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a099840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a099e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a09a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a09a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a09af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a09b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a09bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a09c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a09c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a09cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a09d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a09d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a09db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a09e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a09e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a09ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a09ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a09f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a09f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a09fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a0a0300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a0a0800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a0a0d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a0a1200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15a0a1700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15a0a1c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15a0a2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15a0a2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15a0a2b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15a0a3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15a0a3500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15a0a3a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15a0a3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15a0a4400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a0a4900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a0a5310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a0a5a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a0a6150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a0a6870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a0a6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a0a72c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a0a7580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a0a7a90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1587091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158709660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158709ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15870a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15870a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15870d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15870d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15870d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15870de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15870e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15870e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15870ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15870f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1587100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1587108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158711010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158711730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158711e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158712570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158712ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1587133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158714200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158714920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158715040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158715300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1587155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158715a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158715ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158716310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1587168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158716de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1587174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158717960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1587182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158718740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158718be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158719080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158719520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1587199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158719e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15871a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15871a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15871ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15871b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15871b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15871ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15871bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15871c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15871c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15871cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15871d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15871d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15871da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15871df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15871e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15871e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15871e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15871edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15871f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15871f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15871fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15871ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1587203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158720850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158720cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158721130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1587215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158721a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1587222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158722760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158722bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158723040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1587234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158723920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158723d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158724200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158724670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158724ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1587253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158725830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158725ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158726110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1587269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1587272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158727740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158727bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158728020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158728490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158728900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158728d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1587291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158729d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15872a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15872a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15872adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15872b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15872b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15872bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15872c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15872ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15872cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15872d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15872db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15872e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15872e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15872ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15872f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15872f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15872fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1587300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1587305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158730af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158730ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1587314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1587319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158731ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1587323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1587328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158732df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1587332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1587337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158733cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1587341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1587346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158734bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1587350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1587355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158735ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1587364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1587369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158736ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1587373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1587378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158737df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1587382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1587387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158738cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1587391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1587396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15873a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15873a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15873aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15873aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15873b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15873b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15873bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15873c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15873c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15873cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15873d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15873d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15873dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15873e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15873e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15873ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15873f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15873f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15873faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15873fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1587404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1587409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158740ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1587413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1587418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158741df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1587422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1587427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158742cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1587431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1587436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158743bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1587440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1587445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158744af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158744ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1587454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1587459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1587463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1587468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158746df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1587472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1587477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158747cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1587481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1587487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158748d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158749300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1587498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158749db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15874a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15874a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15874ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15874b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15874b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15874bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15874c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15874c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15874cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15874d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15874d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15874ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15874e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15874e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15874ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15874f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15874f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15874fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158750290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1587513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158751950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158751f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1587524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158752a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158753010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1587535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158754120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1587546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158754c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158755230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1587557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158755d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158756340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1587568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158757450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158757fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158758560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158758b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1587590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158759670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158759c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15875a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15875a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15875ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15875b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15875b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15875be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15875c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15875c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15875cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15875d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15875dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15875e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15875e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15875ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15875f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15875f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15875fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158760280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158760830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158760de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158761940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158761ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1587623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1587628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158762df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1587632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1587637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158763cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1587641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1587646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1587650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1587655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158765af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158765ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1587664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1587669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x158766ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1587673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1587678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x158767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1587682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1587687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x158768cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1587691f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1587696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158769bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15876a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15876ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15876b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15876bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15876be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15876c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15876ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15876cef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.231s
sys	0m0.185s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.02 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.47 sec*proc (2 tests)

Total Test time (real) =   1.48 sec
        1.50 real         0.52 user         0.19 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.27 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.57 sec*proc (2 tests)

Total Test time (real) =   0.58 sec
        0.58 real         0.13 user         0.08 sys
```
