Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- Performing Test GGML_COMPILER_SUPPORT_I8MM
-- Performing Test GGML_COMPILER_SUPPORT_I8MM - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.9s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.144s
user	0m0.758s
sys	0m1.084s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Built target sha1
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target build_info
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 22%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX static library libcommon.a
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 35%] Built target llama-simple
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 35%] Built target llama-simple-chat
[ 35%] Built target llava_static
[ 35%] Built target common
[ 35%] Built target llava_shared
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-sampling
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Built target test-chat-template
[ 63%] Built target test-autorelease
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-barrier
[ 63%] Built target llama-batched-bench
[ 63%] Built target test-rope
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-batched
[ 72%] Linking CXX executable ../../bin/llama-embedding
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-bench
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-lookup
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Linking CXX executable ../../bin/llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-passkey
[ 83%] Built target llama-parallel
[ 83%] Built target llama-cli
[ 83%] Built target llama-perplexity
[ 83%] Built target llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 90%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 91%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 93%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-run
[ 93%] Linking CXX executable ../../bin/llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-speculative
[ 93%] Built target llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.012s
user	0m5.888s
sys	0m9.991s

main: quantize time =  3553.25 ms
main:    total time =  3553.25 ms

main: quantize time =  1819.94 ms
main:    total time =  1819.94 ms

main: quantize time =  1828.33 ms
main:    total time =  1828.33 ms

main: quantize time =  2075.61 ms
main:    total time =  2075.61 ms

main: quantize time =  2391.76 ms
main:    total time =  2391.76 ms

main: quantize time =  5380.81 ms
main:    total time =  5380.81 ms

main: quantize time =  5864.42 ms
main:    total time =  5864.42 ms

main: quantize time =  6930.20 ms
main:    total time =  6930.20 ms

main: quantize time =  5965.25 ms
main:    total time =  5965.25 ms

main: quantize time =  4455.85 ms
main:    total time =  4455.85 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.112 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.239 I main: llama backend init
0.00.000.251 I main: load the model and apply lora adapter, if any
0.00.029.691 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.913 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.936 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.149 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.646 I llama_model_loader: - type  f32:  194 tensors
0.00.059.647 I llama_model_loader: - type  f16:   98 tensors
0.00.093.944 I llm_load_vocab: special tokens cache size = 25
0.00.101.096 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.118 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.119 I llm_load_print_meta: arch             = gptneox
0.00.101.119 I llm_load_print_meta: vocab type       = BPE
0.00.101.120 I llm_load_print_meta: n_vocab          = 50304
0.00.101.120 I llm_load_print_meta: n_merges         = 50009
0.00.101.120 I llm_load_print_meta: vocab_only       = 0
0.00.101.120 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.120 I llm_load_print_meta: n_embd           = 2048
0.00.101.121 I llm_load_print_meta: n_layer          = 24
0.00.101.124 I llm_load_print_meta: n_head           = 16
0.00.101.125 I llm_load_print_meta: n_head_kv        = 16
0.00.101.125 I llm_load_print_meta: n_rot            = 32
0.00.101.125 I llm_load_print_meta: n_swa            = 0
0.00.101.125 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.126 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.127 I llm_load_print_meta: n_gqa            = 1
0.00.101.127 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.128 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.128 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.129 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.129 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.129 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.129 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.130 I llm_load_print_meta: n_ff             = 8192
0.00.101.130 I llm_load_print_meta: n_expert         = 0
0.00.101.132 I llm_load_print_meta: n_expert_used    = 0
0.00.101.132 I llm_load_print_meta: causal attn      = 1
0.00.101.132 I llm_load_print_meta: pooling type     = 0
0.00.101.132 I llm_load_print_meta: rope type        = 2
0.00.101.133 I llm_load_print_meta: rope scaling     = linear
0.00.101.133 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.133 I llm_load_print_meta: freq_scale_train = 1
0.00.101.133 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.138 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.138 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.138 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.138 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.138 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.138 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.101.140 I llm_load_print_meta: model type       = 1.4B
0.00.101.141 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.101.141 I llm_load_print_meta: model params     = 1.41 B
0.00.101.142 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.101.142 I llm_load_print_meta: general.name     = 1.4B
0.00.101.142 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.101.142 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.101.142 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.101.143 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.101.143 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.101.143 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.101.144 I llm_load_print_meta: max token length = 1024
0.00.103.854 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.103.854 I llm_load_tensors: offloading output layer to GPU
0.00.103.854 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.103.873 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.874 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.104.844 I llama_new_context_with_model: n_seq_max     = 1
0.00.104.845 I llama_new_context_with_model: n_ctx         = 2048
0.00.104.845 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.104.846 I llama_new_context_with_model: n_batch       = 2048
0.00.104.846 I llama_new_context_with_model: n_ubatch      = 512
0.00.104.846 I llama_new_context_with_model: flash_attn    = 0
0.00.104.847 I llama_new_context_with_model: freq_base     = 10000.0
0.00.104.847 I llama_new_context_with_model: freq_scale    = 1
0.00.104.847 I ggml_metal_init: allocating
0.00.104.858 I ggml_metal_init: found device: Apple M4
0.00.104.861 I ggml_metal_init: picking default device: Apple M4
0.00.105.577 I ggml_metal_init: using embedded metal library
0.00.115.916 I ggml_metal_init: GPU name:   Apple M4
0.00.115.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.921 I ggml_metal_init: simdgroup reduction   = true
0.00.115.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.922 I ggml_metal_init: has bfloat            = true
0.00.115.922 I ggml_metal_init: use bfloat            = true
0.00.115.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.791 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.162.649 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.655 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.162.683 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.677 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.679 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.679 I llama_new_context_with_model: graph nodes  = 967
0.00.163.679 I llama_new_context_with_model: graph splits = 2
0.00.163.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.163.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.163.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.246.388 I main: llama threadpool init, n_threads = 4
0.00.246.422 I 
0.00.246.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.246.464 I 
0.00.246.538 I sampler seed: 1234
0.00.246.542 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.246.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.246.567 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.246.567 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.091.727 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.02.091.727 I llama_perf_context_print:        load time =     216.69 ms
0.02.091.728 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.34 tokens per second)
0.02.091.729 I llama_perf_context_print:        eval time =    1798.47 ms /    63 runs   (   28.55 ms per token,    35.03 tokens per second)
0.02.091.733 I llama_perf_context_print:       total time =    1845.34 ms /    70 tokens
0.02.091.903 I ggml_metal_free: deallocating

real	0m2.407s
user	0m0.149s
sys	0m0.105s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.862 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.717 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.721 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.730 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.402 I llama_model_loader: - type  f32:  194 tensors
0.00.031.402 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.801 I llm_load_vocab: special tokens cache size = 25
0.00.058.737 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.756 I llm_load_print_meta: arch             = gptneox
0.00.058.756 I llm_load_print_meta: vocab type       = BPE
0.00.058.757 I llm_load_print_meta: n_vocab          = 50304
0.00.058.757 I llm_load_print_meta: n_merges         = 50009
0.00.058.757 I llm_load_print_meta: vocab_only       = 0
0.00.058.757 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.757 I llm_load_print_meta: n_embd           = 2048
0.00.058.758 I llm_load_print_meta: n_layer          = 24
0.00.058.763 I llm_load_print_meta: n_head           = 16
0.00.058.764 I llm_load_print_meta: n_head_kv        = 16
0.00.058.764 I llm_load_print_meta: n_rot            = 32
0.00.058.764 I llm_load_print_meta: n_swa            = 0
0.00.058.766 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.766 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.767 I llm_load_print_meta: n_gqa            = 1
0.00.058.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.768 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.769 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.769 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.769 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.769 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.769 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.770 I llm_load_print_meta: n_ff             = 8192
0.00.058.770 I llm_load_print_meta: n_expert         = 0
0.00.058.770 I llm_load_print_meta: n_expert_used    = 0
0.00.058.770 I llm_load_print_meta: causal attn      = 1
0.00.058.770 I llm_load_print_meta: pooling type     = 0
0.00.058.771 I llm_load_print_meta: rope type        = 2
0.00.058.771 I llm_load_print_meta: rope scaling     = linear
0.00.058.771 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.772 I llm_load_print_meta: freq_scale_train = 1
0.00.058.777 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.777 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.777 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.777 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.777 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.778 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.778 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.778 I llm_load_print_meta: model type       = 1.4B
0.00.058.779 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.779 I llm_load_print_meta: model params     = 1.41 B
0.00.058.779 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.779 I llm_load_print_meta: general.name     = 1.4B
0.00.058.780 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.780 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.780 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.780 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.781 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.058.781 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.782 I llm_load_print_meta: max token length = 1024
0.00.061.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.238 I llm_load_tensors: offloading output layer to GPU
0.00.061.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.250 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.251 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.200 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.201 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.201 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.201 I llama_new_context_with_model: n_batch       = 2048
0.00.062.201 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.201 I llama_new_context_with_model: flash_attn    = 0
0.00.062.202 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.202 I llama_new_context_with_model: freq_scale    = 1
0.00.062.203 I ggml_metal_init: allocating
0.00.062.207 I ggml_metal_init: found device: Apple M4
0.00.062.209 I ggml_metal_init: picking default device: Apple M4
0.00.062.952 I ggml_metal_init: using embedded metal library
0.00.065.544 I ggml_metal_init: GPU name:   Apple M4
0.00.065.546 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.546 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.547 I ggml_metal_init: simdgroup reduction   = true
0.00.065.547 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.547 I ggml_metal_init: has bfloat            = true
0.00.065.547 I ggml_metal_init: use bfloat            = true
0.00.065.548 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.848 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.567 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.588 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.613 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.844 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.847 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.847 I llama_new_context_with_model: graph nodes  = 967
0.00.103.847 I llama_new_context_with_model: graph splits = 2
0.00.103.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.181.569 I main: llama threadpool init, n_threads = 4
0.01.181.612 I 
0.01.181.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.181.655 I 
0.01.181.906 I sampler seed: 1234
0.01.181.909 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.181.920 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.181.921 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.181.921 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.275.446 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.02.275.447 I llama_perf_context_print:        load time =    1171.70 ms
0.02.275.447 I llama_perf_context_print: prompt eval time =      43.25 ms /     7 tokens (    6.18 ms per token,   161.85 tokens per second)
0.02.275.448 I llama_perf_context_print:        eval time =    1047.32 ms /    63 runs   (   16.62 ms per token,    60.15 tokens per second)
0.02.275.448 I llama_perf_context_print:       total time =    1093.88 ms /    70 tokens
0.02.275.637 I ggml_metal_free: deallocating

real	0m2.294s
user	0m0.115s
sys	0m0.207s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.015.556 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.868 I llama_model_loader: - type  f32:  194 tensors
0.00.043.868 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.101 I llm_load_vocab: special tokens cache size = 25
0.00.078.985 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.002 I llm_load_print_meta: arch             = gptneox
0.00.079.002 I llm_load_print_meta: vocab type       = BPE
0.00.079.003 I llm_load_print_meta: n_vocab          = 50304
0.00.079.003 I llm_load_print_meta: n_merges         = 50009
0.00.079.003 I llm_load_print_meta: vocab_only       = 0
0.00.079.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.003 I llm_load_print_meta: n_embd           = 2048
0.00.079.003 I llm_load_print_meta: n_layer          = 24
0.00.079.007 I llm_load_print_meta: n_head           = 16
0.00.079.008 I llm_load_print_meta: n_head_kv        = 16
0.00.079.008 I llm_load_print_meta: n_rot            = 32
0.00.079.009 I llm_load_print_meta: n_swa            = 0
0.00.079.009 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.009 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.010 I llm_load_print_meta: n_gqa            = 1
0.00.079.010 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.011 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.012 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.012 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.012 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.012 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.013 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.013 I llm_load_print_meta: n_ff             = 8192
0.00.079.013 I llm_load_print_meta: n_expert         = 0
0.00.079.014 I llm_load_print_meta: n_expert_used    = 0
0.00.079.014 I llm_load_print_meta: causal attn      = 1
0.00.079.014 I llm_load_print_meta: pooling type     = 0
0.00.079.017 I llm_load_print_meta: rope type        = 2
0.00.079.017 I llm_load_print_meta: rope scaling     = linear
0.00.079.017 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.018 I llm_load_print_meta: freq_scale_train = 1
0.00.079.018 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.018 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.018 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.018 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.018 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.019 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.019 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.019 I llm_load_print_meta: model type       = 1.4B
0.00.079.020 I llm_load_print_meta: model ftype      = Q4_0
0.00.079.020 I llm_load_print_meta: model params     = 1.41 B
0.00.079.022 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.079.022 I llm_load_print_meta: general.name     = 1.4B
0.00.079.022 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.022 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.023 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.023 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.023 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.023 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.024 I llm_load_print_meta: max token length = 1024
0.00.081.516 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.516 I llm_load_tensors: offloading output layer to GPU
0.00.081.517 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.529 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.530 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.082.742 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.743 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.743 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.743 I llama_new_context_with_model: n_batch       = 2048
0.00.082.743 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.743 I llama_new_context_with_model: flash_attn    = 0
0.00.082.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.744 I llama_new_context_with_model: freq_scale    = 1
0.00.082.745 I ggml_metal_init: allocating
0.00.082.753 I ggml_metal_init: found device: Apple M4
0.00.082.756 I ggml_metal_init: picking default device: Apple M4
0.00.083.627 I ggml_metal_init: using embedded metal library
0.00.087.322 I ggml_metal_init: GPU name:   Apple M4
0.00.087.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.326 I ggml_metal_init: simdgroup reduction   = true
0.00.087.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.326 I ggml_metal_init: has bfloat            = true
0.00.087.326 I ggml_metal_init: use bfloat            = true
0.00.087.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.151 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.128 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.136 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.159 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.247 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.249 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.249 I llama_new_context_with_model: graph nodes  = 967
0.00.126.249 I llama_new_context_with_model: graph splits = 2
0.00.126.267 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.258 I main: llama threadpool init, n_threads = 4
0.00.698.301 I 
0.00.698.335 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.338 I 
0.00.698.558 I sampler seed: 1234
0.00.698.562 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.583 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.584 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.384.558 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.384.559 I llama_perf_context_print:        load time =     682.70 ms
0.01.384.560 I llama_perf_context_print: prompt eval time =      39.76 ms /     7 tokens (    5.68 ms per token,   176.07 tokens per second)
0.01.384.560 I llama_perf_context_print:        eval time =     643.05 ms /    63 runs   (   10.21 ms per token,    97.97 tokens per second)
0.01.384.561 I llama_perf_context_print:       total time =     686.30 ms /    70 tokens
0.01.384.717 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.126s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.914 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.920 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.923 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.923 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.270 I llama_model_loader: - type  f32:  194 tensors
0.00.037.270 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.060.250 I llm_load_vocab: special tokens cache size = 25
0.00.066.456 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.470 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.471 I llm_load_print_meta: arch             = gptneox
0.00.066.472 I llm_load_print_meta: vocab type       = BPE
0.00.066.472 I llm_load_print_meta: n_vocab          = 50304
0.00.066.472 I llm_load_print_meta: n_merges         = 50009
0.00.066.472 I llm_load_print_meta: vocab_only       = 0
0.00.066.472 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.473 I llm_load_print_meta: n_embd           = 2048
0.00.066.475 I llm_load_print_meta: n_layer          = 24
0.00.066.477 I llm_load_print_meta: n_head           = 16
0.00.066.478 I llm_load_print_meta: n_head_kv        = 16
0.00.066.478 I llm_load_print_meta: n_rot            = 32
0.00.066.479 I llm_load_print_meta: n_swa            = 0
0.00.066.479 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.479 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.480 I llm_load_print_meta: n_gqa            = 1
0.00.066.481 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.481 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.482 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.482 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.482 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.482 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.482 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.484 I llm_load_print_meta: n_ff             = 8192
0.00.066.484 I llm_load_print_meta: n_expert         = 0
0.00.066.484 I llm_load_print_meta: n_expert_used    = 0
0.00.066.486 I llm_load_print_meta: causal attn      = 1
0.00.066.487 I llm_load_print_meta: pooling type     = 0
0.00.066.487 I llm_load_print_meta: rope type        = 2
0.00.066.487 I llm_load_print_meta: rope scaling     = linear
0.00.066.488 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.488 I llm_load_print_meta: freq_scale_train = 1
0.00.066.488 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.489 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.489 I llm_load_print_meta: model type       = 1.4B
0.00.066.490 I llm_load_print_meta: model ftype      = Q4_1
0.00.066.490 I llm_load_print_meta: model params     = 1.41 B
0.00.066.491 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.066.491 I llm_load_print_meta: general.name     = 1.4B
0.00.066.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.491 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.491 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.493 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.493 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.493 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.493 I llm_load_print_meta: max token length = 1024
0.00.068.527 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.527 I llm_load_tensors: offloading output layer to GPU
0.00.068.527 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.538 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.068.539 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.069.479 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.480 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.480 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.480 I llama_new_context_with_model: n_batch       = 2048
0.00.069.480 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.480 I llama_new_context_with_model: flash_attn    = 0
0.00.069.481 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.481 I llama_new_context_with_model: freq_scale    = 1
0.00.069.481 I ggml_metal_init: allocating
0.00.069.489 I ggml_metal_init: found device: Apple M4
0.00.069.491 I ggml_metal_init: picking default device: Apple M4
0.00.070.115 I ggml_metal_init: using embedded metal library
0.00.072.560 I ggml_metal_init: GPU name:   Apple M4
0.00.072.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.563 I ggml_metal_init: simdgroup reduction   = true
0.00.072.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.563 I ggml_metal_init: has bfloat            = true
0.00.072.564 I ggml_metal_init: use bfloat            = true
0.00.072.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.808 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.349 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.356 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.372 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.345 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.346 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.347 I llama_new_context_with_model: graph nodes  = 967
0.00.103.347 I llama_new_context_with_model: graph splits = 2
0.00.103.362 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.511 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.024 I main: llama threadpool init, n_threads = 4
0.00.817.063 I 
0.00.817.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.093 I 
0.00.817.324 I sampler seed: 1234
0.00.817.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.374 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.375 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.542.811 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.542.811 I llama_perf_context_print:        load time =     808.26 ms
0.01.542.812 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.34 tokens per second)
0.01.542.813 I llama_perf_context_print:        eval time =     679.42 ms /    63 runs   (   10.78 ms per token,    92.73 tokens per second)
0.01.542.813 I llama_perf_context_print:       total time =     725.79 ms /    70 tokens
0.01.542.970 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.113s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.016.713 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.774 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.786 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.787 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.791 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.087 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.090 I llama_model_loader: - type  f32:  194 tensors
0.00.035.091 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.091 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.064 I llm_load_vocab: special tokens cache size = 25
0.00.065.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.420 I llm_load_print_meta: arch             = gptneox
0.00.065.420 I llm_load_print_meta: vocab type       = BPE
0.00.065.421 I llm_load_print_meta: n_vocab          = 50304
0.00.065.421 I llm_load_print_meta: n_merges         = 50009
0.00.065.421 I llm_load_print_meta: vocab_only       = 0
0.00.065.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.421 I llm_load_print_meta: n_embd           = 2048
0.00.065.421 I llm_load_print_meta: n_layer          = 24
0.00.065.424 I llm_load_print_meta: n_head           = 16
0.00.065.425 I llm_load_print_meta: n_head_kv        = 16
0.00.065.427 I llm_load_print_meta: n_rot            = 32
0.00.065.427 I llm_load_print_meta: n_swa            = 0
0.00.065.427 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.427 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.428 I llm_load_print_meta: n_gqa            = 1
0.00.065.428 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.429 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.429 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.431 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.432 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.432 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.432 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.433 I llm_load_print_meta: n_ff             = 8192
0.00.065.433 I llm_load_print_meta: n_expert         = 0
0.00.065.433 I llm_load_print_meta: n_expert_used    = 0
0.00.065.435 I llm_load_print_meta: causal attn      = 1
0.00.065.435 I llm_load_print_meta: pooling type     = 0
0.00.065.435 I llm_load_print_meta: rope type        = 2
0.00.065.435 I llm_load_print_meta: rope scaling     = linear
0.00.065.436 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.436 I llm_load_print_meta: freq_scale_train = 1
0.00.065.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.440 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.444 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.446 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.446 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.446 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.446 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.447 I llm_load_print_meta: model type       = 1.4B
0.00.065.447 I llm_load_print_meta: model ftype      = Q5_0
0.00.065.448 I llm_load_print_meta: model params     = 1.41 B
0.00.065.448 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.065.448 I llm_load_print_meta: general.name     = 1.4B
0.00.065.449 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.449 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.449 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.450 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.451 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.452 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.452 I llm_load_print_meta: max token length = 1024
0.00.067.563 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.563 I llm_load_tensors: offloading output layer to GPU
0.00.067.563 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.574 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.067.575 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.068.545 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.547 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.547 I llama_new_context_with_model: n_batch       = 2048
0.00.068.547 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.547 I llama_new_context_with_model: flash_attn    = 0
0.00.068.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.548 I llama_new_context_with_model: freq_scale    = 1
0.00.068.548 I ggml_metal_init: allocating
0.00.068.551 I ggml_metal_init: found device: Apple M4
0.00.068.553 I ggml_metal_init: picking default device: Apple M4
0.00.069.184 I ggml_metal_init: using embedded metal library
0.00.071.685 I ggml_metal_init: GPU name:   Apple M4
0.00.071.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.689 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.689 I ggml_metal_init: simdgroup reduction   = true
0.00.071.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.690 I ggml_metal_init: has bfloat            = true
0.00.071.690 I ggml_metal_init: use bfloat            = true
0.00.071.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.691 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.642 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.651 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.730 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.732 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.732 I llama_new_context_with_model: graph nodes  = 967
0.00.105.733 I llama_new_context_with_model: graph splits = 2
0.00.105.748 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.896 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.678 I main: llama threadpool init, n_threads = 4
0.00.811.713 I 
0.00.811.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.743 I 
0.00.811.956 I sampler seed: 1234
0.00.811.961 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.985 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.987 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.987 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.603.777 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.603.778 I llama_perf_context_print:        load time =     794.96 ms
0.01.603.779 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.30 tokens per second)
0.01.603.780 I llama_perf_context_print:        eval time =     745.61 ms /    63 runs   (   11.84 ms per token,    84.50 tokens per second)
0.01.603.780 I llama_perf_context_print:       total time =     792.10 ms /    70 tokens
0.01.603.944 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.116s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.107 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.830 I llama_model_loader: - type  f32:  194 tensors
0.00.035.830 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.807 I llm_load_vocab: special tokens cache size = 25
0.00.066.345 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.360 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.361 I llm_load_print_meta: arch             = gptneox
0.00.066.362 I llm_load_print_meta: vocab type       = BPE
0.00.066.362 I llm_load_print_meta: n_vocab          = 50304
0.00.066.362 I llm_load_print_meta: n_merges         = 50009
0.00.066.362 I llm_load_print_meta: vocab_only       = 0
0.00.066.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.362 I llm_load_print_meta: n_embd           = 2048
0.00.066.363 I llm_load_print_meta: n_layer          = 24
0.00.066.365 I llm_load_print_meta: n_head           = 16
0.00.066.366 I llm_load_print_meta: n_head_kv        = 16
0.00.066.366 I llm_load_print_meta: n_rot            = 32
0.00.066.366 I llm_load_print_meta: n_swa            = 0
0.00.066.366 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.366 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.367 I llm_load_print_meta: n_gqa            = 1
0.00.066.368 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.368 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.369 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.370 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.371 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.371 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.371 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.372 I llm_load_print_meta: n_ff             = 8192
0.00.066.380 I llm_load_print_meta: n_expert         = 0
0.00.066.380 I llm_load_print_meta: n_expert_used    = 0
0.00.066.382 I llm_load_print_meta: causal attn      = 1
0.00.066.383 I llm_load_print_meta: pooling type     = 0
0.00.066.384 I llm_load_print_meta: rope type        = 2
0.00.066.384 I llm_load_print_meta: rope scaling     = linear
0.00.066.384 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.385 I llm_load_print_meta: freq_scale_train = 1
0.00.066.385 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.385 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.385 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.385 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.385 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.386 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.386 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.386 I llm_load_print_meta: model type       = 1.4B
0.00.066.386 I llm_load_print_meta: model ftype      = Q5_1
0.00.066.387 I llm_load_print_meta: model params     = 1.41 B
0.00.066.387 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.066.394 I llm_load_print_meta: general.name     = 1.4B
0.00.066.395 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.395 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.395 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.395 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.395 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.396 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.396 I llm_load_print_meta: max token length = 1024
0.00.068.584 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.584 I llm_load_tensors: offloading output layer to GPU
0.00.068.585 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.595 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.068.596 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.069.582 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.583 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.584 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.584 I llama_new_context_with_model: n_batch       = 2048
0.00.069.584 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.584 I llama_new_context_with_model: flash_attn    = 0
0.00.069.585 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.585 I llama_new_context_with_model: freq_scale    = 1
0.00.069.585 I ggml_metal_init: allocating
0.00.069.592 I ggml_metal_init: found device: Apple M4
0.00.069.595 I ggml_metal_init: picking default device: Apple M4
0.00.070.213 I ggml_metal_init: using embedded metal library
0.00.072.894 I ggml_metal_init: GPU name:   Apple M4
0.00.072.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.896 I ggml_metal_init: simdgroup reduction   = true
0.00.072.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.898 I ggml_metal_init: has bfloat            = true
0.00.072.898 I ggml_metal_init: use bfloat            = true
0.00.072.899 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.682 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.692 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.709 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.787 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.789 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.789 I llama_new_context_with_model: graph nodes  = 967
0.00.106.789 I llama_new_context_with_model: graph splits = 2
0.00.106.805 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.934.723 I main: llama threadpool init, n_threads = 4
0.00.934.763 I 
0.00.934.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.934.792 I 
0.00.935.019 I sampler seed: 1234
0.00.935.024 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.935.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.935.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.935.070 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.778.409 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.778.409 I llama_perf_context_print:        load time =     926.04 ms
0.01.778.410 I llama_perf_context_print: prompt eval time =      44.80 ms /     7 tokens (    6.40 ms per token,   156.25 tokens per second)
0.01.778.411 I llama_perf_context_print:        eval time =     795.62 ms /    63 runs   (   12.63 ms per token,    79.18 tokens per second)
0.01.778.411 I llama_perf_context_print:       total time =     843.69 ms /    70 tokens
0.01.778.608 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.116s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.016.245 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.081 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.023.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.086 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.087 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.092 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.093 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.385 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.386 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.387 I llama_model_loader: - type  f32:  194 tensors
0.00.033.388 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.388 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.388 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.008 I llm_load_vocab: special tokens cache size = 25
0.00.068.275 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.291 I llm_load_print_meta: arch             = gptneox
0.00.068.292 I llm_load_print_meta: vocab type       = BPE
0.00.068.292 I llm_load_print_meta: n_vocab          = 50304
0.00.068.292 I llm_load_print_meta: n_merges         = 50009
0.00.068.292 I llm_load_print_meta: vocab_only       = 0
0.00.068.293 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.293 I llm_load_print_meta: n_embd           = 2048
0.00.068.293 I llm_load_print_meta: n_layer          = 24
0.00.068.295 I llm_load_print_meta: n_head           = 16
0.00.068.296 I llm_load_print_meta: n_head_kv        = 16
0.00.068.296 I llm_load_print_meta: n_rot            = 32
0.00.068.298 I llm_load_print_meta: n_swa            = 0
0.00.068.299 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.299 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.299 I llm_load_print_meta: n_gqa            = 1
0.00.068.300 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.305 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.305 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.306 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.306 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.306 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.306 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.307 I llm_load_print_meta: n_ff             = 8192
0.00.068.307 I llm_load_print_meta: n_expert         = 0
0.00.068.308 I llm_load_print_meta: n_expert_used    = 0
0.00.068.308 I llm_load_print_meta: causal attn      = 1
0.00.068.309 I llm_load_print_meta: pooling type     = 0
0.00.068.309 I llm_load_print_meta: rope type        = 2
0.00.068.309 I llm_load_print_meta: rope scaling     = linear
0.00.068.310 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.310 I llm_load_print_meta: freq_scale_train = 1
0.00.068.310 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.310 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.311 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.311 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.311 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.311 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.311 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.311 I llm_load_print_meta: model type       = 1.4B
0.00.068.312 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.068.312 I llm_load_print_meta: model params     = 1.41 B
0.00.068.313 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.068.313 I llm_load_print_meta: general.name     = 1.4B
0.00.068.313 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.313 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.314 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.314 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.314 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.315 I llm_load_print_meta: max token length = 1024
0.00.070.522 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.523 I llm_load_tensors: offloading output layer to GPU
0.00.070.523 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.534 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.070.535 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.071.558 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.559 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.559 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.559 I llama_new_context_with_model: n_batch       = 2048
0.00.071.559 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.559 I llama_new_context_with_model: flash_attn    = 0
0.00.071.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.560 I llama_new_context_with_model: freq_scale    = 1
0.00.071.561 I ggml_metal_init: allocating
0.00.071.565 I ggml_metal_init: found device: Apple M4
0.00.071.567 I ggml_metal_init: picking default device: Apple M4
0.00.072.173 I ggml_metal_init: using embedded metal library
0.00.074.837 I ggml_metal_init: GPU name:   Apple M4
0.00.074.838 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.839 I ggml_metal_init: simdgroup reduction   = true
0.00.074.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.840 I ggml_metal_init: has bfloat            = true
0.00.074.840 I ggml_metal_init: use bfloat            = true
0.00.074.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.595 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.515 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.523 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.541 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.525 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.526 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.526 I llama_new_context_with_model: graph nodes  = 967
0.00.110.527 I llama_new_context_with_model: graph splits = 2
0.00.110.536 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.340 I main: llama threadpool init, n_threads = 4
0.00.536.384 I 
0.00.536.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.418 I 
0.00.536.651 I sampler seed: 1234
0.00.536.659 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.536.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.536.701 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.536.702 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.217.540 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63279.86 tokens per second)
0.01.217.540 I llama_perf_context_print:        load time =     520.09 ms
0.01.217.541 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.217.544 I llama_perf_context_print:        eval time =     642.32 ms /    63 runs   (   10.20 ms per token,    98.08 tokens per second)
0.01.217.544 I llama_perf_context_print:       total time =     681.20 ms /    70 tokens
0.01.217.740 I ggml_metal_free: deallocating

real	0m1.244s
user	0m0.125s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.918 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.026.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.522 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.523 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.524 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.528 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.631 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.634 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.634 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.634 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.635 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.635 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.035.636 I llama_model_loader: - type  f32:  194 tensors
0.00.035.636 I llama_model_loader: - type q3_K:   25 tensors
0.00.035.636 I llama_model_loader: - type q4_K:   71 tensors
0.00.035.637 I llama_model_loader: - type q5_K:    1 tensors
0.00.035.637 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.907 I llm_load_vocab: special tokens cache size = 25
0.00.063.094 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.113 I llm_load_print_meta: arch             = gptneox
0.00.063.113 I llm_load_print_meta: vocab type       = BPE
0.00.063.114 I llm_load_print_meta: n_vocab          = 50304
0.00.063.114 I llm_load_print_meta: n_merges         = 50009
0.00.063.114 I llm_load_print_meta: vocab_only       = 0
0.00.063.114 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.114 I llm_load_print_meta: n_embd           = 2048
0.00.063.114 I llm_load_print_meta: n_layer          = 24
0.00.063.118 I llm_load_print_meta: n_head           = 16
0.00.063.119 I llm_load_print_meta: n_head_kv        = 16
0.00.063.119 I llm_load_print_meta: n_rot            = 32
0.00.063.119 I llm_load_print_meta: n_swa            = 0
0.00.063.119 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.120 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.120 I llm_load_print_meta: n_gqa            = 1
0.00.063.121 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.129 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.129 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.130 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.130 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.130 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.130 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.131 I llm_load_print_meta: n_ff             = 8192
0.00.063.131 I llm_load_print_meta: n_expert         = 0
0.00.063.131 I llm_load_print_meta: n_expert_used    = 0
0.00.063.131 I llm_load_print_meta: causal attn      = 1
0.00.063.131 I llm_load_print_meta: pooling type     = 0
0.00.063.132 I llm_load_print_meta: rope type        = 2
0.00.063.132 I llm_load_print_meta: rope scaling     = linear
0.00.063.132 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.132 I llm_load_print_meta: freq_scale_train = 1
0.00.063.132 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.133 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.133 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.133 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.133 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.133 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.133 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.134 I llm_load_print_meta: model type       = 1.4B
0.00.063.134 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.063.134 I llm_load_print_meta: model params     = 1.41 B
0.00.063.135 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.063.135 I llm_load_print_meta: general.name     = 1.4B
0.00.063.135 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.140 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.140 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.140 I llm_load_print_meta: max token length = 1024
0.00.065.095 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.096 I llm_load_tensors: offloading output layer to GPU
0.00.065.096 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.106 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.065.108 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.066.040 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.041 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.041 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.041 I llama_new_context_with_model: n_batch       = 2048
0.00.066.041 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.042 I llama_new_context_with_model: flash_attn    = 0
0.00.066.042 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.042 I llama_new_context_with_model: freq_scale    = 1
0.00.066.042 I ggml_metal_init: allocating
0.00.066.046 I ggml_metal_init: found device: Apple M4
0.00.066.048 I ggml_metal_init: picking default device: Apple M4
0.00.066.700 I ggml_metal_init: using embedded metal library
0.00.069.116 I ggml_metal_init: GPU name:   Apple M4
0.00.069.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.118 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.118 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.119 I ggml_metal_init: simdgroup reduction   = true
0.00.069.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.119 I ggml_metal_init: has bfloat            = true
0.00.069.119 I ggml_metal_init: use bfloat            = true
0.00.069.120 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.156 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.099.631 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.640 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.708 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.709 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.709 I llama_new_context_with_model: graph nodes  = 967
0.00.100.710 I llama_new_context_with_model: graph splits = 2
0.00.100.725 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.866 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.394 I main: llama threadpool init, n_threads = 4
0.00.803.438 I 
0.00.803.472 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.474 I 
0.00.803.714 I sampler seed: 1234
0.00.803.719 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.777 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.777 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.549.388 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.549.388 I llama_perf_context_print:        load time =     794.47 ms
0.01.549.389 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.549.390 I llama_perf_context_print:        eval time =     699.73 ms /    63 runs   (   11.11 ms per token,    90.04 tokens per second)
0.01.549.390 I llama_perf_context_print:       total time =     746.00 ms /    70 tokens
0.01.549.551 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.485 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.848 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.850 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.850 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.853 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.854 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.854 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.857 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.858 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.861 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.371 I llama_model_loader: - type  f32:  194 tensors
0.00.026.371 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.371 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.372 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.454 I llm_load_vocab: special tokens cache size = 25
0.00.053.384 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.395 I llm_load_print_meta: arch             = gptneox
0.00.053.395 I llm_load_print_meta: vocab type       = BPE
0.00.053.395 I llm_load_print_meta: n_vocab          = 50304
0.00.053.396 I llm_load_print_meta: n_merges         = 50009
0.00.053.396 I llm_load_print_meta: vocab_only       = 0
0.00.053.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.396 I llm_load_print_meta: n_embd           = 2048
0.00.053.396 I llm_load_print_meta: n_layer          = 24
0.00.053.399 I llm_load_print_meta: n_head           = 16
0.00.053.400 I llm_load_print_meta: n_head_kv        = 16
0.00.053.401 I llm_load_print_meta: n_rot            = 32
0.00.053.402 I llm_load_print_meta: n_swa            = 0
0.00.053.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.403 I llm_load_print_meta: n_gqa            = 1
0.00.053.404 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.404 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.408 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.408 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.408 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.409 I llm_load_print_meta: n_ff             = 8192
0.00.053.409 I llm_load_print_meta: n_expert         = 0
0.00.053.409 I llm_load_print_meta: n_expert_used    = 0
0.00.053.409 I llm_load_print_meta: causal attn      = 1
0.00.053.410 I llm_load_print_meta: pooling type     = 0
0.00.053.410 I llm_load_print_meta: rope type        = 2
0.00.053.410 I llm_load_print_meta: rope scaling     = linear
0.00.053.410 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.411 I llm_load_print_meta: freq_scale_train = 1
0.00.053.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.411 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.411 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.411 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.412 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.413 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.413 I llm_load_print_meta: model type       = 1.4B
0.00.053.413 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.414 I llm_load_print_meta: model params     = 1.41 B
0.00.053.414 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.414 I llm_load_print_meta: general.name     = 1.4B
0.00.053.414 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.414 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.415 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.415 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.416 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.416 I llm_load_print_meta: max token length = 1024
0.00.055.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.268 I llm_load_tensors: offloading output layer to GPU
0.00.055.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.274 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.276 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.154 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.154 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.154 I llama_new_context_with_model: n_batch       = 2048
0.00.056.155 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.155 I llama_new_context_with_model: flash_attn    = 0
0.00.056.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.155 I llama_new_context_with_model: freq_scale    = 1
0.00.056.156 I ggml_metal_init: allocating
0.00.056.160 I ggml_metal_init: found device: Apple M4
0.00.056.164 I ggml_metal_init: picking default device: Apple M4
0.00.056.800 I ggml_metal_init: using embedded metal library
0.00.059.174 I ggml_metal_init: GPU name:   Apple M4
0.00.059.175 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.176 I ggml_metal_init: simdgroup reduction   = true
0.00.059.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.177 I ggml_metal_init: has bfloat            = true
0.00.059.177 I ggml_metal_init: use bfloat            = true
0.00.059.177 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.157 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.332 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.338 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.415 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.417 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.417 I llama_new_context_with_model: graph nodes  = 967
0.00.090.417 I llama_new_context_with_model: graph splits = 2
0.00.090.432 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.348 I main: llama threadpool init, n_threads = 4
0.00.618.389 I 
0.00.618.421 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.421 I 
0.00.618.642 I sampler seed: 1234
0.00.618.647 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.618.657 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.618.658 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.618.659 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.870 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.378.870 I llama_perf_context_print:        load time =     607.86 ms
0.01.378.871 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.62 tokens per second)
0.01.378.872 I llama_perf_context_print:        eval time =     710.00 ms /    63 runs   (   11.27 ms per token,    88.73 tokens per second)
0.01.378.872 I llama_perf_context_print:       total time =     760.53 ms /    70 tokens
0.01.379.062 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.112s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.636 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.098 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.100 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.489 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.492 I llama_model_loader: - type  f32:  194 tensors
0.00.023.492 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.492 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.972 I llm_load_vocab: special tokens cache size = 25
0.00.049.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.902 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.903 I llm_load_print_meta: arch             = gptneox
0.00.049.904 I llm_load_print_meta: vocab type       = BPE
0.00.049.904 I llm_load_print_meta: n_vocab          = 50304
0.00.049.904 I llm_load_print_meta: n_merges         = 50009
0.00.049.904 I llm_load_print_meta: vocab_only       = 0
0.00.049.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.905 I llm_load_print_meta: n_embd           = 2048
0.00.049.905 I llm_load_print_meta: n_layer          = 24
0.00.049.908 I llm_load_print_meta: n_head           = 16
0.00.049.909 I llm_load_print_meta: n_head_kv        = 16
0.00.049.909 I llm_load_print_meta: n_rot            = 32
0.00.049.909 I llm_load_print_meta: n_swa            = 0
0.00.049.909 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.909 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.910 I llm_load_print_meta: n_gqa            = 1
0.00.049.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.913 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.914 I llm_load_print_meta: n_ff             = 8192
0.00.049.914 I llm_load_print_meta: n_expert         = 0
0.00.049.915 I llm_load_print_meta: n_expert_used    = 0
0.00.049.915 I llm_load_print_meta: causal attn      = 1
0.00.049.915 I llm_load_print_meta: pooling type     = 0
0.00.049.915 I llm_load_print_meta: rope type        = 2
0.00.049.915 I llm_load_print_meta: rope scaling     = linear
0.00.049.916 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.917 I llm_load_print_meta: freq_scale_train = 1
0.00.049.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.917 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.917 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.919 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.919 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.919 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.920 I llm_load_print_meta: model type       = 1.4B
0.00.049.920 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.920 I llm_load_print_meta: model params     = 1.41 B
0.00.049.922 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.922 I llm_load_print_meta: general.name     = 1.4B
0.00.049.922 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.922 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.923 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.923 I llm_load_print_meta: max token length = 1024
0.00.051.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.923 I llm_load_tensors: offloading output layer to GPU
0.00.051.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.934 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.935 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.831 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.832 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.833 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.833 I llama_new_context_with_model: n_batch       = 2048
0.00.052.833 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.833 I llama_new_context_with_model: flash_attn    = 0
0.00.052.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.834 I llama_new_context_with_model: freq_scale    = 1
0.00.052.835 I ggml_metal_init: allocating
0.00.052.841 I ggml_metal_init: found device: Apple M4
0.00.052.844 I ggml_metal_init: picking default device: Apple M4
0.00.053.433 I ggml_metal_init: using embedded metal library
0.00.055.751 I ggml_metal_init: GPU name:   Apple M4
0.00.055.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.753 I ggml_metal_init: simdgroup reduction   = true
0.00.055.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.754 I ggml_metal_init: has bfloat            = true
0.00.055.754 I ggml_metal_init: use bfloat            = true
0.00.055.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.576 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.698 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.705 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.724 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.708 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.709 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.709 I llama_new_context_with_model: graph nodes  = 967
0.00.086.710 I llama_new_context_with_model: graph splits = 2
0.00.086.726 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.866 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.101 I main: llama threadpool init, n_threads = 4
0.00.704.142 I 
0.00.704.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.173 I 
0.00.704.407 I sampler seed: 1234
0.00.704.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.430 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.430 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.550.493 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.550.494 I llama_perf_context_print:        load time =     695.46 ms
0.01.550.494 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.72 tokens per second)
0.01.550.495 I llama_perf_context_print:        eval time =     791.39 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.550.495 I llama_perf_context_print:       total time =     846.39 ms /    70 tokens
0.01.550.683 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.278 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.283 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.765 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.766 I llama_model_loader: - type  f32:  194 tensors
0.00.024.766 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.329 I llm_load_vocab: special tokens cache size = 25
0.00.051.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.406 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.407 I llm_load_print_meta: arch             = gptneox
0.00.051.407 I llm_load_print_meta: vocab type       = BPE
0.00.051.407 I llm_load_print_meta: n_vocab          = 50304
0.00.051.408 I llm_load_print_meta: n_merges         = 50009
0.00.051.408 I llm_load_print_meta: vocab_only       = 0
0.00.051.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.408 I llm_load_print_meta: n_embd           = 2048
0.00.051.408 I llm_load_print_meta: n_layer          = 24
0.00.051.410 I llm_load_print_meta: n_head           = 16
0.00.051.411 I llm_load_print_meta: n_head_kv        = 16
0.00.051.411 I llm_load_print_meta: n_rot            = 32
0.00.051.411 I llm_load_print_meta: n_swa            = 0
0.00.051.412 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.413 I llm_load_print_meta: n_gqa            = 1
0.00.051.413 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.414 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.415 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.415 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.416 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.416 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.416 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.417 I llm_load_print_meta: n_ff             = 8192
0.00.051.417 I llm_load_print_meta: n_expert         = 0
0.00.051.418 I llm_load_print_meta: n_expert_used    = 0
0.00.051.418 I llm_load_print_meta: causal attn      = 1
0.00.051.418 I llm_load_print_meta: pooling type     = 0
0.00.051.418 I llm_load_print_meta: rope type        = 2
0.00.051.418 I llm_load_print_meta: rope scaling     = linear
0.00.051.419 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.419 I llm_load_print_meta: freq_scale_train = 1
0.00.051.419 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.419 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.419 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.419 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.420 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.420 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.420 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.420 I llm_load_print_meta: model type       = 1.4B
0.00.051.420 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.421 I llm_load_print_meta: model params     = 1.41 B
0.00.051.421 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.421 I llm_load_print_meta: general.name     = 1.4B
0.00.051.422 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.422 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.422 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.422 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.423 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.423 I llm_load_print_meta: max token length = 1024
0.00.053.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.486 I llm_load_tensors: offloading output layer to GPU
0.00.053.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.497 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.498 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.396 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.397 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.397 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.397 I llama_new_context_with_model: n_batch       = 2048
0.00.054.398 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.398 I llama_new_context_with_model: flash_attn    = 0
0.00.054.398 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.399 I llama_new_context_with_model: freq_scale    = 1
0.00.054.399 I ggml_metal_init: allocating
0.00.054.405 I ggml_metal_init: found device: Apple M4
0.00.054.407 I ggml_metal_init: picking default device: Apple M4
0.00.054.995 I ggml_metal_init: using embedded metal library
0.00.057.308 I ggml_metal_init: GPU name:   Apple M4
0.00.057.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.310 I ggml_metal_init: simdgroup reduction   = true
0.00.057.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.310 I ggml_metal_init: has bfloat            = true
0.00.057.311 I ggml_metal_init: use bfloat            = true
0.00.057.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.312 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.953 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.659 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.663 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.681 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.708 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.710 I llama_new_context_with_model: graph nodes  = 967
0.00.087.711 I llama_new_context_with_model: graph splits = 2
0.00.087.720 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.396 I main: llama threadpool init, n_threads = 4
0.00.753.438 I 
0.00.753.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.481 I 
0.00.753.706 I sampler seed: 1234
0.00.753.712 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.764 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.765 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.766 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.636.332 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.636.333 I llama_perf_context_print:        load time =     743.67 ms
0.01.636.334 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.77 tokens per second)
0.01.636.335 I llama_perf_context_print:        eval time =     825.12 ms /    63 runs   (   13.10 ms per token,    76.35 tokens per second)
0.01.636.336 I llama_perf_context_print:       total time =     882.94 ms /    70 tokens
0.01.636.524 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.549 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.098 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.901 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.941 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.946 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.280 I llama_model_loader: - type  f32:  194 tensors
0.00.056.280 I llama_model_loader: - type  f16:   98 tensors
0.00.086.538 I llm_load_vocab: special tokens cache size = 25
0.00.093.041 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.057 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.058 I llm_load_print_meta: arch             = gptneox
0.00.093.058 I llm_load_print_meta: vocab type       = BPE
0.00.093.059 I llm_load_print_meta: n_vocab          = 50304
0.00.093.059 I llm_load_print_meta: n_merges         = 50009
0.00.093.059 I llm_load_print_meta: vocab_only       = 0
0.00.093.059 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.059 I llm_load_print_meta: n_embd           = 2048
0.00.093.060 I llm_load_print_meta: n_layer          = 24
0.00.093.062 I llm_load_print_meta: n_head           = 16
0.00.093.063 I llm_load_print_meta: n_head_kv        = 16
0.00.093.063 I llm_load_print_meta: n_rot            = 32
0.00.093.063 I llm_load_print_meta: n_swa            = 0
0.00.093.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.064 I llm_load_print_meta: n_gqa            = 1
0.00.093.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.066 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.066 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.067 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.067 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.067 I llm_load_print_meta: n_ff             = 8192
0.00.093.068 I llm_load_print_meta: n_expert         = 0
0.00.093.068 I llm_load_print_meta: n_expert_used    = 0
0.00.093.068 I llm_load_print_meta: causal attn      = 1
0.00.093.068 I llm_load_print_meta: pooling type     = 0
0.00.093.068 I llm_load_print_meta: rope type        = 2
0.00.093.068 I llm_load_print_meta: rope scaling     = linear
0.00.093.069 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.069 I llm_load_print_meta: freq_scale_train = 1
0.00.093.069 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.069 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.070 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.070 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.070 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.070 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.070 I llm_load_print_meta: model type       = 1.4B
0.00.093.071 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.073 I llm_load_print_meta: model params     = 1.41 B
0.00.093.073 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.073 I llm_load_print_meta: general.name     = 1.4B
0.00.093.074 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.074 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.074 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.075 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.075 I llm_load_print_meta: max token length = 1024
0.00.095.745 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.745 I llm_load_tensors: offloading output layer to GPU
0.00.095.746 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.757 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.758 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.706 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.707 I llama_new_context_with_model: n_ctx         = 128
0.00.096.708 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.708 I llama_new_context_with_model: n_batch       = 128
0.00.096.708 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.708 I llama_new_context_with_model: flash_attn    = 0
0.00.096.709 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.709 I llama_new_context_with_model: freq_scale    = 1
0.00.096.709 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.710 I ggml_metal_init: allocating
0.00.096.718 I ggml_metal_init: found device: Apple M4
0.00.096.721 I ggml_metal_init: picking default device: Apple M4
0.00.097.367 I ggml_metal_init: using embedded metal library
0.00.099.983 I ggml_metal_init: GPU name:   Apple M4
0.00.099.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.986 I ggml_metal_init: simdgroup reduction   = true
0.00.099.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.986 I ggml_metal_init: has bfloat            = true
0.00.099.986 I ggml_metal_init: use bfloat            = true
0.00.099.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.525 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.110.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.921 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.935 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.863 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.864 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.864 I llama_new_context_with_model: graph nodes  = 967
0.00.111.864 I llama_new_context_with_model: graph splits = 2
0.00.111.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.114 I 
0.00.936.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.192 I perplexity: tokenizing the input ..
0.00.948.392 I perplexity: tokenization took 12.196 ms
0.00.948.397 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.069.377 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.071.178 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.071.209 I llama_perf_context_print:        load time =     912.00 ms
0.01.071.211 I llama_perf_context_print: prompt eval time =     120.60 ms /   128 tokens (    0.94 ms per token,  1061.33 tokens per second)
0.01.071.212 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.071.213 I llama_perf_context_print:       total time =     135.10 ms /   129 tokens
0.01.071.939 I ggml_metal_free: deallocating

real	0m1.265s
user	0m0.126s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.137 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.208 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.441 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.452 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.453 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.844 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.196 I llama_model_loader: - type  f32:  194 tensors
0.00.032.197 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.816 I llm_load_vocab: special tokens cache size = 25
0.00.063.910 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.927 I llm_load_print_meta: arch             = gptneox
0.00.063.928 I llm_load_print_meta: vocab type       = BPE
0.00.063.928 I llm_load_print_meta: n_vocab          = 50304
0.00.063.928 I llm_load_print_meta: n_merges         = 50009
0.00.063.928 I llm_load_print_meta: vocab_only       = 0
0.00.063.928 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.928 I llm_load_print_meta: n_embd           = 2048
0.00.063.929 I llm_load_print_meta: n_layer          = 24
0.00.063.934 I llm_load_print_meta: n_head           = 16
0.00.063.935 I llm_load_print_meta: n_head_kv        = 16
0.00.063.935 I llm_load_print_meta: n_rot            = 32
0.00.063.935 I llm_load_print_meta: n_swa            = 0
0.00.063.935 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.936 I llm_load_print_meta: n_gqa            = 1
0.00.063.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.940 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.941 I llm_load_print_meta: n_ff             = 8192
0.00.063.941 I llm_load_print_meta: n_expert         = 0
0.00.063.941 I llm_load_print_meta: n_expert_used    = 0
0.00.063.942 I llm_load_print_meta: causal attn      = 1
0.00.063.942 I llm_load_print_meta: pooling type     = 0
0.00.063.942 I llm_load_print_meta: rope type        = 2
0.00.063.942 I llm_load_print_meta: rope scaling     = linear
0.00.063.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.943 I llm_load_print_meta: freq_scale_train = 1
0.00.063.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.944 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.944 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.944 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.944 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.944 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.944 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.945 I llm_load_print_meta: model type       = 1.4B
0.00.063.945 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.945 I llm_load_print_meta: model params     = 1.41 B
0.00.063.946 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.946 I llm_load_print_meta: general.name     = 1.4B
0.00.063.946 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.946 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.947 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.948 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.948 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.949 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.949 I llm_load_print_meta: max token length = 1024
0.00.066.027 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.027 I llm_load_tensors: offloading output layer to GPU
0.00.066.028 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.039 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.040 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.947 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.948 I llama_new_context_with_model: n_ctx         = 128
0.00.066.948 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.948 I llama_new_context_with_model: n_batch       = 128
0.00.066.949 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.949 I llama_new_context_with_model: flash_attn    = 0
0.00.066.949 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.949 I llama_new_context_with_model: freq_scale    = 1
0.00.066.950 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.951 I ggml_metal_init: allocating
0.00.066.955 I ggml_metal_init: found device: Apple M4
0.00.066.957 I ggml_metal_init: picking default device: Apple M4
0.00.067.555 I ggml_metal_init: using embedded metal library
0.00.070.150 I ggml_metal_init: GPU name:   Apple M4
0.00.070.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.152 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.152 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.153 I ggml_metal_init: simdgroup reduction   = true
0.00.070.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.153 I ggml_metal_init: has bfloat            = true
0.00.070.153 I ggml_metal_init: use bfloat            = true
0.00.070.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.137 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.080.446 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.453 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.469 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.435 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.436 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.436 I llama_new_context_with_model: graph nodes  = 967
0.00.081.437 I llama_new_context_with_model: graph splits = 2
0.00.081.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.815 I 
0.00.821.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.861 I perplexity: tokenizing the input ..
0.00.829.365 I perplexity: tokenization took 7.503 ms
0.00.829.370 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.951.979 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.953.188 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.953.201 I llama_perf_context_print:        load time =     810.60 ms
0.00.953.201 I llama_perf_context_print: prompt eval time =     122.38 ms /   128 tokens (    0.96 ms per token,  1045.96 tokens per second)
0.00.953.202 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.953.203 I llama_perf_context_print:       total time =     131.39 ms /   129 tokens
0.00.953.762 I ggml_metal_free: deallocating

real	0m0.972s
user	0m0.092s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.003 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.930 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.934 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.947 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.028 I llama_model_loader: - type  f32:  194 tensors
0.00.025.028 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.218 I llm_load_vocab: special tokens cache size = 25
0.00.052.132 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.147 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.148 I llm_load_print_meta: arch             = gptneox
0.00.052.148 I llm_load_print_meta: vocab type       = BPE
0.00.052.149 I llm_load_print_meta: n_vocab          = 50304
0.00.052.149 I llm_load_print_meta: n_merges         = 50009
0.00.052.149 I llm_load_print_meta: vocab_only       = 0
0.00.052.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.149 I llm_load_print_meta: n_embd           = 2048
0.00.052.150 I llm_load_print_meta: n_layer          = 24
0.00.052.152 I llm_load_print_meta: n_head           = 16
0.00.052.153 I llm_load_print_meta: n_head_kv        = 16
0.00.052.153 I llm_load_print_meta: n_rot            = 32
0.00.052.153 I llm_load_print_meta: n_swa            = 0
0.00.052.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.154 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.155 I llm_load_print_meta: n_gqa            = 1
0.00.052.155 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.161 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.162 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.162 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.164 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.166 I llm_load_print_meta: n_ff             = 8192
0.00.052.166 I llm_load_print_meta: n_expert         = 0
0.00.052.166 I llm_load_print_meta: n_expert_used    = 0
0.00.052.167 I llm_load_print_meta: causal attn      = 1
0.00.052.167 I llm_load_print_meta: pooling type     = 0
0.00.052.167 I llm_load_print_meta: rope type        = 2
0.00.052.167 I llm_load_print_meta: rope scaling     = linear
0.00.052.167 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.168 I llm_load_print_meta: freq_scale_train = 1
0.00.052.168 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.169 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.169 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.169 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.169 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.169 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.169 I llm_load_print_meta: model type       = 1.4B
0.00.052.171 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.171 I llm_load_print_meta: model params     = 1.41 B
0.00.052.172 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.172 I llm_load_print_meta: general.name     = 1.4B
0.00.052.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.174 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.174 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.174 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.174 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.174 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.175 I llm_load_print_meta: max token length = 1024
0.00.054.083 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.083 I llm_load_tensors: offloading output layer to GPU
0.00.054.084 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.094 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.095 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.992 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.993 I llama_new_context_with_model: n_ctx         = 128
0.00.054.993 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.993 I llama_new_context_with_model: n_batch       = 128
0.00.054.993 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.993 I llama_new_context_with_model: flash_attn    = 0
0.00.054.994 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.994 I llama_new_context_with_model: freq_scale    = 1
0.00.054.995 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.995 I ggml_metal_init: allocating
0.00.055.002 I ggml_metal_init: found device: Apple M4
0.00.055.005 I ggml_metal_init: picking default device: Apple M4
0.00.055.595 I ggml_metal_init: using embedded metal library
0.00.058.005 I ggml_metal_init: GPU name:   Apple M4
0.00.058.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.008 I ggml_metal_init: simdgroup reduction   = true
0.00.058.008 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.008 I ggml_metal_init: has bfloat            = true
0.00.058.008 I ggml_metal_init: use bfloat            = true
0.00.058.009 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.857 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.859 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.874 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.869 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.870 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.870 I llama_new_context_with_model: graph nodes  = 967
0.00.070.870 I llama_new_context_with_model: graph splits = 2
0.00.070.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.451 I 
0.00.601.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.504 I perplexity: tokenizing the input ..
0.00.609.909 I perplexity: tokenization took 8.404 ms
0.00.609.917 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.654 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.824 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.839 I llama_perf_context_print:        load time =     591.44 ms
0.00.733.840 I llama_perf_context_print: prompt eval time =     122.51 ms /   128 tokens (    0.96 ms per token,  1044.80 tokens per second)
0.00.733.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.842 I llama_perf_context_print:       total time =     132.39 ms /   129 tokens
0.00.734.309 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.081s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.858 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.679 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.772 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.715 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.716 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.716 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.717 I llama_model_loader: - type  f32:  194 tensors
0.00.023.717 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.718 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.156 I llm_load_vocab: special tokens cache size = 25
0.00.050.084 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.098 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.099 I llm_load_print_meta: arch             = gptneox
0.00.050.099 I llm_load_print_meta: vocab type       = BPE
0.00.050.100 I llm_load_print_meta: n_vocab          = 50304
0.00.050.100 I llm_load_print_meta: n_merges         = 50009
0.00.050.100 I llm_load_print_meta: vocab_only       = 0
0.00.050.100 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.100 I llm_load_print_meta: n_embd           = 2048
0.00.050.100 I llm_load_print_meta: n_layer          = 24
0.00.050.103 I llm_load_print_meta: n_head           = 16
0.00.050.104 I llm_load_print_meta: n_head_kv        = 16
0.00.050.104 I llm_load_print_meta: n_rot            = 32
0.00.050.104 I llm_load_print_meta: n_swa            = 0
0.00.050.106 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.106 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.107 I llm_load_print_meta: n_gqa            = 1
0.00.050.108 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.108 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.109 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.109 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.110 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.110 I llm_load_print_meta: n_ff             = 8192
0.00.050.111 I llm_load_print_meta: n_expert         = 0
0.00.050.111 I llm_load_print_meta: n_expert_used    = 0
0.00.050.112 I llm_load_print_meta: causal attn      = 1
0.00.050.112 I llm_load_print_meta: pooling type     = 0
0.00.050.112 I llm_load_print_meta: rope type        = 2
0.00.050.113 I llm_load_print_meta: rope scaling     = linear
0.00.050.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.113 I llm_load_print_meta: freq_scale_train = 1
0.00.050.113 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.121 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.121 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.122 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.123 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.123 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.124 I llm_load_print_meta: model type       = 1.4B
0.00.050.125 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.125 I llm_load_print_meta: model params     = 1.41 B
0.00.050.126 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.126 I llm_load_print_meta: general.name     = 1.4B
0.00.050.126 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.126 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.127 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.130 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.130 I llm_load_print_meta: max token length = 1024
0.00.052.048 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.049 I llm_load_tensors: offloading output layer to GPU
0.00.052.049 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.059 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.060 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.927 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.928 I llama_new_context_with_model: n_ctx         = 128
0.00.052.928 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.928 I llama_new_context_with_model: n_batch       = 128
0.00.052.928 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.929 I llama_new_context_with_model: flash_attn    = 0
0.00.052.929 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.929 I llama_new_context_with_model: freq_scale    = 1
0.00.052.930 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.930 I ggml_metal_init: allocating
0.00.052.933 I ggml_metal_init: found device: Apple M4
0.00.052.935 I ggml_metal_init: picking default device: Apple M4
0.00.053.486 I ggml_metal_init: using embedded metal library
0.00.055.837 I ggml_metal_init: GPU name:   Apple M4
0.00.055.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.840 I ggml_metal_init: simdgroup reduction   = true
0.00.055.840 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.840 I ggml_metal_init: has bfloat            = true
0.00.055.840 I ggml_metal_init: use bfloat            = true
0.00.055.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.366 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.841 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.844 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.859 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.731 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.732 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.735 I llama_new_context_with_model: graph nodes  = 967
0.00.067.735 I llama_new_context_with_model: graph splits = 2
0.00.067.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.650 I 
0.00.623.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.712 I perplexity: tokenizing the input ..
0.00.631.442 I perplexity: tokenization took 7.728 ms
0.00.631.445 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.285 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.755.513 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.755.526 I llama_perf_context_print:        load time =     614.79 ms
0.00.755.527 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.94 tokens per second)
0.00.755.528 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.529 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.755.980 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.078s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.026 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.979 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.986 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.993 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.993 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.057 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.058 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.059 I llama_model_loader: - type  f32:  194 tensors
0.00.025.059 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.060 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.690 I llm_load_vocab: special tokens cache size = 25
0.00.052.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.655 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.657 I llm_load_print_meta: arch             = gptneox
0.00.052.657 I llm_load_print_meta: vocab type       = BPE
0.00.052.657 I llm_load_print_meta: n_vocab          = 50304
0.00.052.657 I llm_load_print_meta: n_merges         = 50009
0.00.052.658 I llm_load_print_meta: vocab_only       = 0
0.00.052.658 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.658 I llm_load_print_meta: n_embd           = 2048
0.00.052.658 I llm_load_print_meta: n_layer          = 24
0.00.052.662 I llm_load_print_meta: n_head           = 16
0.00.052.662 I llm_load_print_meta: n_head_kv        = 16
0.00.052.663 I llm_load_print_meta: n_rot            = 32
0.00.052.663 I llm_load_print_meta: n_swa            = 0
0.00.052.663 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.663 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.664 I llm_load_print_meta: n_gqa            = 1
0.00.052.665 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.665 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.666 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.666 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.666 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.666 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.666 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.667 I llm_load_print_meta: n_ff             = 8192
0.00.052.667 I llm_load_print_meta: n_expert         = 0
0.00.052.667 I llm_load_print_meta: n_expert_used    = 0
0.00.052.667 I llm_load_print_meta: causal attn      = 1
0.00.052.667 I llm_load_print_meta: pooling type     = 0
0.00.052.667 I llm_load_print_meta: rope type        = 2
0.00.052.668 I llm_load_print_meta: rope scaling     = linear
0.00.052.669 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.669 I llm_load_print_meta: freq_scale_train = 1
0.00.052.669 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.670 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.670 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.670 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.672 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.672 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.672 I llm_load_print_meta: model type       = 1.4B
0.00.052.673 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.673 I llm_load_print_meta: model params     = 1.41 B
0.00.052.674 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.674 I llm_load_print_meta: general.name     = 1.4B
0.00.052.677 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.677 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.678 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.678 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.678 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.678 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.679 I llm_load_print_meta: max token length = 1024
0.00.054.802 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.802 I llm_load_tensors: offloading output layer to GPU
0.00.054.802 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.813 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.814 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.786 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.787 I llama_new_context_with_model: n_ctx         = 128
0.00.055.787 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.787 I llama_new_context_with_model: n_batch       = 128
0.00.055.787 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.788 I llama_new_context_with_model: flash_attn    = 0
0.00.055.788 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.788 I llama_new_context_with_model: freq_scale    = 1
0.00.055.789 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.789 I ggml_metal_init: allocating
0.00.055.792 I ggml_metal_init: found device: Apple M4
0.00.055.794 I ggml_metal_init: picking default device: Apple M4
0.00.056.389 I ggml_metal_init: using embedded metal library
0.00.058.777 I ggml_metal_init: GPU name:   Apple M4
0.00.058.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.780 I ggml_metal_init: simdgroup reduction   = true
0.00.058.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.780 I ggml_metal_init: has bfloat            = true
0.00.058.780 I ggml_metal_init: use bfloat            = true
0.00.058.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.754 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.029 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.983 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.984 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.985 I llama_new_context_with_model: graph nodes  = 967
0.00.070.985 I llama_new_context_with_model: graph splits = 2
0.00.070.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.999 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.747 I 
0.00.721.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.786 I perplexity: tokenizing the input ..
0.00.729.954 I perplexity: tokenization took 8.167 ms
0.00.729.958 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.864.970 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.138 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.157 I llama_perf_context_print:        load time =     711.72 ms
0.00.866.158 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.71 tokens per second)
0.00.866.159 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.159 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.866.696 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.804 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.768 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.769 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.769 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.770 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.770 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.771 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.774 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.910 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.912 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.913 I llama_model_loader: - type  f32:  194 tensors
0.00.023.914 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.914 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.267 I llm_load_vocab: special tokens cache size = 25
0.00.050.173 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.186 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.187 I llm_load_print_meta: arch             = gptneox
0.00.050.187 I llm_load_print_meta: vocab type       = BPE
0.00.050.188 I llm_load_print_meta: n_vocab          = 50304
0.00.050.188 I llm_load_print_meta: n_merges         = 50009
0.00.050.188 I llm_load_print_meta: vocab_only       = 0
0.00.050.188 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.188 I llm_load_print_meta: n_embd           = 2048
0.00.050.189 I llm_load_print_meta: n_layer          = 24
0.00.050.191 I llm_load_print_meta: n_head           = 16
0.00.050.192 I llm_load_print_meta: n_head_kv        = 16
0.00.050.192 I llm_load_print_meta: n_rot            = 32
0.00.050.192 I llm_load_print_meta: n_swa            = 0
0.00.050.192 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.194 I llm_load_print_meta: n_gqa            = 1
0.00.050.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.196 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.196 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.197 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.198 I llm_load_print_meta: n_ff             = 8192
0.00.050.198 I llm_load_print_meta: n_expert         = 0
0.00.050.198 I llm_load_print_meta: n_expert_used    = 0
0.00.050.198 I llm_load_print_meta: causal attn      = 1
0.00.050.198 I llm_load_print_meta: pooling type     = 0
0.00.050.198 I llm_load_print_meta: rope type        = 2
0.00.050.198 I llm_load_print_meta: rope scaling     = linear
0.00.050.200 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.200 I llm_load_print_meta: freq_scale_train = 1
0.00.050.201 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.201 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.201 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.201 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.201 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.205 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.205 I llm_load_print_meta: model type       = 1.4B
0.00.050.206 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.206 I llm_load_print_meta: model params     = 1.41 B
0.00.050.207 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.207 I llm_load_print_meta: general.name     = 1.4B
0.00.050.207 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.209 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.209 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.209 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.209 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.209 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.210 I llm_load_print_meta: max token length = 1024
0.00.052.162 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.162 I llm_load_tensors: offloading output layer to GPU
0.00.052.162 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.172 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.173 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.085 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.086 I llama_new_context_with_model: n_ctx         = 128
0.00.053.086 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.086 I llama_new_context_with_model: n_batch       = 128
0.00.053.087 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.087 I llama_new_context_with_model: flash_attn    = 0
0.00.053.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.087 I llama_new_context_with_model: freq_scale    = 1
0.00.053.088 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.088 I ggml_metal_init: allocating
0.00.053.092 I ggml_metal_init: found device: Apple M4
0.00.053.094 I ggml_metal_init: picking default device: Apple M4
0.00.053.657 I ggml_metal_init: using embedded metal library
0.00.056.022 I ggml_metal_init: GPU name:   Apple M4
0.00.056.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.025 I ggml_metal_init: simdgroup reduction   = true
0.00.056.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.025 I ggml_metal_init: has bfloat            = true
0.00.056.025 I ggml_metal_init: use bfloat            = true
0.00.056.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.690 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.921 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.923 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.937 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.871 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.872 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.873 I llama_new_context_with_model: graph nodes  = 967
0.00.067.873 I llama_new_context_with_model: graph splits = 2
0.00.067.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.886 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.693 I 
0.00.743.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.744 I perplexity: tokenizing the input ..
0.00.751.954 I perplexity: tokenization took 8.209 ms
0.00.751.962 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.838 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.887.993 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.888.009 I llama_perf_context_print:        load time =     734.88 ms
0.00.888.010 I llama_perf_context_print: prompt eval time =     134.65 ms /   128 tokens (    1.05 ms per token,   950.61 tokens per second)
0.00.888.011 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.888.012 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.888.413 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.991 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.341 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.345 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.354 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.609 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.740 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.741 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.742 I llama_model_loader: - type  f32:  194 tensors
0.00.024.743 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.743 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.743 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.188 I llm_load_vocab: special tokens cache size = 25
0.00.051.116 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.131 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.132 I llm_load_print_meta: arch             = gptneox
0.00.051.132 I llm_load_print_meta: vocab type       = BPE
0.00.051.132 I llm_load_print_meta: n_vocab          = 50304
0.00.051.133 I llm_load_print_meta: n_merges         = 50009
0.00.051.133 I llm_load_print_meta: vocab_only       = 0
0.00.051.133 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.133 I llm_load_print_meta: n_embd           = 2048
0.00.051.133 I llm_load_print_meta: n_layer          = 24
0.00.051.136 I llm_load_print_meta: n_head           = 16
0.00.051.137 I llm_load_print_meta: n_head_kv        = 16
0.00.051.137 I llm_load_print_meta: n_rot            = 32
0.00.051.137 I llm_load_print_meta: n_swa            = 0
0.00.051.138 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.138 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.138 I llm_load_print_meta: n_gqa            = 1
0.00.051.140 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.141 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.141 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.143 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.143 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.143 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.143 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.144 I llm_load_print_meta: n_ff             = 8192
0.00.051.144 I llm_load_print_meta: n_expert         = 0
0.00.051.144 I llm_load_print_meta: n_expert_used    = 0
0.00.051.145 I llm_load_print_meta: causal attn      = 1
0.00.051.145 I llm_load_print_meta: pooling type     = 0
0.00.051.145 I llm_load_print_meta: rope type        = 2
0.00.051.145 I llm_load_print_meta: rope scaling     = linear
0.00.051.145 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.146 I llm_load_print_meta: freq_scale_train = 1
0.00.051.146 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.147 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.148 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.148 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.148 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.148 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.148 I llm_load_print_meta: model type       = 1.4B
0.00.051.149 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.149 I llm_load_print_meta: model params     = 1.41 B
0.00.051.151 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.151 I llm_load_print_meta: general.name     = 1.4B
0.00.051.151 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.151 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.152 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.152 I llm_load_print_meta: max token length = 1024
0.00.053.072 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.072 I llm_load_tensors: offloading output layer to GPU
0.00.053.072 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.083 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.084 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.963 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.964 I llama_new_context_with_model: n_ctx         = 128
0.00.053.964 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.964 I llama_new_context_with_model: n_batch       = 128
0.00.053.964 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.964 I llama_new_context_with_model: flash_attn    = 0
0.00.053.965 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.965 I llama_new_context_with_model: freq_scale    = 1
0.00.053.966 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.966 I ggml_metal_init: allocating
0.00.053.969 I ggml_metal_init: found device: Apple M4
0.00.053.971 I ggml_metal_init: picking default device: Apple M4
0.00.054.549 I ggml_metal_init: using embedded metal library
0.00.056.851 I ggml_metal_init: GPU name:   Apple M4
0.00.056.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.854 I ggml_metal_init: simdgroup reduction   = true
0.00.056.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.854 I ggml_metal_init: has bfloat            = true
0.00.056.854 I ggml_metal_init: use bfloat            = true
0.00.056.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.550 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.788 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.790 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.814 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.740 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.741 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.742 I llama_new_context_with_model: graph nodes  = 967
0.00.068.742 I llama_new_context_with_model: graph splits = 2
0.00.068.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.562 I 
0.00.451.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.641 I perplexity: tokenizing the input ..
0.00.459.897 I perplexity: tokenization took 8.253 ms
0.00.459.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.591.883 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.593.166 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.593.186 I llama_perf_context_print:        load time =     441.56 ms
0.00.593.187 I llama_perf_context_print: prompt eval time =     131.76 ms /   128 tokens (    1.03 ms per token,   971.49 tokens per second)
0.00.593.188 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.593.188 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.593.722 I ggml_metal_free: deallocating

real	0m0.609s
user	0m0.079s
sys	0m0.072s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.635 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.635 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.636 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.636 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.876 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.869 I llama_model_loader: - type  f32:  194 tensors
0.00.023.869 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.870 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.870 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.543 I llm_load_vocab: special tokens cache size = 25
0.00.050.468 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.484 I llm_load_print_meta: arch             = gptneox
0.00.050.485 I llm_load_print_meta: vocab type       = BPE
0.00.050.485 I llm_load_print_meta: n_vocab          = 50304
0.00.050.485 I llm_load_print_meta: n_merges         = 50009
0.00.050.485 I llm_load_print_meta: vocab_only       = 0
0.00.050.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.486 I llm_load_print_meta: n_embd           = 2048
0.00.050.486 I llm_load_print_meta: n_layer          = 24
0.00.050.489 I llm_load_print_meta: n_head           = 16
0.00.050.489 I llm_load_print_meta: n_head_kv        = 16
0.00.050.489 I llm_load_print_meta: n_rot            = 32
0.00.050.490 I llm_load_print_meta: n_swa            = 0
0.00.050.490 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.490 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.491 I llm_load_print_meta: n_gqa            = 1
0.00.050.492 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.492 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.493 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.493 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.493 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.494 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.494 I llm_load_print_meta: n_ff             = 8192
0.00.050.495 I llm_load_print_meta: n_expert         = 0
0.00.050.495 I llm_load_print_meta: n_expert_used    = 0
0.00.050.495 I llm_load_print_meta: causal attn      = 1
0.00.050.495 I llm_load_print_meta: pooling type     = 0
0.00.050.496 I llm_load_print_meta: rope type        = 2
0.00.050.498 I llm_load_print_meta: rope scaling     = linear
0.00.050.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.498 I llm_load_print_meta: freq_scale_train = 1
0.00.050.498 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.499 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.499 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.499 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.499 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.499 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.500 I llm_load_print_meta: model type       = 1.4B
0.00.050.500 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.501 I llm_load_print_meta: model params     = 1.41 B
0.00.050.501 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.501 I llm_load_print_meta: general.name     = 1.4B
0.00.050.503 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.503 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.504 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.504 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.504 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.504 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.504 I llm_load_print_meta: max token length = 1024
0.00.052.542 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.542 I llm_load_tensors: offloading output layer to GPU
0.00.052.542 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.552 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.554 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.570 I llama_new_context_with_model: n_ctx         = 128
0.00.053.570 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.570 I llama_new_context_with_model: n_batch       = 128
0.00.053.570 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.571 I llama_new_context_with_model: flash_attn    = 0
0.00.053.571 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.571 I llama_new_context_with_model: freq_scale    = 1
0.00.053.572 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.572 I ggml_metal_init: allocating
0.00.053.575 I ggml_metal_init: found device: Apple M4
0.00.053.577 I ggml_metal_init: picking default device: Apple M4
0.00.054.155 I ggml_metal_init: using embedded metal library
0.00.056.502 I ggml_metal_init: GPU name:   Apple M4
0.00.056.504 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.505 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.506 I ggml_metal_init: simdgroup reduction   = true
0.00.056.506 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.506 I ggml_metal_init: has bfloat            = true
0.00.056.506 I ggml_metal_init: use bfloat            = true
0.00.056.507 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.278 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.581 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.584 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.552 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.553 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.553 I llama_new_context_with_model: graph nodes  = 967
0.00.068.553 I llama_new_context_with_model: graph splits = 2
0.00.068.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.483 I 
0.00.508.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.540 I perplexity: tokenizing the input ..
0.00.516.429 I perplexity: tokenization took 7.888 ms
0.00.516.433 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.072 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.523 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.550 I llama_perf_context_print:        load time =     499.68 ms
0.00.649.551 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.12 tokens per second)
0.00.649.552 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.552 I llama_perf_context_print:       total time =     141.07 ms /   129 tokens
0.00.650.046 I ggml_metal_free: deallocating

real	0m0.664s
user	0m0.080s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.542 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.356 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.357 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.357 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.359 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.509 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.510 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.511 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.511 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.512 I llama_model_loader: - type  f32:  194 tensors
0.00.024.512 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.512 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.513 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.665 I llm_load_vocab: special tokens cache size = 25
0.00.051.616 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.631 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.632 I llm_load_print_meta: arch             = gptneox
0.00.051.632 I llm_load_print_meta: vocab type       = BPE
0.00.051.632 I llm_load_print_meta: n_vocab          = 50304
0.00.051.633 I llm_load_print_meta: n_merges         = 50009
0.00.051.633 I llm_load_print_meta: vocab_only       = 0
0.00.051.633 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.635 I llm_load_print_meta: n_embd           = 2048
0.00.051.635 I llm_load_print_meta: n_layer          = 24
0.00.051.638 I llm_load_print_meta: n_head           = 16
0.00.051.639 I llm_load_print_meta: n_head_kv        = 16
0.00.051.639 I llm_load_print_meta: n_rot            = 32
0.00.051.639 I llm_load_print_meta: n_swa            = 0
0.00.051.639 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.640 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.640 I llm_load_print_meta: n_gqa            = 1
0.00.051.641 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.642 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.642 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.642 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.642 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.643 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.643 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.643 I llm_load_print_meta: n_ff             = 8192
0.00.051.643 I llm_load_print_meta: n_expert         = 0
0.00.051.643 I llm_load_print_meta: n_expert_used    = 0
0.00.051.644 I llm_load_print_meta: causal attn      = 1
0.00.051.645 I llm_load_print_meta: pooling type     = 0
0.00.051.645 I llm_load_print_meta: rope type        = 2
0.00.051.645 I llm_load_print_meta: rope scaling     = linear
0.00.051.646 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.646 I llm_load_print_meta: freq_scale_train = 1
0.00.051.646 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.646 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.647 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.647 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.647 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.647 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.647 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.647 I llm_load_print_meta: model type       = 1.4B
0.00.051.648 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.649 I llm_load_print_meta: model params     = 1.41 B
0.00.051.649 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.650 I llm_load_print_meta: general.name     = 1.4B
0.00.051.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.650 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.651 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.652 I llm_load_print_meta: max token length = 1024
0.00.053.589 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.589 I llm_load_tensors: offloading output layer to GPU
0.00.053.589 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.600 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.601 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.475 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.476 I llama_new_context_with_model: n_ctx         = 128
0.00.054.476 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.476 I llama_new_context_with_model: n_batch       = 128
0.00.054.476 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.476 I llama_new_context_with_model: flash_attn    = 0
0.00.054.477 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.477 I llama_new_context_with_model: freq_scale    = 1
0.00.054.477 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.478 I ggml_metal_init: allocating
0.00.054.481 I ggml_metal_init: found device: Apple M4
0.00.054.483 I ggml_metal_init: picking default device: Apple M4
0.00.055.019 I ggml_metal_init: using embedded metal library
0.00.057.342 I ggml_metal_init: GPU name:   Apple M4
0.00.057.343 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.344 I ggml_metal_init: simdgroup reduction   = true
0.00.057.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.344 I ggml_metal_init: has bfloat            = true
0.00.057.345 I ggml_metal_init: use bfloat            = true
0.00.057.345 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.351 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.733 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.740 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.651 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.652 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.652 I llama_new_context_with_model: graph nodes  = 967
0.00.069.653 I llama_new_context_with_model: graph splits = 2
0.00.069.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.102 I 
0.00.561.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.226 I perplexity: tokenizing the input ..
0.00.569.289 I perplexity: tokenization took 8.061 ms
0.00.569.292 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.703.850 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.705.120 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.705.137 I llama_perf_context_print:        load time =     551.55 ms
0.00.705.138 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.87 tokens per second)
0.00.705.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.705.140 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.705.642 I ggml_metal_free: deallocating

real	0m0.722s
user	0m0.079s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.863 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.432 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.377 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.378 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.378 I llama_model_loader: - type  f32:  194 tensors
0.00.023.379 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.379 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.889 I llm_load_vocab: special tokens cache size = 25
0.00.049.805 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.820 I llm_load_print_meta: arch             = gptneox
0.00.049.820 I llm_load_print_meta: vocab type       = BPE
0.00.049.821 I llm_load_print_meta: n_vocab          = 50304
0.00.049.821 I llm_load_print_meta: n_merges         = 50009
0.00.049.821 I llm_load_print_meta: vocab_only       = 0
0.00.049.821 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.821 I llm_load_print_meta: n_embd           = 2048
0.00.049.822 I llm_load_print_meta: n_layer          = 24
0.00.049.825 I llm_load_print_meta: n_head           = 16
0.00.049.826 I llm_load_print_meta: n_head_kv        = 16
0.00.049.826 I llm_load_print_meta: n_rot            = 32
0.00.049.826 I llm_load_print_meta: n_swa            = 0
0.00.049.826 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.826 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.827 I llm_load_print_meta: n_gqa            = 1
0.00.049.828 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.829 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.834 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.835 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.838 I llm_load_print_meta: n_ff             = 8192
0.00.049.838 I llm_load_print_meta: n_expert         = 0
0.00.049.838 I llm_load_print_meta: n_expert_used    = 0
0.00.049.838 I llm_load_print_meta: causal attn      = 1
0.00.049.838 I llm_load_print_meta: pooling type     = 0
0.00.049.839 I llm_load_print_meta: rope type        = 2
0.00.049.839 I llm_load_print_meta: rope scaling     = linear
0.00.049.839 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.839 I llm_load_print_meta: freq_scale_train = 1
0.00.049.840 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.840 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.840 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.840 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.840 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.841 I llm_load_print_meta: model type       = 1.4B
0.00.049.841 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.841 I llm_load_print_meta: model params     = 1.41 B
0.00.049.842 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.842 I llm_load_print_meta: general.name     = 1.4B
0.00.049.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.843 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.844 I llm_load_print_meta: max token length = 1024
0.00.051.846 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.846 I llm_load_tensors: offloading output layer to GPU
0.00.051.846 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.857 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.858 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.707 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.708 I llama_new_context_with_model: n_ctx         = 128
0.00.052.708 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.708 I llama_new_context_with_model: n_batch       = 128
0.00.052.708 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.709 I llama_new_context_with_model: flash_attn    = 0
0.00.052.709 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.709 I llama_new_context_with_model: freq_scale    = 1
0.00.052.710 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.710 I ggml_metal_init: allocating
0.00.052.713 I ggml_metal_init: found device: Apple M4
0.00.052.715 I ggml_metal_init: picking default device: Apple M4
0.00.053.299 I ggml_metal_init: using embedded metal library
0.00.055.639 I ggml_metal_init: GPU name:   Apple M4
0.00.055.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.641 I ggml_metal_init: simdgroup reduction   = true
0.00.055.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.642 I ggml_metal_init: has bfloat            = true
0.00.055.642 I ggml_metal_init: use bfloat            = true
0.00.055.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.241 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.567 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.571 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.587 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.474 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.476 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.476 I llama_new_context_with_model: graph nodes  = 967
0.00.067.476 I llama_new_context_with_model: graph splits = 2
0.00.067.490 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.982 I 
0.00.627.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.040 I perplexity: tokenizing the input ..
0.00.635.127 I perplexity: tokenization took 8.085 ms
0.00.635.130 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.208 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.776.532 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.776.547 I llama_perf_context_print:        load time =     618.11 ms
0.00.776.548 I llama_perf_context_print: prompt eval time =     139.85 ms /   128 tokens (    1.09 ms per token,   915.25 tokens per second)
0.00.776.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.549 I llama_perf_context_print:       total time =     149.57 ms /   129 tokens
0.00.776.984 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.983 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.352 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.355 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.359 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.625 I llama_model_loader: - type  f32:  194 tensors
0.00.024.625 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.925 I llm_load_vocab: special tokens cache size = 25
0.00.050.733 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.743 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.744 I llm_load_print_meta: arch             = gptneox
0.00.050.744 I llm_load_print_meta: vocab type       = BPE
0.00.050.744 I llm_load_print_meta: n_vocab          = 50304
0.00.050.745 I llm_load_print_meta: n_merges         = 50009
0.00.050.745 I llm_load_print_meta: vocab_only       = 0
0.00.050.745 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.745 I llm_load_print_meta: n_embd           = 2048
0.00.050.745 I llm_load_print_meta: n_layer          = 24
0.00.050.748 I llm_load_print_meta: n_head           = 16
0.00.050.748 I llm_load_print_meta: n_head_kv        = 16
0.00.050.749 I llm_load_print_meta: n_rot            = 32
0.00.050.749 I llm_load_print_meta: n_swa            = 0
0.00.050.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.749 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.750 I llm_load_print_meta: n_gqa            = 1
0.00.050.751 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.751 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.752 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.752 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.752 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.753 I llm_load_print_meta: n_ff             = 8192
0.00.050.753 I llm_load_print_meta: n_expert         = 0
0.00.050.754 I llm_load_print_meta: n_expert_used    = 0
0.00.050.754 I llm_load_print_meta: causal attn      = 1
0.00.050.754 I llm_load_print_meta: pooling type     = 0
0.00.050.754 I llm_load_print_meta: rope type        = 2
0.00.050.754 I llm_load_print_meta: rope scaling     = linear
0.00.050.755 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.755 I llm_load_print_meta: freq_scale_train = 1
0.00.050.755 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.756 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.758 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.758 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.758 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.758 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.758 I llm_load_print_meta: model type       = 1.4B
0.00.050.759 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.759 I llm_load_print_meta: model params     = 1.41 B
0.00.050.760 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.760 I llm_load_print_meta: general.name     = 1.4B
0.00.050.760 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.761 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: max token length = 1024
0.00.052.535 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.535 I llm_load_tensors: offloading output layer to GPU
0.00.052.536 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.541 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.542 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.421 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.422 I llama_new_context_with_model: n_ctx         = 128
0.00.053.422 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.423 I llama_new_context_with_model: n_batch       = 128
0.00.053.423 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.423 I llama_new_context_with_model: flash_attn    = 0
0.00.053.423 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.424 I llama_new_context_with_model: freq_scale    = 1
0.00.053.424 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.425 I ggml_metal_init: allocating
0.00.053.428 I ggml_metal_init: found device: Apple M4
0.00.053.430 I ggml_metal_init: picking default device: Apple M4
0.00.053.993 I ggml_metal_init: using embedded metal library
0.00.056.322 I ggml_metal_init: GPU name:   Apple M4
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.325 I ggml_metal_init: simdgroup reduction   = true
0.00.056.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.325 I ggml_metal_init: has bfloat            = true
0.00.056.325 I ggml_metal_init: use bfloat            = true
0.00.056.326 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.808 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.082 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.959 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.960 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.960 I llama_new_context_with_model: graph nodes  = 967
0.00.067.961 I llama_new_context_with_model: graph splits = 2
0.00.067.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.250.866 I 
0.00.250.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.250.918 I perplexity: tokenizing the input ..
0.00.258.308 I perplexity: tokenization took 7.389 ms
0.00.258.312 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.398.279 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.399.684 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.399.695 I llama_perf_context_print:        load time =     240.88 ms
0.00.399.696 I llama_perf_context_print: prompt eval time =     139.61 ms /   128 tokens (    1.09 ms per token,   916.86 tokens per second)
0.00.399.696 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.399.697 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.400.018 I ggml_metal_free: deallocating

real	0m0.416s
user	0m0.078s
sys	0m0.051s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.183 I build: 4370 (5de36876) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.889 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.899 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.899 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.902 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.906 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.349 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.864 I llama_model_loader: - type  f32:  194 tensors
0.00.033.864 I llama_model_loader: - type  f16:   98 tensors
0.00.054.975 I llm_load_vocab: special tokens cache size = 25
0.00.060.824 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.841 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.842 I llm_load_print_meta: arch             = gptneox
0.00.060.842 I llm_load_print_meta: vocab type       = BPE
0.00.060.842 I llm_load_print_meta: n_vocab          = 50304
0.00.060.843 I llm_load_print_meta: n_merges         = 50009
0.00.060.843 I llm_load_print_meta: vocab_only       = 0
0.00.060.843 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.843 I llm_load_print_meta: n_embd           = 2048
0.00.060.843 I llm_load_print_meta: n_layer          = 24
0.00.060.847 I llm_load_print_meta: n_head           = 16
0.00.060.847 I llm_load_print_meta: n_head_kv        = 16
0.00.060.848 I llm_load_print_meta: n_rot            = 32
0.00.060.848 I llm_load_print_meta: n_swa            = 0
0.00.060.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.849 I llm_load_print_meta: n_gqa            = 1
0.00.060.849 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.850 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.851 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.851 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.851 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.852 I llm_load_print_meta: n_ff             = 8192
0.00.060.852 I llm_load_print_meta: n_expert         = 0
0.00.060.852 I llm_load_print_meta: n_expert_used    = 0
0.00.060.852 I llm_load_print_meta: causal attn      = 1
0.00.060.852 I llm_load_print_meta: pooling type     = 0
0.00.060.852 I llm_load_print_meta: rope type        = 2
0.00.060.852 I llm_load_print_meta: rope scaling     = linear
0.00.060.853 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.853 I llm_load_print_meta: freq_scale_train = 1
0.00.060.853 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.854 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.854 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.854 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.854 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.854 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.855 I llm_load_print_meta: model type       = 1.4B
0.00.060.855 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.060.856 I llm_load_print_meta: model params     = 1.41 B
0.00.060.856 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.060.856 I llm_load_print_meta: general.name     = 1.4B
0.00.060.857 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.857 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.857 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.857 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.857 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.858 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.858 I llm_load_print_meta: max token length = 1024
0.00.063.251 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.251 I llm_load_tensors: offloading output layer to GPU
0.00.063.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.262 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.063.263 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.064.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.265 I llama_new_context_with_model: n_ctx         = 128
0.00.064.265 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.265 I llama_new_context_with_model: n_batch       = 128
0.00.064.266 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.266 I llama_new_context_with_model: flash_attn    = 0
0.00.064.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.267 I llama_new_context_with_model: freq_scale    = 1
0.00.064.267 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.268 I ggml_metal_init: allocating
0.00.064.278 I ggml_metal_init: found device: Apple M4
0.00.064.280 I ggml_metal_init: picking default device: Apple M4
0.00.064.925 I ggml_metal_init: using embedded metal library
0.00.067.667 I ggml_metal_init: GPU name:   Apple M4
0.00.067.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.670 I ggml_metal_init: simdgroup reduction   = true
0.00.067.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.670 I ggml_metal_init: has bfloat            = true
0.00.067.670 I ggml_metal_init: use bfloat            = true
0.00.067.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.891 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.078.179 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.183 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.099 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.100 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.101 I llama_new_context_with_model: graph nodes  = 967
0.00.079.101 I llama_new_context_with_model: graph splits = 2
0.00.079.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.117 I 
0.00.079.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.079.154 I compute_imatrix: tokenizing the input ..
0.00.086.484 I compute_imatrix: tokenization took 7.329 ms
0.00.086.487 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.557.328 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.559.875 I llama_perf_context_print:        load time =    1541.02 ms
0.01.559.876 I llama_perf_context_print: prompt eval time =    1470.11 ms /   128 tokens (   11.49 ms per token,    87.07 tokens per second)
0.01.559.877 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.559.877 I llama_perf_context_print:       total time =    1543.57 ms /   129 tokens
0.01.560.444 I ggml_metal_free: deallocating

real	0m1.743s
user	0m0.146s
sys	0m0.218s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4370 (5de36876)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143d0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143d0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143d0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143d0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143d0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143d0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143d0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143d0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143d0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143d0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143d0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143d0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143d0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143d0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143d0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143d101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143d10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143d11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143d11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143d11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143d12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143d12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143d13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143d13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143d14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143d14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143d14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143d15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143d15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143d16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143d16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143d168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143d17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143d176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143d17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143d17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143d182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143d18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143d18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143d19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143d19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143d199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143d19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143d1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143d1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143d1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143d1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143d1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143d1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143d1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143d1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143d1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143d1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143d1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143d1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143d1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143d1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143d1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143d1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143d20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143d20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143d208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143d20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143d21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143d216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143d21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143d21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143d22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143d22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143d22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143d23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143d23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143d23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143d240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143d24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143d24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143d250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143d25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143d25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143d260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143d26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143d26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143d270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143d27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143d27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143d280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143d28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143d28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143d290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143d295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143d29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143d2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143d2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143d2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143d2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143d2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143d2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143d1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143d2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143d2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143d2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143d2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143d2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143d2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143d2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143d2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143d2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143d2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143d2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143d2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143d301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143d30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143d30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143d310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143d31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143d31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143d31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143d32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143d32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143d32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143d33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143d335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143d33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143d33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143d343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143d34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143d34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143d351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143d35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143d35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143d35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143d36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143d368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143d36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143d37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143d376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143d37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143d37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143d38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143d38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143d38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143d39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143d39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143d39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143d3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143d3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143d3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143d3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143d3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143d3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143d3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143d3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143d3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143d3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143d3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143d3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143d3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143d3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143d3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143d3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143d3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143d3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143d3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143d3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143d3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143d40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143d40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143d40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143d40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143d413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143d41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143d41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143d421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143d42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143d42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143d42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143d43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143d438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143d43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143d44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143d446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143d44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143d45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143d454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143d45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143d45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143d46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143d46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143d46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143d47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143d47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143d479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143d47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143d483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143d488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143d48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143d49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143d49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143d49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143d4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143d4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143d4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143d4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143d4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143d4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143d4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143d4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143d4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143d4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143d4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143d4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143d4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143d4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143d4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143d4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143d4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143d50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143d506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143d50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143d51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143d51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143d51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143d52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143d52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143d52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143d53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143d53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143d53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143d54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143d54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143d54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143d55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143d55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143d55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143d560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143d56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143d56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143d570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143d57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143d57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143d580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143d58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143d58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143d590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143d59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143d59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143d5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143d5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143d5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143d5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143d5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143d5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143d5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143d5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143d5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143d5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143d5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143d5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143d5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143d5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143d5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143d5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143d5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143d5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143d60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143d605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143d60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143d60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143d61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143d618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143d61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143d62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143d626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143d62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143d62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143d63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143d63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143d63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143d64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143d64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143d64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143d65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143d655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143d65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143d663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143d66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143d67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143d674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143d67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143d67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143d685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.138.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.138.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143d25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143d257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143d25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143d260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143d26520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143d26990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143d26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143d27270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143d276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143d27b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143d27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143d285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143d28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143d29610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143d29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143d2a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143d2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143d2b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143d2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143d2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143d2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143d2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143d2d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143d2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143d2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143d2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143d2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143d2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143d2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143d2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143d30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143d304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143d30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143d30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143d31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143d31500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143d31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143d31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143d32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143d326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143d32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143d32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143d33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143d33880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143d33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143d34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143d345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143d34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143d34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143d35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143d35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143d35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143d36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143d364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143d36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143d36dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143d37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143d376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143d37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143d37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143d383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143d38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143d38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143d39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143d395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143d39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143d39e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143d3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143d3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143d3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143d3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143d3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143d3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143d3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143d3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143d3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143d3caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143d3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143d3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143d3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143d3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143d3e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143d3e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143d3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143d3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143d3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143d3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143d3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143d40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143d404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143d40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143d40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143d411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143d41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143d41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143d41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143d423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143d42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143d42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143d43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143d43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143d439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143d43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143d442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143d44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143d44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143d45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143d45480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143d458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143d45d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143d461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143d46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143d46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143d46f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143d47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143d47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143d47c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143d480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143d48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143d489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143d48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143d492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143d49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143d49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143d49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143d4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143d4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143d4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143d4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143d4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143d4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143d4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143d4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143d4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143d4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143d4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143d4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143d4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143d4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143d4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143d4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143d4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143d4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143d4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143d4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143d4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143d50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143d50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143d50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143d50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143d51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143d517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143d51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143d520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143d52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143d52980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143d52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143d53260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143d536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143d53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143d53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143d54420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143d54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143d54d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143d55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143d555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143d55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143d55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143d56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143d567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143d56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143d57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143d574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143d57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143d57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143d58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143d586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143d58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143d58f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143d59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143d59870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143d59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143d5a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143d5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143d5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143d5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143d5b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143d5b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143d5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143d5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143d5c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143d5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143d5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143d5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143d5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143d5db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143d5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143d5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143d5e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143d5ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143d5f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143d5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143d5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143d5fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143d602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143d60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143d60bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143d61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143d614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143d61920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143d620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143d62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143d62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143d62df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143d63260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143d636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143d63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143d63fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143d64420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143d64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143d64d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143d65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143d655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143d65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143d65ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143d66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143d667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143d66c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143d67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143d674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143d67960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143d67dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143d68240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143d686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143d0b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143d0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143d0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143d176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143d17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143d17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143d18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143d186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143d18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143d18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143d19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143d19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143d19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143d1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143d1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143d1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143d1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143d1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143d1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143d1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143d1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143d1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143d1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143d1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143d1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143d1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143d1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143d1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143d1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143d1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143d1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143d1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143d1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143d1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143d1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143d202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143d20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143d20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143d21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143d214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143d21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143d21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143d22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143d22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143d22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143d22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143d233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143d23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143d23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143d24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143d16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143d16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143d16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143d0d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143d0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143d0de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143d0e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143d15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143d16320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143d16790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143d16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143d17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143d0a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143d17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143d17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143d18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143d185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143d18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143d18ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143d198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143d1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143d1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143d1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143d1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143d1bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143d1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143d1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143d1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143d1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143d1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143d1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143d1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143d1f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143d1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143d1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143d201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143d20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143d20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143d20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143d213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143d21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143d21ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143d21f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143d223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143d22830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143d22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143d23110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143d23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143d239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143d23e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143d242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143d0ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143d0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143d24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143d251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143d25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143d25aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143d25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143d26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143d267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143d26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143d270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143d27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143d279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143d27e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143d28290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143d28700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143d28b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143d28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143d29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143d298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143d29d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143d2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143d2a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143d2aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143d2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143d2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143d2b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143d2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143d2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143d2c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143d2c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143d2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143d2d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143d2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143d2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143d2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143d2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143d2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143d2ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143d2f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143d2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143d2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143d2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143d30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143d307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143d30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143d31090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143d31500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143d31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143d31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143d32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143d326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143d32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143d32fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143d33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143d33880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143d33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143d34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143d345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143d34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143d34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143d35320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143d35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143d35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143d36070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143d364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143d36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143d36dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143d37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143d376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143d37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143d37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143d383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143d38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143d38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143d39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143d395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143d39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143d39e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143d3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143d3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143d3abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143d3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143d3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143d3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143d3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143d3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143d3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143d3caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143d3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143d3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143d3d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143d3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143d3e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143d3e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143d3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143d3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143d3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143d3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143d3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143d40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143d404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143d40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143d40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143d411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143d41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143d41ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143d41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143d423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143d42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143d42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143d43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143d43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143d439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143d43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143d442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143d44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143d44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143d45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143d45480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143d458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143d45d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143d461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143d46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143d46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143d46f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143d47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143d47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143d47c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143d480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143d48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143d489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143d48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143d492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143d49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143d49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143d49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143d4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143d4a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143d4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143d4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143d4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143d4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143d4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143d4c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143d4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143d4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143d4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143d4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143d4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143d4de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143d4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143d4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143d4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143d4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143d4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143d4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143d4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143d50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143d50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143d50a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143d50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143d51350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143d517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143d51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143d520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143d52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143d52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143d53100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143d53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143d539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143d53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143d542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143d54730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143d54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143d55010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143d55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143d558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143d55d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143d561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143d56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143d56ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143d56f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143d57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143d57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143d57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143d580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143d58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143d589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143d58e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143d592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143d59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143d59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143d59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143d5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143d5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143d5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143d5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143d5b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143d5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143d5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143d5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143d5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143d5cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143d5d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143d5d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143d5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143d5de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143d5e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143d5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143d5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143d5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143d5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143d5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143d5fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143d60190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143d60600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143d60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143d60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143d61350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143d617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143d61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143d620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143d62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143d62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143d62df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143d63260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143d636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143d63b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143d63fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143d64420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143d64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143d64d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143d65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143d655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143d65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143d65ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143d66330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143d667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143d67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143d676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143d67de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143d684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143d0d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143d0da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143d0de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143d0e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.740s
user	0m0.291s
sys	0m0.282s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4370 (5de36876)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147809250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147809800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147809db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14780a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14780a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14780aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14780b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14780ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14780bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14780c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14780c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14780d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14780dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14780e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14780eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14780f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14780f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147810080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147810850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147810f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147811690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147811db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147812650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147812d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147813030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147813640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1478142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1478147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147814ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147814f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147815aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147815fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1478162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147816740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147816be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147817080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147817520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1478179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147817e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147818300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1478187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147818c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147818f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147819510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147819b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14781a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14781aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14781b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14781b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14781bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14781c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14781c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14781d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14781d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14781d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14781dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14781e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14781ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14781ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14781f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14781f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14781fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14781ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147820470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147820910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147820db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147821250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1478216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147821b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147822030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1478224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147822a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1478234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147823a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147823f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1478244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147824a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1478254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1478259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147825f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147826490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1478269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147826f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147827480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1478279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147827f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147828470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1478289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147828f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147829460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1478299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147829f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14782a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14781a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14782a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14782b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14782b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14782bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14782c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14782c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14782cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14782d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14782d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14782daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14782e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14782e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14782eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14782f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14782f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14782fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14782fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147830360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147830800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147830ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147831140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1478315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147831a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147831f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1478323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147832860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147832d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1478331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147833640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147833ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147833f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147834420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1478348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147835200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1478356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147835b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147835fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147836480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147836920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147836dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147837260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147837700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147837ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147838040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1478384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147838e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1478392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147839760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147839c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14783a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14783a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14783a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14783ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14783b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14783b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14783bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14783c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14783c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14783ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14783cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14783d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14783d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14783dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14783e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14783e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14783eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14783ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14783f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14783f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14783fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1478401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147840660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147840b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147840fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147841440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1478418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147841d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147842220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1478426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147842b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147843000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1478434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147843940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147843de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147844280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147844720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147844bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147845060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1478459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147845e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1478462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147846cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147847220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147847770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147847cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147847f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147848590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147848ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1478491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1478499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147849e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14784a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14784a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14784ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14784b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14784b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14784be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14784c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14784caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14784cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14784d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14784da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14784dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14784e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14784ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14784efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14784f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14784fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14784ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147850510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147850a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147850fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147851500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147851a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147851fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1478524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147852a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147852f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1478534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147853a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147853f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1478544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147854f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1478554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147855a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147855f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1478564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147856a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147856f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1478574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1478579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147857f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147858490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1478589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147858f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147859480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1478599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147859f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14785a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14785a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14785af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14785b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14785b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14785bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14785c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14785c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14785cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14785d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14785d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14785dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14785e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14785e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14785eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14785f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14785f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14785fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147860200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1478606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147860b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147860fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147861480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147861920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147861dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147862260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147862700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147862ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147863040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1478634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147863980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147863ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1478645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147864d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147865430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147865b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147865e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147866600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1478668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147866ed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1267056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1267075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1267093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12670a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12670aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12670b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12670b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12670bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12670c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12670cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12670d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12670dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12670e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12670e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12670e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12670ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12670f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12670f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12670fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12670ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1267106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1267118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126712190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1267137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1267140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1267156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126717300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126717be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126718050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1267184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12671a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12671a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12671acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12671b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12671b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12671ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12671be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12671c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12671c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12671cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12671d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12671d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12671d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12671dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12671e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12671e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12671ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12671ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12671f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12671f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12671fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1267209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126720e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1267212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1267228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1267231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126723f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1267250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1267259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1267262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1267278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126727d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1267281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126728620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126728f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126729370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1267297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12672a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12672a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12672a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12672ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12672b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12672b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12672bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12672bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12672c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12672c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12672cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12672d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12672d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12672da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12672dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12672e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12672e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12672ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12672f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12672f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12672f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12672fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1267306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126730fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126731890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126732170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1267325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126732a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126733330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1267337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126733c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126734080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1267344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126735240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1267356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126736400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126736870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126736ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126737150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1267375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126737a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126737ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126738310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126738780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126738bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126739060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1267394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126739db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12673a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12673a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12673ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12673af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12673b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12673b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12673bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12673c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12673c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12673ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12673ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12673d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12673d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12673dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12673e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12673e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12673e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12673ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12673f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12673f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12673fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12673ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1267403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126740830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126740dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126741230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1267416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1267421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1267424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126742770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126742be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126743050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1267434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126743da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126744210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126744680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126744af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126744f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1267453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126745840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126745cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126746120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126746590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126746a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126746e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1267472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126747750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126747bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126748030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1267484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126748910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126748d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1267491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126749660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126749ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126749f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12674a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12674a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12674ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12674b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12674b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12674b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12674be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12674c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12674c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12674cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12674d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12674d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12674d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12674dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12674e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12674e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12674eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12674ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12674f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12674f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12674fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1267500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126750550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1267509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126750e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1267512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126751710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126751b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126751ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126752460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1267528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126752d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1267531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126753620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126753a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126753f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126754370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1267547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126754c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1267550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126755530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1267559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126755e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126756880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126756fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1267576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126757de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1267580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126758510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126758b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126759120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1466075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146607a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146607e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14660b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14660b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14660ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14660c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14660c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14660cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14660d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14660d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14660dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14660e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14660ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14660f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14660fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146610240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146610960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146611310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146611a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146612150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146612870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1466136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146613970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146613f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146614590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146614ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146615390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146615830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146615af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146616380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1466168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146616b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146617020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1466174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146617960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146617e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1466182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146618be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146619080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1466197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14661a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14661aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14661b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14661b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14661bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14661c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14661c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14661ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14661d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14661db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14661dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14661e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14661e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14661f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14661f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14661f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14661fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1466202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146620780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146620c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1466210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146621560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146621a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146621ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146622340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1466227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1466231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146623c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1466241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146624710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1466251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146625700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1466261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1466266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146627190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1466276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146627c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146628180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1466286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146629170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1466296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146629c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14662a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14662a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14662ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14662b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14662b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14662bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14662c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14662c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14662cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14662d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14662d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14662dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14662e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14662e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14662ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14662f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14662f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14662fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146630100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1466305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146630a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146630ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146631380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146631820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146631cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146632600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146632aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146632f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1466333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146633880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146633d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1466341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146634660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146634b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146634fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146635440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1466358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146635d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146636220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1466366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146636b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146637000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1466374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146637940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146637de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146638280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146638720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146638bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146639060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146639500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1466399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146639e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14663a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14663a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14663ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14663b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14663b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14663ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14663bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14663c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14663c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14663cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14663d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14663d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14663da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14663df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14663e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14663e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14663ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14663f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14663f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14663fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14663ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146640400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1466408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146640d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1466411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146641680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146641b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146641fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146642460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146642900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146642da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146643240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1466436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146643b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146644020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1466444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146644960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146644e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1466452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146645740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146645be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146646080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146646520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1466469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146646e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146647300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146647850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146647da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1466482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146648840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146648b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146649110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146649720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146649d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14664a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14664a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14664ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14664b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14664b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14664c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14664c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14664c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14664ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14664d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14664db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14664e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14664e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14664eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14664f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14664f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14664fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1466500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1466505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146650b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146651090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1466515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146651b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146652080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1466525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146652b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146653070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1466535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146653b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146654060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1466545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146654b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146655050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1466555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146655af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146656040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146656ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146658020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146658570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146658ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146659010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146659560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14665a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14665a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14665aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14665aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14665b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14665ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14665bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14665c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14665ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14665cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14665d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14665da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14665dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14665e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14665ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14665efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14665f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14665fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14665ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146660440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1466608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146660d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146661220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1466616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146661b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146662000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1466624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146662940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146662de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146663280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146663720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146663bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146664060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146664500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146664a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146665890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146665fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1466666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146667180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146667440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146667a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.245s
sys	0m0.155s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
