Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.570s
user	0m0.880s
sys	0m1.228s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha1
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library ../../bin/libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 36%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Built target llava
[ 36%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-chat
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-chat
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Linking CXX executable ../bin/test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-arg-parser
[ 63%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-backend-ops
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Built target test-barrier
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Built target test-quantize-perf
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gguf-split
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-eval-callback
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Built target llama-infill
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookahead
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-stats
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Built target llama-perplexity
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-speculative
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-run
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-gen-docs
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.048s
user	0m6.508s
sys	0m9.803s

main: quantize time =  6084.40 ms
main:    total time =  6084.40 ms

main: quantize time =  3617.67 ms
main:    total time =  3617.67 ms

main: quantize time =  3854.99 ms
main:    total time =  3854.99 ms

main: quantize time =  1896.55 ms
main:    total time =  1896.55 ms

main: quantize time =  2148.66 ms
main:    total time =  2148.66 ms

main: quantize time =  5233.83 ms
main:    total time =  5233.83 ms

main: quantize time =  5680.62 ms
main:    total time =  5680.62 ms

main: quantize time =  6748.89 ms
main:    total time =  6748.89 ms

main: quantize time =  5966.17 ms
main:    total time =  5966.17 ms

main: quantize time =  4719.62 ms
main:    total time =  4719.62 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.182 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.334 I main: llama backend init
0.00.000.341 I main: load the model and apply lora adapter, if any
0.00.094.292 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.106.717 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.106.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.106.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.106.738 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.106.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.106.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.106.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.106.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.106.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.106.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.106.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.106.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.106.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.106.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.106.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.106.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.106.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.113.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.115.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.122.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.122.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.122.534 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.122.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.122.536 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.122.537 I llama_model_loader: - type  f32:  194 tensors
0.00.122.538 I llama_model_loader: - type  f16:   98 tensors
0.00.122.539 I print_info: file format = GGUF V3 (latest)
0.00.122.541 I print_info: file type   = all F32 (guessed)
0.00.122.544 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.140.771 I load: special tokens cache size = 25
0.00.151.270 I load: token to piece cache size = 0.2984 MB
0.00.151.276 I print_info: arch             = gptneox
0.00.151.276 I print_info: vocab_only       = 0
0.00.151.276 I print_info: n_ctx_train      = 2048
0.00.151.277 I print_info: n_embd           = 2048
0.00.151.277 I print_info: n_layer          = 24
0.00.151.282 I print_info: n_head           = 16
0.00.151.283 I print_info: n_head_kv        = 16
0.00.151.285 I print_info: n_rot            = 32
0.00.151.285 I print_info: n_swa            = 0
0.00.151.285 I print_info: n_embd_head_k    = 128
0.00.151.285 I print_info: n_embd_head_v    = 128
0.00.151.288 I print_info: n_gqa            = 1
0.00.151.289 I print_info: n_embd_k_gqa     = 2048
0.00.151.292 I print_info: n_embd_v_gqa     = 2048
0.00.151.292 I print_info: f_norm_eps       = 1.0e-05
0.00.151.293 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.151.293 I print_info: f_clamp_kqv      = 0.0e+00
0.00.151.293 I print_info: f_max_alibi_bias = 0.0e+00
0.00.151.294 I print_info: f_logit_scale    = 0.0e+00
0.00.151.295 I print_info: n_ff             = 8192
0.00.151.295 I print_info: n_expert         = 0
0.00.151.295 I print_info: n_expert_used    = 0
0.00.151.295 I print_info: causal attn      = 1
0.00.151.295 I print_info: pooling type     = 0
0.00.151.296 I print_info: rope type        = 2
0.00.151.296 I print_info: rope scaling     = linear
0.00.151.297 I print_info: freq_base_train  = 10000.0
0.00.151.297 I print_info: freq_scale_train = 1
0.00.151.297 I print_info: n_ctx_orig_yarn  = 2048
0.00.151.297 I print_info: rope_finetuned   = unknown
0.00.151.298 I print_info: ssm_d_conv       = 0
0.00.151.299 I print_info: ssm_d_inner      = 0
0.00.151.299 I print_info: ssm_d_state      = 0
0.00.151.300 I print_info: ssm_dt_rank      = 0
0.00.151.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.151.302 I print_info: model type       = 1.4B
0.00.151.302 I print_info: model params     = 1.41 B
0.00.151.302 I print_info: general.name     = 1.4B
0.00.151.303 I print_info: vocab type       = BPE
0.00.151.304 I print_info: n_vocab          = 50304
0.00.151.304 I print_info: n_merges         = 50009
0.00.151.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.151.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.151.305 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.151.305 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.151.305 I print_info: LF token         = 187 'Ċ'
0.00.151.306 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.151.306 I print_info: max token length = 1024
0.00.199.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.199.972 I load_tensors: offloading output layer to GPU
0.00.199.973 I load_tensors: offloaded 25/25 layers to GPU
0.00.199.998 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.199.999 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.200.545 I llama_context: n_seq_max     = 1
0.00.200.546 I llama_context: n_ctx         = 2048
0.00.200.546 I llama_context: n_ctx_per_seq = 2048
0.00.200.546 I llama_context: n_batch       = 2048
0.00.200.546 I llama_context: n_ubatch      = 512
0.00.200.546 I llama_context: flash_attn    = 0
0.00.200.547 I llama_context: freq_base     = 10000.0
0.00.200.547 I llama_context: freq_scale    = 1
0.00.200.547 I ggml_metal_init: allocating
0.00.200.586 I ggml_metal_init: found device: Apple M4
0.00.200.592 I ggml_metal_init: picking default device: Apple M4
0.00.201.195 I ggml_metal_init: using embedded metal library
0.00.379.717 I ggml_metal_init: GPU name:   Apple M4
0.00.379.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.379.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.379.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.379.727 I ggml_metal_init: simdgroup reduction   = true
0.00.379.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.379.727 I ggml_metal_init: has residency sets    = true
0.00.379.728 I ggml_metal_init: has bfloat            = true
0.00.379.728 I ggml_metal_init: use bfloat            = true
0.00.379.729 I ggml_metal_init: hasUnifiedMemory      = true
0.00.379.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.436.918 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.843 I init:      Metal KV buffer size =   384.00 MiB
0.00.473.850 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.473.873 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.477.383 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.477.386 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.477.386 I llama_context: graph nodes  = 967
0.00.477.386 I llama_context: graph splits = 2
0.00.477.391 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.477.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.477.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.642 I main: llama threadpool init, n_threads = 4
0.00.548.679 I 
0.00.548.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.715 I 
0.00.548.939 I sampler seed: 1234
0.00.548.944 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.548.976 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.548.979 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.548.979 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.394.000 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.02.394.001 I llama_perf_context_print:        load time =     452.87 ms
0.02.394.002 I llama_perf_context_print: prompt eval time =      44.29 ms /     7 tokens (    6.33 ms per token,   158.04 tokens per second)
0.02.394.002 I llama_perf_context_print:        eval time =    1797.83 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.394.003 I llama_perf_context_print:       total time =    1846.83 ms /    70 tokens
0.02.396.446 I ggml_metal_free: deallocating

real	0m2.735s
user	0m0.149s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.968 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.958 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.958 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.960 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.064 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.378 I llama_model_loader: - type  f32:  194 tensors
0.00.040.378 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.379 I print_info: file format = GGUF V3 (latest)
0.00.040.380 I print_info: file type   = Q8_0
0.00.040.381 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.592 I load: special tokens cache size = 25
0.00.057.025 I load: token to piece cache size = 0.2984 MB
0.00.057.029 I print_info: arch             = gptneox
0.00.057.029 I print_info: vocab_only       = 0
0.00.057.029 I print_info: n_ctx_train      = 2048
0.00.057.030 I print_info: n_embd           = 2048
0.00.057.030 I print_info: n_layer          = 24
0.00.057.034 I print_info: n_head           = 16
0.00.057.035 I print_info: n_head_kv        = 16
0.00.057.035 I print_info: n_rot            = 32
0.00.057.036 I print_info: n_swa            = 0
0.00.057.036 I print_info: n_embd_head_k    = 128
0.00.057.036 I print_info: n_embd_head_v    = 128
0.00.057.037 I print_info: n_gqa            = 1
0.00.057.037 I print_info: n_embd_k_gqa     = 2048
0.00.057.038 I print_info: n_embd_v_gqa     = 2048
0.00.057.040 I print_info: f_norm_eps       = 1.0e-05
0.00.057.042 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.042 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.042 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.042 I print_info: f_logit_scale    = 0.0e+00
0.00.057.043 I print_info: n_ff             = 8192
0.00.057.044 I print_info: n_expert         = 0
0.00.057.044 I print_info: n_expert_used    = 0
0.00.057.044 I print_info: causal attn      = 1
0.00.057.044 I print_info: pooling type     = 0
0.00.057.044 I print_info: rope type        = 2
0.00.057.045 I print_info: rope scaling     = linear
0.00.057.045 I print_info: freq_base_train  = 10000.0
0.00.057.045 I print_info: freq_scale_train = 1
0.00.057.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.046 I print_info: rope_finetuned   = unknown
0.00.057.046 I print_info: ssm_d_conv       = 0
0.00.057.046 I print_info: ssm_d_inner      = 0
0.00.057.046 I print_info: ssm_d_state      = 0
0.00.057.047 I print_info: ssm_dt_rank      = 0
0.00.057.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.047 I print_info: model type       = 1.4B
0.00.057.048 I print_info: model params     = 1.41 B
0.00.057.048 I print_info: general.name     = 1.4B
0.00.057.048 I print_info: vocab type       = BPE
0.00.057.049 I print_info: n_vocab          = 50304
0.00.057.049 I print_info: n_merges         = 50009
0.00.057.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.052 I print_info: LF token         = 187 'Ċ'
0.00.057.052 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.053 I print_info: max token length = 1024
0.01.204.224 I load_tensors: offloading 24 repeating layers to GPU
0.01.204.230 I load_tensors: offloading output layer to GPU
0.01.204.231 I load_tensors: offloaded 25/25 layers to GPU
0.01.204.254 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.204.255 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.205.423 I llama_context: n_seq_max     = 1
0.01.205.425 I llama_context: n_ctx         = 2048
0.01.205.425 I llama_context: n_ctx_per_seq = 2048
0.01.205.426 I llama_context: n_batch       = 2048
0.01.205.426 I llama_context: n_ubatch      = 512
0.01.205.427 I llama_context: flash_attn    = 0
0.01.205.427 I llama_context: freq_base     = 10000.0
0.01.205.428 I llama_context: freq_scale    = 1
0.01.205.429 I ggml_metal_init: allocating
0.01.205.443 I ggml_metal_init: found device: Apple M4
0.01.205.451 I ggml_metal_init: picking default device: Apple M4
0.01.206.648 I ggml_metal_init: using embedded metal library
0.01.212.269 I ggml_metal_init: GPU name:   Apple M4
0.01.212.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.212.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.212.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.212.274 I ggml_metal_init: simdgroup reduction   = true
0.01.212.274 I ggml_metal_init: simdgroup matrix mul. = true
0.01.212.275 I ggml_metal_init: has residency sets    = true
0.01.212.275 I ggml_metal_init: has bfloat            = true
0.01.212.275 I ggml_metal_init: use bfloat            = true
0.01.212.276 I ggml_metal_init: hasUnifiedMemory      = true
0.01.212.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.228.425 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.283.712 I init:      Metal KV buffer size =   384.00 MiB
0.01.283.720 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.283.744 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.288.166 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.288.168 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.288.168 I llama_context: graph nodes  = 967
0.01.288.168 I llama_context: graph splits = 2
0.01.288.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.288.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.288.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.341.994 I main: llama threadpool init, n_threads = 4
0.01.342.039 I 
0.01.342.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.342.062 I 
0.01.342.227 I sampler seed: 1234
0.01.342.231 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.342.254 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.342.255 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.342.255 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.442.987 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48431.11 tokens per second)
0.02.442.988 I llama_perf_context_print:        load time =    1331.09 ms
0.02.442.990 I llama_perf_context_print: prompt eval time =      49.40 ms /     7 tokens (    7.06 ms per token,   141.69 tokens per second)
0.02.442.991 I llama_perf_context_print:        eval time =    1048.67 ms /    63 runs   (   16.65 ms per token,    60.08 tokens per second)
0.02.442.992 I llama_perf_context_print:       total time =    1101.93 ms /    70 tokens
0.02.446.144 I ggml_metal_free: deallocating

real	0m2.463s
user	0m0.111s
sys	0m0.263s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.400 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.108 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.112 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.114 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.114 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.625 I llama_model_loader: - type  f32:  194 tensors
0.00.025.625 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.626 I print_info: file format = GGUF V3 (latest)
0.00.025.627 I print_info: file type   = Q4_0
0.00.025.628 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.754 I load: special tokens cache size = 25
0.00.039.745 I load: token to piece cache size = 0.2984 MB
0.00.039.748 I print_info: arch             = gptneox
0.00.039.748 I print_info: vocab_only       = 0
0.00.039.748 I print_info: n_ctx_train      = 2048
0.00.039.748 I print_info: n_embd           = 2048
0.00.039.748 I print_info: n_layer          = 24
0.00.039.752 I print_info: n_head           = 16
0.00.039.753 I print_info: n_head_kv        = 16
0.00.039.753 I print_info: n_rot            = 32
0.00.039.753 I print_info: n_swa            = 0
0.00.039.753 I print_info: n_embd_head_k    = 128
0.00.039.754 I print_info: n_embd_head_v    = 128
0.00.039.754 I print_info: n_gqa            = 1
0.00.039.756 I print_info: n_embd_k_gqa     = 2048
0.00.039.757 I print_info: n_embd_v_gqa     = 2048
0.00.039.757 I print_info: f_norm_eps       = 1.0e-05
0.00.039.759 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.760 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.760 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.760 I print_info: f_logit_scale    = 0.0e+00
0.00.039.762 I print_info: n_ff             = 8192
0.00.039.762 I print_info: n_expert         = 0
0.00.039.762 I print_info: n_expert_used    = 0
0.00.039.762 I print_info: causal attn      = 1
0.00.039.763 I print_info: pooling type     = 0
0.00.039.763 I print_info: rope type        = 2
0.00.039.763 I print_info: rope scaling     = linear
0.00.039.763 I print_info: freq_base_train  = 10000.0
0.00.039.764 I print_info: freq_scale_train = 1
0.00.039.764 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.764 I print_info: rope_finetuned   = unknown
0.00.039.764 I print_info: ssm_d_conv       = 0
0.00.039.765 I print_info: ssm_d_inner      = 0
0.00.039.765 I print_info: ssm_d_state      = 0
0.00.039.765 I print_info: ssm_dt_rank      = 0
0.00.039.765 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.765 I print_info: model type       = 1.4B
0.00.039.766 I print_info: model params     = 1.41 B
0.00.039.766 I print_info: general.name     = 1.4B
0.00.039.766 I print_info: vocab type       = BPE
0.00.039.767 I print_info: n_vocab          = 50304
0.00.039.767 I print_info: n_merges         = 50009
0.00.039.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: LF token         = 187 'Ċ'
0.00.039.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: max token length = 1024
0.00.600.855 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.866 I load_tensors: offloading output layer to GPU
0.00.600.867 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.897 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.600.899 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.602.046 I llama_context: n_seq_max     = 1
0.00.602.056 I llama_context: n_ctx         = 2048
0.00.602.056 I llama_context: n_ctx_per_seq = 2048
0.00.602.057 I llama_context: n_batch       = 2048
0.00.602.057 I llama_context: n_ubatch      = 512
0.00.602.058 I llama_context: flash_attn    = 0
0.00.602.059 I llama_context: freq_base     = 10000.0
0.00.602.060 I llama_context: freq_scale    = 1
0.00.602.067 I ggml_metal_init: allocating
0.00.602.142 I ggml_metal_init: found device: Apple M4
0.00.602.154 I ggml_metal_init: picking default device: Apple M4
0.00.604.098 I ggml_metal_init: using embedded metal library
0.00.610.786 I ggml_metal_init: GPU name:   Apple M4
0.00.610.791 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.793 I ggml_metal_init: simdgroup reduction   = true
0.00.610.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.794 I ggml_metal_init: has residency sets    = true
0.00.610.794 I ggml_metal_init: has bfloat            = true
0.00.610.794 I ggml_metal_init: use bfloat            = true
0.00.610.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.794 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.688.213 I init:      Metal KV buffer size =   384.00 MiB
0.00.688.220 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.688.243 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.693.412 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.693.415 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.693.415 I llama_context: graph nodes  = 967
0.00.693.415 I llama_context: graph splits = 2
0.00.693.420 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.693.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.693.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.979 I main: llama threadpool init, n_threads = 4
0.00.753.023 I 
0.00.753.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.047 I 
0.00.753.227 I sampler seed: 1234
0.00.753.232 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.247 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.247 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.247 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.436.141 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.436.141 I llama_perf_context_print:        load time =     741.61 ms
0.01.436.143 I llama_perf_context_print: prompt eval time =      49.08 ms /     7 tokens (    7.01 ms per token,   142.61 tokens per second)
0.01.436.144 I llama_perf_context_print:        eval time =     630.98 ms /    63 runs   (   10.02 ms per token,    99.85 tokens per second)
0.01.436.144 I llama_perf_context_print:       total time =     684.13 ms /    70 tokens
0.01.439.981 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.620 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.280 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.281 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.282 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.952 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.953 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.954 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.955 I llama_model_loader: - type  f32:  194 tensors
0.00.023.955 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.955 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.956 I print_info: file format = GGUF V3 (latest)
0.00.023.957 I print_info: file type   = Q4_1
0.00.023.957 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.122 I load: special tokens cache size = 25
0.00.038.239 I load: token to piece cache size = 0.2984 MB
0.00.038.242 I print_info: arch             = gptneox
0.00.038.242 I print_info: vocab_only       = 0
0.00.038.242 I print_info: n_ctx_train      = 2048
0.00.038.242 I print_info: n_embd           = 2048
0.00.038.243 I print_info: n_layer          = 24
0.00.038.246 I print_info: n_head           = 16
0.00.038.246 I print_info: n_head_kv        = 16
0.00.038.248 I print_info: n_rot            = 32
0.00.038.248 I print_info: n_swa            = 0
0.00.038.249 I print_info: n_embd_head_k    = 128
0.00.038.249 I print_info: n_embd_head_v    = 128
0.00.038.250 I print_info: n_gqa            = 1
0.00.038.250 I print_info: n_embd_k_gqa     = 2048
0.00.038.251 I print_info: n_embd_v_gqa     = 2048
0.00.038.252 I print_info: f_norm_eps       = 1.0e-05
0.00.038.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.252 I print_info: f_logit_scale    = 0.0e+00
0.00.038.253 I print_info: n_ff             = 8192
0.00.038.253 I print_info: n_expert         = 0
0.00.038.254 I print_info: n_expert_used    = 0
0.00.038.254 I print_info: causal attn      = 1
0.00.038.254 I print_info: pooling type     = 0
0.00.038.254 I print_info: rope type        = 2
0.00.038.254 I print_info: rope scaling     = linear
0.00.038.255 I print_info: freq_base_train  = 10000.0
0.00.038.255 I print_info: freq_scale_train = 1
0.00.038.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.255 I print_info: rope_finetuned   = unknown
0.00.038.255 I print_info: ssm_d_conv       = 0
0.00.038.256 I print_info: ssm_d_inner      = 0
0.00.038.256 I print_info: ssm_d_state      = 0
0.00.038.256 I print_info: ssm_dt_rank      = 0
0.00.038.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.257 I print_info: model type       = 1.4B
0.00.038.257 I print_info: model params     = 1.41 B
0.00.038.257 I print_info: general.name     = 1.4B
0.00.038.258 I print_info: vocab type       = BPE
0.00.038.258 I print_info: n_vocab          = 50304
0.00.038.258 I print_info: n_merges         = 50009
0.00.038.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.260 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.260 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.260 I print_info: LF token         = 187 'Ċ'
0.00.038.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.261 I print_info: max token length = 1024
0.00.630.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.919 I load_tensors: offloading output layer to GPU
0.00.630.919 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.963 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.630.964 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.632.545 I llama_context: n_seq_max     = 1
0.00.632.555 I llama_context: n_ctx         = 2048
0.00.632.555 I llama_context: n_ctx_per_seq = 2048
0.00.632.555 I llama_context: n_batch       = 2048
0.00.632.556 I llama_context: n_ubatch      = 512
0.00.632.556 I llama_context: flash_attn    = 0
0.00.632.562 I llama_context: freq_base     = 10000.0
0.00.632.563 I llama_context: freq_scale    = 1
0.00.632.565 I ggml_metal_init: allocating
0.00.632.659 I ggml_metal_init: found device: Apple M4
0.00.632.711 I ggml_metal_init: picking default device: Apple M4
0.00.634.514 I ggml_metal_init: using embedded metal library
0.00.641.154 I ggml_metal_init: GPU name:   Apple M4
0.00.641.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.160 I ggml_metal_init: simdgroup reduction   = true
0.00.641.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.161 I ggml_metal_init: has residency sets    = true
0.00.641.161 I ggml_metal_init: has bfloat            = true
0.00.641.161 I ggml_metal_init: use bfloat            = true
0.00.641.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.417 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.182 I init:      Metal KV buffer size =   384.00 MiB
0.00.714.190 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.218 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.585 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.719.588 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.719.588 I llama_context: graph nodes  = 967
0.00.719.588 I llama_context: graph splits = 2
0.00.719.593 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.531 I main: llama threadpool init, n_threads = 4
0.00.772.576 I 
0.00.772.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.601 I 
0.00.772.772 I sampler seed: 1234
0.00.772.776 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.793 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.793 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.085 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.497.086 I llama_perf_context_print:        load time =     762.88 ms
0.01.497.087 I llama_perf_context_print: prompt eval time =      39.18 ms /     7 tokens (    5.60 ms per token,   178.64 tokens per second)
0.01.497.088 I llama_perf_context_print:        eval time =     682.32 ms /    63 runs   (   10.83 ms per token,    92.33 tokens per second)
0.01.497.089 I llama_perf_context_print:       total time =     725.59 ms /    70 tokens
0.01.500.819 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.109s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.511 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.040 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.053 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.053 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.054 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.865 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.654 I llama_model_loader: - type  f32:  194 tensors
0.00.023.655 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.655 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.655 I print_info: file format = GGUF V3 (latest)
0.00.023.656 I print_info: file type   = Q5_0
0.00.023.657 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.031.481 I load: special tokens cache size = 25
0.00.037.441 I load: token to piece cache size = 0.2984 MB
0.00.037.444 I print_info: arch             = gptneox
0.00.037.444 I print_info: vocab_only       = 0
0.00.037.444 I print_info: n_ctx_train      = 2048
0.00.037.445 I print_info: n_embd           = 2048
0.00.037.445 I print_info: n_layer          = 24
0.00.037.448 I print_info: n_head           = 16
0.00.037.449 I print_info: n_head_kv        = 16
0.00.037.449 I print_info: n_rot            = 32
0.00.037.449 I print_info: n_swa            = 0
0.00.037.449 I print_info: n_embd_head_k    = 128
0.00.037.449 I print_info: n_embd_head_v    = 128
0.00.037.450 I print_info: n_gqa            = 1
0.00.037.451 I print_info: n_embd_k_gqa     = 2048
0.00.037.452 I print_info: n_embd_v_gqa     = 2048
0.00.037.452 I print_info: f_norm_eps       = 1.0e-05
0.00.037.453 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.453 I print_info: f_logit_scale    = 0.0e+00
0.00.037.454 I print_info: n_ff             = 8192
0.00.037.454 I print_info: n_expert         = 0
0.00.037.454 I print_info: n_expert_used    = 0
0.00.037.454 I print_info: causal attn      = 1
0.00.037.455 I print_info: pooling type     = 0
0.00.037.455 I print_info: rope type        = 2
0.00.037.455 I print_info: rope scaling     = linear
0.00.037.456 I print_info: freq_base_train  = 10000.0
0.00.037.456 I print_info: freq_scale_train = 1
0.00.037.456 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.456 I print_info: rope_finetuned   = unknown
0.00.037.457 I print_info: ssm_d_conv       = 0
0.00.037.457 I print_info: ssm_d_inner      = 0
0.00.037.457 I print_info: ssm_d_state      = 0
0.00.037.457 I print_info: ssm_dt_rank      = 0
0.00.037.457 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.457 I print_info: model type       = 1.4B
0.00.037.458 I print_info: model params     = 1.41 B
0.00.037.461 I print_info: general.name     = 1.4B
0.00.037.461 I print_info: vocab type       = BPE
0.00.037.461 I print_info: n_vocab          = 50304
0.00.037.462 I print_info: n_merges         = 50009
0.00.037.462 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.463 I print_info: LF token         = 187 'Ċ'
0.00.037.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.463 I print_info: max token length = 1024
0.00.690.300 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.316 I load_tensors: offloading output layer to GPU
0.00.690.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.354 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.690.355 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.691.865 I llama_context: n_seq_max     = 1
0.00.691.871 I llama_context: n_ctx         = 2048
0.00.691.871 I llama_context: n_ctx_per_seq = 2048
0.00.691.872 I llama_context: n_batch       = 2048
0.00.691.872 I llama_context: n_ubatch      = 512
0.00.691.872 I llama_context: flash_attn    = 0
0.00.691.875 I llama_context: freq_base     = 10000.0
0.00.691.875 I llama_context: freq_scale    = 1
0.00.691.877 I ggml_metal_init: allocating
0.00.691.990 I ggml_metal_init: found device: Apple M4
0.00.692.004 I ggml_metal_init: picking default device: Apple M4
0.00.693.828 I ggml_metal_init: using embedded metal library
0.00.700.501 I ggml_metal_init: GPU name:   Apple M4
0.00.700.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.507 I ggml_metal_init: simdgroup reduction   = true
0.00.700.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.508 I ggml_metal_init: has residency sets    = true
0.00.700.508 I ggml_metal_init: has bfloat            = true
0.00.700.508 I ggml_metal_init: use bfloat            = true
0.00.700.509 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.718.452 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.951 I init:      Metal KV buffer size =   384.00 MiB
0.00.773.958 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.773.982 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.778.228 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.778.231 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.778.231 I llama_context: graph nodes  = 967
0.00.778.231 I llama_context: graph splits = 2
0.00.778.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.778.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.778.365 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.238 I main: llama threadpool init, n_threads = 4
0.00.836.283 I 
0.00.836.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.308 I 
0.00.836.485 I sampler seed: 1234
0.00.836.490 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.540 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.544 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.544 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.634.984 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.01.634.984 I llama_perf_context_print:        load time =     826.69 ms
0.01.634.985 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.634.986 I llama_perf_context_print:        eval time =     742.55 ms /    63 runs   (   11.79 ms per token,    84.84 tokens per second)
0.01.634.986 I llama_perf_context_print:       total time =     799.78 ms /    70 tokens
0.01.638.946 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.108s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.070 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.074 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.075 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.613 I llama_model_loader: - type  f32:  194 tensors
0.00.024.613 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.613 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.614 I print_info: file format = GGUF V3 (latest)
0.00.024.614 I print_info: file type   = Q5_1
0.00.024.616 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.491 I load: special tokens cache size = 25
0.00.038.512 I load: token to piece cache size = 0.2984 MB
0.00.038.515 I print_info: arch             = gptneox
0.00.038.515 I print_info: vocab_only       = 0
0.00.038.516 I print_info: n_ctx_train      = 2048
0.00.038.516 I print_info: n_embd           = 2048
0.00.038.516 I print_info: n_layer          = 24
0.00.038.519 I print_info: n_head           = 16
0.00.038.520 I print_info: n_head_kv        = 16
0.00.038.520 I print_info: n_rot            = 32
0.00.038.520 I print_info: n_swa            = 0
0.00.038.520 I print_info: n_embd_head_k    = 128
0.00.038.520 I print_info: n_embd_head_v    = 128
0.00.038.521 I print_info: n_gqa            = 1
0.00.038.522 I print_info: n_embd_k_gqa     = 2048
0.00.038.523 I print_info: n_embd_v_gqa     = 2048
0.00.038.523 I print_info: f_norm_eps       = 1.0e-05
0.00.038.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.524 I print_info: f_logit_scale    = 0.0e+00
0.00.038.525 I print_info: n_ff             = 8192
0.00.038.525 I print_info: n_expert         = 0
0.00.038.525 I print_info: n_expert_used    = 0
0.00.038.525 I print_info: causal attn      = 1
0.00.038.526 I print_info: pooling type     = 0
0.00.038.526 I print_info: rope type        = 2
0.00.038.526 I print_info: rope scaling     = linear
0.00.038.526 I print_info: freq_base_train  = 10000.0
0.00.038.527 I print_info: freq_scale_train = 1
0.00.038.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.529 I print_info: rope_finetuned   = unknown
0.00.038.529 I print_info: ssm_d_conv       = 0
0.00.038.530 I print_info: ssm_d_inner      = 0
0.00.038.530 I print_info: ssm_d_state      = 0
0.00.038.530 I print_info: ssm_dt_rank      = 0
0.00.038.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.530 I print_info: model type       = 1.4B
0.00.038.531 I print_info: model params     = 1.41 B
0.00.038.531 I print_info: general.name     = 1.4B
0.00.038.531 I print_info: vocab type       = BPE
0.00.038.531 I print_info: n_vocab          = 50304
0.00.038.531 I print_info: n_merges         = 50009
0.00.038.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.533 I print_info: LF token         = 187 'Ċ'
0.00.038.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: max token length = 1024
0.00.606.524 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.540 I load_tensors: offloading output layer to GPU
0.00.606.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.577 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.606.579 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.608.146 I llama_context: n_seq_max     = 1
0.00.608.151 I llama_context: n_ctx         = 2048
0.00.608.151 I llama_context: n_ctx_per_seq = 2048
0.00.608.152 I llama_context: n_batch       = 2048
0.00.608.152 I llama_context: n_ubatch      = 512
0.00.608.152 I llama_context: flash_attn    = 0
0.00.608.155 I llama_context: freq_base     = 10000.0
0.00.608.155 I llama_context: freq_scale    = 1
0.00.608.162 I ggml_metal_init: allocating
0.00.608.271 I ggml_metal_init: found device: Apple M4
0.00.608.286 I ggml_metal_init: picking default device: Apple M4
0.00.610.052 I ggml_metal_init: using embedded metal library
0.00.616.481 I ggml_metal_init: GPU name:   Apple M4
0.00.616.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.488 I ggml_metal_init: simdgroup reduction   = true
0.00.616.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.488 I ggml_metal_init: has residency sets    = true
0.00.616.488 I ggml_metal_init: has bfloat            = true
0.00.616.489 I ggml_metal_init: use bfloat            = true
0.00.616.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.491 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.247 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.689 I init:      Metal KV buffer size =   384.00 MiB
0.00.686.697 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.686.721 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.691.333 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.691.335 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.691.336 I llama_context: graph nodes  = 967
0.00.691.336 I llama_context: graph splits = 2
0.00.691.341 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.249 I main: llama threadpool init, n_threads = 4
0.00.747.292 I 
0.00.747.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.320 I 
0.00.747.493 I sampler seed: 1234
0.00.747.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.521 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.522 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.522 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.590.802 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46710.53 tokens per second)
0.01.590.802 I llama_perf_context_print:        load time =     736.69 ms
0.01.590.803 I llama_perf_context_print: prompt eval time =      41.94 ms /     7 tokens (    5.99 ms per token,   166.91 tokens per second)
0.01.590.804 I llama_perf_context_print:        eval time =     798.43 ms /    63 runs   (   12.67 ms per token,    78.90 tokens per second)
0.01.590.804 I llama_perf_context_print:       total time =     844.48 ms /    70 tokens
0.01.594.824 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.317 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.319 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.320 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.052 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.052 I llama_model_loader: - type  f32:  194 tensors
0.00.024.052 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.053 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.053 I print_info: file format = GGUF V3 (latest)
0.00.024.054 I print_info: file type   = Q2_K - Medium
0.00.024.055 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.205 I load: special tokens cache size = 25
0.00.038.207 I load: token to piece cache size = 0.2984 MB
0.00.038.210 I print_info: arch             = gptneox
0.00.038.210 I print_info: vocab_only       = 0
0.00.038.210 I print_info: n_ctx_train      = 2048
0.00.038.211 I print_info: n_embd           = 2048
0.00.038.211 I print_info: n_layer          = 24
0.00.038.214 I print_info: n_head           = 16
0.00.038.214 I print_info: n_head_kv        = 16
0.00.038.215 I print_info: n_rot            = 32
0.00.038.215 I print_info: n_swa            = 0
0.00.038.215 I print_info: n_embd_head_k    = 128
0.00.038.215 I print_info: n_embd_head_v    = 128
0.00.038.216 I print_info: n_gqa            = 1
0.00.038.217 I print_info: n_embd_k_gqa     = 2048
0.00.038.218 I print_info: n_embd_v_gqa     = 2048
0.00.038.218 I print_info: f_norm_eps       = 1.0e-05
0.00.038.219 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.219 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.219 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.219 I print_info: f_logit_scale    = 0.0e+00
0.00.038.220 I print_info: n_ff             = 8192
0.00.038.220 I print_info: n_expert         = 0
0.00.038.221 I print_info: n_expert_used    = 0
0.00.038.221 I print_info: causal attn      = 1
0.00.038.221 I print_info: pooling type     = 0
0.00.038.221 I print_info: rope type        = 2
0.00.038.222 I print_info: rope scaling     = linear
0.00.038.224 I print_info: freq_base_train  = 10000.0
0.00.038.224 I print_info: freq_scale_train = 1
0.00.038.224 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.224 I print_info: rope_finetuned   = unknown
0.00.038.225 I print_info: ssm_d_conv       = 0
0.00.038.225 I print_info: ssm_d_inner      = 0
0.00.038.225 I print_info: ssm_d_state      = 0
0.00.038.225 I print_info: ssm_dt_rank      = 0
0.00.038.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.226 I print_info: model type       = 1.4B
0.00.038.226 I print_info: model params     = 1.41 B
0.00.038.226 I print_info: general.name     = 1.4B
0.00.038.227 I print_info: vocab type       = BPE
0.00.038.227 I print_info: n_vocab          = 50304
0.00.038.227 I print_info: n_merges         = 50009
0.00.038.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.228 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.228 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.228 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.228 I print_info: LF token         = 187 'Ċ'
0.00.038.230 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.230 I print_info: max token length = 1024
0.00.352.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.352.085 I load_tensors: offloading output layer to GPU
0.00.352.086 I load_tensors: offloaded 25/25 layers to GPU
0.00.352.120 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.352.125 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.353.538 I llama_context: n_seq_max     = 1
0.00.353.545 I llama_context: n_ctx         = 2048
0.00.353.546 I llama_context: n_ctx_per_seq = 2048
0.00.353.546 I llama_context: n_batch       = 2048
0.00.353.546 I llama_context: n_ubatch      = 512
0.00.353.547 I llama_context: flash_attn    = 0
0.00.353.549 I llama_context: freq_base     = 10000.0
0.00.353.553 I llama_context: freq_scale    = 1
0.00.353.555 I ggml_metal_init: allocating
0.00.353.625 I ggml_metal_init: found device: Apple M4
0.00.353.640 I ggml_metal_init: picking default device: Apple M4
0.00.355.353 I ggml_metal_init: using embedded metal library
0.00.360.884 I ggml_metal_init: GPU name:   Apple M4
0.00.360.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.902 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.902 I ggml_metal_init: simdgroup reduction   = true
0.00.360.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.903 I ggml_metal_init: has residency sets    = true
0.00.360.903 I ggml_metal_init: has bfloat            = true
0.00.360.903 I ggml_metal_init: use bfloat            = true
0.00.360.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.795 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.414.153 I init:      Metal KV buffer size =   384.00 MiB
0.00.414.160 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.414.185 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.419.711 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.419.713 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.419.713 I llama_context: graph nodes  = 967
0.00.419.713 I llama_context: graph splits = 2
0.00.419.719 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.419.849 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.419.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.481.097 I main: llama threadpool init, n_threads = 4
0.00.481.138 I 
0.00.481.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.160 I 
0.00.481.332 I sampler seed: 1234
0.00.481.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.481.379 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.481.383 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.481.383 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.161.314 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.161.315 I llama_perf_context_print:        load time =     471.50 ms
0.01.161.316 I llama_perf_context_print: prompt eval time =      40.22 ms /     7 tokens (    5.75 ms per token,   174.05 tokens per second)
0.01.161.317 I llama_perf_context_print:        eval time =     636.87 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.161.317 I llama_perf_context_print:       total time =     681.15 ms /    70 tokens
0.01.164.928 I ggml_metal_free: deallocating

real	0m1.180s
user	0m0.106s
sys	0m0.154s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.509 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.067 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.724 I llama_model_loader: - type  f32:  194 tensors
0.00.023.725 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.725 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.725 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.726 I print_info: file format = GGUF V3 (latest)
0.00.023.726 I print_info: file type   = Q3_K - Medium
0.00.023.727 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.571 I load: special tokens cache size = 25
0.00.037.564 I load: token to piece cache size = 0.2984 MB
0.00.037.566 I print_info: arch             = gptneox
0.00.037.567 I print_info: vocab_only       = 0
0.00.037.567 I print_info: n_ctx_train      = 2048
0.00.037.567 I print_info: n_embd           = 2048
0.00.037.567 I print_info: n_layer          = 24
0.00.037.570 I print_info: n_head           = 16
0.00.037.571 I print_info: n_head_kv        = 16
0.00.037.571 I print_info: n_rot            = 32
0.00.037.571 I print_info: n_swa            = 0
0.00.037.571 I print_info: n_embd_head_k    = 128
0.00.037.571 I print_info: n_embd_head_v    = 128
0.00.037.572 I print_info: n_gqa            = 1
0.00.037.573 I print_info: n_embd_k_gqa     = 2048
0.00.037.575 I print_info: n_embd_v_gqa     = 2048
0.00.037.575 I print_info: f_norm_eps       = 1.0e-05
0.00.037.576 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.576 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.576 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.576 I print_info: f_logit_scale    = 0.0e+00
0.00.037.577 I print_info: n_ff             = 8192
0.00.037.577 I print_info: n_expert         = 0
0.00.037.577 I print_info: n_expert_used    = 0
0.00.037.577 I print_info: causal attn      = 1
0.00.037.577 I print_info: pooling type     = 0
0.00.037.578 I print_info: rope type        = 2
0.00.037.578 I print_info: rope scaling     = linear
0.00.037.580 I print_info: freq_base_train  = 10000.0
0.00.037.581 I print_info: freq_scale_train = 1
0.00.037.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.581 I print_info: rope_finetuned   = unknown
0.00.037.582 I print_info: ssm_d_conv       = 0
0.00.037.582 I print_info: ssm_d_inner      = 0
0.00.037.583 I print_info: ssm_d_state      = 0
0.00.037.583 I print_info: ssm_dt_rank      = 0
0.00.037.583 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.583 I print_info: model type       = 1.4B
0.00.037.583 I print_info: model params     = 1.41 B
0.00.037.584 I print_info: general.name     = 1.4B
0.00.037.584 I print_info: vocab type       = BPE
0.00.037.584 I print_info: n_vocab          = 50304
0.00.037.585 I print_info: n_merges         = 50009
0.00.037.593 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.593 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.593 I print_info: LF token         = 187 'Ċ'
0.00.037.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.594 I print_info: max token length = 1024
0.00.432.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.572 I load_tensors: offloading output layer to GPU
0.00.432.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.609 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.610 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.183 I llama_context: n_seq_max     = 1
0.00.434.187 I llama_context: n_ctx         = 2048
0.00.434.188 I llama_context: n_ctx_per_seq = 2048
0.00.434.188 I llama_context: n_batch       = 2048
0.00.434.189 I llama_context: n_ubatch      = 512
0.00.434.189 I llama_context: flash_attn    = 0
0.00.434.191 I llama_context: freq_base     = 10000.0
0.00.434.192 I llama_context: freq_scale    = 1
0.00.434.194 I ggml_metal_init: allocating
0.00.434.311 I ggml_metal_init: found device: Apple M4
0.00.434.326 I ggml_metal_init: picking default device: Apple M4
0.00.436.186 I ggml_metal_init: using embedded metal library
0.00.442.223 I ggml_metal_init: GPU name:   Apple M4
0.00.442.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.231 I ggml_metal_init: simdgroup reduction   = true
0.00.442.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.231 I ggml_metal_init: has residency sets    = true
0.00.442.232 I ggml_metal_init: has bfloat            = true
0.00.442.232 I ggml_metal_init: use bfloat            = true
0.00.442.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.351 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.518.621 I init:      Metal KV buffer size =   384.00 MiB
0.00.518.629 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.518.658 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.522.856 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.522.858 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.522.858 I llama_context: graph nodes  = 967
0.00.522.859 I llama_context: graph splits = 2
0.00.522.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.522.980 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.522.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.001 I main: llama threadpool init, n_threads = 4
0.00.581.045 I 
0.00.581.072 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.073 I 
0.00.581.253 I sampler seed: 1234
0.00.581.258 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.581.303 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.581.307 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.581.307 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.328.159 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.328.159 I llama_perf_context_print:        load time =     571.53 ms
0.01.328.160 I llama_perf_context_print: prompt eval time =      49.00 ms /     7 tokens (    7.00 ms per token,   142.87 tokens per second)
0.01.328.161 I llama_perf_context_print:        eval time =     694.89 ms /    63 runs   (   11.03 ms per token,    90.66 tokens per second)
0.01.328.161 I llama_perf_context_print:       total time =     748.12 ms /    70 tokens
0.01.331.925 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.109s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.319 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.324 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.329 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.330 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.332 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.333 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.333 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.333 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.334 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.334 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.336 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.336 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.336 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.045 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.047 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.047 I llama_model_loader: - type  f32:  194 tensors
0.00.025.047 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.047 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.048 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.048 I print_info: file format = GGUF V3 (latest)
0.00.025.049 I print_info: file type   = Q4_K - Medium
0.00.025.050 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.222 I load: special tokens cache size = 25
0.00.039.355 I load: token to piece cache size = 0.2984 MB
0.00.039.358 I print_info: arch             = gptneox
0.00.039.358 I print_info: vocab_only       = 0
0.00.039.358 I print_info: n_ctx_train      = 2048
0.00.039.358 I print_info: n_embd           = 2048
0.00.039.358 I print_info: n_layer          = 24
0.00.039.361 I print_info: n_head           = 16
0.00.039.362 I print_info: n_head_kv        = 16
0.00.039.362 I print_info: n_rot            = 32
0.00.039.362 I print_info: n_swa            = 0
0.00.039.363 I print_info: n_embd_head_k    = 128
0.00.039.363 I print_info: n_embd_head_v    = 128
0.00.039.364 I print_info: n_gqa            = 1
0.00.039.364 I print_info: n_embd_k_gqa     = 2048
0.00.039.365 I print_info: n_embd_v_gqa     = 2048
0.00.039.366 I print_info: f_norm_eps       = 1.0e-05
0.00.039.370 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.370 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.370 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.372 I print_info: f_logit_scale    = 0.0e+00
0.00.039.373 I print_info: n_ff             = 8192
0.00.039.373 I print_info: n_expert         = 0
0.00.039.373 I print_info: n_expert_used    = 0
0.00.039.374 I print_info: causal attn      = 1
0.00.039.374 I print_info: pooling type     = 0
0.00.039.374 I print_info: rope type        = 2
0.00.039.374 I print_info: rope scaling     = linear
0.00.039.375 I print_info: freq_base_train  = 10000.0
0.00.039.375 I print_info: freq_scale_train = 1
0.00.039.378 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.378 I print_info: rope_finetuned   = unknown
0.00.039.379 I print_info: ssm_d_conv       = 0
0.00.039.379 I print_info: ssm_d_inner      = 0
0.00.039.379 I print_info: ssm_d_state      = 0
0.00.039.379 I print_info: ssm_dt_rank      = 0
0.00.039.379 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.379 I print_info: model type       = 1.4B
0.00.039.380 I print_info: model params     = 1.41 B
0.00.039.380 I print_info: general.name     = 1.4B
0.00.039.380 I print_info: vocab type       = BPE
0.00.039.380 I print_info: n_vocab          = 50304
0.00.039.381 I print_info: n_merges         = 50009
0.00.039.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: LF token         = 187 'Ċ'
0.00.039.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.382 I print_info: max token length = 1024
0.00.521.763 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.779 I load_tensors: offloading output layer to GPU
0.00.521.780 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.816 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.817 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.523.369 I llama_context: n_seq_max     = 1
0.00.523.374 I llama_context: n_ctx         = 2048
0.00.523.374 I llama_context: n_ctx_per_seq = 2048
0.00.523.375 I llama_context: n_batch       = 2048
0.00.523.375 I llama_context: n_ubatch      = 512
0.00.523.376 I llama_context: flash_attn    = 0
0.00.523.377 I llama_context: freq_base     = 10000.0
0.00.523.378 I llama_context: freq_scale    = 1
0.00.523.381 I ggml_metal_init: allocating
0.00.523.456 I ggml_metal_init: found device: Apple M4
0.00.523.470 I ggml_metal_init: picking default device: Apple M4
0.00.525.246 I ggml_metal_init: using embedded metal library
0.00.531.945 I ggml_metal_init: GPU name:   Apple M4
0.00.531.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.951 I ggml_metal_init: simdgroup reduction   = true
0.00.531.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.951 I ggml_metal_init: has residency sets    = true
0.00.531.952 I ggml_metal_init: has bfloat            = true
0.00.531.952 I ggml_metal_init: use bfloat            = true
0.00.531.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.172 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.602.417 I init:      Metal KV buffer size =   384.00 MiB
0.00.602.424 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.602.491 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.606.699 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.606.701 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.606.701 I llama_context: graph nodes  = 967
0.00.606.702 I llama_context: graph splits = 2
0.00.606.708 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.606.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.606.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.410 I main: llama threadpool init, n_threads = 4
0.00.665.455 I 
0.00.665.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.480 I 
0.00.665.658 I sampler seed: 1234
0.00.665.662 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.665.709 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.665.713 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.665.713 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.423.723 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.423.723 I llama_perf_context_print:        load time =     654.68 ms
0.01.423.724 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.61 tokens per second)
0.01.423.725 I llama_perf_context_print:        eval time =     703.39 ms /    63 runs   (   11.16 ms per token,    89.57 tokens per second)
0.01.423.729 I llama_perf_context_print:       total time =     759.27 ms /    70 tokens
0.01.427.857 I ggml_metal_free: deallocating

real	0m1.447s
user	0m0.110s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.404 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.404 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.405 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.407 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.017 I llama_model_loader: - type  f32:  194 tensors
0.00.024.017 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.018 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.018 I print_info: file format = GGUF V3 (latest)
0.00.024.019 I print_info: file type   = Q5_K - Medium
0.00.024.019 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.889 I load: special tokens cache size = 25
0.00.037.881 I load: token to piece cache size = 0.2984 MB
0.00.037.884 I print_info: arch             = gptneox
0.00.037.884 I print_info: vocab_only       = 0
0.00.037.884 I print_info: n_ctx_train      = 2048
0.00.037.884 I print_info: n_embd           = 2048
0.00.037.885 I print_info: n_layer          = 24
0.00.037.888 I print_info: n_head           = 16
0.00.037.888 I print_info: n_head_kv        = 16
0.00.037.889 I print_info: n_rot            = 32
0.00.037.889 I print_info: n_swa            = 0
0.00.037.889 I print_info: n_embd_head_k    = 128
0.00.037.889 I print_info: n_embd_head_v    = 128
0.00.037.890 I print_info: n_gqa            = 1
0.00.037.891 I print_info: n_embd_k_gqa     = 2048
0.00.037.892 I print_info: n_embd_v_gqa     = 2048
0.00.037.892 I print_info: f_norm_eps       = 1.0e-05
0.00.037.892 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.893 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.894 I print_info: f_logit_scale    = 0.0e+00
0.00.037.895 I print_info: n_ff             = 8192
0.00.037.895 I print_info: n_expert         = 0
0.00.037.895 I print_info: n_expert_used    = 0
0.00.037.895 I print_info: causal attn      = 1
0.00.037.895 I print_info: pooling type     = 0
0.00.037.897 I print_info: rope type        = 2
0.00.037.897 I print_info: rope scaling     = linear
0.00.037.898 I print_info: freq_base_train  = 10000.0
0.00.037.898 I print_info: freq_scale_train = 1
0.00.037.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.898 I print_info: rope_finetuned   = unknown
0.00.037.898 I print_info: ssm_d_conv       = 0
0.00.037.899 I print_info: ssm_d_inner      = 0
0.00.037.899 I print_info: ssm_d_state      = 0
0.00.037.899 I print_info: ssm_dt_rank      = 0
0.00.037.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.899 I print_info: model type       = 1.4B
0.00.037.900 I print_info: model params     = 1.41 B
0.00.037.900 I print_info: general.name     = 1.4B
0.00.037.900 I print_info: vocab type       = BPE
0.00.037.900 I print_info: n_vocab          = 50304
0.00.037.901 I print_info: n_merges         = 50009
0.00.037.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.903 I print_info: LF token         = 187 'Ċ'
0.00.037.903 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.904 I print_info: max token length = 1024
0.00.604.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.697 I load_tensors: offloading output layer to GPU
0.00.604.698 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.729 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.731 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.606.042 I llama_context: n_seq_max     = 1
0.00.606.048 I llama_context: n_ctx         = 2048
0.00.606.049 I llama_context: n_ctx_per_seq = 2048
0.00.606.049 I llama_context: n_batch       = 2048
0.00.606.050 I llama_context: n_ubatch      = 512
0.00.606.050 I llama_context: flash_attn    = 0
0.00.606.052 I llama_context: freq_base     = 10000.0
0.00.606.053 I llama_context: freq_scale    = 1
0.00.606.055 I ggml_metal_init: allocating
0.00.606.158 I ggml_metal_init: found device: Apple M4
0.00.606.172 I ggml_metal_init: picking default device: Apple M4
0.00.607.851 I ggml_metal_init: using embedded metal library
0.00.611.664 I ggml_metal_init: GPU name:   Apple M4
0.00.611.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.672 I ggml_metal_init: simdgroup reduction   = true
0.00.611.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.672 I ggml_metal_init: has residency sets    = true
0.00.611.673 I ggml_metal_init: has bfloat            = true
0.00.611.673 I ggml_metal_init: use bfloat            = true
0.00.611.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.715 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.653.157 I init:      Metal KV buffer size =   384.00 MiB
0.00.653.164 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.653.200 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.657.477 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.657.479 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.657.479 I llama_context: graph nodes  = 967
0.00.657.479 I llama_context: graph splits = 2
0.00.657.486 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.657.618 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.657.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.988 I main: llama threadpool init, n_threads = 4
0.00.723.027 I 
0.00.723.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.050 I 
0.00.723.217 I sampler seed: 1234
0.00.723.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.237 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.239 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.561.850 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.561.850 I llama_perf_context_print:        load time =     713.19 ms
0.01.561.851 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.561.852 I llama_perf_context_print:        eval time =     784.64 ms /    63 runs   (   12.45 ms per token,    80.29 tokens per second)
0.01.561.852 I llama_perf_context_print:       total time =     839.82 ms /    70 tokens
0.01.564.608 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.099s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.360 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.366 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.366 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.367 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.367 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.369 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.370 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.372 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.373 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.343 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.206 I llama_model_loader: - type  f32:  194 tensors
0.00.027.206 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.207 I print_info: file format = GGUF V3 (latest)
0.00.027.208 I print_info: file type   = Q6_K
0.00.027.208 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.661 I load: special tokens cache size = 25
0.00.041.941 I load: token to piece cache size = 0.2984 MB
0.00.041.946 I print_info: arch             = gptneox
0.00.041.947 I print_info: vocab_only       = 0
0.00.041.947 I print_info: n_ctx_train      = 2048
0.00.041.947 I print_info: n_embd           = 2048
0.00.041.947 I print_info: n_layer          = 24
0.00.041.952 I print_info: n_head           = 16
0.00.041.952 I print_info: n_head_kv        = 16
0.00.041.953 I print_info: n_rot            = 32
0.00.041.953 I print_info: n_swa            = 0
0.00.041.953 I print_info: n_embd_head_k    = 128
0.00.041.953 I print_info: n_embd_head_v    = 128
0.00.041.954 I print_info: n_gqa            = 1
0.00.041.955 I print_info: n_embd_k_gqa     = 2048
0.00.041.955 I print_info: n_embd_v_gqa     = 2048
0.00.041.956 I print_info: f_norm_eps       = 1.0e-05
0.00.041.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.959 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.959 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.959 I print_info: f_logit_scale    = 0.0e+00
0.00.041.960 I print_info: n_ff             = 8192
0.00.041.960 I print_info: n_expert         = 0
0.00.041.960 I print_info: n_expert_used    = 0
0.00.041.960 I print_info: causal attn      = 1
0.00.041.961 I print_info: pooling type     = 0
0.00.041.961 I print_info: rope type        = 2
0.00.041.961 I print_info: rope scaling     = linear
0.00.041.961 I print_info: freq_base_train  = 10000.0
0.00.041.962 I print_info: freq_scale_train = 1
0.00.041.962 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.962 I print_info: rope_finetuned   = unknown
0.00.041.962 I print_info: ssm_d_conv       = 0
0.00.041.962 I print_info: ssm_d_inner      = 0
0.00.041.963 I print_info: ssm_d_state      = 0
0.00.041.963 I print_info: ssm_dt_rank      = 0
0.00.041.963 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.963 I print_info: model type       = 1.4B
0.00.041.964 I print_info: model params     = 1.41 B
0.00.041.964 I print_info: general.name     = 1.4B
0.00.041.965 I print_info: vocab type       = BPE
0.00.041.965 I print_info: n_vocab          = 50304
0.00.041.966 I print_info: n_merges         = 50009
0.00.041.966 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.966 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.966 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.966 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.966 I print_info: LF token         = 187 'Ċ'
0.00.041.967 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.967 I print_info: max token length = 1024
0.00.612.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.612.598 I load_tensors: offloading output layer to GPU
0.00.612.599 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.633 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.612.635 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.613.540 I llama_context: n_seq_max     = 1
0.00.613.544 I llama_context: n_ctx         = 2048
0.00.613.544 I llama_context: n_ctx_per_seq = 2048
0.00.613.544 I llama_context: n_batch       = 2048
0.00.613.545 I llama_context: n_ubatch      = 512
0.00.613.545 I llama_context: flash_attn    = 0
0.00.613.546 I llama_context: freq_base     = 10000.0
0.00.613.547 I llama_context: freq_scale    = 1
0.00.613.548 I ggml_metal_init: allocating
0.00.613.584 I ggml_metal_init: found device: Apple M4
0.00.613.595 I ggml_metal_init: picking default device: Apple M4
0.00.614.572 I ggml_metal_init: using embedded metal library
0.00.619.525 I ggml_metal_init: GPU name:   Apple M4
0.00.619.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.535 I ggml_metal_init: simdgroup reduction   = true
0.00.619.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.535 I ggml_metal_init: has residency sets    = true
0.00.619.535 I ggml_metal_init: has bfloat            = true
0.00.619.536 I ggml_metal_init: use bfloat            = true
0.00.619.537 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.089 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.152 I init:      Metal KV buffer size =   384.00 MiB
0.00.661.159 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.661.184 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.665.714 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.665.716 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.665.716 I llama_context: graph nodes  = 967
0.00.665.716 I llama_context: graph splits = 2
0.00.665.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.665.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.665.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.152 I main: llama threadpool init, n_threads = 4
0.00.731.206 I 
0.00.731.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.233 I 
0.00.731.412 I sampler seed: 1234
0.00.731.416 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.433 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.433 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.615.669 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.615.669 I llama_perf_context_print:        load time =     721.16 ms
0.01.615.670 I llama_perf_context_print: prompt eval time =      54.10 ms /     7 tokens (    7.73 ms per token,   129.39 tokens per second)
0.01.615.672 I llama_perf_context_print:        eval time =     827.31 ms /    63 runs   (   13.13 ms per token,    76.15 tokens per second)
0.01.615.672 I llama_perf_context_print:       total time =     885.45 ms /    70 tokens
0.01.619.498 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.104s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.005.576 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.034.461 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.259 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.279 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.280 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.293 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.226 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.230 I llama_model_loader: - type  f32:  194 tensors
0.00.063.230 I llama_model_loader: - type  f16:   98 tensors
0.00.063.232 I print_info: file format = GGUF V3 (latest)
0.00.063.234 I print_info: file type   = all F32 (guessed)
0.00.063.238 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.082 I load: special tokens cache size = 25
0.00.092.176 I load: token to piece cache size = 0.2984 MB
0.00.092.182 I print_info: arch             = gptneox
0.00.092.182 I print_info: vocab_only       = 0
0.00.092.182 I print_info: n_ctx_train      = 2048
0.00.092.183 I print_info: n_embd           = 2048
0.00.092.183 I print_info: n_layer          = 24
0.00.092.188 I print_info: n_head           = 16
0.00.092.193 I print_info: n_head_kv        = 16
0.00.092.193 I print_info: n_rot            = 32
0.00.092.193 I print_info: n_swa            = 0
0.00.092.194 I print_info: n_embd_head_k    = 128
0.00.092.194 I print_info: n_embd_head_v    = 128
0.00.092.195 I print_info: n_gqa            = 1
0.00.092.196 I print_info: n_embd_k_gqa     = 2048
0.00.092.197 I print_info: n_embd_v_gqa     = 2048
0.00.092.198 I print_info: f_norm_eps       = 1.0e-05
0.00.092.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.200 I print_info: f_logit_scale    = 0.0e+00
0.00.092.201 I print_info: n_ff             = 8192
0.00.092.202 I print_info: n_expert         = 0
0.00.092.202 I print_info: n_expert_used    = 0
0.00.092.202 I print_info: causal attn      = 1
0.00.092.202 I print_info: pooling type     = 0
0.00.092.203 I print_info: rope type        = 2
0.00.092.203 I print_info: rope scaling     = linear
0.00.092.203 I print_info: freq_base_train  = 10000.0
0.00.092.204 I print_info: freq_scale_train = 1
0.00.092.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.210 I print_info: rope_finetuned   = unknown
0.00.092.210 I print_info: ssm_d_conv       = 0
0.00.092.210 I print_info: ssm_d_inner      = 0
0.00.092.210 I print_info: ssm_d_state      = 0
0.00.092.210 I print_info: ssm_dt_rank      = 0
0.00.092.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.211 I print_info: model type       = 1.4B
0.00.092.212 I print_info: model params     = 1.41 B
0.00.092.212 I print_info: general.name     = 1.4B
0.00.092.213 I print_info: vocab type       = BPE
0.00.092.213 I print_info: n_vocab          = 50304
0.00.092.213 I print_info: n_merges         = 50009
0.00.092.214 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.215 I print_info: LF token         = 187 'Ċ'
0.00.092.215 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.216 I print_info: max token length = 1024
0.01.477.433 I load_tensors: offloading 24 repeating layers to GPU
0.01.477.437 I load_tensors: offloading output layer to GPU
0.01.477.437 I load_tensors: offloaded 25/25 layers to GPU
0.01.477.461 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.477.462 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.478.561 I llama_context: n_seq_max     = 1
0.01.478.563 I llama_context: n_ctx         = 128
0.01.478.563 I llama_context: n_ctx_per_seq = 128
0.01.478.563 I llama_context: n_batch       = 128
0.01.478.564 I llama_context: n_ubatch      = 128
0.01.478.564 I llama_context: flash_attn    = 0
0.01.478.564 I llama_context: freq_base     = 10000.0
0.01.478.565 I llama_context: freq_scale    = 1
0.01.478.565 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.478.566 I ggml_metal_init: allocating
0.01.478.631 I ggml_metal_init: found device: Apple M4
0.01.478.635 I ggml_metal_init: picking default device: Apple M4
0.01.479.671 I ggml_metal_init: using embedded metal library
0.01.483.774 I ggml_metal_init: GPU name:   Apple M4
0.01.483.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.483.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.483.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.483.779 I ggml_metal_init: simdgroup reduction   = true
0.01.483.779 I ggml_metal_init: simdgroup matrix mul. = true
0.01.483.779 I ggml_metal_init: has residency sets    = true
0.01.483.779 I ggml_metal_init: has bfloat            = true
0.01.483.779 I ggml_metal_init: use bfloat            = true
0.01.483.780 I ggml_metal_init: hasUnifiedMemory      = true
0.01.483.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.494.656 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.496.452 I init:      Metal KV buffer size =    24.00 MiB
0.01.496.455 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.496.477 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.498.311 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.498.313 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.498.313 I llama_context: graph nodes  = 967
0.01.498.313 I llama_context: graph splits = 2
0.01.498.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.498.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.532.778 I 
0.01.532.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.532.836 I perplexity: tokenizing the input ..
0.01.537.835 I perplexity: tokenization took 4.997 ms
0.01.537.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.656.276 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.657.619 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.657.650 I llama_perf_context_print:        load time =    1498.30 ms
0.01.657.652 I llama_perf_context_print: prompt eval time =     118.12 ms /   128 tokens (    0.92 ms per token,  1083.60 tokens per second)
0.01.657.653 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.657.653 I llama_perf_context_print:       total time =     124.87 ms /   129 tokens
0.01.658.207 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.102s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.866 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.866 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.869 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.870 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.871 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.871 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.876 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.801 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.659 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.662 I llama_model_loader: - type  f32:  194 tensors
0.00.029.662 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.663 I print_info: file format = GGUF V3 (latest)
0.00.029.664 I print_info: file type   = Q8_0
0.00.029.669 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.038.066 I load: special tokens cache size = 25
0.00.044.238 I load: token to piece cache size = 0.2984 MB
0.00.044.242 I print_info: arch             = gptneox
0.00.044.243 I print_info: vocab_only       = 0
0.00.044.243 I print_info: n_ctx_train      = 2048
0.00.044.243 I print_info: n_embd           = 2048
0.00.044.243 I print_info: n_layer          = 24
0.00.044.247 I print_info: n_head           = 16
0.00.044.248 I print_info: n_head_kv        = 16
0.00.044.248 I print_info: n_rot            = 32
0.00.044.248 I print_info: n_swa            = 0
0.00.044.248 I print_info: n_embd_head_k    = 128
0.00.044.249 I print_info: n_embd_head_v    = 128
0.00.044.251 I print_info: n_gqa            = 1
0.00.044.251 I print_info: n_embd_k_gqa     = 2048
0.00.044.252 I print_info: n_embd_v_gqa     = 2048
0.00.044.253 I print_info: f_norm_eps       = 1.0e-05
0.00.044.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.253 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.253 I print_info: f_logit_scale    = 0.0e+00
0.00.044.254 I print_info: n_ff             = 8192
0.00.044.254 I print_info: n_expert         = 0
0.00.044.254 I print_info: n_expert_used    = 0
0.00.044.254 I print_info: causal attn      = 1
0.00.044.254 I print_info: pooling type     = 0
0.00.044.255 I print_info: rope type        = 2
0.00.044.255 I print_info: rope scaling     = linear
0.00.044.255 I print_info: freq_base_train  = 10000.0
0.00.044.255 I print_info: freq_scale_train = 1
0.00.044.257 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.258 I print_info: rope_finetuned   = unknown
0.00.044.259 I print_info: ssm_d_conv       = 0
0.00.044.259 I print_info: ssm_d_inner      = 0
0.00.044.259 I print_info: ssm_d_state      = 0
0.00.044.259 I print_info: ssm_dt_rank      = 0
0.00.044.259 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.259 I print_info: model type       = 1.4B
0.00.044.260 I print_info: model params     = 1.41 B
0.00.044.260 I print_info: general.name     = 1.4B
0.00.044.260 I print_info: vocab type       = BPE
0.00.044.260 I print_info: n_vocab          = 50304
0.00.044.261 I print_info: n_merges         = 50009
0.00.044.261 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.261 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.261 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.261 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.261 I print_info: LF token         = 187 'Ċ'
0.00.044.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.263 I print_info: max token length = 1024
0.00.975.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.975.213 I load_tensors: offloading output layer to GPU
0.00.975.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.975.238 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.975.240 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.976.554 I llama_context: n_seq_max     = 1
0.00.976.556 I llama_context: n_ctx         = 128
0.00.976.556 I llama_context: n_ctx_per_seq = 128
0.00.976.556 I llama_context: n_batch       = 128
0.00.976.557 I llama_context: n_ubatch      = 128
0.00.976.557 I llama_context: flash_attn    = 0
0.00.976.558 I llama_context: freq_base     = 10000.0
0.00.976.558 I llama_context: freq_scale    = 1
0.00.976.559 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.976.560 I ggml_metal_init: allocating
0.00.976.578 I ggml_metal_init: found device: Apple M4
0.00.976.585 I ggml_metal_init: picking default device: Apple M4
0.00.977.815 I ggml_metal_init: using embedded metal library
0.00.983.458 I ggml_metal_init: GPU name:   Apple M4
0.00.983.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.983.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.983.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.983.464 I ggml_metal_init: simdgroup reduction   = true
0.00.983.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.983.464 I ggml_metal_init: has residency sets    = true
0.00.983.464 I ggml_metal_init: has bfloat            = true
0.00.983.464 I ggml_metal_init: use bfloat            = true
0.00.983.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.983.469 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.998.832 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.002.186 I init:      Metal KV buffer size =    24.00 MiB
0.01.002.193 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.002.235 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.005.399 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.005.401 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.005.401 I llama_context: graph nodes  = 967
0.01.005.401 I llama_context: graph splits = 2
0.01.005.405 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.005.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.034.921 I 
0.01.035.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.035.029 I perplexity: tokenizing the input ..
0.01.042.301 I perplexity: tokenization took 7.268 ms
0.01.042.307 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.181.480 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.182.816 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.182.843 I llama_perf_context_print:        load time =    1025.72 ms
0.01.182.844 I llama_perf_context_print: prompt eval time =     138.27 ms /   128 tokens (    1.08 ms per token,   925.75 tokens per second)
0.01.182.845 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.182.845 I llama_perf_context_print:       total time =     147.93 ms /   129 tokens
0.01.183.421 I ggml_metal_free: deallocating

real	0m1.197s
user	0m0.078s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.072 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.069 I llama_model_loader: - type  f32:  194 tensors
0.00.035.069 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.069 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.070 I print_info: file format = GGUF V3 (latest)
0.00.035.070 I print_info: file type   = Q4_0
0.00.035.071 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.716 I load: special tokens cache size = 25
0.00.049.915 I load: token to piece cache size = 0.2984 MB
0.00.049.918 I print_info: arch             = gptneox
0.00.049.918 I print_info: vocab_only       = 0
0.00.049.918 I print_info: n_ctx_train      = 2048
0.00.049.919 I print_info: n_embd           = 2048
0.00.049.919 I print_info: n_layer          = 24
0.00.049.922 I print_info: n_head           = 16
0.00.049.923 I print_info: n_head_kv        = 16
0.00.049.923 I print_info: n_rot            = 32
0.00.049.923 I print_info: n_swa            = 0
0.00.049.923 I print_info: n_embd_head_k    = 128
0.00.049.925 I print_info: n_embd_head_v    = 128
0.00.049.926 I print_info: n_gqa            = 1
0.00.049.926 I print_info: n_embd_k_gqa     = 2048
0.00.049.927 I print_info: n_embd_v_gqa     = 2048
0.00.049.932 I print_info: f_norm_eps       = 1.0e-05
0.00.049.933 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.934 I print_info: f_logit_scale    = 0.0e+00
0.00.049.935 I print_info: n_ff             = 8192
0.00.049.935 I print_info: n_expert         = 0
0.00.049.935 I print_info: n_expert_used    = 0
0.00.049.937 I print_info: causal attn      = 1
0.00.049.937 I print_info: pooling type     = 0
0.00.049.937 I print_info: rope type        = 2
0.00.049.937 I print_info: rope scaling     = linear
0.00.049.938 I print_info: freq_base_train  = 10000.0
0.00.049.938 I print_info: freq_scale_train = 1
0.00.049.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.939 I print_info: rope_finetuned   = unknown
0.00.049.939 I print_info: ssm_d_conv       = 0
0.00.049.940 I print_info: ssm_d_inner      = 0
0.00.049.940 I print_info: ssm_d_state      = 0
0.00.049.940 I print_info: ssm_dt_rank      = 0
0.00.049.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.941 I print_info: model type       = 1.4B
0.00.049.941 I print_info: model params     = 1.41 B
0.00.049.941 I print_info: general.name     = 1.4B
0.00.049.942 I print_info: vocab type       = BPE
0.00.049.942 I print_info: n_vocab          = 50304
0.00.049.942 I print_info: n_merges         = 50009
0.00.049.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.943 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.945 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.945 I print_info: LF token         = 187 'Ċ'
0.00.049.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.945 I print_info: max token length = 1024
0.00.614.655 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.670 I load_tensors: offloading output layer to GPU
0.00.614.670 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.707 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.614.712 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.616.089 I llama_context: n_seq_max     = 1
0.00.616.094 I llama_context: n_ctx         = 128
0.00.616.095 I llama_context: n_ctx_per_seq = 128
0.00.616.100 I llama_context: n_batch       = 128
0.00.616.100 I llama_context: n_ubatch      = 128
0.00.616.101 I llama_context: flash_attn    = 0
0.00.616.106 I llama_context: freq_base     = 10000.0
0.00.616.107 I llama_context: freq_scale    = 1
0.00.616.108 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.111 I ggml_metal_init: allocating
0.00.616.198 I ggml_metal_init: found device: Apple M4
0.00.616.213 I ggml_metal_init: picking default device: Apple M4
0.00.618.060 I ggml_metal_init: using embedded metal library
0.00.623.598 I ggml_metal_init: GPU name:   Apple M4
0.00.623.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.606 I ggml_metal_init: simdgroup reduction   = true
0.00.623.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.606 I ggml_metal_init: has residency sets    = true
0.00.623.607 I ggml_metal_init: has bfloat            = true
0.00.623.607 I ggml_metal_init: use bfloat            = true
0.00.623.609 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.054 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.602 I init:      Metal KV buffer size =    24.00 MiB
0.00.646.608 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.642 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.649.841 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.649.843 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.649.843 I llama_context: graph nodes  = 967
0.00.649.843 I llama_context: graph splits = 2
0.00.649.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.941 I 
0.00.675.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.029 I perplexity: tokenizing the input ..
0.00.682.998 I perplexity: tokenization took 7.965 ms
0.00.683.009 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.409 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.820.754 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.820.776 I llama_perf_context_print:        load time =     659.00 ms
0.00.820.777 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.66 tokens per second)
0.00.820.779 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.780 I llama_perf_context_print:       total time =     145.84 ms /   129 tokens
0.00.821.335 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.082s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.754 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.424 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.425 I llama_model_loader: - type  f32:  194 tensors
0.00.025.426 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.426 I print_info: file format = GGUF V3 (latest)
0.00.025.427 I print_info: file type   = Q4_1
0.00.025.428 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.539 I load: special tokens cache size = 25
0.00.039.548 I load: token to piece cache size = 0.2984 MB
0.00.039.551 I print_info: arch             = gptneox
0.00.039.551 I print_info: vocab_only       = 0
0.00.039.552 I print_info: n_ctx_train      = 2048
0.00.039.552 I print_info: n_embd           = 2048
0.00.039.552 I print_info: n_layer          = 24
0.00.039.555 I print_info: n_head           = 16
0.00.039.556 I print_info: n_head_kv        = 16
0.00.039.556 I print_info: n_rot            = 32
0.00.039.556 I print_info: n_swa            = 0
0.00.039.556 I print_info: n_embd_head_k    = 128
0.00.039.556 I print_info: n_embd_head_v    = 128
0.00.039.557 I print_info: n_gqa            = 1
0.00.039.558 I print_info: n_embd_k_gqa     = 2048
0.00.039.559 I print_info: n_embd_v_gqa     = 2048
0.00.039.559 I print_info: f_norm_eps       = 1.0e-05
0.00.039.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.560 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.560 I print_info: f_logit_scale    = 0.0e+00
0.00.039.561 I print_info: n_ff             = 8192
0.00.039.561 I print_info: n_expert         = 0
0.00.039.561 I print_info: n_expert_used    = 0
0.00.039.561 I print_info: causal attn      = 1
0.00.039.561 I print_info: pooling type     = 0
0.00.039.561 I print_info: rope type        = 2
0.00.039.562 I print_info: rope scaling     = linear
0.00.039.563 I print_info: freq_base_train  = 10000.0
0.00.039.564 I print_info: freq_scale_train = 1
0.00.039.564 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.564 I print_info: rope_finetuned   = unknown
0.00.039.564 I print_info: ssm_d_conv       = 0
0.00.039.564 I print_info: ssm_d_inner      = 0
0.00.039.564 I print_info: ssm_d_state      = 0
0.00.039.566 I print_info: ssm_dt_rank      = 0
0.00.039.566 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.566 I print_info: model type       = 1.4B
0.00.039.567 I print_info: model params     = 1.41 B
0.00.039.567 I print_info: general.name     = 1.4B
0.00.039.567 I print_info: vocab type       = BPE
0.00.039.568 I print_info: n_vocab          = 50304
0.00.039.568 I print_info: n_merges         = 50009
0.00.039.568 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.568 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.568 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.572 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.572 I print_info: LF token         = 187 'Ċ'
0.00.039.573 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.577 I print_info: max token length = 1024
0.00.651.575 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.591 I load_tensors: offloading output layer to GPU
0.00.651.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.627 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.651.628 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.653.207 I llama_context: n_seq_max     = 1
0.00.653.213 I llama_context: n_ctx         = 128
0.00.653.214 I llama_context: n_ctx_per_seq = 128
0.00.653.215 I llama_context: n_batch       = 128
0.00.653.215 I llama_context: n_ubatch      = 128
0.00.653.216 I llama_context: flash_attn    = 0
0.00.653.218 I llama_context: freq_base     = 10000.0
0.00.653.218 I llama_context: freq_scale    = 1
0.00.653.219 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.653.221 I ggml_metal_init: allocating
0.00.653.306 I ggml_metal_init: found device: Apple M4
0.00.653.321 I ggml_metal_init: picking default device: Apple M4
0.00.655.014 I ggml_metal_init: using embedded metal library
0.00.661.794 I ggml_metal_init: GPU name:   Apple M4
0.00.661.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.801 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.802 I ggml_metal_init: simdgroup reduction   = true
0.00.661.802 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.802 I ggml_metal_init: has residency sets    = true
0.00.661.802 I ggml_metal_init: has bfloat            = true
0.00.661.803 I ggml_metal_init: use bfloat            = true
0.00.661.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.591 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.940 I init:      Metal KV buffer size =    24.00 MiB
0.00.682.947 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.977 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.686.093 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.686.095 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.686.096 I llama_context: graph nodes  = 967
0.00.686.096 I llama_context: graph splits = 2
0.00.686.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.686.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.027 I 
0.00.716.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.125 I perplexity: tokenizing the input ..
0.00.723.774 I perplexity: tokenization took 7.646 ms
0.00.723.781 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.150 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.862.488 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.862.509 I llama_perf_context_print:        load time =     707.07 ms
0.00.862.510 I llama_perf_context_print: prompt eval time =     136.42 ms /   128 tokens (    1.07 ms per token,   938.25 tokens per second)
0.00.862.511 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.511 I llama_perf_context_print:       total time =     146.49 ms /   129 tokens
0.00.863.124 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.080s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.687 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.697 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.698 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.262 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.264 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.265 I llama_model_loader: - type  f32:  194 tensors
0.00.024.265 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.265 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.266 I print_info: file format = GGUF V3 (latest)
0.00.024.266 I print_info: file type   = Q5_0
0.00.024.267 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.032 I load: special tokens cache size = 25
0.00.038.068 I load: token to piece cache size = 0.2984 MB
0.00.038.071 I print_info: arch             = gptneox
0.00.038.071 I print_info: vocab_only       = 0
0.00.038.071 I print_info: n_ctx_train      = 2048
0.00.038.072 I print_info: n_embd           = 2048
0.00.038.072 I print_info: n_layer          = 24
0.00.038.075 I print_info: n_head           = 16
0.00.038.079 I print_info: n_head_kv        = 16
0.00.038.079 I print_info: n_rot            = 32
0.00.038.079 I print_info: n_swa            = 0
0.00.038.080 I print_info: n_embd_head_k    = 128
0.00.038.080 I print_info: n_embd_head_v    = 128
0.00.038.086 I print_info: n_gqa            = 1
0.00.038.090 I print_info: n_embd_k_gqa     = 2048
0.00.038.091 I print_info: n_embd_v_gqa     = 2048
0.00.038.091 I print_info: f_norm_eps       = 1.0e-05
0.00.038.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.094 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.095 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.095 I print_info: f_logit_scale    = 0.0e+00
0.00.038.096 I print_info: n_ff             = 8192
0.00.038.096 I print_info: n_expert         = 0
0.00.038.096 I print_info: n_expert_used    = 0
0.00.038.096 I print_info: causal attn      = 1
0.00.038.096 I print_info: pooling type     = 0
0.00.038.096 I print_info: rope type        = 2
0.00.038.097 I print_info: rope scaling     = linear
0.00.038.097 I print_info: freq_base_train  = 10000.0
0.00.038.097 I print_info: freq_scale_train = 1
0.00.038.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.098 I print_info: rope_finetuned   = unknown
0.00.038.098 I print_info: ssm_d_conv       = 0
0.00.038.098 I print_info: ssm_d_inner      = 0
0.00.038.099 I print_info: ssm_d_state      = 0
0.00.038.099 I print_info: ssm_dt_rank      = 0
0.00.038.099 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.100 I print_info: model type       = 1.4B
0.00.038.100 I print_info: model params     = 1.41 B
0.00.038.100 I print_info: general.name     = 1.4B
0.00.038.101 I print_info: vocab type       = BPE
0.00.038.101 I print_info: n_vocab          = 50304
0.00.038.101 I print_info: n_merges         = 50009
0.00.038.103 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.103 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.103 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.103 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.104 I print_info: LF token         = 187 'Ċ'
0.00.038.104 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.104 I print_info: max token length = 1024
0.00.687.150 I load_tensors: offloading 24 repeating layers to GPU
0.00.687.166 I load_tensors: offloading output layer to GPU
0.00.687.167 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.198 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.687.199 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.688.627 I llama_context: n_seq_max     = 1
0.00.688.630 I llama_context: n_ctx         = 128
0.00.688.630 I llama_context: n_ctx_per_seq = 128
0.00.688.635 I llama_context: n_batch       = 128
0.00.688.636 I llama_context: n_ubatch      = 128
0.00.688.636 I llama_context: flash_attn    = 0
0.00.688.637 I llama_context: freq_base     = 10000.0
0.00.688.638 I llama_context: freq_scale    = 1
0.00.688.640 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.688.645 I ggml_metal_init: allocating
0.00.688.668 I ggml_metal_init: found device: Apple M4
0.00.688.679 I ggml_metal_init: picking default device: Apple M4
0.00.690.166 I ggml_metal_init: using embedded metal library
0.00.696.382 I ggml_metal_init: GPU name:   Apple M4
0.00.696.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.388 I ggml_metal_init: simdgroup reduction   = true
0.00.696.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.389 I ggml_metal_init: has residency sets    = true
0.00.696.389 I ggml_metal_init: has bfloat            = true
0.00.696.389 I ggml_metal_init: use bfloat            = true
0.00.696.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.713.090 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.531 I init:      Metal KV buffer size =    24.00 MiB
0.00.716.535 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.716.567 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.719.666 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.719.667 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.719.668 I llama_context: graph nodes  = 967
0.00.719.668 I llama_context: graph splits = 2
0.00.719.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.719.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.162 I 
0.00.747.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.260 I perplexity: tokenizing the input ..
0.00.754.454 I perplexity: tokenization took 7.192 ms
0.00.754.459 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.496 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.890.814 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.890.841 I llama_perf_context_print:        load time =     738.36 ms
0.00.890.841 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.71 tokens per second)
0.00.890.842 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.842 I llama_perf_context_print:       total time =     143.68 ms /   129 tokens
0.00.891.409 I ggml_metal_free: deallocating

real	0m0.906s
user	0m0.077s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.827 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.848 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.850 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.678 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.496 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.497 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.498 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.499 I llama_model_loader: - type  f32:  194 tensors
0.00.025.499 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.499 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.500 I print_info: file format = GGUF V3 (latest)
0.00.025.500 I print_info: file type   = Q5_1
0.00.025.501 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.695 I load: special tokens cache size = 25
0.00.039.665 I load: token to piece cache size = 0.2984 MB
0.00.039.668 I print_info: arch             = gptneox
0.00.039.668 I print_info: vocab_only       = 0
0.00.039.668 I print_info: n_ctx_train      = 2048
0.00.039.668 I print_info: n_embd           = 2048
0.00.039.669 I print_info: n_layer          = 24
0.00.039.672 I print_info: n_head           = 16
0.00.039.672 I print_info: n_head_kv        = 16
0.00.039.673 I print_info: n_rot            = 32
0.00.039.673 I print_info: n_swa            = 0
0.00.039.673 I print_info: n_embd_head_k    = 128
0.00.039.673 I print_info: n_embd_head_v    = 128
0.00.039.674 I print_info: n_gqa            = 1
0.00.039.675 I print_info: n_embd_k_gqa     = 2048
0.00.039.676 I print_info: n_embd_v_gqa     = 2048
0.00.039.676 I print_info: f_norm_eps       = 1.0e-05
0.00.039.677 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.677 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.677 I print_info: f_logit_scale    = 0.0e+00
0.00.039.678 I print_info: n_ff             = 8192
0.00.039.678 I print_info: n_expert         = 0
0.00.039.678 I print_info: n_expert_used    = 0
0.00.039.678 I print_info: causal attn      = 1
0.00.039.678 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.679 I print_info: freq_base_train  = 10000.0
0.00.039.682 I print_info: freq_scale_train = 1
0.00.039.682 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.682 I print_info: rope_finetuned   = unknown
0.00.039.682 I print_info: ssm_d_conv       = 0
0.00.039.682 I print_info: ssm_d_inner      = 0
0.00.039.683 I print_info: ssm_d_state      = 0
0.00.039.683 I print_info: ssm_dt_rank      = 0
0.00.039.683 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.683 I print_info: model type       = 1.4B
0.00.039.683 I print_info: model params     = 1.41 B
0.00.039.684 I print_info: general.name     = 1.4B
0.00.039.684 I print_info: vocab type       = BPE
0.00.039.684 I print_info: n_vocab          = 50304
0.00.039.684 I print_info: n_merges         = 50009
0.00.039.685 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.687 I print_info: LF token         = 187 'Ċ'
0.00.039.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.688 I print_info: max token length = 1024
0.00.630.533 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.550 I load_tensors: offloading output layer to GPU
0.00.630.551 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.588 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.630.589 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.632.125 I llama_context: n_seq_max     = 1
0.00.632.129 I llama_context: n_ctx         = 128
0.00.632.129 I llama_context: n_ctx_per_seq = 128
0.00.632.133 I llama_context: n_batch       = 128
0.00.632.134 I llama_context: n_ubatch      = 128
0.00.632.134 I llama_context: flash_attn    = 0
0.00.632.137 I llama_context: freq_base     = 10000.0
0.00.632.137 I llama_context: freq_scale    = 1
0.00.632.138 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.141 I ggml_metal_init: allocating
0.00.632.241 I ggml_metal_init: found device: Apple M4
0.00.632.255 I ggml_metal_init: picking default device: Apple M4
0.00.633.962 I ggml_metal_init: using embedded metal library
0.00.640.362 I ggml_metal_init: GPU name:   Apple M4
0.00.640.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.367 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.368 I ggml_metal_init: simdgroup reduction   = true
0.00.640.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.369 I ggml_metal_init: has residency sets    = true
0.00.640.369 I ggml_metal_init: has bfloat            = true
0.00.640.369 I ggml_metal_init: use bfloat            = true
0.00.640.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.712 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.269 I init:      Metal KV buffer size =    24.00 MiB
0.00.661.274 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.301 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.664.701 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.664.703 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.664.704 I llama_context: graph nodes  = 967
0.00.664.704 I llama_context: graph splits = 2
0.00.664.707 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.933 I 
0.00.696.013 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.033 I perplexity: tokenizing the input ..
0.00.702.410 I perplexity: tokenization took 6.369 ms
0.00.702.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.519 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.845.015 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.845.039 I llama_perf_context_print:        load time =     685.93 ms
0.00.845.040 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.30 tokens per second)
0.00.845.042 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.043 I llama_perf_context_print:       total time =     149.11 ms /   129 tokens
0.00.845.580 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.077s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.796 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.396 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.396 I llama_model_loader: - type  f32:  194 tensors
0.00.024.396 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.397 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.397 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.398 I print_info: file type   = Q2_K - Medium
0.00.024.399 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.273 I load: special tokens cache size = 25
0.00.038.027 I load: token to piece cache size = 0.2984 MB
0.00.038.030 I print_info: arch             = gptneox
0.00.038.030 I print_info: vocab_only       = 0
0.00.038.031 I print_info: n_ctx_train      = 2048
0.00.038.031 I print_info: n_embd           = 2048
0.00.038.031 I print_info: n_layer          = 24
0.00.038.034 I print_info: n_head           = 16
0.00.038.035 I print_info: n_head_kv        = 16
0.00.038.035 I print_info: n_rot            = 32
0.00.038.036 I print_info: n_swa            = 0
0.00.038.037 I print_info: n_embd_head_k    = 128
0.00.038.038 I print_info: n_embd_head_v    = 128
0.00.038.038 I print_info: n_gqa            = 1
0.00.038.039 I print_info: n_embd_k_gqa     = 2048
0.00.038.040 I print_info: n_embd_v_gqa     = 2048
0.00.038.040 I print_info: f_norm_eps       = 1.0e-05
0.00.038.041 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.041 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.041 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.041 I print_info: f_logit_scale    = 0.0e+00
0.00.038.042 I print_info: n_ff             = 8192
0.00.038.042 I print_info: n_expert         = 0
0.00.038.042 I print_info: n_expert_used    = 0
0.00.038.042 I print_info: causal attn      = 1
0.00.038.042 I print_info: pooling type     = 0
0.00.038.042 I print_info: rope type        = 2
0.00.038.044 I print_info: rope scaling     = linear
0.00.038.045 I print_info: freq_base_train  = 10000.0
0.00.038.046 I print_info: freq_scale_train = 1
0.00.038.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.046 I print_info: rope_finetuned   = unknown
0.00.038.046 I print_info: ssm_d_conv       = 0
0.00.038.046 I print_info: ssm_d_inner      = 0
0.00.038.046 I print_info: ssm_d_state      = 0
0.00.038.047 I print_info: ssm_dt_rank      = 0
0.00.038.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.047 I print_info: model type       = 1.4B
0.00.038.047 I print_info: model params     = 1.41 B
0.00.038.048 I print_info: general.name     = 1.4B
0.00.038.048 I print_info: vocab type       = BPE
0.00.038.048 I print_info: n_vocab          = 50304
0.00.038.049 I print_info: n_merges         = 50009
0.00.038.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.050 I print_info: LF token         = 187 'Ċ'
0.00.038.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.050 I print_info: max token length = 1024
0.00.351.054 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.068 I load_tensors: offloading output layer to GPU
0.00.351.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.103 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.105 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.352.491 I llama_context: n_seq_max     = 1
0.00.352.495 I llama_context: n_ctx         = 128
0.00.352.496 I llama_context: n_ctx_per_seq = 128
0.00.352.496 I llama_context: n_batch       = 128
0.00.352.496 I llama_context: n_ubatch      = 128
0.00.352.497 I llama_context: flash_attn    = 0
0.00.352.499 I llama_context: freq_base     = 10000.0
0.00.352.500 I llama_context: freq_scale    = 1
0.00.352.500 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.352.507 I ggml_metal_init: allocating
0.00.352.588 I ggml_metal_init: found device: Apple M4
0.00.352.603 I ggml_metal_init: picking default device: Apple M4
0.00.354.370 I ggml_metal_init: using embedded metal library
0.00.359.776 I ggml_metal_init: GPU name:   Apple M4
0.00.359.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.792 I ggml_metal_init: simdgroup reduction   = true
0.00.359.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.792 I ggml_metal_init: has residency sets    = true
0.00.359.793 I ggml_metal_init: has bfloat            = true
0.00.359.793 I ggml_metal_init: use bfloat            = true
0.00.359.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.726 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.384.328 I init:      Metal KV buffer size =    24.00 MiB
0.00.384.343 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.384.394 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.387.664 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.387.666 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.387.667 I llama_context: graph nodes  = 967
0.00.387.667 I llama_context: graph splits = 2
0.00.387.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.387.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.164 I 
0.00.422.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.272 I perplexity: tokenizing the input ..
0.00.428.854 I perplexity: tokenization took 6.579 ms
0.00.428.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.569.514 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.570.850 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.570.874 I llama_perf_context_print:        load time =     413.22 ms
0.00.570.874 I llama_perf_context_print: prompt eval time =     140.11 ms /   128 tokens (    1.09 ms per token,   913.57 tokens per second)
0.00.570.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.570.875 I llama_perf_context_print:       total time =     148.71 ms /   129 tokens
0.00.571.431 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.080s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.831 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.838 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.839 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.851 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.439 I llama_model_loader: - type  f32:  194 tensors
0.00.024.439 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.439 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.439 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.440 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.440 I print_info: file format = GGUF V3 (latest)
0.00.024.441 I print_info: file type   = Q3_K - Medium
0.00.024.442 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.176 I load: special tokens cache size = 25
0.00.038.201 I load: token to piece cache size = 0.2984 MB
0.00.038.203 I print_info: arch             = gptneox
0.00.038.204 I print_info: vocab_only       = 0
0.00.038.204 I print_info: n_ctx_train      = 2048
0.00.038.204 I print_info: n_embd           = 2048
0.00.038.204 I print_info: n_layer          = 24
0.00.038.207 I print_info: n_head           = 16
0.00.038.208 I print_info: n_head_kv        = 16
0.00.038.208 I print_info: n_rot            = 32
0.00.038.209 I print_info: n_swa            = 0
0.00.038.209 I print_info: n_embd_head_k    = 128
0.00.038.209 I print_info: n_embd_head_v    = 128
0.00.038.211 I print_info: n_gqa            = 1
0.00.038.212 I print_info: n_embd_k_gqa     = 2048
0.00.038.212 I print_info: n_embd_v_gqa     = 2048
0.00.038.213 I print_info: f_norm_eps       = 1.0e-05
0.00.038.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.214 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.214 I print_info: f_logit_scale    = 0.0e+00
0.00.038.215 I print_info: n_ff             = 8192
0.00.038.215 I print_info: n_expert         = 0
0.00.038.215 I print_info: n_expert_used    = 0
0.00.038.215 I print_info: causal attn      = 1
0.00.038.215 I print_info: pooling type     = 0
0.00.038.216 I print_info: rope type        = 2
0.00.038.217 I print_info: rope scaling     = linear
0.00.038.218 I print_info: freq_base_train  = 10000.0
0.00.038.218 I print_info: freq_scale_train = 1
0.00.038.218 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.219 I print_info: rope_finetuned   = unknown
0.00.038.219 I print_info: ssm_d_conv       = 0
0.00.038.219 I print_info: ssm_d_inner      = 0
0.00.038.219 I print_info: ssm_d_state      = 0
0.00.038.219 I print_info: ssm_dt_rank      = 0
0.00.038.219 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.219 I print_info: model type       = 1.4B
0.00.038.220 I print_info: model params     = 1.41 B
0.00.038.220 I print_info: general.name     = 1.4B
0.00.038.220 I print_info: vocab type       = BPE
0.00.038.221 I print_info: n_vocab          = 50304
0.00.038.221 I print_info: n_merges         = 50009
0.00.038.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.225 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.225 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.226 I print_info: LF token         = 187 'Ċ'
0.00.038.227 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.227 I print_info: max token length = 1024
0.00.432.463 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.479 I load_tensors: offloading output layer to GPU
0.00.432.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.512 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.513 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.085 I llama_context: n_seq_max     = 1
0.00.434.090 I llama_context: n_ctx         = 128
0.00.434.091 I llama_context: n_ctx_per_seq = 128
0.00.434.091 I llama_context: n_batch       = 128
0.00.434.091 I llama_context: n_ubatch      = 128
0.00.434.092 I llama_context: flash_attn    = 0
0.00.434.094 I llama_context: freq_base     = 10000.0
0.00.434.095 I llama_context: freq_scale    = 1
0.00.434.095 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.434.098 I ggml_metal_init: allocating
0.00.434.172 I ggml_metal_init: found device: Apple M4
0.00.434.186 I ggml_metal_init: picking default device: Apple M4
0.00.435.874 I ggml_metal_init: using embedded metal library
0.00.441.375 I ggml_metal_init: GPU name:   Apple M4
0.00.441.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.441.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.441.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.441.395 I ggml_metal_init: simdgroup reduction   = true
0.00.441.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.441.395 I ggml_metal_init: has residency sets    = true
0.00.441.396 I ggml_metal_init: has bfloat            = true
0.00.441.396 I ggml_metal_init: use bfloat            = true
0.00.441.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.441.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.614 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.465.151 I init:      Metal KV buffer size =    24.00 MiB
0.00.465.155 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.465.202 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.468.483 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.468.485 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.468.486 I llama_context: graph nodes  = 967
0.00.468.486 I llama_context: graph splits = 2
0.00.468.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.468.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.641 I 
0.00.499.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.745 I perplexity: tokenizing the input ..
0.00.507.057 I perplexity: tokenization took 7.309 ms
0.00.507.063 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.650.224 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.651.567 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.651.591 I llama_perf_context_print:        load time =     490.70 ms
0.00.651.592 I llama_perf_context_print: prompt eval time =     142.26 ms /   128 tokens (    1.11 ms per token,   899.79 tokens per second)
0.00.651.593 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.651.593 I llama_perf_context_print:       total time =     151.95 ms /   129 tokens
0.00.652.122 I ggml_metal_free: deallocating

real	0m0.666s
user	0m0.080s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.138 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.365 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.370 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.379 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.380 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.380 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.381 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.383 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.962 I llama_model_loader: - type  f32:  194 tensors
0.00.026.962 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.962 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.962 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.963 I print_info: file format = GGUF V3 (latest)
0.00.026.963 I print_info: file type   = Q4_K - Medium
0.00.026.966 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.110 I load: special tokens cache size = 25
0.00.041.147 I load: token to piece cache size = 0.2984 MB
0.00.041.150 I print_info: arch             = gptneox
0.00.041.150 I print_info: vocab_only       = 0
0.00.041.150 I print_info: n_ctx_train      = 2048
0.00.041.150 I print_info: n_embd           = 2048
0.00.041.151 I print_info: n_layer          = 24
0.00.041.153 I print_info: n_head           = 16
0.00.041.154 I print_info: n_head_kv        = 16
0.00.041.154 I print_info: n_rot            = 32
0.00.041.154 I print_info: n_swa            = 0
0.00.041.155 I print_info: n_embd_head_k    = 128
0.00.041.155 I print_info: n_embd_head_v    = 128
0.00.041.157 I print_info: n_gqa            = 1
0.00.041.158 I print_info: n_embd_k_gqa     = 2048
0.00.041.159 I print_info: n_embd_v_gqa     = 2048
0.00.041.165 I print_info: f_norm_eps       = 1.0e-05
0.00.041.167 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.167 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.168 I print_info: f_logit_scale    = 0.0e+00
0.00.041.170 I print_info: n_ff             = 8192
0.00.041.171 I print_info: n_expert         = 0
0.00.041.171 I print_info: n_expert_used    = 0
0.00.041.171 I print_info: causal attn      = 1
0.00.041.171 I print_info: pooling type     = 0
0.00.041.171 I print_info: rope type        = 2
0.00.041.172 I print_info: rope scaling     = linear
0.00.041.173 I print_info: freq_base_train  = 10000.0
0.00.041.173 I print_info: freq_scale_train = 1
0.00.041.173 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.174 I print_info: rope_finetuned   = unknown
0.00.041.174 I print_info: ssm_d_conv       = 0
0.00.041.174 I print_info: ssm_d_inner      = 0
0.00.041.174 I print_info: ssm_d_state      = 0
0.00.041.174 I print_info: ssm_dt_rank      = 0
0.00.041.174 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.174 I print_info: model type       = 1.4B
0.00.041.175 I print_info: model params     = 1.41 B
0.00.041.176 I print_info: general.name     = 1.4B
0.00.041.176 I print_info: vocab type       = BPE
0.00.041.177 I print_info: n_vocab          = 50304
0.00.041.177 I print_info: n_merges         = 50009
0.00.041.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.177 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.178 I print_info: LF token         = 187 'Ċ'
0.00.041.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.178 I print_info: max token length = 1024
0.00.524.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.524.943 I load_tensors: offloading output layer to GPU
0.00.524.944 I load_tensors: offloaded 25/25 layers to GPU
0.00.524.978 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.524.979 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.526.526 I llama_context: n_seq_max     = 1
0.00.526.530 I llama_context: n_ctx         = 128
0.00.526.531 I llama_context: n_ctx_per_seq = 128
0.00.526.531 I llama_context: n_batch       = 128
0.00.526.531 I llama_context: n_ubatch      = 128
0.00.526.532 I llama_context: flash_attn    = 0
0.00.526.534 I llama_context: freq_base     = 10000.0
0.00.526.534 I llama_context: freq_scale    = 1
0.00.526.535 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.526.537 I ggml_metal_init: allocating
0.00.526.584 I ggml_metal_init: found device: Apple M4
0.00.526.595 I ggml_metal_init: picking default device: Apple M4
0.00.528.295 I ggml_metal_init: using embedded metal library
0.00.533.642 I ggml_metal_init: GPU name:   Apple M4
0.00.533.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.650 I ggml_metal_init: simdgroup reduction   = true
0.00.533.650 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.651 I ggml_metal_init: has residency sets    = true
0.00.533.651 I ggml_metal_init: has bfloat            = true
0.00.533.651 I ggml_metal_init: use bfloat            = true
0.00.533.652 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.015 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.602 I init:      Metal KV buffer size =    24.00 MiB
0.00.556.610 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.680 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.560.658 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.560.660 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.560.661 I llama_context: graph nodes  = 967
0.00.560.661 I llama_context: graph splits = 2
0.00.560.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.560.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.552 I 
0.00.588.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.644 I perplexity: tokenizing the input ..
0.00.595.981 I perplexity: tokenization took 7.335 ms
0.00.595.987 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.703 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.146 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.170 I llama_perf_context_print:        load time =     578.41 ms
0.00.733.170 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   949.99 tokens per second)
0.00.733.171 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.171 I llama_perf_context_print:       total time =     144.62 ms /   129 tokens
0.00.733.731 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.035 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.614 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.616 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.145 I llama_model_loader: - type  f32:  194 tensors
0.00.026.145 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.145 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.146 I print_info: file format = GGUF V3 (latest)
0.00.026.146 I print_info: file type   = Q5_K - Medium
0.00.026.148 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.099 I load: special tokens cache size = 25
0.00.040.001 I load: token to piece cache size = 0.2984 MB
0.00.040.004 I print_info: arch             = gptneox
0.00.040.004 I print_info: vocab_only       = 0
0.00.040.005 I print_info: n_ctx_train      = 2048
0.00.040.005 I print_info: n_embd           = 2048
0.00.040.005 I print_info: n_layer          = 24
0.00.040.007 I print_info: n_head           = 16
0.00.040.008 I print_info: n_head_kv        = 16
0.00.040.008 I print_info: n_rot            = 32
0.00.040.009 I print_info: n_swa            = 0
0.00.040.009 I print_info: n_embd_head_k    = 128
0.00.040.009 I print_info: n_embd_head_v    = 128
0.00.040.010 I print_info: n_gqa            = 1
0.00.040.011 I print_info: n_embd_k_gqa     = 2048
0.00.040.011 I print_info: n_embd_v_gqa     = 2048
0.00.040.012 I print_info: f_norm_eps       = 1.0e-05
0.00.040.012 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.012 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.012 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.014 I print_info: f_logit_scale    = 0.0e+00
0.00.040.015 I print_info: n_ff             = 8192
0.00.040.015 I print_info: n_expert         = 0
0.00.040.015 I print_info: n_expert_used    = 0
0.00.040.015 I print_info: causal attn      = 1
0.00.040.016 I print_info: pooling type     = 0
0.00.040.016 I print_info: rope type        = 2
0.00.040.016 I print_info: rope scaling     = linear
0.00.040.016 I print_info: freq_base_train  = 10000.0
0.00.040.017 I print_info: freq_scale_train = 1
0.00.040.019 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.019 I print_info: rope_finetuned   = unknown
0.00.040.019 I print_info: ssm_d_conv       = 0
0.00.040.019 I print_info: ssm_d_inner      = 0
0.00.040.019 I print_info: ssm_d_state      = 0
0.00.040.019 I print_info: ssm_dt_rank      = 0
0.00.040.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.020 I print_info: model type       = 1.4B
0.00.040.020 I print_info: model params     = 1.41 B
0.00.040.020 I print_info: general.name     = 1.4B
0.00.040.021 I print_info: vocab type       = BPE
0.00.040.021 I print_info: n_vocab          = 50304
0.00.040.021 I print_info: n_merges         = 50009
0.00.040.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.022 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.022 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.026 I print_info: LF token         = 187 'Ċ'
0.00.040.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.027 I print_info: max token length = 1024
0.00.597.478 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.495 I load_tensors: offloading output layer to GPU
0.00.597.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.536 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.537 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.599.169 I llama_context: n_seq_max     = 1
0.00.599.172 I llama_context: n_ctx         = 128
0.00.599.173 I llama_context: n_ctx_per_seq = 128
0.00.599.177 I llama_context: n_batch       = 128
0.00.599.178 I llama_context: n_ubatch      = 128
0.00.599.178 I llama_context: flash_attn    = 0
0.00.599.179 I llama_context: freq_base     = 10000.0
0.00.599.180 I llama_context: freq_scale    = 1
0.00.599.181 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.187 I ggml_metal_init: allocating
0.00.599.267 I ggml_metal_init: found device: Apple M4
0.00.599.279 I ggml_metal_init: picking default device: Apple M4
0.00.600.794 I ggml_metal_init: using embedded metal library
0.00.607.101 I ggml_metal_init: GPU name:   Apple M4
0.00.607.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.107 I ggml_metal_init: simdgroup reduction   = true
0.00.607.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.107 I ggml_metal_init: has residency sets    = true
0.00.607.108 I ggml_metal_init: has bfloat            = true
0.00.607.108 I ggml_metal_init: use bfloat            = true
0.00.607.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.406 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.888 I init:      Metal KV buffer size =    24.00 MiB
0.00.627.892 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.923 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.631.183 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.631.185 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.631.185 I llama_context: graph nodes  = 967
0.00.631.185 I llama_context: graph splits = 2
0.00.631.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.610 I 
0.00.660.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.711 I perplexity: tokenizing the input ..
0.00.667.834 I perplexity: tokenization took 7.121 ms
0.00.667.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.673 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.811.004 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.811.030 I llama_perf_context_print:        load time =     651.57 ms
0.00.811.030 I llama_perf_context_print: prompt eval time =     140.97 ms /   128 tokens (    1.10 ms per token,   908.01 tokens per second)
0.00.811.031 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.032 I llama_perf_context_print:       total time =     150.42 ms /   129 tokens
0.00.811.597 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.080s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.046 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.654 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.666 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.668 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.669 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.330 I llama_model_loader: - type  f32:  194 tensors
0.00.024.331 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.331 I print_info: file format = GGUF V3 (latest)
0.00.024.332 I print_info: file type   = Q6_K
0.00.024.333 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.432 I load: special tokens cache size = 25
0.00.038.610 I load: token to piece cache size = 0.2984 MB
0.00.038.613 I print_info: arch             = gptneox
0.00.038.613 I print_info: vocab_only       = 0
0.00.038.613 I print_info: n_ctx_train      = 2048
0.00.038.614 I print_info: n_embd           = 2048
0.00.038.614 I print_info: n_layer          = 24
0.00.038.617 I print_info: n_head           = 16
0.00.038.618 I print_info: n_head_kv        = 16
0.00.038.618 I print_info: n_rot            = 32
0.00.038.621 I print_info: n_swa            = 0
0.00.038.621 I print_info: n_embd_head_k    = 128
0.00.038.621 I print_info: n_embd_head_v    = 128
0.00.038.622 I print_info: n_gqa            = 1
0.00.038.623 I print_info: n_embd_k_gqa     = 2048
0.00.038.623 I print_info: n_embd_v_gqa     = 2048
0.00.038.624 I print_info: f_norm_eps       = 1.0e-05
0.00.038.625 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.625 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.625 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.625 I print_info: f_logit_scale    = 0.0e+00
0.00.038.627 I print_info: n_ff             = 8192
0.00.038.629 I print_info: n_expert         = 0
0.00.038.629 I print_info: n_expert_used    = 0
0.00.038.630 I print_info: causal attn      = 1
0.00.038.630 I print_info: pooling type     = 0
0.00.038.630 I print_info: rope type        = 2
0.00.038.634 I print_info: rope scaling     = linear
0.00.038.634 I print_info: freq_base_train  = 10000.0
0.00.038.634 I print_info: freq_scale_train = 1
0.00.038.635 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.635 I print_info: rope_finetuned   = unknown
0.00.038.635 I print_info: ssm_d_conv       = 0
0.00.038.636 I print_info: ssm_d_inner      = 0
0.00.038.640 I print_info: ssm_d_state      = 0
0.00.038.641 I print_info: ssm_dt_rank      = 0
0.00.038.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.641 I print_info: model type       = 1.4B
0.00.038.642 I print_info: model params     = 1.41 B
0.00.038.642 I print_info: general.name     = 1.4B
0.00.038.643 I print_info: vocab type       = BPE
0.00.038.643 I print_info: n_vocab          = 50304
0.00.038.643 I print_info: n_merges         = 50009
0.00.038.643 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.643 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.645 I print_info: LF token         = 187 'Ċ'
0.00.038.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.646 I print_info: max token length = 1024
0.00.587.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.390 I load_tensors: offloading output layer to GPU
0.00.587.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.414 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.587.417 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.588.859 I llama_context: n_seq_max     = 1
0.00.588.861 I llama_context: n_ctx         = 128
0.00.588.861 I llama_context: n_ctx_per_seq = 128
0.00.588.862 I llama_context: n_batch       = 128
0.00.588.865 I llama_context: n_ubatch      = 128
0.00.588.866 I llama_context: flash_attn    = 0
0.00.588.867 I llama_context: freq_base     = 10000.0
0.00.588.867 I llama_context: freq_scale    = 1
0.00.588.868 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.588.871 I ggml_metal_init: allocating
0.00.588.889 I ggml_metal_init: found device: Apple M4
0.00.588.896 I ggml_metal_init: picking default device: Apple M4
0.00.590.124 I ggml_metal_init: using embedded metal library
0.00.595.999 I ggml_metal_init: GPU name:   Apple M4
0.00.596.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.596.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.596.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.596.004 I ggml_metal_init: simdgroup reduction   = true
0.00.596.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.596.005 I ggml_metal_init: has residency sets    = true
0.00.596.005 I ggml_metal_init: has bfloat            = true
0.00.596.005 I ggml_metal_init: use bfloat            = true
0.00.596.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.596.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.242 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.564 I init:      Metal KV buffer size =    24.00 MiB
0.00.615.568 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.598 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.618.641 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.618.643 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.618.644 I llama_context: graph nodes  = 967
0.00.618.644 I llama_context: graph splits = 2
0.00.618.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.646 I 
0.00.650.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.740 I perplexity: tokenizing the input ..
0.00.657.921 I perplexity: tokenization took 7.179 ms
0.00.657.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.331 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.798.692 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.798.711 I llama_perf_context_print:        load time =     641.59 ms
0.00.798.712 I llama_perf_context_print: prompt eval time =     139.14 ms /   128 tokens (    1.09 ms per token,   919.94 tokens per second)
0.00.798.713 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.713 I llama_perf_context_print:       total time =     148.07 ms /   129 tokens
0.00.799.249 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.076s
sys	0m0.129s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.295 I build: 4627 (5d3491e7) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.072 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.090 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.250 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.251 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.251 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.251 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.252 I llama_model_loader: - type  f32:  194 tensors
0.00.059.253 I llama_model_loader: - type  f16:   98 tensors
0.00.059.253 I print_info: file format = GGUF V3 (latest)
0.00.059.254 I print_info: file type   = all F32 (guessed)
0.00.059.255 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.071.449 I load: special tokens cache size = 25
0.00.078.864 I load: token to piece cache size = 0.2984 MB
0.00.078.867 I print_info: arch             = gptneox
0.00.078.867 I print_info: vocab_only       = 0
0.00.078.868 I print_info: n_ctx_train      = 2048
0.00.078.868 I print_info: n_embd           = 2048
0.00.078.868 I print_info: n_layer          = 24
0.00.078.872 I print_info: n_head           = 16
0.00.078.874 I print_info: n_head_kv        = 16
0.00.078.874 I print_info: n_rot            = 32
0.00.078.874 I print_info: n_swa            = 0
0.00.078.874 I print_info: n_embd_head_k    = 128
0.00.078.874 I print_info: n_embd_head_v    = 128
0.00.078.875 I print_info: n_gqa            = 1
0.00.078.876 I print_info: n_embd_k_gqa     = 2048
0.00.078.876 I print_info: n_embd_v_gqa     = 2048
0.00.078.877 I print_info: f_norm_eps       = 1.0e-05
0.00.078.877 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.878 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.878 I print_info: f_logit_scale    = 0.0e+00
0.00.078.879 I print_info: n_ff             = 8192
0.00.078.879 I print_info: n_expert         = 0
0.00.078.879 I print_info: n_expert_used    = 0
0.00.078.879 I print_info: causal attn      = 1
0.00.078.879 I print_info: pooling type     = 0
0.00.078.880 I print_info: rope type        = 2
0.00.078.880 I print_info: rope scaling     = linear
0.00.078.880 I print_info: freq_base_train  = 10000.0
0.00.078.880 I print_info: freq_scale_train = 1
0.00.078.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.881 I print_info: rope_finetuned   = unknown
0.00.078.881 I print_info: ssm_d_conv       = 0
0.00.078.881 I print_info: ssm_d_inner      = 0
0.00.078.881 I print_info: ssm_d_state      = 0
0.00.078.881 I print_info: ssm_dt_rank      = 0
0.00.078.882 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.882 I print_info: model type       = 1.4B
0.00.078.882 I print_info: model params     = 1.41 B
0.00.078.883 I print_info: general.name     = 1.4B
0.00.078.883 I print_info: vocab type       = BPE
0.00.078.883 I print_info: n_vocab          = 50304
0.00.078.883 I print_info: n_merges         = 50009
0.00.078.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.884 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.884 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.884 I print_info: LF token         = 187 'Ċ'
0.00.078.885 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.886 I print_info: max token length = 1024
0.01.265.652 I load_tensors: offloading 24 repeating layers to GPU
0.01.265.657 I load_tensors: offloading output layer to GPU
0.01.265.657 I load_tensors: offloaded 25/25 layers to GPU
0.01.265.689 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.265.690 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.266.520 I llama_context: n_seq_max     = 1
0.01.266.521 I llama_context: n_ctx         = 128
0.01.266.522 I llama_context: n_ctx_per_seq = 128
0.01.266.522 I llama_context: n_batch       = 128
0.01.266.522 I llama_context: n_ubatch      = 128
0.01.266.522 I llama_context: flash_attn    = 0
0.01.266.523 I llama_context: freq_base     = 10000.0
0.01.266.523 I llama_context: freq_scale    = 1
0.01.266.524 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.266.527 I ggml_metal_init: allocating
0.01.266.626 I ggml_metal_init: found device: Apple M4
0.01.266.645 I ggml_metal_init: picking default device: Apple M4
0.01.267.786 I ggml_metal_init: using embedded metal library
0.01.271.634 I ggml_metal_init: GPU name:   Apple M4
0.01.271.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.271.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.271.638 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.271.638 I ggml_metal_init: simdgroup reduction   = true
0.01.271.638 I ggml_metal_init: simdgroup matrix mul. = true
0.01.271.638 I ggml_metal_init: has residency sets    = true
0.01.271.639 I ggml_metal_init: has bfloat            = true
0.01.271.639 I ggml_metal_init: use bfloat            = true
0.01.271.639 I ggml_metal_init: hasUnifiedMemory      = true
0.01.271.640 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.282.123 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.283.836 I init:      Metal KV buffer size =    24.00 MiB
0.01.283.838 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.283.852 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.285.456 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.285.457 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.285.458 I llama_context: graph nodes  = 967
0.01.285.458 I llama_context: graph splits = 2
0.01.285.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.285.460 I 
0.01.285.497 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.285.498 I compute_imatrix: tokenizing the input ..
0.01.289.603 I compute_imatrix: tokenization took 4.104 ms
0.01.289.605 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.553.357 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.555.739 I llama_perf_context_print:        load time =    1527.15 ms
0.01.555.740 I llama_perf_context_print: prompt eval time =     262.02 ms /   128 tokens (    2.05 ms per token,   488.51 tokens per second)
0.01.555.745 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.555.745 I llama_perf_context_print:       total time =    1529.53 ms /   129 tokens
0.01.556.414 I ggml_metal_free: deallocating

real	0m1.746s
user	0m0.126s
sys	0m0.239s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4627 (5d3491e7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138b05520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138b05b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138b06000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138b091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138b09730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138b09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138b0a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138b0a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138b0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138b0aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138b0b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138b0b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138b0c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138b0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138b0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138b0d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138b0dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138b0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138b0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138b0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138b0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138b10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138b10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138b113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138b11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138b11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138b123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138b13030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138b13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138b13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138b13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138b13f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138b14820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138b14d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138b15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138b154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138b15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138b15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138b162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138b16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138b16be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138b17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138b17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138b179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138b17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138b18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138b188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138b191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138b197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138b19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138b1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138b1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138b1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138b1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138b1be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138b1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138b1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138b1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138b1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138b1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138b1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138b1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138b1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138b1e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138b1ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138b1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138b1f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138b1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138b1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138b20470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138b20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138b20db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138b21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138b217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138b21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138b22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138b22790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138b22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138b23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138b23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138b23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138b24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138b24770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138b24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138b25210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138b25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138b25cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138b26200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138b26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138b26ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138b271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138b27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138b27c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138b281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138b28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138b28c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138b291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138b18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138b29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138b29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138b2a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138b2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138b2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138b2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138b2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138b2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138b2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138b2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138b2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138b2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138b2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138b2ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138b2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138b2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138b2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138b2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138b2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138b2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138b2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138b30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138b30800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138b30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138b31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138b315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138b31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138b31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138b323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138b32860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138b32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138b331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138b33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138b33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138b33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138b34420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138b348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138b34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138b35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138b356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138b35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138b35fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138b36480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138b36920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138b36dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138b37260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138b37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138b37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138b38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138b384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138b38980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138b38e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138b392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138b39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138b39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138b3a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138b3a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138b3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138b3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138b3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138b3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138b3bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138b3c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138b3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138b3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138b3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138b3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138b3d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138b3dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138b3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138b3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138b3eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138b3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138b3f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138b3f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138b3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138b401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138b40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138b40b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138b40fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138b41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138b418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138b41d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138b42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138b426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138b42b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138b43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138b434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138b43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138b43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138b44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138b44720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138b44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138b45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138b45500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138b45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138b45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138b464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138b46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138b46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138b47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138b47920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138b47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138b48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138b48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138b48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138b49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138b49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138b4a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138b4a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138b4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138b4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138b4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138b4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138b4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138b4c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138b4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138b4d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138b4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138b4dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138b4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138b4e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138b4ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138b4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138b4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138b4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138b50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138b507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138b50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138b51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138b517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138b51d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138b52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138b527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138b52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138b53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138b537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138b53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138b54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138b54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138b54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138b55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138b55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138b55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138b56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138b56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138b56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138b57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138b57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138b57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138b58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138b58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138b58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138b591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138b59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138b59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138b5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138b5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138b5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138b5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138b5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138b5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138b5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138b5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138b5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138b5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138b5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138b5dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138b5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138b5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138b5eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138b5ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138b5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138b5f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138b5fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138b60200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138b606a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138b60b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138b60fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138b61480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138b61920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138b61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138b62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138b62700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138b62c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138b63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138b63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138b641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138b648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138b64b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138b65380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138b65640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138b65c50 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.746.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138b65900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138b475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138b46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138b47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138b1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138b1a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138b12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138b18b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138b19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138b19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138b18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138b17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138b1a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138b0b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138b1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138b29900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138b64e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138b14380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138b481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138b12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138b12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138b12c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138b660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138b66370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138b66630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138b668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138b66bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138b66e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138b67130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138b673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138b676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138b67970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138b67c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138b67ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138b681b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138b68470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138b68730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138b689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138b68cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138b68f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138b69230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138b694f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138b697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138b69a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138b69d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138b69ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138b6a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138b6a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138b6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138b6aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138b6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138b6b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138b6b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138b6b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138b6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138b6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138b6be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138b6c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138b6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138b6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138b6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138b6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138b6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138b6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138b6d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138b6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138b6d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138b6dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138b6df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138b6e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138b6e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138b6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138b6ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138b6ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138b6efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138b6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138b6f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138b6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138b6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138b6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138b70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138b702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138b705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138b70870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138b70b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138b70df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138b710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138b71370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138b71630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138b718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138b71bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138b71e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138b72130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138b723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138b726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138b72970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138b72c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138b72ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138b731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138b73470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138b73730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138b739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138b73cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138b73f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138b74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138b744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138b747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138b74a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138b74d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138b74ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138b752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138b75570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138b75830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138b75af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138b75db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138b76070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138b76330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138b765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138b768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138b76b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138b76e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138b770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138b773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138b77670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138b77930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138b77bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138b77eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138b78170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138b78430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138b786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138b789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138b78c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138b78f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138b791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138b794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138b79770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138b79a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138b79cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138b79fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138b7a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138b7a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138b7a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138b7aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138b7ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138b7b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138b7b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138b7b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138b7b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138b7bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138b7bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138b7c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138b7c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138b7c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138b7c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138b7cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138b7ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138b7d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138b7d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138b7d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138b7d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138b7dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138b7def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138b7e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138b7e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138b7e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138b7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138b7ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138b7ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138b7f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138b7f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138b7f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138b7fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138b7fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138b7fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138b802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138b80570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138b80830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138b80af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138b80db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138b81070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138b81330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138b815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138b818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138b81b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138b81e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138b820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138b823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138b82670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138b82930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138b82bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138b82eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138b83170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138b83430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138b836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138b839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138b83c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138b83f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138b841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138b844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138b84770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138b84a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138b84cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138b84fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138b854f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138b85a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138b85f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138b86230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138b86630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138b86ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138b86f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138b87720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138b879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138b87ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138b88110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138b88580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138b889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138b88e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138b892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138b89740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138b89bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138b8a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138b8a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138b8a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138b8ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138b8b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138b8b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138b8bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138b8bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138b8c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138b8c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138b8cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138b8d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138b8d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138b8d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138b8de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138b8e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138b8e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138b8eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138b8f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138b8f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138b8f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138b8fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138b901c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138b90630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138b90aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138b90f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138b91380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138b917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138b91c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138b920d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138b92540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138b929b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138b92e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138b93290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138b93700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138b93b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138b93fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138b94450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138b948c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138b94d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138b951a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138b95610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138b95a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138b95ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138b96360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138b967d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138b96c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138b970b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138b97520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138b97990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138b97e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138b98270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138b986e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138b98b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138b98fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138b99430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138b998a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138b99d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138b9a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138b9a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138b9aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138b9aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138b9b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138b9bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138b9c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138b9cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138b9d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138b9d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138b9ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138b9e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138b9e690 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1237044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1237056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1237063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1237078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1237083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12370a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12370a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12370b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12370b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12370bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12370c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12370cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12370d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12370db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12370de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12370e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12370e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12370e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12370ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12370f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12370f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12370fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12370ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1237107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1237110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1237119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1237138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1237141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1237157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1237160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1237185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12371a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12371a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12371a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12371adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12371b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12371b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12371bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12371bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12371c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12371c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12371ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12371d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12371d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12371da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12371de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12371e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12371e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12371ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12371f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12371f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12371f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12371fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1237213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1237229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1237232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1237258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1237261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1237277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1237280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1237296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12372a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12372a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12372ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12372b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12372b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12372ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12372bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12372c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12372c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12372cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12372d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12372d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12372d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12372dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12372e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12372e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12372eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12372efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12372f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12372f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12372fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1237305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1237324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1237336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1237343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1237355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1237374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1237393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12373a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12373a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12373aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12373ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12373b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12373b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12373bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12373c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12373c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12373c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12373cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12373d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12373d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12373dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12373df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12373e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12373e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12373ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12373f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12373f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12373f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12373fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1237402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1237429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1237432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1237451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1237463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1237470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1237479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1237482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1237498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12374a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12374a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12374aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12374af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12374b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12374b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12374bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12374c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12374c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12374c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12374ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12374d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12374d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12374db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12374dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12374e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12374e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12374ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12374f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12374f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12374fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12374fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1237507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1237510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1237526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1237538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1237545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1237557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1237584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123758ac0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.804s
user	0m0.281s
sys	0m0.329s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4627 (5d3491e7)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ef0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ef0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ef0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ef0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ef0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ef0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ef0d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ef0dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ef0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ef0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ef0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ef0f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ef0fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ef10370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ef10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ef112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ef119c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ef120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ef12800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ef12fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ef136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ef13e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ef14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ef14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ef154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ef157b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ef15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ef16a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ef16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ef17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ef176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ef17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ef18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ef18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ef18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ef18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ef19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ef19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ef19ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ef1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ef1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ef1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ef1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ef1b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ef1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ef1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ef1c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ef1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ef1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ef1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ef1ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ef1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ef1ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ef1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ef1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ef1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ef20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ef20410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ef20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ef21210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ef214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ef21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ef21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ef222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ef22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ef22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ef23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ef23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ef239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ef23e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ef24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ef247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ef24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ef251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ef256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ef25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ef26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ef266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ef26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ef27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ef276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ef27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ef28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ef286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ef28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ef29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ef296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ef29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ef2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ef2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ef2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ef2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ef2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ef2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ef2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ef2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ef2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ef1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ef2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ef2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ef2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ef2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ef2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ef2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ef2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ef2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ef2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ef30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ef307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ef30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ef31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ef317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ef31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ef321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ef32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ef32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ef32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ef33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ef338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ef33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ef34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ef346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ef34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ef34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ef35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ef35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ef35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ef36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ef36700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ef36ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ef37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ef374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ef37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ef37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ef382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ef38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ef38c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ef390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ef39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ef399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ef39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ef3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ef3a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ef3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ef3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ef3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ef3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ef3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ef3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ef3c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ef3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ef3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ef3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ef3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ef3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ef3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ef3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ef3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ef3f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ef3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ef3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ef3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ef40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ef408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ef40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ef41220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ef416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ef41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ef42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ef424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ef42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ef42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ef43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ef43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ef43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ef44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ef44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ef449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ef44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ef452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ef45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ef45c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ef460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ef46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ef46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ef46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ef47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ef477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ef47c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ef48120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ef485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ef48a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ef48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ef49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ef499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ef49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ef4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ef4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ef4ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ef4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ef4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ef4c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ef4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ef4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ef4ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef4d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef61100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef61ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef64540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef64e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef65320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef65c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef66100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef66d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef67490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef67bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ef68d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ef69040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ef69650 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.097.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130806150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1308065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x130806a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130806ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130807310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130807780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130807bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x130808060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1308084d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x130808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130808db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130809470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130809f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13080a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13080af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13080b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13080bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13080c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13080cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13080d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13080dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13080e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13080e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13080f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13080f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13080fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13080fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x130810130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1308105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x130810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x130810e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1308113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x130811820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130811ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130811f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1308123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x130812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130812ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130813580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1308139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130813e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1308142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x130814740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x130815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x130815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1308161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130816ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x130816f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1308173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x130817810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1308181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1308186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130818b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1308198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x130819d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13081a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13081a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13081aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13081aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13081b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13081b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13081bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13081c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13081c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13081c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13081cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13081d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13081d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13081db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13081dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13081e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13081e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13081ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13081f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13081f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13081fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13081fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x130820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1308207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x130820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x130821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1308214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1308226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x130822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1308245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x130824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x130825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1308264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x130826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x130827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1308283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1308295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x130829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13082a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13082a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13082abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13082b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13082b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13082b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13082bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13082c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13082c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13082cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13082cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13082d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13082d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13082dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13082e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13082e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13082e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13082ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13082f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13082f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13082fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x130830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x130830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x130830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x130830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1308311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1308323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x130832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1308330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1308339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x130833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1308342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x130834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x130835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130835470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1308358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130835d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1308361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130836630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130836de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1308370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130837510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x130837980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130838260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1308386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130838b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x130838fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130839420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x130839890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130839d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13083a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13083a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13083aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13083aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13083b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13083b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13083bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13083c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13083c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13083c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13083cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13083d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13083d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13083db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13083df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13083e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13083e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13083ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13083f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13083f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13083fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13083fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x130840310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x130840780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x130840bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x130841060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1308414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130841940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x130841db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130842220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130842690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130842b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1308436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130843970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130843c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1308440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130844510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130844980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x130844df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130845260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1308456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130845b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130845fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130846890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130846d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130847170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1308475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130847a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x130847ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130848330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1308487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x130848c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130849080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1308494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130849960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x130849dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13084a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13084a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13084ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13084af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13084b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13084b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13084bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13084c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13084c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13084ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13084cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13084d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13084d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13084dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13084e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13084e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13084e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13084edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13084f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13084f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13084fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13084ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1308503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130850850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130850cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130851130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1308515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x130851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130851e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1308522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130852760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x130852bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130853040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1308534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130853920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130853d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130854200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130854670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x130854ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130854f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1308553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130855830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130855ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x130856110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130856580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1308569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130856e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1308572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130857d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130858460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130858b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1308592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130859560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1308599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130859fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13085a5e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ef69300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ef4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ef4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ef4b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ef1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ef1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ef206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ef4d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ef15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ef1c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ef1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ef1d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ef1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ef1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ef14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ef20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ef2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ef68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ef17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ef17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ef4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ef4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ef16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ef16340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ef16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ef69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ef69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ef6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ef6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ef6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ef6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ef6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ef6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ef6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ef6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ef6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ef6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ef6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ef6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ef6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ef6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ef6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ef6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ef6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ef6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ef6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ef6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ef6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ef6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ef6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ef6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ef6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ef6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ef6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ef6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ef6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ef6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ef6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ef6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ef6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ef6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ef6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ef70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ef70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ef705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ef708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ef70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ef70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ef710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ef713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ef71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ef71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ef71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ef71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ef72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ef72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ef726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ef729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ef72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ef72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ef731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ef734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ef73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ef73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ef73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ef73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ef74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ef74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ef747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ef74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ef74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ef75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ef752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ef755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ef75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ef75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ef75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ef760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ef76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ef76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ef768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ef76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ef76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ef77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ef773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ef776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ef77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ef77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ef77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ef781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ef78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ef78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ef789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ef78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ef78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ef79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ef794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ef797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ef79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ef79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ef79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ef7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ef7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ef7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ef7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ef7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ef7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ef7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ef7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ef7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ef7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ef7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ef7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ef7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ef7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ef7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ef7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ef7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ef7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ef7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ef7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ef7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ef7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ef7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ef7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ef7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ef7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ef7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ef7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ef7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ef7f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ef7f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ef7f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ef7fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ef7fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ef80030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ef802f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ef805b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ef80870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ef80b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ef80df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ef810b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ef81370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ef81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ef818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ef81bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ef81e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ef82130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ef823f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ef826b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ef82970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ef82c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ef82ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ef831b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ef83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ef83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ef839f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ef83cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ef83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ef84230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ef844f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ef847b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ef84a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ef84d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ef84ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ef852b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ef85570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ef85830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ef85af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ef85db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ef86070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ef86330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ef865f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ef868b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ef86b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ef86e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ef870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ef873b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ef87670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ef87930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ef87bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ef87eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ef88170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ef88430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ef886f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef889b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef88c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef88f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef89330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef89f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef8a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef8a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef8a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef8ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef8b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef8b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef8bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef8bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef8c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef8c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef8ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef8d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef8d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef8da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef8deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef8e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef8e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef8ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef8f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef8f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef8f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef8fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef90230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef906a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef90b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef90f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef913f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef91860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef91cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef92140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef925b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef92a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef92e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef93300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef93770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef93be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef94050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef944c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef94930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef94da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef95210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef95680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef95af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef95f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef963d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef96840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef96cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef97120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef97590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef97a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef97e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef982e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef98750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef98bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef99030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef994a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef99910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef99d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef9a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef9a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef9aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef9af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef9b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef9b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef9bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef9c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef9c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef9c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef9ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef9d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef9d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef9dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef9e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef9ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef9f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef9fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef9fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12efa0620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12efa08e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12efa0ef0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.968s
user	0m0.236s
sys	0m0.189s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
