Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:49 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:305 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.642s
user	0m0.721s
sys	0m0.992s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Built target build_info
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.c.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX shared library libllama.dylib
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-run
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Built target llava
[ 34%] Linking CXX static library libcommon.a
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-run
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX shared library libllava_shared.dylib
[ 35%] Built target llama-quantize-stats
[ 35%] Built target common
[ 35%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-log
[ 48%] Built target test-chat-template
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Built target test-llama-grammar
[ 58%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Built target test-autorelease
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 63%] Built target test-barrier
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-json-schema-to-grammar
[ 65%] Built target test-quantize-perf
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-fns
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target test-rope
[ 67%] Built target llama-batched-bench
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Built target llama-gguf-split
[ 75%] Built target llama-gbnf-validator
[ 75%] Built target llama-eval-callback
[ 75%] Built target llama-embedding
[ 75%] Built target llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Built target llama-gritlm
[ 75%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-bench
[ 76%] Built target llama-lookahead
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-cli
[ 85%] Linking CXX executable ../../bin/llama-lookup-stats
[ 85%] Linking CXX executable ../../bin/llama-perplexity
[ 85%] Built target llama-lookup
[ 85%] Linking CXX executable ../../bin/llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Generating loading.html.hpp
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Generating index.html.hpp
[ 87%] Built target llama-parallel
[ 87%] Built target llama-lookup-create
[ 87%] Built target llama-lookup-merge
[ 87%] Built target llama-perplexity
[ 87%] Built target llama-lookup-stats
[ 87%] Built target llama-cli
[ 87%] Built target llama-passkey
[ 87%] Built target llama-quantize
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Built target llama-retrieval
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 91%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-save-load-state
[ 95%] Linking CXX executable ../../bin/llama-speculative
[ 95%] Linking CXX executable ../../bin/llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-speculative-simple
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-speculative
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-llava-cli
[ 96%] Built target llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.428s
user	0m5.051s
sys	0m8.365s

main: quantize time =  4361.90 ms
main:    total time =  4361.90 ms

main: quantize time =  1929.20 ms
main:    total time =  1929.20 ms

main: quantize time =  2027.11 ms
main:    total time =  2027.11 ms

main: quantize time =  2792.81 ms
main:    total time =  2792.81 ms

main: quantize time =  2398.01 ms
main:    total time =  2398.01 ms

main: quantize time =  5051.09 ms
main:    total time =  5051.09 ms

main: quantize time =  5598.60 ms
main:    total time =  5598.60 ms

main: quantize time =  6778.12 ms
main:    total time =  6778.12 ms

main: quantize time =  5773.03 ms
main:    total time =  5773.03 ms

main: quantize time =  4470.12 ms
main:    total time =  4470.12 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.235 I main: llama backend init
0.00.000.241 I main: load the model and apply lora adapter, if any
0.00.032.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.877 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.908 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.913 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.913 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.064.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.064.247 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.064.248 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.064.248 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.064.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.064.250 I llama_model_loader: - type  f32:  194 tensors
0.00.064.250 I llama_model_loader: - type  f16:   98 tensors
0.00.099.128 I llm_load_vocab: special tokens cache size = 25
0.00.106.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.106.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.106.409 I llm_load_print_meta: arch             = gptneox
0.00.106.409 I llm_load_print_meta: vocab type       = BPE
0.00.106.409 I llm_load_print_meta: n_vocab          = 50304
0.00.106.409 I llm_load_print_meta: n_merges         = 50009
0.00.106.410 I llm_load_print_meta: vocab_only       = 0
0.00.106.410 I llm_load_print_meta: n_ctx_train      = 2048
0.00.106.410 I llm_load_print_meta: n_embd           = 2048
0.00.106.410 I llm_load_print_meta: n_layer          = 24
0.00.106.414 I llm_load_print_meta: n_head           = 16
0.00.106.414 I llm_load_print_meta: n_head_kv        = 16
0.00.106.434 I llm_load_print_meta: n_rot            = 32
0.00.106.435 I llm_load_print_meta: n_swa            = 0
0.00.106.435 I llm_load_print_meta: n_embd_head_k    = 128
0.00.106.436 I llm_load_print_meta: n_embd_head_v    = 128
0.00.106.436 I llm_load_print_meta: n_gqa            = 1
0.00.106.437 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.106.438 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.106.438 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.106.438 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.106.439 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.106.439 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.106.439 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.106.440 I llm_load_print_meta: n_ff             = 8192
0.00.106.440 I llm_load_print_meta: n_expert         = 0
0.00.106.440 I llm_load_print_meta: n_expert_used    = 0
0.00.106.440 I llm_load_print_meta: causal attn      = 1
0.00.106.440 I llm_load_print_meta: pooling type     = 0
0.00.106.440 I llm_load_print_meta: rope type        = 2
0.00.106.441 I llm_load_print_meta: rope scaling     = linear
0.00.106.441 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.106.441 I llm_load_print_meta: freq_scale_train = 1
0.00.106.441 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.106.441 I llm_load_print_meta: rope_finetuned   = unknown
0.00.106.442 I llm_load_print_meta: ssm_d_conv       = 0
0.00.106.442 I llm_load_print_meta: ssm_d_inner      = 0
0.00.106.442 I llm_load_print_meta: ssm_d_state      = 0
0.00.106.442 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.106.442 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.106.452 I llm_load_print_meta: model type       = 1.4B
0.00.106.452 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.106.453 I llm_load_print_meta: model params     = 1.41 B
0.00.106.453 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.106.453 I llm_load_print_meta: general.name     = 1.4B
0.00.106.454 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.106.454 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.106.454 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.106.454 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.106.455 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.106.455 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.106.455 I llm_load_print_meta: max token length = 1024
0.00.109.090 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.109.090 I llm_load_tensors: offloading output layer to GPU
0.00.109.090 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.109.109 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.109.111 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.110.146 I llama_new_context_with_model: n_seq_max     = 1
0.00.110.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.110.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.110.148 I llama_new_context_with_model: n_batch       = 2048
0.00.110.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.110.148 I llama_new_context_with_model: flash_attn    = 0
0.00.110.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.110.149 I llama_new_context_with_model: freq_scale    = 1
0.00.110.149 I ggml_metal_init: allocating
0.00.110.153 I ggml_metal_init: found device: Apple M4
0.00.110.155 I ggml_metal_init: picking default device: Apple M4
0.00.110.825 I ggml_metal_init: using embedded metal library
0.00.119.709 I ggml_metal_init: GPU name:   Apple M4
0.00.119.711 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.712 I ggml_metal_init: simdgroup reduction   = true
0.00.119.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.712 I ggml_metal_init: has bfloat            = true
0.00.119.712 I ggml_metal_init: use bfloat            = true
0.00.119.713 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.162.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.236 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.162.256 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.115 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.117 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.117 I llama_new_context_with_model: graph nodes  = 967
0.00.163.117 I llama_new_context_with_model: graph splits = 2
0.00.163.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.234.727 I main: llama threadpool init, n_threads = 4
0.00.234.760 I 
0.00.234.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.234.798 I 
0.00.234.883 I sampler seed: 1234
0.00.234.888 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.234.911 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.234.913 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.234.913 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.087.237 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.02.087.238 I llama_perf_context_print:        load time =     201.72 ms
0.02.087.238 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.93 tokens per second)
0.02.087.240 I llama_perf_context_print:        eval time =    1805.55 ms /    63 runs   (   28.66 ms per token,    34.89 tokens per second)
0.02.087.240 I llama_perf_context_print:       total time =    1852.51 ms /    70 tokens
0.02.087.429 I ggml_metal_free: deallocating

real	0m2.377s
user	0m0.148s
sys	0m0.092s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.776 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.619 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.622 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.623 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.624 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.625 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.625 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.626 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.626 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.628 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.909 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.558 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.560 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.561 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.561 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.562 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.562 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.563 I llama_model_loader: - type  f32:  194 tensors
0.00.039.563 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.015 I llm_load_vocab: special tokens cache size = 25
0.00.070.675 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.680 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.681 I llm_load_print_meta: arch             = gptneox
0.00.070.681 I llm_load_print_meta: vocab type       = BPE
0.00.070.681 I llm_load_print_meta: n_vocab          = 50304
0.00.070.682 I llm_load_print_meta: n_merges         = 50009
0.00.070.682 I llm_load_print_meta: vocab_only       = 0
0.00.070.682 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.682 I llm_load_print_meta: n_embd           = 2048
0.00.070.682 I llm_load_print_meta: n_layer          = 24
0.00.070.688 I llm_load_print_meta: n_head           = 16
0.00.070.689 I llm_load_print_meta: n_head_kv        = 16
0.00.070.701 I llm_load_print_meta: n_rot            = 32
0.00.070.702 I llm_load_print_meta: n_swa            = 0
0.00.070.702 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.703 I llm_load_print_meta: n_gqa            = 1
0.00.070.704 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.705 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.705 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.706 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.706 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.706 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.706 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.707 I llm_load_print_meta: n_ff             = 8192
0.00.070.707 I llm_load_print_meta: n_expert         = 0
0.00.070.707 I llm_load_print_meta: n_expert_used    = 0
0.00.070.707 I llm_load_print_meta: causal attn      = 1
0.00.070.707 I llm_load_print_meta: pooling type     = 0
0.00.070.710 I llm_load_print_meta: rope type        = 2
0.00.070.710 I llm_load_print_meta: rope scaling     = linear
0.00.070.710 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.711 I llm_load_print_meta: freq_scale_train = 1
0.00.070.711 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.711 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.711 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.711 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.712 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.712 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.712 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.722 I llm_load_print_meta: model type       = 1.4B
0.00.070.723 I llm_load_print_meta: model ftype      = Q8_0
0.00.070.723 I llm_load_print_meta: model params     = 1.41 B
0.00.070.724 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.070.724 I llm_load_print_meta: general.name     = 1.4B
0.00.070.724 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.724 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.724 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.725 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.725 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.070.725 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.725 I llm_load_print_meta: max token length = 1024
0.00.073.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.250 I llm_load_tensors: offloading output layer to GPU
0.00.073.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.262 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.264 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.399 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.400 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.400 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.401 I llama_new_context_with_model: n_batch       = 2048
0.00.074.401 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.401 I llama_new_context_with_model: flash_attn    = 0
0.00.074.402 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.402 I llama_new_context_with_model: freq_scale    = 1
0.00.074.403 I ggml_metal_init: allocating
0.00.074.410 I ggml_metal_init: found device: Apple M4
0.00.074.413 I ggml_metal_init: picking default device: Apple M4
0.00.075.206 I ggml_metal_init: using embedded metal library
0.00.078.174 I ggml_metal_init: GPU name:   Apple M4
0.00.078.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.177 I ggml_metal_init: simdgroup reduction   = true
0.00.078.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.177 I ggml_metal_init: has bfloat            = true
0.00.078.178 I ggml_metal_init: use bfloat            = true
0.00.078.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.061 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.068 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.182 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.183 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.184 I llama_new_context_with_model: graph nodes  = 967
0.00.115.184 I llama_new_context_with_model: graph splits = 2
0.00.115.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.207.794 I main: llama threadpool init, n_threads = 4
0.01.207.873 I 
0.01.207.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.207.924 I 
0.01.208.307 I sampler seed: 1234
0.01.208.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.208.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.208.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.208.375 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.316.422 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48864.42 tokens per second)
0.02.316.423 I llama_perf_context_print:        load time =    1198.01 ms
0.02.316.424 I llama_perf_context_print: prompt eval time =      50.04 ms /     7 tokens (    7.15 ms per token,   139.90 tokens per second)
0.02.316.424 I llama_perf_context_print:        eval time =    1054.79 ms /    63 runs   (   16.74 ms per token,    59.73 tokens per second)
0.02.316.425 I llama_perf_context_print:       total time =    1108.63 ms /    70 tokens
0.02.316.613 I ggml_metal_free: deallocating

real	0m2.335s
user	0m0.132s
sys	0m0.217s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.586 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.477 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.477 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.079 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.080 I llama_model_loader: - type  f32:  194 tensors
0.00.027.080 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.080 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.937 I llm_load_vocab: special tokens cache size = 25
0.00.053.907 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.910 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.910 I llm_load_print_meta: arch             = gptneox
0.00.053.911 I llm_load_print_meta: vocab type       = BPE
0.00.053.911 I llm_load_print_meta: n_vocab          = 50304
0.00.053.911 I llm_load_print_meta: n_merges         = 50009
0.00.053.911 I llm_load_print_meta: vocab_only       = 0
0.00.053.912 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.914 I llm_load_print_meta: n_embd           = 2048
0.00.053.914 I llm_load_print_meta: n_layer          = 24
0.00.053.919 I llm_load_print_meta: n_head           = 16
0.00.053.920 I llm_load_print_meta: n_head_kv        = 16
0.00.053.933 I llm_load_print_meta: n_rot            = 32
0.00.053.933 I llm_load_print_meta: n_swa            = 0
0.00.053.934 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.934 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.934 I llm_load_print_meta: n_gqa            = 1
0.00.053.935 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.936 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.936 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.937 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.937 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.937 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.937 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.938 I llm_load_print_meta: n_ff             = 8192
0.00.053.938 I llm_load_print_meta: n_expert         = 0
0.00.053.938 I llm_load_print_meta: n_expert_used    = 0
0.00.053.938 I llm_load_print_meta: causal attn      = 1
0.00.053.938 I llm_load_print_meta: pooling type     = 0
0.00.053.938 I llm_load_print_meta: rope type        = 2
0.00.053.939 I llm_load_print_meta: rope scaling     = linear
0.00.053.939 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.939 I llm_load_print_meta: freq_scale_train = 1
0.00.053.940 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.940 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.940 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.940 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.942 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.942 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.952 I llm_load_print_meta: model type       = 1.4B
0.00.053.953 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.953 I llm_load_print_meta: model params     = 1.41 B
0.00.053.953 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.953 I llm_load_print_meta: general.name     = 1.4B
0.00.053.954 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.954 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.954 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.954 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.955 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.955 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.955 I llm_load_print_meta: max token length = 1024
0.00.056.273 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.273 I llm_load_tensors: offloading output layer to GPU
0.00.056.273 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.285 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.286 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.295 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.296 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.296 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.296 I llama_new_context_with_model: n_batch       = 2048
0.00.057.296 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.296 I llama_new_context_with_model: flash_attn    = 0
0.00.057.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.297 I llama_new_context_with_model: freq_scale    = 1
0.00.057.298 I ggml_metal_init: allocating
0.00.057.306 I ggml_metal_init: found device: Apple M4
0.00.057.309 I ggml_metal_init: picking default device: Apple M4
0.00.058.007 I ggml_metal_init: using embedded metal library
0.00.060.595 I ggml_metal_init: GPU name:   Apple M4
0.00.060.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.597 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.598 I ggml_metal_init: simdgroup reduction   = true
0.00.060.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.598 I ggml_metal_init: has bfloat            = true
0.00.060.598 I ggml_metal_init: use bfloat            = true
0.00.060.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.505 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.515 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.537 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.687 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.689 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.689 I llama_new_context_with_model: graph nodes  = 967
0.00.095.690 I llama_new_context_with_model: graph splits = 2
0.00.095.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.566 I main: llama threadpool init, n_threads = 4
0.00.676.605 I 
0.00.676.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.635 I 
0.00.676.871 I sampler seed: 1234
0.00.676.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.912 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.914 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.359.588 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.359.588 I llama_perf_context_print:        load time =     665.97 ms
0.01.359.589 I llama_perf_context_print: prompt eval time =      43.51 ms /     7 tokens (    6.22 ms per token,   160.87 tokens per second)
0.01.359.590 I llama_perf_context_print:        eval time =     636.22 ms /    63 runs   (   10.10 ms per token,    99.02 tokens per second)
0.01.359.593 I llama_perf_context_print:       total time =     683.03 ms /    70 tokens
0.01.359.784 I ggml_metal_free: deallocating

real	0m1.378s
user	0m0.112s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.686 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.643 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.644 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.644 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.648 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.648 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.190 I llama_model_loader: - type  f32:  194 tensors
0.00.025.191 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.191 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.610 I llm_load_vocab: special tokens cache size = 25
0.00.051.545 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.548 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.548 I llm_load_print_meta: arch             = gptneox
0.00.051.549 I llm_load_print_meta: vocab type       = BPE
0.00.051.549 I llm_load_print_meta: n_vocab          = 50304
0.00.051.549 I llm_load_print_meta: n_merges         = 50009
0.00.051.549 I llm_load_print_meta: vocab_only       = 0
0.00.051.549 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.549 I llm_load_print_meta: n_embd           = 2048
0.00.051.550 I llm_load_print_meta: n_layer          = 24
0.00.051.553 I llm_load_print_meta: n_head           = 16
0.00.051.555 I llm_load_print_meta: n_head_kv        = 16
0.00.051.567 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.572 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.573 I llm_load_print_meta: n_gqa            = 1
0.00.051.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.574 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.575 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.576 I llm_load_print_meta: n_ff             = 8192
0.00.051.576 I llm_load_print_meta: n_expert         = 0
0.00.051.576 I llm_load_print_meta: n_expert_used    = 0
0.00.051.576 I llm_load_print_meta: causal attn      = 1
0.00.051.577 I llm_load_print_meta: pooling type     = 0
0.00.051.577 I llm_load_print_meta: rope type        = 2
0.00.051.577 I llm_load_print_meta: rope scaling     = linear
0.00.051.577 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.577 I llm_load_print_meta: freq_scale_train = 1
0.00.051.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.589 I llm_load_print_meta: model type       = 1.4B
0.00.051.589 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.590 I llm_load_print_meta: model params     = 1.41 B
0.00.051.590 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.590 I llm_load_print_meta: general.name     = 1.4B
0.00.051.591 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.591 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.592 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.592 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.593 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.593 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.593 I llm_load_print_meta: max token length = 1024
0.00.053.590 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.590 I llm_load_tensors: offloading output layer to GPU
0.00.053.590 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.601 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.602 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.543 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.543 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.544 I llama_new_context_with_model: n_batch       = 2048
0.00.054.544 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.544 I llama_new_context_with_model: flash_attn    = 0
0.00.054.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.545 I llama_new_context_with_model: freq_scale    = 1
0.00.054.545 I ggml_metal_init: allocating
0.00.054.548 I ggml_metal_init: found device: Apple M4
0.00.054.550 I ggml_metal_init: picking default device: Apple M4
0.00.055.109 I ggml_metal_init: using embedded metal library
0.00.057.435 I ggml_metal_init: GPU name:   Apple M4
0.00.057.437 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.438 I ggml_metal_init: simdgroup reduction   = true
0.00.057.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.438 I ggml_metal_init: has bfloat            = true
0.00.057.438 I ggml_metal_init: use bfloat            = true
0.00.057.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.871 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.879 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.896 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.903 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.904 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.905 I llama_new_context_with_model: graph nodes  = 967
0.00.087.905 I llama_new_context_with_model: graph splits = 2
0.00.087.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.785 I main: llama threadpool init, n_threads = 4
0.00.715.822 I 
0.00.715.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.863 I 
0.00.716.081 I sampler seed: 1234
0.00.716.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.098 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.100 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.449.528 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66479.40 tokens per second)
0.01.449.529 I llama_perf_context_print:        load time =     707.10 ms
0.01.449.530 I llama_perf_context_print: prompt eval time =      43.38 ms /     7 tokens (    6.20 ms per token,   161.38 tokens per second)
0.01.449.532 I llama_perf_context_print:        eval time =     687.17 ms /    63 runs   (   10.91 ms per token,    91.68 tokens per second)
0.01.449.532 I llama_perf_context_print:       total time =     733.75 ms /    70 tokens
0.01.449.735 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.111s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.939 I llama_model_loader: - type  f32:  194 tensors
0.00.025.939 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.073 I llm_load_vocab: special tokens cache size = 25
0.00.053.073 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.076 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.076 I llm_load_print_meta: arch             = gptneox
0.00.053.077 I llm_load_print_meta: vocab type       = BPE
0.00.053.077 I llm_load_print_meta: n_vocab          = 50304
0.00.053.077 I llm_load_print_meta: n_merges         = 50009
0.00.053.077 I llm_load_print_meta: vocab_only       = 0
0.00.053.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.078 I llm_load_print_meta: n_embd           = 2048
0.00.053.078 I llm_load_print_meta: n_layer          = 24
0.00.053.081 I llm_load_print_meta: n_head           = 16
0.00.053.082 I llm_load_print_meta: n_head_kv        = 16
0.00.053.093 I llm_load_print_meta: n_rot            = 32
0.00.053.093 I llm_load_print_meta: n_swa            = 0
0.00.053.093 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.093 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.094 I llm_load_print_meta: n_gqa            = 1
0.00.053.095 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.095 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.096 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.096 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.097 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.097 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.097 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.098 I llm_load_print_meta: n_ff             = 8192
0.00.053.098 I llm_load_print_meta: n_expert         = 0
0.00.053.098 I llm_load_print_meta: n_expert_used    = 0
0.00.053.099 I llm_load_print_meta: causal attn      = 1
0.00.053.100 I llm_load_print_meta: pooling type     = 0
0.00.053.100 I llm_load_print_meta: rope type        = 2
0.00.053.100 I llm_load_print_meta: rope scaling     = linear
0.00.053.100 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.101 I llm_load_print_meta: freq_scale_train = 1
0.00.053.101 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.102 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.102 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.102 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.102 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.102 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.103 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.111 I llm_load_print_meta: model type       = 1.4B
0.00.053.112 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.112 I llm_load_print_meta: model params     = 1.41 B
0.00.053.113 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.113 I llm_load_print_meta: general.name     = 1.4B
0.00.053.113 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.113 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.113 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.113 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.114 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.115 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.115 I llm_load_print_meta: max token length = 1024
0.00.054.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.715 I llm_load_tensors: offloading output layer to GPU
0.00.054.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.726 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.727 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.570 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.571 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.572 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.572 I llama_new_context_with_model: n_batch       = 2048
0.00.055.572 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.572 I llama_new_context_with_model: flash_attn    = 0
0.00.055.573 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.573 I llama_new_context_with_model: freq_scale    = 1
0.00.055.573 I ggml_metal_init: allocating
0.00.055.577 I ggml_metal_init: found device: Apple M4
0.00.055.579 I ggml_metal_init: picking default device: Apple M4
0.00.056.139 I ggml_metal_init: using embedded metal library
0.00.058.478 I ggml_metal_init: GPU name:   Apple M4
0.00.058.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.481 I ggml_metal_init: simdgroup reduction   = true
0.00.058.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.481 I ggml_metal_init: has bfloat            = true
0.00.058.481 I ggml_metal_init: use bfloat            = true
0.00.058.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.176 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.182 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.201 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.257 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.259 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.259 I llama_new_context_with_model: graph nodes  = 967
0.00.089.260 I llama_new_context_with_model: graph splits = 2
0.00.089.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.804 I main: llama threadpool init, n_threads = 4
0.00.719.840 I 
0.00.719.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.869 I 
0.00.720.102 I sampler seed: 1234
0.00.720.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.119 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.119 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.515.360 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.515.361 I llama_perf_context_print:        load time =     710.16 ms
0.01.515.362 I llama_perf_context_print: prompt eval time =      49.77 ms /     7 tokens (    7.11 ms per token,   140.64 tokens per second)
0.01.515.364 I llama_perf_context_print:        eval time =     742.46 ms /    63 runs   (   11.79 ms per token,    84.85 tokens per second)
0.01.515.364 I llama_perf_context_print:       total time =     795.56 ms /    70 tokens
0.01.515.562 I ggml_metal_free: deallocating

real	0m1.531s
user	0m0.112s
sys	0m0.165s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.631 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.482 I llama_model_loader: - type  f32:  194 tensors
0.00.025.483 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.865 I llm_load_vocab: special tokens cache size = 25
0.00.051.819 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.822 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.822 I llm_load_print_meta: arch             = gptneox
0.00.051.823 I llm_load_print_meta: vocab type       = BPE
0.00.051.823 I llm_load_print_meta: n_vocab          = 50304
0.00.051.823 I llm_load_print_meta: n_merges         = 50009
0.00.051.823 I llm_load_print_meta: vocab_only       = 0
0.00.051.823 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.824 I llm_load_print_meta: n_embd           = 2048
0.00.051.824 I llm_load_print_meta: n_layer          = 24
0.00.051.826 I llm_load_print_meta: n_head           = 16
0.00.051.827 I llm_load_print_meta: n_head_kv        = 16
0.00.051.839 I llm_load_print_meta: n_rot            = 32
0.00.051.839 I llm_load_print_meta: n_swa            = 0
0.00.051.840 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.840 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.840 I llm_load_print_meta: n_gqa            = 1
0.00.051.842 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.843 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.844 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.844 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.844 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.844 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.845 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.845 I llm_load_print_meta: n_ff             = 8192
0.00.051.845 I llm_load_print_meta: n_expert         = 0
0.00.051.845 I llm_load_print_meta: n_expert_used    = 0
0.00.051.847 I llm_load_print_meta: causal attn      = 1
0.00.051.848 I llm_load_print_meta: pooling type     = 0
0.00.051.848 I llm_load_print_meta: rope type        = 2
0.00.051.848 I llm_load_print_meta: rope scaling     = linear
0.00.051.849 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.849 I llm_load_print_meta: freq_scale_train = 1
0.00.051.849 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.850 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.850 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.850 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.851 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.851 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.851 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.860 I llm_load_print_meta: model type       = 1.4B
0.00.051.860 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.861 I llm_load_print_meta: model params     = 1.41 B
0.00.051.861 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.862 I llm_load_print_meta: general.name     = 1.4B
0.00.051.862 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.862 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.862 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.863 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.863 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.864 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.864 I llm_load_print_meta: max token length = 1024
0.00.053.880 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.880 I llm_load_tensors: offloading output layer to GPU
0.00.053.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.891 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.892 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.810 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.810 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.811 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.811 I llama_new_context_with_model: n_batch       = 2048
0.00.054.811 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.811 I llama_new_context_with_model: flash_attn    = 0
0.00.054.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.812 I llama_new_context_with_model: freq_scale    = 1
0.00.054.812 I ggml_metal_init: allocating
0.00.054.816 I ggml_metal_init: found device: Apple M4
0.00.054.818 I ggml_metal_init: picking default device: Apple M4
0.00.055.381 I ggml_metal_init: using embedded metal library
0.00.057.682 I ggml_metal_init: GPU name:   Apple M4
0.00.057.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.684 I ggml_metal_init: simdgroup reduction   = true
0.00.057.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.686 I ggml_metal_init: has bfloat            = true
0.00.057.686 I ggml_metal_init: use bfloat            = true
0.00.057.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.389 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.399 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.417 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.490 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.491 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.492 I llama_new_context_with_model: graph nodes  = 967
0.00.088.492 I llama_new_context_with_model: graph splits = 2
0.00.088.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.428 I main: llama threadpool init, n_threads = 4
0.00.757.466 I 
0.00.757.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.514 I 
0.00.757.767 I sampler seed: 1234
0.00.757.770 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.781 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.781 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.781 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.596.792 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.596.793 I llama_perf_context_print:        load time =     748.79 ms
0.01.596.794 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.37 tokens per second)
0.01.596.795 I llama_perf_context_print:        eval time =     793.72 ms /    63 runs   (   12.60 ms per token,    79.37 tokens per second)
0.01.596.795 I llama_perf_context_print:       total time =     839.37 ms /    70 tokens
0.01.596.982 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.109s
sys	0m0.179s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.868 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.803 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.806 I llama_model_loader: - type  f32:  194 tensors
0.00.024.807 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.807 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.807 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.068 I llm_load_vocab: special tokens cache size = 25
0.00.051.940 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.943 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.943 I llm_load_print_meta: arch             = gptneox
0.00.051.944 I llm_load_print_meta: vocab type       = BPE
0.00.051.944 I llm_load_print_meta: n_vocab          = 50304
0.00.051.944 I llm_load_print_meta: n_merges         = 50009
0.00.051.944 I llm_load_print_meta: vocab_only       = 0
0.00.051.944 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.944 I llm_load_print_meta: n_embd           = 2048
0.00.051.945 I llm_load_print_meta: n_layer          = 24
0.00.051.948 I llm_load_print_meta: n_head           = 16
0.00.051.949 I llm_load_print_meta: n_head_kv        = 16
0.00.051.961 I llm_load_print_meta: n_rot            = 32
0.00.051.961 I llm_load_print_meta: n_swa            = 0
0.00.051.962 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.962 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.962 I llm_load_print_meta: n_gqa            = 1
0.00.051.963 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.964 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.965 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.965 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.965 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.965 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.965 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.966 I llm_load_print_meta: n_ff             = 8192
0.00.051.966 I llm_load_print_meta: n_expert         = 0
0.00.051.966 I llm_load_print_meta: n_expert_used    = 0
0.00.051.966 I llm_load_print_meta: causal attn      = 1
0.00.051.967 I llm_load_print_meta: pooling type     = 0
0.00.051.967 I llm_load_print_meta: rope type        = 2
0.00.051.967 I llm_load_print_meta: rope scaling     = linear
0.00.051.967 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.967 I llm_load_print_meta: freq_scale_train = 1
0.00.051.967 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.968 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.968 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.968 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.968 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.968 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.968 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.978 I llm_load_print_meta: model type       = 1.4B
0.00.051.978 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.978 I llm_load_print_meta: model params     = 1.41 B
0.00.051.979 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.979 I llm_load_print_meta: general.name     = 1.4B
0.00.051.979 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.979 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.980 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.980 I llm_load_print_meta: max token length = 1024
0.00.053.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.844 I llm_load_tensors: offloading output layer to GPU
0.00.053.844 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.854 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.856 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.777 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.778 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.778 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.778 I llama_new_context_with_model: n_batch       = 2048
0.00.054.779 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.779 I llama_new_context_with_model: flash_attn    = 0
0.00.054.779 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.780 I llama_new_context_with_model: freq_scale    = 1
0.00.054.780 I ggml_metal_init: allocating
0.00.054.787 I ggml_metal_init: found device: Apple M4
0.00.054.790 I ggml_metal_init: picking default device: Apple M4
0.00.055.342 I ggml_metal_init: using embedded metal library
0.00.057.674 I ggml_metal_init: GPU name:   Apple M4
0.00.057.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.677 I ggml_metal_init: simdgroup reduction   = true
0.00.057.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.677 I ggml_metal_init: has bfloat            = true
0.00.057.677 I ggml_metal_init: use bfloat            = true
0.00.057.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.135 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.149 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.177 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.121 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.123 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.123 I llama_new_context_with_model: graph nodes  = 967
0.00.088.123 I llama_new_context_with_model: graph splits = 2
0.00.088.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.109 I main: llama threadpool init, n_threads = 4
0.00.472.162 I 
0.00.472.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.201 I 
0.00.472.454 I sampler seed: 1234
0.00.472.460 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.472.498 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.472.518 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.472.518 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.150.660 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65018.32 tokens per second)
0.01.150.661 I llama_perf_context_print:        load time =     462.24 ms
0.01.150.662 I llama_perf_context_print: prompt eval time =      35.92 ms /     7 tokens (    5.13 ms per token,   194.88 tokens per second)
0.01.150.665 I llama_perf_context_print:        eval time =     639.37 ms /    63 runs   (   10.15 ms per token,    98.53 tokens per second)
0.01.150.666 I llama_perf_context_print:       total time =     678.56 ms /    70 tokens
0.01.150.860 I ggml_metal_free: deallocating

real	0m1.168s
user	0m0.110s
sys	0m0.114s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.239 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.797 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.004 I llama_model_loader: - type  f32:  194 tensors
0.00.026.004 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.004 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.005 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.435 I llm_load_vocab: special tokens cache size = 25
0.00.052.319 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.322 I llm_load_print_meta: arch             = gptneox
0.00.052.323 I llm_load_print_meta: vocab type       = BPE
0.00.052.323 I llm_load_print_meta: n_vocab          = 50304
0.00.052.323 I llm_load_print_meta: n_merges         = 50009
0.00.052.323 I llm_load_print_meta: vocab_only       = 0
0.00.052.324 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.324 I llm_load_print_meta: n_embd           = 2048
0.00.052.324 I llm_load_print_meta: n_layer          = 24
0.00.052.327 I llm_load_print_meta: n_head           = 16
0.00.052.327 I llm_load_print_meta: n_head_kv        = 16
0.00.052.339 I llm_load_print_meta: n_rot            = 32
0.00.052.340 I llm_load_print_meta: n_swa            = 0
0.00.052.341 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.342 I llm_load_print_meta: n_gqa            = 1
0.00.052.342 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.343 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.344 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.344 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.344 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.344 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.344 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.345 I llm_load_print_meta: n_ff             = 8192
0.00.052.345 I llm_load_print_meta: n_expert         = 0
0.00.052.345 I llm_load_print_meta: n_expert_used    = 0
0.00.052.345 I llm_load_print_meta: causal attn      = 1
0.00.052.345 I llm_load_print_meta: pooling type     = 0
0.00.052.347 I llm_load_print_meta: rope type        = 2
0.00.052.347 I llm_load_print_meta: rope scaling     = linear
0.00.052.347 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.348 I llm_load_print_meta: freq_scale_train = 1
0.00.052.348 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.348 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.348 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.348 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.349 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.349 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.350 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.359 I llm_load_print_meta: model type       = 1.4B
0.00.052.359 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.360 I llm_load_print_meta: model params     = 1.41 B
0.00.052.360 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.360 I llm_load_print_meta: general.name     = 1.4B
0.00.052.360 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.361 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.361 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.361 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.362 I llm_load_print_meta: max token length = 1024
0.00.054.290 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.290 I llm_load_tensors: offloading output layer to GPU
0.00.054.290 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.301 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.302 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.234 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.235 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.235 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.235 I llama_new_context_with_model: n_batch       = 2048
0.00.055.236 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.236 I llama_new_context_with_model: flash_attn    = 0
0.00.055.236 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.236 I llama_new_context_with_model: freq_scale    = 1
0.00.055.237 I ggml_metal_init: allocating
0.00.055.243 I ggml_metal_init: found device: Apple M4
0.00.055.245 I ggml_metal_init: picking default device: Apple M4
0.00.055.813 I ggml_metal_init: using embedded metal library
0.00.058.131 I ggml_metal_init: GPU name:   Apple M4
0.00.058.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.134 I ggml_metal_init: simdgroup reduction   = true
0.00.058.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.134 I ggml_metal_init: has bfloat            = true
0.00.058.134 I ggml_metal_init: use bfloat            = true
0.00.058.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.923 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.934 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.951 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.959 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.961 I llama_new_context_with_model: graph nodes  = 967
0.00.088.961 I llama_new_context_with_model: graph splits = 2
0.00.088.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.906 I main: llama threadpool init, n_threads = 4
0.00.554.950 I 
0.00.554.989 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.990 I 
0.00.555.225 I sampler seed: 1234
0.00.555.229 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.555.270 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.555.270 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.555.270 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.301.698 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.01.301.699 I llama_perf_context_print:        load time =     543.66 ms
0.01.301.699 I llama_perf_context_print: prompt eval time =      40.53 ms /     7 tokens (    5.79 ms per token,   172.71 tokens per second)
0.01.301.700 I llama_perf_context_print:        eval time =     702.79 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.301.703 I llama_perf_context_print:       total time =     746.80 ms /    70 tokens
0.01.301.882 I ggml_metal_free: deallocating

real	0m1.317s
user	0m0.111s
sys	0m0.127s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.258 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.985 I llama_model_loader: - type  f32:  194 tensors
0.00.024.985 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.985 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.985 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.415 I llm_load_vocab: special tokens cache size = 25
0.00.051.516 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.519 I llm_load_print_meta: arch             = gptneox
0.00.051.520 I llm_load_print_meta: vocab type       = BPE
0.00.051.520 I llm_load_print_meta: n_vocab          = 50304
0.00.051.520 I llm_load_print_meta: n_merges         = 50009
0.00.051.520 I llm_load_print_meta: vocab_only       = 0
0.00.051.520 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.520 I llm_load_print_meta: n_embd           = 2048
0.00.051.520 I llm_load_print_meta: n_layer          = 24
0.00.051.523 I llm_load_print_meta: n_head           = 16
0.00.051.524 I llm_load_print_meta: n_head_kv        = 16
0.00.051.535 I llm_load_print_meta: n_rot            = 32
0.00.051.536 I llm_load_print_meta: n_swa            = 0
0.00.051.536 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.536 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.537 I llm_load_print_meta: n_gqa            = 1
0.00.051.537 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.540 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.541 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.541 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.541 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.541 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.541 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.542 I llm_load_print_meta: n_ff             = 8192
0.00.051.542 I llm_load_print_meta: n_expert         = 0
0.00.051.543 I llm_load_print_meta: n_expert_used    = 0
0.00.051.544 I llm_load_print_meta: causal attn      = 1
0.00.051.544 I llm_load_print_meta: pooling type     = 0
0.00.051.544 I llm_load_print_meta: rope type        = 2
0.00.051.544 I llm_load_print_meta: rope scaling     = linear
0.00.051.545 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.545 I llm_load_print_meta: freq_scale_train = 1
0.00.051.545 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.545 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.546 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.546 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.546 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.546 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.546 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.555 I llm_load_print_meta: model type       = 1.4B
0.00.051.555 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.556 I llm_load_print_meta: model params     = 1.41 B
0.00.051.556 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.556 I llm_load_print_meta: general.name     = 1.4B
0.00.051.556 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.557 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.557 I llm_load_print_meta: max token length = 1024
0.00.053.142 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.142 I llm_load_tensors: offloading output layer to GPU
0.00.053.142 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.152 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.153 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.038 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.039 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.039 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.039 I llama_new_context_with_model: n_batch       = 2048
0.00.054.040 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.040 I llama_new_context_with_model: flash_attn    = 0
0.00.054.040 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.041 I llama_new_context_with_model: freq_scale    = 1
0.00.054.041 I ggml_metal_init: allocating
0.00.054.047 I ggml_metal_init: found device: Apple M4
0.00.054.050 I ggml_metal_init: picking default device: Apple M4
0.00.054.591 I ggml_metal_init: using embedded metal library
0.00.056.898 I ggml_metal_init: GPU name:   Apple M4
0.00.056.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.901 I ggml_metal_init: simdgroup reduction   = true
0.00.056.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.901 I ggml_metal_init: has bfloat            = true
0.00.056.902 I ggml_metal_init: use bfloat            = true
0.00.056.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.598 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.617 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.573 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.575 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.575 I llama_new_context_with_model: graph nodes  = 967
0.00.087.575 I llama_new_context_with_model: graph splits = 2
0.00.087.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.875 I main: llama threadpool init, n_threads = 4
0.00.636.917 I 
0.00.636.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.946 I 
0.00.637.179 I sampler seed: 1234
0.00.637.183 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.235 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.237 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.237 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.401.582 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.401.583 I llama_perf_context_print:        load time =     627.61 ms
0.01.401.584 I llama_perf_context_print: prompt eval time =      51.09 ms /     7 tokens (    7.30 ms per token,   137.02 tokens per second)
0.01.401.584 I llama_perf_context_print:        eval time =     710.19 ms /    63 runs   (   11.27 ms per token,    88.71 tokens per second)
0.01.401.585 I llama_perf_context_print:       total time =     764.71 ms /    70 tokens
0.01.401.774 I ggml_metal_free: deallocating

real	0m1.419s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.578 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.137 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.138 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.138 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.139 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.139 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.140 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.269 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.412 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.415 I llama_model_loader: - type  f32:  194 tensors
0.00.023.415 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.415 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.929 I llm_load_vocab: special tokens cache size = 25
0.00.049.870 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.873 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.874 I llm_load_print_meta: arch             = gptneox
0.00.049.874 I llm_load_print_meta: vocab type       = BPE
0.00.049.874 I llm_load_print_meta: n_vocab          = 50304
0.00.049.874 I llm_load_print_meta: n_merges         = 50009
0.00.049.875 I llm_load_print_meta: vocab_only       = 0
0.00.049.875 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.875 I llm_load_print_meta: n_embd           = 2048
0.00.049.875 I llm_load_print_meta: n_layer          = 24
0.00.049.878 I llm_load_print_meta: n_head           = 16
0.00.049.880 I llm_load_print_meta: n_head_kv        = 16
0.00.049.891 I llm_load_print_meta: n_rot            = 32
0.00.049.892 I llm_load_print_meta: n_swa            = 0
0.00.049.892 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.892 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.893 I llm_load_print_meta: n_gqa            = 1
0.00.049.893 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.896 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.896 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.897 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.897 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.897 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.897 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.898 I llm_load_print_meta: n_ff             = 8192
0.00.049.899 I llm_load_print_meta: n_expert         = 0
0.00.049.899 I llm_load_print_meta: n_expert_used    = 0
0.00.049.899 I llm_load_print_meta: causal attn      = 1
0.00.049.899 I llm_load_print_meta: pooling type     = 0
0.00.049.899 I llm_load_print_meta: rope type        = 2
0.00.049.899 I llm_load_print_meta: rope scaling     = linear
0.00.049.900 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.900 I llm_load_print_meta: freq_scale_train = 1
0.00.049.900 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.900 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.901 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.902 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.902 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.902 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.911 I llm_load_print_meta: model type       = 1.4B
0.00.049.912 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.912 I llm_load_print_meta: model params     = 1.41 B
0.00.049.913 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.914 I llm_load_print_meta: general.name     = 1.4B
0.00.049.914 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.915 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: max token length = 1024
0.00.051.909 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.909 I llm_load_tensors: offloading output layer to GPU
0.00.051.909 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.919 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.921 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.832 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.833 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.833 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.833 I llama_new_context_with_model: n_batch       = 2048
0.00.052.833 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.834 I llama_new_context_with_model: flash_attn    = 0
0.00.052.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.834 I llama_new_context_with_model: freq_scale    = 1
0.00.052.835 I ggml_metal_init: allocating
0.00.052.841 I ggml_metal_init: found device: Apple M4
0.00.052.844 I ggml_metal_init: picking default device: Apple M4
0.00.053.368 I ggml_metal_init: using embedded metal library
0.00.055.667 I ggml_metal_init: GPU name:   Apple M4
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.670 I ggml_metal_init: simdgroup reduction   = true
0.00.055.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.670 I ggml_metal_init: has bfloat            = true
0.00.055.670 I ggml_metal_init: use bfloat            = true
0.00.055.671 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.257 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.265 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.285 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.300 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.301 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.302 I llama_new_context_with_model: graph nodes  = 967
0.00.086.302 I llama_new_context_with_model: graph splits = 2
0.00.086.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.156 I main: llama threadpool init, n_threads = 4
0.00.711.203 I 
0.00.711.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.257 I 
0.00.711.504 I sampler seed: 1234
0.00.711.509 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.542 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.562 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.563 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.560.813 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.560.815 I llama_perf_context_print:        load time =     702.57 ms
0.01.560.815 I llama_perf_context_print: prompt eval time =      51.59 ms /     7 tokens (    7.37 ms per token,   135.68 tokens per second)
0.01.560.816 I llama_perf_context_print:        eval time =     794.72 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.560.817 I llama_perf_context_print:       total time =     849.66 ms /    70 tokens
0.01.561.006 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.110 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.846 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.847 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.108 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.108 I llama_model_loader: - type  f32:  194 tensors
0.00.025.109 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.537 I llm_load_vocab: special tokens cache size = 25
0.00.051.401 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.403 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.404 I llm_load_print_meta: arch             = gptneox
0.00.051.404 I llm_load_print_meta: vocab type       = BPE
0.00.051.404 I llm_load_print_meta: n_vocab          = 50304
0.00.051.405 I llm_load_print_meta: n_merges         = 50009
0.00.051.405 I llm_load_print_meta: vocab_only       = 0
0.00.051.405 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.405 I llm_load_print_meta: n_embd           = 2048
0.00.051.405 I llm_load_print_meta: n_layer          = 24
0.00.051.408 I llm_load_print_meta: n_head           = 16
0.00.051.409 I llm_load_print_meta: n_head_kv        = 16
0.00.051.421 I llm_load_print_meta: n_rot            = 32
0.00.051.421 I llm_load_print_meta: n_swa            = 0
0.00.051.421 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.422 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.425 I llm_load_print_meta: n_gqa            = 1
0.00.051.425 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.426 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.426 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.427 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.427 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.427 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.427 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.428 I llm_load_print_meta: n_ff             = 8192
0.00.051.428 I llm_load_print_meta: n_expert         = 0
0.00.051.428 I llm_load_print_meta: n_expert_used    = 0
0.00.051.428 I llm_load_print_meta: causal attn      = 1
0.00.051.430 I llm_load_print_meta: pooling type     = 0
0.00.051.431 I llm_load_print_meta: rope type        = 2
0.00.051.431 I llm_load_print_meta: rope scaling     = linear
0.00.051.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.431 I llm_load_print_meta: freq_scale_train = 1
0.00.051.432 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.432 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.432 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.432 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.432 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.432 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.441 I llm_load_print_meta: model type       = 1.4B
0.00.051.442 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.442 I llm_load_print_meta: model params     = 1.41 B
0.00.051.443 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.443 I llm_load_print_meta: general.name     = 1.4B
0.00.051.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.444 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.444 I llm_load_print_meta: max token length = 1024
0.00.053.435 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.436 I llm_load_tensors: offloading output layer to GPU
0.00.053.436 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.446 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.448 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.360 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.361 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.361 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.361 I llama_new_context_with_model: n_batch       = 2048
0.00.054.361 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.361 I llama_new_context_with_model: flash_attn    = 0
0.00.054.362 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.362 I llama_new_context_with_model: freq_scale    = 1
0.00.054.362 I ggml_metal_init: allocating
0.00.054.365 I ggml_metal_init: found device: Apple M4
0.00.054.369 I ggml_metal_init: picking default device: Apple M4
0.00.054.925 I ggml_metal_init: using embedded metal library
0.00.057.237 I ggml_metal_init: GPU name:   Apple M4
0.00.057.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.239 I ggml_metal_init: simdgroup reduction   = true
0.00.057.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.241 I ggml_metal_init: has bfloat            = true
0.00.057.241 I ggml_metal_init: use bfloat            = true
0.00.057.241 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.648 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.715 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.717 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.717 I llama_new_context_with_model: graph nodes  = 967
0.00.086.718 I llama_new_context_with_model: graph splits = 2
0.00.086.731 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.818 I main: llama threadpool init, n_threads = 4
0.00.776.851 I 
0.00.776.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.882 I 
0.00.777.118 I sampler seed: 1234
0.00.777.123 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.144 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.144 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.144 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.659.609 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61365.60 tokens per second)
0.01.659.609 I llama_perf_context_print:        load time =     767.70 ms
0.01.659.610 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.659.612 I llama_perf_context_print:        eval time =     825.23 ms /    63 runs   (   13.10 ms per token,    76.34 tokens per second)
0.01.659.612 I llama_perf_context_print:       total time =     882.79 ms /    70 tokens
0.01.659.812 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.550 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.236 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.106 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.709 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.713 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.714 I llama_model_loader: - type  f32:  194 tensors
0.00.053.715 I llama_model_loader: - type  f16:   98 tensors
0.00.084.265 I llm_load_vocab: special tokens cache size = 25
0.00.090.807 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.810 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.810 I llm_load_print_meta: arch             = gptneox
0.00.090.810 I llm_load_print_meta: vocab type       = BPE
0.00.090.811 I llm_load_print_meta: n_vocab          = 50304
0.00.090.811 I llm_load_print_meta: n_merges         = 50009
0.00.090.811 I llm_load_print_meta: vocab_only       = 0
0.00.090.811 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.811 I llm_load_print_meta: n_embd           = 2048
0.00.090.811 I llm_load_print_meta: n_layer          = 24
0.00.090.815 I llm_load_print_meta: n_head           = 16
0.00.090.815 I llm_load_print_meta: n_head_kv        = 16
0.00.090.826 I llm_load_print_meta: n_rot            = 32
0.00.090.826 I llm_load_print_meta: n_swa            = 0
0.00.090.827 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.827 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.827 I llm_load_print_meta: n_gqa            = 1
0.00.090.828 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.829 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.829 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.830 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.830 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.830 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.831 I llm_load_print_meta: n_ff             = 8192
0.00.090.831 I llm_load_print_meta: n_expert         = 0
0.00.090.831 I llm_load_print_meta: n_expert_used    = 0
0.00.090.831 I llm_load_print_meta: causal attn      = 1
0.00.090.831 I llm_load_print_meta: pooling type     = 0
0.00.090.831 I llm_load_print_meta: rope type        = 2
0.00.090.831 I llm_load_print_meta: rope scaling     = linear
0.00.090.832 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.832 I llm_load_print_meta: freq_scale_train = 1
0.00.090.832 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.832 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.833 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.833 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.842 I llm_load_print_meta: model type       = 1.4B
0.00.090.843 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.843 I llm_load_print_meta: model params     = 1.41 B
0.00.090.844 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.844 I llm_load_print_meta: general.name     = 1.4B
0.00.090.847 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.847 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.847 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.847 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.848 I llm_load_print_meta: max token length = 1024
0.00.092.640 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.640 I llm_load_tensors: offloading output layer to GPU
0.00.092.641 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.650 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.652 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.536 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.536 I llama_new_context_with_model: n_ctx         = 128
0.00.093.536 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.537 I llama_new_context_with_model: n_batch       = 128
0.00.093.537 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.537 I llama_new_context_with_model: flash_attn    = 0
0.00.093.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.538 I llama_new_context_with_model: freq_scale    = 1
0.00.093.538 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.538 I ggml_metal_init: allocating
0.00.093.541 I ggml_metal_init: found device: Apple M4
0.00.093.543 I ggml_metal_init: picking default device: Apple M4
0.00.094.118 I ggml_metal_init: using embedded metal library
0.00.096.657 I ggml_metal_init: GPU name:   Apple M4
0.00.096.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.660 I ggml_metal_init: simdgroup reduction   = true
0.00.096.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.660 I ggml_metal_init: has bfloat            = true
0.00.096.660 I ggml_metal_init: use bfloat            = true
0.00.096.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.445 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.447 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.462 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.339 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.340 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.340 I llama_new_context_with_model: graph nodes  = 967
0.00.108.340 I llama_new_context_with_model: graph splits = 2
0.00.108.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.019.495 I 
0.01.019.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.019.642 I perplexity: tokenizing the input ..
0.01.032.578 I perplexity: tokenization took 12.93 ms
0.01.032.606 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.154.512 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.156.483 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.156.563 I llama_perf_context_print:        load time =     998.09 ms
0.01.156.565 I llama_perf_context_print: prompt eval time =     120.97 ms /   128 tokens (    0.95 ms per token,  1058.14 tokens per second)
0.01.156.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.156.567 I llama_perf_context_print:       total time =     137.08 ms /   129 tokens
0.01.157.361 I ggml_metal_free: deallocating

real	0m1.345s
user	0m0.130s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.144 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.645 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.956 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.414 I llama_model_loader: - type  f32:  194 tensors
0.00.036.414 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.058 I llm_load_vocab: special tokens cache size = 25
0.00.069.134 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.137 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.137 I llm_load_print_meta: arch             = gptneox
0.00.069.137 I llm_load_print_meta: vocab type       = BPE
0.00.069.137 I llm_load_print_meta: n_vocab          = 50304
0.00.069.138 I llm_load_print_meta: n_merges         = 50009
0.00.069.138 I llm_load_print_meta: vocab_only       = 0
0.00.069.138 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.138 I llm_load_print_meta: n_embd           = 2048
0.00.069.138 I llm_load_print_meta: n_layer          = 24
0.00.069.141 I llm_load_print_meta: n_head           = 16
0.00.069.142 I llm_load_print_meta: n_head_kv        = 16
0.00.069.153 I llm_load_print_meta: n_rot            = 32
0.00.069.154 I llm_load_print_meta: n_swa            = 0
0.00.069.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.154 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.155 I llm_load_print_meta: n_gqa            = 1
0.00.069.155 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.156 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.157 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.157 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.157 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.157 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.158 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.158 I llm_load_print_meta: n_ff             = 8192
0.00.069.158 I llm_load_print_meta: n_expert         = 0
0.00.069.159 I llm_load_print_meta: n_expert_used    = 0
0.00.069.160 I llm_load_print_meta: causal attn      = 1
0.00.069.160 I llm_load_print_meta: pooling type     = 0
0.00.069.161 I llm_load_print_meta: rope type        = 2
0.00.069.161 I llm_load_print_meta: rope scaling     = linear
0.00.069.162 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.162 I llm_load_print_meta: freq_scale_train = 1
0.00.069.162 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.163 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.163 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.163 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.163 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.163 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.163 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.172 I llm_load_print_meta: model type       = 1.4B
0.00.069.172 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.172 I llm_load_print_meta: model params     = 1.41 B
0.00.069.173 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.173 I llm_load_print_meta: general.name     = 1.4B
0.00.069.173 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.174 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.174 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.175 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.175 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.175 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.175 I llm_load_print_meta: max token length = 1024
0.00.070.813 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.813 I llm_load_tensors: offloading output layer to GPU
0.00.070.814 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.823 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.825 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.696 I llama_new_context_with_model: n_ctx         = 128
0.00.071.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.697 I llama_new_context_with_model: n_batch       = 128
0.00.071.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.697 I llama_new_context_with_model: flash_attn    = 0
0.00.071.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.698 I llama_new_context_with_model: freq_scale    = 1
0.00.071.698 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.698 I ggml_metal_init: allocating
0.00.071.704 I ggml_metal_init: found device: Apple M4
0.00.071.707 I ggml_metal_init: picking default device: Apple M4
0.00.072.282 I ggml_metal_init: using embedded metal library
0.00.074.791 I ggml_metal_init: GPU name:   Apple M4
0.00.074.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.794 I ggml_metal_init: simdgroup reduction   = true
0.00.074.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.794 I ggml_metal_init: has bfloat            = true
0.00.074.794 I ggml_metal_init: use bfloat            = true
0.00.074.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.102 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.013 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.014 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.014 I llama_new_context_with_model: graph nodes  = 967
0.00.086.015 I llama_new_context_with_model: graph splits = 2
0.00.086.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.852.532 I 
0.00.852.586 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.852.607 I perplexity: tokenizing the input ..
0.00.860.738 I perplexity: tokenization took 8.13 ms
0.00.860.753 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.985.307 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.986.658 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.986.674 I llama_perf_context_print:        load time =     839.88 ms
0.00.986.675 I llama_perf_context_print: prompt eval time =     124.33 ms /   128 tokens (    0.97 ms per token,  1029.52 tokens per second)
0.00.986.676 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.986.676 I llama_perf_context_print:       total time =     134.15 ms /   129 tokens
0.00.986.977 I ggml_metal_free: deallocating

real	0m1.007s
user	0m0.098s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.544 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.759 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.760 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.874 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.875 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.876 I llama_model_loader: - type  f32:  194 tensors
0.00.024.876 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.876 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.023 I llm_load_vocab: special tokens cache size = 25
0.00.051.816 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.819 I llm_load_print_meta: arch             = gptneox
0.00.051.820 I llm_load_print_meta: vocab type       = BPE
0.00.051.820 I llm_load_print_meta: n_vocab          = 50304
0.00.051.820 I llm_load_print_meta: n_merges         = 50009
0.00.051.820 I llm_load_print_meta: vocab_only       = 0
0.00.051.820 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.820 I llm_load_print_meta: n_embd           = 2048
0.00.051.821 I llm_load_print_meta: n_layer          = 24
0.00.051.823 I llm_load_print_meta: n_head           = 16
0.00.051.824 I llm_load_print_meta: n_head_kv        = 16
0.00.051.836 I llm_load_print_meta: n_rot            = 32
0.00.051.836 I llm_load_print_meta: n_swa            = 0
0.00.051.836 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.836 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.837 I llm_load_print_meta: n_gqa            = 1
0.00.051.838 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.839 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.839 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.842 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.842 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.843 I llm_load_print_meta: n_ff             = 8192
0.00.051.843 I llm_load_print_meta: n_expert         = 0
0.00.051.843 I llm_load_print_meta: n_expert_used    = 0
0.00.051.843 I llm_load_print_meta: causal attn      = 1
0.00.051.845 I llm_load_print_meta: pooling type     = 0
0.00.051.845 I llm_load_print_meta: rope type        = 2
0.00.051.845 I llm_load_print_meta: rope scaling     = linear
0.00.051.845 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.846 I llm_load_print_meta: freq_scale_train = 1
0.00.051.847 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.847 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.847 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.847 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.847 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.847 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.847 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.856 I llm_load_print_meta: model type       = 1.4B
0.00.051.857 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.857 I llm_load_print_meta: model params     = 1.41 B
0.00.051.858 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.858 I llm_load_print_meta: general.name     = 1.4B
0.00.051.858 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.858 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.858 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.859 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.859 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.859 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.859 I llm_load_print_meta: max token length = 1024
0.00.053.427 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.428 I llm_load_tensors: offloading output layer to GPU
0.00.053.428 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.438 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.439 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.315 I llama_new_context_with_model: n_ctx         = 128
0.00.054.315 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.315 I llama_new_context_with_model: n_batch       = 128
0.00.054.315 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.315 I llama_new_context_with_model: flash_attn    = 0
0.00.054.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.316 I llama_new_context_with_model: freq_scale    = 1
0.00.054.316 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.317 I ggml_metal_init: allocating
0.00.054.320 I ggml_metal_init: found device: Apple M4
0.00.054.322 I ggml_metal_init: picking default device: Apple M4
0.00.054.873 I ggml_metal_init: using embedded metal library
0.00.057.247 I ggml_metal_init: GPU name:   Apple M4
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.250 I ggml_metal_init: simdgroup reduction   = true
0.00.057.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.250 I ggml_metal_init: has bfloat            = true
0.00.057.250 I ggml_metal_init: use bfloat            = true
0.00.057.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.585 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.587 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.601 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.489 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.490 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.491 I llama_new_context_with_model: graph nodes  = 967
0.00.069.491 I llama_new_context_with_model: graph splits = 2
0.00.069.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.368 I 
0.00.630.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.413 I perplexity: tokenizing the input ..
0.00.638.758 I perplexity: tokenization took 8.344 ms
0.00.638.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.137 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.762.451 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.762.466 I llama_perf_context_print:        load time =     620.82 ms
0.00.762.467 I llama_perf_context_print: prompt eval time =     122.14 ms /   128 tokens (    0.95 ms per token,  1047.99 tokens per second)
0.00.762.470 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.471 I llama_perf_context_print:       total time =     132.10 ms /   129 tokens
0.00.762.876 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.082s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.704 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.029 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.030 I llama_model_loader: - type  f32:  194 tensors
0.00.024.030 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.333 I llm_load_vocab: special tokens cache size = 25
0.00.050.247 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.249 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.250 I llm_load_print_meta: arch             = gptneox
0.00.050.250 I llm_load_print_meta: vocab type       = BPE
0.00.050.250 I llm_load_print_meta: n_vocab          = 50304
0.00.050.250 I llm_load_print_meta: n_merges         = 50009
0.00.050.251 I llm_load_print_meta: vocab_only       = 0
0.00.050.251 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.251 I llm_load_print_meta: n_embd           = 2048
0.00.050.251 I llm_load_print_meta: n_layer          = 24
0.00.050.254 I llm_load_print_meta: n_head           = 16
0.00.050.255 I llm_load_print_meta: n_head_kv        = 16
0.00.050.267 I llm_load_print_meta: n_rot            = 32
0.00.050.267 I llm_load_print_meta: n_swa            = 0
0.00.050.267 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.268 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.269 I llm_load_print_meta: n_gqa            = 1
0.00.050.269 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.270 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.271 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.271 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.272 I llm_load_print_meta: n_ff             = 8192
0.00.050.272 I llm_load_print_meta: n_expert         = 0
0.00.050.272 I llm_load_print_meta: n_expert_used    = 0
0.00.050.272 I llm_load_print_meta: causal attn      = 1
0.00.050.273 I llm_load_print_meta: pooling type     = 0
0.00.050.273 I llm_load_print_meta: rope type        = 2
0.00.050.273 I llm_load_print_meta: rope scaling     = linear
0.00.050.273 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.274 I llm_load_print_meta: freq_scale_train = 1
0.00.050.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.274 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.274 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.274 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.274 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.274 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.275 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.284 I llm_load_print_meta: model type       = 1.4B
0.00.050.284 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.284 I llm_load_print_meta: model params     = 1.41 B
0.00.050.285 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.285 I llm_load_print_meta: general.name     = 1.4B
0.00.050.285 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.285 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.286 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.286 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.288 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.288 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.288 I llm_load_print_meta: max token length = 1024
0.00.052.176 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.176 I llm_load_tensors: offloading output layer to GPU
0.00.052.176 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.187 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.188 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.065 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.065 I llama_new_context_with_model: n_ctx         = 128
0.00.053.066 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.066 I llama_new_context_with_model: n_batch       = 128
0.00.053.066 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.066 I llama_new_context_with_model: flash_attn    = 0
0.00.053.067 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.067 I llama_new_context_with_model: freq_scale    = 1
0.00.053.067 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.068 I ggml_metal_init: allocating
0.00.053.073 I ggml_metal_init: found device: Apple M4
0.00.053.076 I ggml_metal_init: picking default device: Apple M4
0.00.053.614 I ggml_metal_init: using embedded metal library
0.00.055.938 I ggml_metal_init: GPU name:   Apple M4
0.00.055.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.940 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.940 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.940 I ggml_metal_init: simdgroup reduction   = true
0.00.055.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.941 I ggml_metal_init: has bfloat            = true
0.00.055.941 I ggml_metal_init: use bfloat            = true
0.00.055.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.649 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.654 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.557 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.558 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.559 I llama_new_context_with_model: graph nodes  = 967
0.00.067.559 I llama_new_context_with_model: graph splits = 2
0.00.067.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.415 I 
0.00.652.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.483 I perplexity: tokenizing the input ..
0.00.660.563 I perplexity: tokenization took 8.078 ms
0.00.660.574 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.521 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.784.923 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.784.942 I llama_perf_context_print:        load time =     643.70 ms
0.00.784.943 I llama_perf_context_print: prompt eval time =     122.71 ms /   128 tokens (    0.96 ms per token,  1043.11 tokens per second)
0.00.784.944 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.945 I llama_perf_context_print:       total time =     132.53 ms /   129 tokens
0.00.785.394 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.946 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.983 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.983 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.984 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.984 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.337 I llama_model_loader: - type  f32:  194 tensors
0.00.025.337 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.072 I llm_load_vocab: special tokens cache size = 25
0.00.052.027 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.030 I llm_load_print_meta: arch             = gptneox
0.00.052.030 I llm_load_print_meta: vocab type       = BPE
0.00.052.030 I llm_load_print_meta: n_vocab          = 50304
0.00.052.030 I llm_load_print_meta: n_merges         = 50009
0.00.052.031 I llm_load_print_meta: vocab_only       = 0
0.00.052.031 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.031 I llm_load_print_meta: n_embd           = 2048
0.00.052.031 I llm_load_print_meta: n_layer          = 24
0.00.052.034 I llm_load_print_meta: n_head           = 16
0.00.052.037 I llm_load_print_meta: n_head_kv        = 16
0.00.052.049 I llm_load_print_meta: n_rot            = 32
0.00.052.049 I llm_load_print_meta: n_swa            = 0
0.00.052.049 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.049 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.050 I llm_load_print_meta: n_gqa            = 1
0.00.052.051 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.052 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.052 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.052 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.053 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.053 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.053 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.054 I llm_load_print_meta: n_ff             = 8192
0.00.052.054 I llm_load_print_meta: n_expert         = 0
0.00.052.054 I llm_load_print_meta: n_expert_used    = 0
0.00.052.054 I llm_load_print_meta: causal attn      = 1
0.00.052.054 I llm_load_print_meta: pooling type     = 0
0.00.052.054 I llm_load_print_meta: rope type        = 2
0.00.052.054 I llm_load_print_meta: rope scaling     = linear
0.00.052.055 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.055 I llm_load_print_meta: freq_scale_train = 1
0.00.052.055 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.055 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.055 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.056 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.056 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.065 I llm_load_print_meta: model type       = 1.4B
0.00.052.065 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.066 I llm_load_print_meta: model params     = 1.41 B
0.00.052.067 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.067 I llm_load_print_meta: general.name     = 1.4B
0.00.052.067 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.068 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.068 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.068 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.068 I llm_load_print_meta: max token length = 1024
0.00.054.052 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.053 I llm_load_tensors: offloading output layer to GPU
0.00.054.053 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.063 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.064 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.009 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.009 I llama_new_context_with_model: n_ctx         = 128
0.00.055.010 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.010 I llama_new_context_with_model: n_batch       = 128
0.00.055.010 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.010 I llama_new_context_with_model: flash_attn    = 0
0.00.055.011 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.011 I llama_new_context_with_model: freq_scale    = 1
0.00.055.011 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.012 I ggml_metal_init: allocating
0.00.055.015 I ggml_metal_init: found device: Apple M4
0.00.055.017 I ggml_metal_init: picking default device: Apple M4
0.00.055.561 I ggml_metal_init: using embedded metal library
0.00.057.861 I ggml_metal_init: GPU name:   Apple M4
0.00.057.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.864 I ggml_metal_init: simdgroup reduction   = true
0.00.057.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.864 I ggml_metal_init: has bfloat            = true
0.00.057.864 I ggml_metal_init: use bfloat            = true
0.00.057.864 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.764 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.779 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.686 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.687 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.688 I llama_new_context_with_model: graph nodes  = 967
0.00.069.688 I llama_new_context_with_model: graph splits = 2
0.00.069.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.272 I 
0.00.647.311 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.322 I perplexity: tokenizing the input ..
0.00.655.225 I perplexity: tokenization took 7.901 ms
0.00.655.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.763 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.792.184 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.792.202 I llama_perf_context_print:        load time =     637.32 ms
0.00.792.205 I llama_perf_context_print: prompt eval time =     135.30 ms /   128 tokens (    1.06 ms per token,   946.02 tokens per second)
0.00.792.206 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.206 I llama_perf_context_print:       total time =     144.93 ms /   129 tokens
0.00.792.670 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.843 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.948 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.366 I llama_model_loader: - type  f32:  194 tensors
0.00.024.366 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.593 I llm_load_vocab: special tokens cache size = 25
0.00.051.515 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.518 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.519 I llm_load_print_meta: arch             = gptneox
0.00.051.519 I llm_load_print_meta: vocab type       = BPE
0.00.051.519 I llm_load_print_meta: n_vocab          = 50304
0.00.051.519 I llm_load_print_meta: n_merges         = 50009
0.00.051.519 I llm_load_print_meta: vocab_only       = 0
0.00.051.520 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.520 I llm_load_print_meta: n_embd           = 2048
0.00.051.520 I llm_load_print_meta: n_layer          = 24
0.00.051.522 I llm_load_print_meta: n_head           = 16
0.00.051.523 I llm_load_print_meta: n_head_kv        = 16
0.00.051.535 I llm_load_print_meta: n_rot            = 32
0.00.051.536 I llm_load_print_meta: n_swa            = 0
0.00.051.536 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.536 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.537 I llm_load_print_meta: n_gqa            = 1
0.00.051.539 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.539 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.540 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.540 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.540 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.541 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.541 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.541 I llm_load_print_meta: n_ff             = 8192
0.00.051.542 I llm_load_print_meta: n_expert         = 0
0.00.051.542 I llm_load_print_meta: n_expert_used    = 0
0.00.051.542 I llm_load_print_meta: causal attn      = 1
0.00.051.542 I llm_load_print_meta: pooling type     = 0
0.00.051.543 I llm_load_print_meta: rope type        = 2
0.00.051.544 I llm_load_print_meta: rope scaling     = linear
0.00.051.544 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.544 I llm_load_print_meta: freq_scale_train = 1
0.00.051.544 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.545 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.545 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.545 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.545 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.545 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.545 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.555 I llm_load_print_meta: model type       = 1.4B
0.00.051.556 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.556 I llm_load_print_meta: model params     = 1.41 B
0.00.051.557 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.557 I llm_load_print_meta: general.name     = 1.4B
0.00.051.557 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.559 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.560 I llm_load_print_meta: max token length = 1024
0.00.053.621 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.621 I llm_load_tensors: offloading output layer to GPU
0.00.053.622 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.632 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.633 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.581 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.581 I llama_new_context_with_model: n_ctx         = 128
0.00.054.582 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.582 I llama_new_context_with_model: n_batch       = 128
0.00.054.582 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.582 I llama_new_context_with_model: flash_attn    = 0
0.00.054.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.583 I llama_new_context_with_model: freq_scale    = 1
0.00.054.583 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.584 I ggml_metal_init: allocating
0.00.054.587 I ggml_metal_init: found device: Apple M4
0.00.054.589 I ggml_metal_init: picking default device: Apple M4
0.00.055.136 I ggml_metal_init: using embedded metal library
0.00.057.478 I ggml_metal_init: GPU name:   Apple M4
0.00.057.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.480 I ggml_metal_init: simdgroup reduction   = true
0.00.057.480 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.481 I ggml_metal_init: has bfloat            = true
0.00.057.481 I ggml_metal_init: use bfloat            = true
0.00.057.481 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.482 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.551 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.553 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.453 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.454 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.454 I llama_new_context_with_model: graph nodes  = 967
0.00.069.454 I llama_new_context_with_model: graph splits = 2
0.00.069.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.448 I 
0.00.690.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.488 I perplexity: tokenizing the input ..
0.00.698.007 I perplexity: tokenization took 7.517 ms
0.00.698.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.156 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.834.692 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.834.705 I llama_perf_context_print:        load time =     681.60 ms
0.00.834.707 I llama_perf_context_print: prompt eval time =     134.91 ms /   128 tokens (    1.05 ms per token,   948.75 tokens per second)
0.00.834.710 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.710 I llama_perf_context_print:       total time =     144.26 ms /   129 tokens
0.00.835.117 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.081s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.465 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.290 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.290 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.293 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.294 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.760 I llama_model_loader: - type  f32:  194 tensors
0.00.025.760 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.760 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.970 I llm_load_vocab: special tokens cache size = 25
0.00.052.918 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.922 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.922 I llm_load_print_meta: arch             = gptneox
0.00.052.923 I llm_load_print_meta: vocab type       = BPE
0.00.052.923 I llm_load_print_meta: n_vocab          = 50304
0.00.052.923 I llm_load_print_meta: n_merges         = 50009
0.00.052.923 I llm_load_print_meta: vocab_only       = 0
0.00.052.924 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.924 I llm_load_print_meta: n_embd           = 2048
0.00.052.924 I llm_load_print_meta: n_layer          = 24
0.00.052.927 I llm_load_print_meta: n_head           = 16
0.00.052.928 I llm_load_print_meta: n_head_kv        = 16
0.00.052.940 I llm_load_print_meta: n_rot            = 32
0.00.052.941 I llm_load_print_meta: n_swa            = 0
0.00.052.941 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.941 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.942 I llm_load_print_meta: n_gqa            = 1
0.00.052.943 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.943 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.944 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.944 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.944 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.944 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.945 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.945 I llm_load_print_meta: n_ff             = 8192
0.00.052.945 I llm_load_print_meta: n_expert         = 0
0.00.052.945 I llm_load_print_meta: n_expert_used    = 0
0.00.052.945 I llm_load_print_meta: causal attn      = 1
0.00.052.946 I llm_load_print_meta: pooling type     = 0
0.00.052.946 I llm_load_print_meta: rope type        = 2
0.00.052.948 I llm_load_print_meta: rope scaling     = linear
0.00.052.949 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.950 I llm_load_print_meta: freq_scale_train = 1
0.00.052.950 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.950 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.950 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.950 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.951 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.951 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.960 I llm_load_print_meta: model type       = 1.4B
0.00.052.960 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.961 I llm_load_print_meta: model params     = 1.41 B
0.00.052.961 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.961 I llm_load_print_meta: general.name     = 1.4B
0.00.052.962 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.962 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.963 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.963 I llm_load_print_meta: max token length = 1024
0.00.054.937 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.937 I llm_load_tensors: offloading output layer to GPU
0.00.054.938 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.948 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.949 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.986 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.987 I llama_new_context_with_model: n_ctx         = 128
0.00.055.987 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.987 I llama_new_context_with_model: n_batch       = 128
0.00.055.987 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.988 I llama_new_context_with_model: flash_attn    = 0
0.00.055.988 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.988 I llama_new_context_with_model: freq_scale    = 1
0.00.055.989 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.989 I ggml_metal_init: allocating
0.00.055.992 I ggml_metal_init: found device: Apple M4
0.00.055.994 I ggml_metal_init: picking default device: Apple M4
0.00.056.565 I ggml_metal_init: using embedded metal library
0.00.058.886 I ggml_metal_init: GPU name:   Apple M4
0.00.058.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.888 I ggml_metal_init: simdgroup reduction   = true
0.00.058.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.888 I ggml_metal_init: has bfloat            = true
0.00.058.889 I ggml_metal_init: use bfloat            = true
0.00.058.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.065 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.067 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.027 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.028 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.028 I llama_new_context_with_model: graph nodes  = 967
0.00.071.028 I llama_new_context_with_model: graph splits = 2
0.00.071.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.775 I 
0.00.414.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.873 I perplexity: tokenizing the input ..
0.00.422.519 I perplexity: tokenization took 7.645 ms
0.00.422.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.554.478 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.555.879 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.555.895 I llama_perf_context_print:        load time =     404.30 ms
0.00.555.896 I llama_perf_context_print: prompt eval time =     131.72 ms /   128 tokens (    1.03 ms per token,   971.74 tokens per second)
0.00.555.897 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.555.897 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.556.299 I ggml_metal_free: deallocating

real	0m0.570s
user	0m0.081s
sys	0m0.072s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.381 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.135 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.135 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.136 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.137 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.138 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.138 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.200 I llama_model_loader: - type  f32:  194 tensors
0.00.023.200 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.201 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.201 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.201 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.311 I llm_load_vocab: special tokens cache size = 25
0.00.049.157 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.160 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.160 I llm_load_print_meta: arch             = gptneox
0.00.049.161 I llm_load_print_meta: vocab type       = BPE
0.00.049.161 I llm_load_print_meta: n_vocab          = 50304
0.00.049.161 I llm_load_print_meta: n_merges         = 50009
0.00.049.161 I llm_load_print_meta: vocab_only       = 0
0.00.049.161 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.161 I llm_load_print_meta: n_embd           = 2048
0.00.049.162 I llm_load_print_meta: n_layer          = 24
0.00.049.164 I llm_load_print_meta: n_head           = 16
0.00.049.165 I llm_load_print_meta: n_head_kv        = 16
0.00.049.176 I llm_load_print_meta: n_rot            = 32
0.00.049.176 I llm_load_print_meta: n_swa            = 0
0.00.049.177 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.178 I llm_load_print_meta: n_gqa            = 1
0.00.049.178 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.179 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.180 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.180 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.180 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.180 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.180 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.181 I llm_load_print_meta: n_ff             = 8192
0.00.049.181 I llm_load_print_meta: n_expert         = 0
0.00.049.181 I llm_load_print_meta: n_expert_used    = 0
0.00.049.181 I llm_load_print_meta: causal attn      = 1
0.00.049.182 I llm_load_print_meta: pooling type     = 0
0.00.049.182 I llm_load_print_meta: rope type        = 2
0.00.049.182 I llm_load_print_meta: rope scaling     = linear
0.00.049.183 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.183 I llm_load_print_meta: freq_scale_train = 1
0.00.049.183 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.184 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.184 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.184 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.184 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.184 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.184 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.193 I llm_load_print_meta: model type       = 1.4B
0.00.049.193 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.193 I llm_load_print_meta: model params     = 1.41 B
0.00.049.194 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.195 I llm_load_print_meta: general.name     = 1.4B
0.00.049.196 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.196 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.196 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.196 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.197 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.197 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.197 I llm_load_print_meta: max token length = 1024
0.00.050.781 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.781 I llm_load_tensors: offloading output layer to GPU
0.00.050.781 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.791 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.792 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.634 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.634 I llama_new_context_with_model: n_ctx         = 128
0.00.051.635 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.635 I llama_new_context_with_model: n_batch       = 128
0.00.051.635 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.635 I llama_new_context_with_model: flash_attn    = 0
0.00.051.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.636 I llama_new_context_with_model: freq_scale    = 1
0.00.051.636 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.637 I ggml_metal_init: allocating
0.00.051.643 I ggml_metal_init: found device: Apple M4
0.00.051.645 I ggml_metal_init: picking default device: Apple M4
0.00.052.174 I ggml_metal_init: using embedded metal library
0.00.054.549 I ggml_metal_init: GPU name:   Apple M4
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.552 I ggml_metal_init: simdgroup reduction   = true
0.00.054.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.552 I ggml_metal_init: has bfloat            = true
0.00.054.552 I ggml_metal_init: use bfloat            = true
0.00.054.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.410 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.412 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.428 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.395 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.396 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.396 I llama_new_context_with_model: graph nodes  = 967
0.00.066.397 I llama_new_context_with_model: graph splits = 2
0.00.066.409 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.202 I 
0.00.492.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.251 I perplexity: tokenizing the input ..
0.00.500.309 I perplexity: tokenization took 8.055 ms
0.00.500.322 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.632.702 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.634.046 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.634.067 I llama_perf_context_print:        load time =     483.82 ms
0.00.634.068 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.66 tokens per second)
0.00.634.069 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.634.070 I llama_perf_context_print:       total time =     141.87 ms /   129 tokens
0.00.634.517 I ggml_metal_free: deallocating

real	0m0.647s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.853 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.854 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.855 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.855 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.855 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.856 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.856 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.856 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.858 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.667 I llama_model_loader: - type  f32:  194 tensors
0.00.023.668 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.668 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.668 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.057 I llm_load_vocab: special tokens cache size = 25
0.00.049.898 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.904 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.904 I llm_load_print_meta: arch             = gptneox
0.00.049.905 I llm_load_print_meta: vocab type       = BPE
0.00.049.905 I llm_load_print_meta: n_vocab          = 50304
0.00.049.905 I llm_load_print_meta: n_merges         = 50009
0.00.049.905 I llm_load_print_meta: vocab_only       = 0
0.00.049.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.906 I llm_load_print_meta: n_embd           = 2048
0.00.049.906 I llm_load_print_meta: n_layer          = 24
0.00.049.909 I llm_load_print_meta: n_head           = 16
0.00.049.910 I llm_load_print_meta: n_head_kv        = 16
0.00.049.923 I llm_load_print_meta: n_rot            = 32
0.00.049.923 I llm_load_print_meta: n_swa            = 0
0.00.049.924 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.924 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.924 I llm_load_print_meta: n_gqa            = 1
0.00.049.925 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.926 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.926 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.926 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.927 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.927 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.927 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.927 I llm_load_print_meta: n_ff             = 8192
0.00.049.928 I llm_load_print_meta: n_expert         = 0
0.00.049.928 I llm_load_print_meta: n_expert_used    = 0
0.00.049.928 I llm_load_print_meta: causal attn      = 1
0.00.049.928 I llm_load_print_meta: pooling type     = 0
0.00.049.928 I llm_load_print_meta: rope type        = 2
0.00.049.928 I llm_load_print_meta: rope scaling     = linear
0.00.049.929 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.932 I llm_load_print_meta: freq_scale_train = 1
0.00.049.933 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.933 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.933 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.933 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.933 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.933 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.933 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.943 I llm_load_print_meta: model type       = 1.4B
0.00.049.943 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.943 I llm_load_print_meta: model params     = 1.41 B
0.00.049.944 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.945 I llm_load_print_meta: general.name     = 1.4B
0.00.049.945 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.945 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.946 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.946 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.946 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.946 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.946 I llm_load_print_meta: max token length = 1024
0.00.051.911 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.911 I llm_load_tensors: offloading output layer to GPU
0.00.051.911 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.922 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.923 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.844 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.845 I llama_new_context_with_model: n_ctx         = 128
0.00.052.845 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.845 I llama_new_context_with_model: n_batch       = 128
0.00.052.846 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.846 I llama_new_context_with_model: flash_attn    = 0
0.00.052.846 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.846 I llama_new_context_with_model: freq_scale    = 1
0.00.052.847 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.847 I ggml_metal_init: allocating
0.00.052.850 I ggml_metal_init: found device: Apple M4
0.00.052.852 I ggml_metal_init: picking default device: Apple M4
0.00.053.397 I ggml_metal_init: using embedded metal library
0.00.055.679 I ggml_metal_init: GPU name:   Apple M4
0.00.055.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.681 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.681 I ggml_metal_init: simdgroup reduction   = true
0.00.055.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.682 I ggml_metal_init: has bfloat            = true
0.00.055.682 I ggml_metal_init: use bfloat            = true
0.00.055.682 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.676 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.678 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.589 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.590 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.590 I llama_new_context_with_model: graph nodes  = 967
0.00.067.590 I llama_new_context_with_model: graph splits = 2
0.00.067.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.331 I 
0.00.588.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.409 I perplexity: tokenizing the input ..
0.00.596.185 I perplexity: tokenization took 7.774 ms
0.00.596.196 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.788 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.732.132 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.732.145 I llama_perf_context_print:        load time =     579.50 ms
0.00.732.146 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.72 tokens per second)
0.00.732.147 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.148 I llama_perf_context_print:       total time =     143.82 ms /   129 tokens
0.00.732.562 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.079s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.234 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.976 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.976 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.977 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.954 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.081 I llama_model_loader: - type  f32:  194 tensors
0.00.024.081 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.082 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.398 I llm_load_vocab: special tokens cache size = 25
0.00.050.356 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.359 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.359 I llm_load_print_meta: arch             = gptneox
0.00.050.359 I llm_load_print_meta: vocab type       = BPE
0.00.050.360 I llm_load_print_meta: n_vocab          = 50304
0.00.050.360 I llm_load_print_meta: n_merges         = 50009
0.00.050.360 I llm_load_print_meta: vocab_only       = 0
0.00.050.360 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.360 I llm_load_print_meta: n_embd           = 2048
0.00.050.360 I llm_load_print_meta: n_layer          = 24
0.00.050.363 I llm_load_print_meta: n_head           = 16
0.00.050.364 I llm_load_print_meta: n_head_kv        = 16
0.00.050.376 I llm_load_print_meta: n_rot            = 32
0.00.050.376 I llm_load_print_meta: n_swa            = 0
0.00.050.376 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.376 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.377 I llm_load_print_meta: n_gqa            = 1
0.00.050.378 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.382 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.382 I llm_load_print_meta: n_ff             = 8192
0.00.050.382 I llm_load_print_meta: n_expert         = 0
0.00.050.383 I llm_load_print_meta: n_expert_used    = 0
0.00.050.383 I llm_load_print_meta: causal attn      = 1
0.00.050.383 I llm_load_print_meta: pooling type     = 0
0.00.050.383 I llm_load_print_meta: rope type        = 2
0.00.050.383 I llm_load_print_meta: rope scaling     = linear
0.00.050.383 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.384 I llm_load_print_meta: freq_scale_train = 1
0.00.050.384 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.384 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.384 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.385 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.385 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.385 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.385 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.395 I llm_load_print_meta: model type       = 1.4B
0.00.050.395 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.395 I llm_load_print_meta: model params     = 1.41 B
0.00.050.396 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.396 I llm_load_print_meta: general.name     = 1.4B
0.00.050.396 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.396 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.398 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.398 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.398 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.398 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.398 I llm_load_print_meta: max token length = 1024
0.00.052.350 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.351 I llm_load_tensors: offloading output layer to GPU
0.00.052.351 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.361 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.362 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.276 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.277 I llama_new_context_with_model: n_ctx         = 128
0.00.053.277 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.277 I llama_new_context_with_model: n_batch       = 128
0.00.053.277 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.278 I llama_new_context_with_model: flash_attn    = 0
0.00.053.278 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.278 I llama_new_context_with_model: freq_scale    = 1
0.00.053.279 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.279 I ggml_metal_init: allocating
0.00.053.284 I ggml_metal_init: found device: Apple M4
0.00.053.286 I ggml_metal_init: picking default device: Apple M4
0.00.053.811 I ggml_metal_init: using embedded metal library
0.00.056.100 I ggml_metal_init: GPU name:   Apple M4
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.102 I ggml_metal_init: simdgroup reduction   = true
0.00.056.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.102 I ggml_metal_init: has bfloat            = true
0.00.056.103 I ggml_metal_init: use bfloat            = true
0.00.056.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.745 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.750 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.765 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.645 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.646 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.646 I llama_new_context_with_model: graph nodes  = 967
0.00.067.647 I llama_new_context_with_model: graph splits = 2
0.00.067.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.783 I 
0.00.648.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.852 I perplexity: tokenizing the input ..
0.00.656.667 I perplexity: tokenization took 7.811 ms
0.00.656.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.866 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.253 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.276 I llama_perf_context_print:        load time =     639.54 ms
0.00.799.277 I llama_perf_context_print: prompt eval time =     140.96 ms /   128 tokens (    1.10 ms per token,   908.09 tokens per second)
0.00.799.278 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.278 I llama_perf_context_print:       total time =     150.49 ms /   129 tokens
0.00.799.728 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.795 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.977 I llama_model_loader: - type  f32:  194 tensors
0.00.023.977 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.298 I llm_load_vocab: special tokens cache size = 25
0.00.050.006 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.009 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.009 I llm_load_print_meta: arch             = gptneox
0.00.050.010 I llm_load_print_meta: vocab type       = BPE
0.00.050.010 I llm_load_print_meta: n_vocab          = 50304
0.00.050.010 I llm_load_print_meta: n_merges         = 50009
0.00.050.010 I llm_load_print_meta: vocab_only       = 0
0.00.050.010 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.011 I llm_load_print_meta: n_embd           = 2048
0.00.050.011 I llm_load_print_meta: n_layer          = 24
0.00.050.013 I llm_load_print_meta: n_head           = 16
0.00.050.014 I llm_load_print_meta: n_head_kv        = 16
0.00.050.021 I llm_load_print_meta: n_rot            = 32
0.00.050.021 I llm_load_print_meta: n_swa            = 0
0.00.050.021 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.021 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.022 I llm_load_print_meta: n_gqa            = 1
0.00.050.023 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.024 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.024 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.025 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.026 I llm_load_print_meta: n_ff             = 8192
0.00.050.026 I llm_load_print_meta: n_expert         = 0
0.00.050.026 I llm_load_print_meta: n_expert_used    = 0
0.00.050.026 I llm_load_print_meta: causal attn      = 1
0.00.050.027 I llm_load_print_meta: pooling type     = 0
0.00.050.029 I llm_load_print_meta: rope type        = 2
0.00.050.029 I llm_load_print_meta: rope scaling     = linear
0.00.050.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.030 I llm_load_print_meta: freq_scale_train = 1
0.00.050.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.032 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.036 I llm_load_print_meta: model type       = 1.4B
0.00.050.037 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.037 I llm_load_print_meta: model params     = 1.41 B
0.00.050.038 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.038 I llm_load_print_meta: general.name     = 1.4B
0.00.050.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.038 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.038 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.039 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.039 I llm_load_print_meta: max token length = 1024
0.00.051.805 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.805 I llm_load_tensors: offloading output layer to GPU
0.00.051.805 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.811 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.811 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.660 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.661 I llama_new_context_with_model: n_ctx         = 128
0.00.052.661 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.661 I llama_new_context_with_model: n_batch       = 128
0.00.052.661 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.662 I llama_new_context_with_model: flash_attn    = 0
0.00.052.662 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.662 I llama_new_context_with_model: freq_scale    = 1
0.00.052.662 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.663 I ggml_metal_init: allocating
0.00.052.666 I ggml_metal_init: found device: Apple M4
0.00.052.667 I ggml_metal_init: picking default device: Apple M4
0.00.053.218 I ggml_metal_init: using embedded metal library
0.00.055.519 I ggml_metal_init: GPU name:   Apple M4
0.00.055.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.521 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.521 I ggml_metal_init: simdgroup reduction   = true
0.00.055.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.522 I ggml_metal_init: has bfloat            = true
0.00.055.522 I ggml_metal_init: use bfloat            = true
0.00.055.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.524 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.121 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.123 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.135 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.011 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.012 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.012 I llama_new_context_with_model: graph nodes  = 967
0.00.067.013 I llama_new_context_with_model: graph splits = 2
0.00.067.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.338.012 I 
0.00.338.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.338.066 I perplexity: tokenizing the input ..
0.00.346.329 I perplexity: tokenization took 8.262 ms
0.00.346.345 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.485.851 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.487.185 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.487.198 I llama_perf_context_print:        load time =     329.21 ms
0.00.487.198 I llama_perf_context_print: prompt eval time =     139.28 ms /   128 tokens (    1.09 ms per token,   919.01 tokens per second)
0.00.487.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.487.201 I llama_perf_context_print:       total time =     149.19 ms /   129 tokens
0.00.487.517 I ggml_metal_free: deallocating

real	0m0.500s
user	0m0.080s
sys	0m0.076s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.370 I build: 4265 (59f4db10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.852 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.259 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.277 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.474 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.475 I llama_model_loader: - type  f32:  194 tensors
0.00.050.475 I llama_model_loader: - type  f16:   98 tensors
0.00.076.273 I llm_load_vocab: special tokens cache size = 25
0.00.082.299 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.302 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.302 I llm_load_print_meta: arch             = gptneox
0.00.082.302 I llm_load_print_meta: vocab type       = BPE
0.00.082.303 I llm_load_print_meta: n_vocab          = 50304
0.00.082.303 I llm_load_print_meta: n_merges         = 50009
0.00.082.303 I llm_load_print_meta: vocab_only       = 0
0.00.082.303 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.303 I llm_load_print_meta: n_embd           = 2048
0.00.082.303 I llm_load_print_meta: n_layer          = 24
0.00.082.306 I llm_load_print_meta: n_head           = 16
0.00.082.307 I llm_load_print_meta: n_head_kv        = 16
0.00.082.319 I llm_load_print_meta: n_rot            = 32
0.00.082.319 I llm_load_print_meta: n_swa            = 0
0.00.082.319 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.319 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.320 I llm_load_print_meta: n_gqa            = 1
0.00.082.320 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.321 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.322 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.322 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.322 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.322 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.323 I llm_load_print_meta: n_ff             = 8192
0.00.082.323 I llm_load_print_meta: n_expert         = 0
0.00.082.323 I llm_load_print_meta: n_expert_used    = 0
0.00.082.323 I llm_load_print_meta: causal attn      = 1
0.00.082.323 I llm_load_print_meta: pooling type     = 0
0.00.082.323 I llm_load_print_meta: rope type        = 2
0.00.082.324 I llm_load_print_meta: rope scaling     = linear
0.00.082.324 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.324 I llm_load_print_meta: freq_scale_train = 1
0.00.082.324 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.325 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.325 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.325 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.325 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.325 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.325 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.335 I llm_load_print_meta: model type       = 1.4B
0.00.082.335 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.082.335 I llm_load_print_meta: model params     = 1.41 B
0.00.082.336 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.082.336 I llm_load_print_meta: general.name     = 1.4B
0.00.082.336 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.336 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.337 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.337 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.337 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.338 I llm_load_print_meta: max token length = 1024
0.00.084.835 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.084.835 I llm_load_tensors: offloading output layer to GPU
0.00.084.835 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.084.846 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.084.847 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.085.829 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.830 I llama_new_context_with_model: n_ctx         = 128
0.00.085.830 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.085.830 I llama_new_context_with_model: n_batch       = 128
0.00.085.830 I llama_new_context_with_model: n_ubatch      = 128
0.00.085.830 I llama_new_context_with_model: flash_attn    = 0
0.00.085.831 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.831 I llama_new_context_with_model: freq_scale    = 1
0.00.085.831 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.832 I ggml_metal_init: allocating
0.00.085.836 I ggml_metal_init: found device: Apple M4
0.00.085.838 I ggml_metal_init: picking default device: Apple M4
0.00.086.386 I ggml_metal_init: using embedded metal library
0.00.088.796 I ggml_metal_init: GPU name:   Apple M4
0.00.088.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.799 I ggml_metal_init: simdgroup reduction   = true
0.00.088.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.799 I ggml_metal_init: has bfloat            = true
0.00.088.799 I ggml_metal_init: use bfloat            = true
0.00.088.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.784 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.797 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.633 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.634 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.635 I llama_new_context_with_model: graph nodes  = 967
0.00.099.635 I llama_new_context_with_model: graph splits = 2
0.00.099.647 I 
0.00.099.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.099.681 I compute_imatrix: tokenizing the input ..
0.00.106.684 I compute_imatrix: tokenization took 7.003 ms
0.00.106.686 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.608.014 I compute_imatrix: 1.50 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.610.453 I llama_perf_context_print:        load time =    1586.16 ms
0.01.610.454 I llama_perf_context_print: prompt eval time =    1500.68 ms /   128 tokens (   11.72 ms per token,    85.29 tokens per second)
0.01.610.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.610.456 I llama_perf_context_print:       total time =    1588.60 ms /   129 tokens
0.01.611.113 I ggml_metal_free: deallocating

real	0m1.800s
user	0m0.163s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4265 (59f4db10)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14870a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14870a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14870af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14870b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14870baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14870c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14870c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14870cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14870d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14870d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14870dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14870e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14870ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14870f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14870fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1487102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1487109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1487110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148711810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148711fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148712e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148713540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148714500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1487147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148714dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148715a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148715f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1487166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1487169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148717230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148717ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148718370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148718cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148719150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1487195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148719a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148719f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14871a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14871a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14871aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14871b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14871bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14871c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14871c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14871ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14871d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14871da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14871e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14871e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14871ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14871f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14871f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14871fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148720220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1487204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148720980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148720e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1487212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148721760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148721c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1487220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148722540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1487229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148722e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148723320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1487237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148723c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1487241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1487251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1487256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148726190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1487266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148727180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1487276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148727c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148728170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1487286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148728c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148729160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1487296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148729c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14872a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14872a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14872abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14872b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14872b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14872bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14871b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14872c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14872c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14872cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14872d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14872d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14872dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14872e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14872e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14872ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14872f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14872f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14872fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148730270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1487307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148730d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1487311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148731650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148731af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1487328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148732d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148733210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1487336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148733b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148734490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148734930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148735710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148735bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148736050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1487364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148736990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148736e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1487372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148737c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1487380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1487389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148738e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148739330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1487397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148739c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14873a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14873a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14873aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14873aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14873b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14873b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14873bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14873c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14873c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14873cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14873cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14873d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14873d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14873dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14873e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14873e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14873eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14873efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14873f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14873f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14873fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148740230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1487406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1487414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148741950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148741df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148742290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148742730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148742bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148743070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148743510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1487439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148743e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1487442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148744790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148744c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1487450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148745a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148746350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1487467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148746c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148747130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1487475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148747a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148747f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1487489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148748f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148749450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148749710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148749d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14874a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14874a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14874b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14874b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14874b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14874bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14874c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14874cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14874cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14874d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14874dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14874e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14874e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14874ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14874f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14874f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14874fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1487506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1487560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1487570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1487580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1487590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14875a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14875a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14875ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14875b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14875b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14875bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14875c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14875c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14875cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14875d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14875d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14875db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14875e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14875e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14875eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14875f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14875f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14875fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1487605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148760a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148760ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148761380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148761820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148761cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148762160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148762600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148762aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148762f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1487633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148763880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148763d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1487641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148764e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148765550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148765c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148766390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148766650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148766e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148767100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148767710 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14870db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14870df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14870e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14870e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14870ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14870f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14870f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14870fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14870fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148710310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148710780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148710bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1487114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148711c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148712440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148712b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148713220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148713910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148714000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148715070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148715760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148715e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148716c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1487170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148717510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148717980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148717df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148718260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1487186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148718b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148718fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148719270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1487196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148719b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148719fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14871a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14871a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14871ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14871b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14871b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14871ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14871bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14871c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14871c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14871cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14871d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14871d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14871d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14871dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14871e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14871e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14871eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14871efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14871f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14871f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14871fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1487205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148720a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148720eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148721320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148721790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148721c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148722070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1487224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1487236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148723b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148723f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1487243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148724860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148724cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148725140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1487255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148725e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148726300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148726770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148726be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148727050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1487274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148727930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148727da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148728210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148728680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148728f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1487293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148729840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148729cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14872a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14872a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14872aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14872ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14872b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14872b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14872bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14872c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14872c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14872c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14872cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14872d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14872d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14872dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14872df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14872e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14872e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14872ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14872f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14872f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14872f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14872fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1487302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148730730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148730ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148731010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148731480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1487318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148731d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1487321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148732640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148732f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148733390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148733800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148733c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1487340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148734550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1487349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148734e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1487352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148735710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148735b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148735ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1487368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148736d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1487371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148737620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148737a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148737f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148738370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1487387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148738c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1487390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148739530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1487399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148739e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14873a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14873a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14873ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14873afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14873b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14873b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14873bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14873c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14873c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14873ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14873cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14873d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14873d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14873dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14873e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14873e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14873e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14873edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14873f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14873f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14873fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14873ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148740420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148740890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148741170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1487415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148741a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148741ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148742330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1487427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148742c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148743080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1487434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148743960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148744240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1487446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148744b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148744f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148745400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148745870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148745ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148746150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1487465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148746a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148746ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148747310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148747780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148747bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148748060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1487484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148748940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148748db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148749220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148749690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148749b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14874a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14874a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14874ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14874afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14874b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14874b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14874bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14874c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14874c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14874ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14874cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14874d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14874d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14874dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14874e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14874e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14874e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14874edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14874f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14874f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14874fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14874ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148750420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148750890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148750d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148751170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1487515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148751a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148751ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148752330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1487527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148752c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148753080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1487534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148753960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148753dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1487546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148754b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148754f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148755400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148755870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148755ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148756150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1487565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148756a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148758060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1487584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148758940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148758db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148759220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148759690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148759b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148759f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14875a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14875a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14875acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14875b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14875b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14875ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14875be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14875c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14875c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14875cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14875d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14875d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14875d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14875e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14875e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14875edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14875f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14875f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14875fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148760230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1487606a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1486046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148604b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148604fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148605430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1486058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148605d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148606180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1486065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148606a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148606fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148607440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148607ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1486085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148608d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1486095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148609cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14860a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14860ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14860b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14860b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14860c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14860c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14860cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14860d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14860dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14860e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14860e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14860e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14860ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14860f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14860f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14860fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14860fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148610130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1486105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1486112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1486124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148613200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148613670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1486143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148614ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148615110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148615580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1486159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148615e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1486162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148616840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1486171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148617620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148617f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1486187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1486190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1486199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14861a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14861a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14861ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14861afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14861b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14861b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14861bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14861c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14861c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14861ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14861cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14861d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14861d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14861dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14861e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14861e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14861e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14861edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14861f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14861f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14861fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14861ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148621170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1486215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148621a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148621ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148622330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1486227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148623080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1486234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148623960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148623dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1486246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148624f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148625870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148625ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148626150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1486265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148627310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148627780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148627bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148628060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1486284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148629220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148629690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148629f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14862a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14862a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14862acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14862b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14862b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14862ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14862be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14862c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14862c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14862cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14862d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14862d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14862d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14862dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14862e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14862e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14862eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14862ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14862f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14862f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14862fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148630110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148630580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1486309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148630e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1486312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148631740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148631bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148632900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148632d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1486331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148633ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1486343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148634810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148634c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1486350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1486359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148635e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1486362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148636720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148636b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148637000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148637470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1486378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148637d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1486381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148638630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148638aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148638f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148639380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1486397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148639c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14863a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14863a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14863a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14863ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14863b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14863b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14863bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14863bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14863c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14863c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14863cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14863d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14863d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14863da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14863def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14863e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14863e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14863ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14863f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14863f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14863f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14863fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148640270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1486406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1486416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148641980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148641c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1486420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148642520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148642990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148643270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1486436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148643b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148643fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1486448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1486455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148646340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1486467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148646c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148647500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148647970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148647de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148648250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1486486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148649cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14864a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14864a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14864aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14864b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14864b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14864bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14864c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14864c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14864ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14864d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14864d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14864df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14864e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14864eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14864f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14864f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14864fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1486501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148650760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1486512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148651870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148651e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1486523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148652980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148652f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148653430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148653e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148654330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148654830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148654d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148655730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148655c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148656130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148656630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148656b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148657a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148658160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148658880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148658fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148659260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148659a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148659d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14865a320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.815s
user	0m0.297s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4265 (59f4db10)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f70edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f70f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f70fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f710030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f7105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f710b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f711140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f7116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f711ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f7121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f7126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f712ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f7136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f713e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f714680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f714da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f7154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f715be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f716300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f716ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f7171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f717910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f7188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f718ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f7192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f7198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f71a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f71aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f71ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f71b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f71b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f71bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f71c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f71c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f71c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f71ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f71d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f71d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f71dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f71e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f71e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f71ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f71eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f71f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f71f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f71fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f7206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f720cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f7212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f7218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f721f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f722510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f722b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f723310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f7237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f723c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f723f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f724d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f724fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f725470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f725910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f725db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f726250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f7266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f726b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f727030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f7274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f727970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f727e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f7282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f728750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f728ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f7291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f729740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f729c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f72a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f72a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f72ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f72b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f72b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f72bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f72c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f72c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f72cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f72d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f72d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f72dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f72e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f72e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f72ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f72f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f72f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f72fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f730180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f7306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f7203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f7312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f731840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f731d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f7322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f732830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f732d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f7332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f733820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f733d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f7342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f734810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f734d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f7352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f735800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f735ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f736140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f7365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f736a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f736f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f7373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f737860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f737d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f7381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f738640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f738ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f738f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f739420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f7398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f739d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f73a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f73a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f73ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f73afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f73b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f73b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f73bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f73c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f73c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f73cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f73d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f73d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f73d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f73de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f73e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f73e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f73ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f73f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f73f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f73fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f740320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f7407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f740c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f741100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f7415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f741a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f741ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f742380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f742820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f742cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f743160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f743600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f743aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f7443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f744880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f744d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f7451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f745660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f745b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f745fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f746440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f7468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f746d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f747220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f7476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f747b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f7484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f748940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f748de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f749280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f749720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f749bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f74a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f74a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f74a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f74ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f74b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f74b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f74bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f74c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f74c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f74ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f74cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f74d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f74d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f74df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f74e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f74e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f74ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f74f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f74fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f7500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f750380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f751180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f751620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f751ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f751f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f752710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f752c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f7531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f753700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f753c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f7541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f7546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f754c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f755190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f7556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f755c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f756180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f7566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f756c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f7576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f758160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f7586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f758c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f759150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f7596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f759bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f75a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f75a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f75abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f75b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f75b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f75bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f75c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f75c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f75cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f75d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f75d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f75dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f75e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f75e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f75eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f75f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f75f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f75fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f7600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f760630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f760b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f7610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f761620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f7620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f762b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f7630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f763600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f763b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f7640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f7645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f764b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f7659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f7667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f7670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f768810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f768cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f769200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f769920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f76a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f76a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f76ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f76b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f76b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f76bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f76c200 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.030 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1310064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1310083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131009390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131009eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13100a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13100ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13100b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13100bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13100c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13100caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13100d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13100d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13100e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13100e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13100ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13100f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13100f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13100fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131010050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1310104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131010930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131010da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1310112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131011740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131011a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1310122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131012750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131012bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131013030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1310134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131013910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131013d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1310141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131014ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131014f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1310153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131015820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131015c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131016100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1310169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1310172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131017730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131017ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131018110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131018610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131019360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1310197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131019c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13101a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13101a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13101a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13101ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13101b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13101b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13101bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13101bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13101c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13101c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13101cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13101d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13101d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13101da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13101ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13101e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13101e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13101ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13101f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13101f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13101f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13101fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131020250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1310206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131020b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131020fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131021410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131021880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131021cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1310225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131022a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131023320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131023790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131023c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131024070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1310244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131024950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131024dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131025230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1310256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131025b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1310263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131026860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131026cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131027140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1310275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131027e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131028770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131028be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131029050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1310294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131029930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131029da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13102a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13102a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13102aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13102af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13102b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13102b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13102bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13102c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13102c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13102ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13102ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13102d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13102d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13102dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13102e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13102e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13102e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13102ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13102f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13102f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13102fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13102ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1310303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131030820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131030c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131031100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131031570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1310319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131031e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1310322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131032730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131032ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131033010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131033480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1310338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131033d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1310341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131034640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131034ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131034f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131035390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131035800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131035c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1310360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131036550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1310369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131036e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1310372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131037710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131037b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131037ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131038460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1310388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131038d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1310391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131039a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131039f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13103a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13103a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13103ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13103b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13103b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13103b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13103be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13103c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13103c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13103cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13103cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13103d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13103d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13103dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13103e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13103e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13103ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13103eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13103f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13103f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13103fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1310400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131040510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131040980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131040df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131041260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1310416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131041b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131041fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131042420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131042f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131043250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131043510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131043980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131043df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131044260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1310446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131044b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131044fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131045420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131045890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131045d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1310465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131046a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131046ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131047330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1310477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131047c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131048080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1310484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131048960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131048dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131049240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1310496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131049b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131049f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13104a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13104a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13104ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13104b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13104b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13104ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13104bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13104c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13104c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13104cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13104d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13104d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13104d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13104ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13104e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13104e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13104eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13104ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13104f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13104f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13104fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131050130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1310505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131050a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131050e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1310512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131051760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131051bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131052040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1310524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131052920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131052d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131053200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131053670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131053ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131053f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1310543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131054830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131054ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131055110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131055580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1310559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131055e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1310562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131056e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131057530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131057c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131058370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131058630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1310588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131058d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1310591d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12f605300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12f605770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12f605be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12f606050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12f6064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12f606930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12f606da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12f607210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12f607680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12f607af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12f607f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12f608680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12f6091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12f609950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12f60a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12f60a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12f60afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12f60b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12f60bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12f60c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12f60cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12f60d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12f60da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12f60e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12f60e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12f60eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f60ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12f60f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12f60f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12f60fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12f60fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12f610520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12f610990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12f610c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12f6110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12f611530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12f6119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12f611e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12f612280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12f6126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12f612b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12f612fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12f613440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12f6138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12f613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12f614190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12f614600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12f614a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12f614ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12f615350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12f6157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12f615c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12f6160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12f616510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12f616980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12f616df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12f617360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12f617860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12f617cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12f618140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12f6185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12f618a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12f618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12f619300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12f619770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12f619be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12f61a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12f61a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12f61a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f61ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12f61b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12f61b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12f61baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12f61bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12f61c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12f61c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12f61ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12f61d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12f61d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12f61da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12f61de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12f61e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12f61e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12f61ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12f61f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12f61f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f61f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12f61fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12f6201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12f620660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12f620ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12f620f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12f6213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12f621820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12f621c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12f622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f622570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12f6229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12f622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12f6232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12f623730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12f623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12f624010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12f624480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12f6248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12f624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12f6251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12f625640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12f625ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12f625f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12f626390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12f626800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12f626c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12f6270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12f627550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12f6279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12f627e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12f6282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f628710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12f628b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12f628ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12f629460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12f6298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12f629d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12f62a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12f62a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12f62aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12f62af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12f62b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12f62b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12f62bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12f62c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12f62c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12f62c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12f62ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12f62d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12f62d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12f62db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12f62dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12f62e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12f62e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12f62ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12f62f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12f62f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12f62fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12f62fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12f630350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12f6307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12f630c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12f6310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12f631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12f631980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12f631df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12f632260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12f6326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12f632b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12f632fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12f633420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12f633890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12f633d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12f634170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12f6345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12f634a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12f634ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12f635330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12f6357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12f635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12f636080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12f6364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12f636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12f636dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12f637240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12f6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12f637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12f637f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12f638400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12f638870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12f638ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12f639150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12f6395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12f639a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12f639ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12f63a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12f63a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12f63abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12f63b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12f63b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12f63b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12f63bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12f63c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12f63c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12f63cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12f63cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12f63d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12f63d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12f63dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12f63e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12f63e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12f63ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12f63ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12f63f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12f63f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12f63fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12f640040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12f6404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12f640920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12f640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12f641200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12f641670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12f6421e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12f6424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12f642760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12f642bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12f643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12f6434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12f643920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12f643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12f644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12f644670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12f644ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12f644f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12f6453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12f645830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12f645ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12f646110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12f646580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12f6469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12f646e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12f6472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12f647740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12f647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12f648020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12f648490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12f648900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12f648d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12f6491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12f649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12f649ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12f649f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12f64a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12f64a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12f64ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12f64b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12f64b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12f64bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12f64c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12f64c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12f64ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12f64d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12f64d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12f64df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12f64e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12f64eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12f64f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12f64f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12f64fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12f650170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12f650720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12f650cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12f651280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12f651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12f651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12f652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12f652940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12f652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12f6534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12f653a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12f653f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12f654450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12f654950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12f654e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12f655350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12f655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12f655d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12f656250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12f656750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12f656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12f657150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12f657650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12f657b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12f658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12f658c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12f6593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12f659ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12f659d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12f65a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12f65a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12f65ae40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.927s
user	0m0.243s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
