Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.6s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.848s
user	0m0.893s
sys	0m1.228s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-embedding
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-infill
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-parallel
[ 84%] Generating loading.html.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-passkey
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Built target llama-retrieval
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.029s
user	0m6.177s
sys	0m10.187s

main: quantize time =  3468.86 ms
main:    total time =  3468.86 ms

main: quantize time =  2650.13 ms
main:    total time =  2650.13 ms

main: quantize time =  1896.07 ms
main:    total time =  1896.07 ms

main: quantize time =  1792.38 ms
main:    total time =  1792.38 ms

main: quantize time =  1756.27 ms
main:    total time =  1756.27 ms

main: quantize time =  5026.63 ms
main:    total time =  5026.63 ms

main: quantize time =  5684.07 ms
main:    total time =  5684.07 ms

main: quantize time =  6787.60 ms
main:    total time =  6787.60 ms

main: quantize time =  6032.89 ms
main:    total time =  6032.89 ms

main: quantize time =  4402.83 ms
main:    total time =  4402.83 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.119 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.208 I main: llama backend init
0.00.000.213 I main: load the model and apply lora adapter, if any
0.00.025.044 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.168 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.558 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.559 I llama_model_loader: - type  f32:  194 tensors
0.00.052.560 I llama_model_loader: - type  f16:   98 tensors
0.00.052.560 I print_info: file format = GGUF V3 (latest)
0.00.052.565 I print_info: file type   = all F32 (guessed)
0.00.052.567 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.386 I load: special tokens cache size = 25
0.00.078.481 I load: token to piece cache size = 0.2984 MB
0.00.078.486 I print_info: arch             = gptneox
0.00.078.486 I print_info: vocab_only       = 0
0.00.078.486 I print_info: n_ctx_train      = 2048
0.00.078.486 I print_info: n_embd           = 2048
0.00.078.487 I print_info: n_layer          = 24
0.00.078.491 I print_info: n_head           = 16
0.00.078.494 I print_info: n_head_kv        = 16
0.00.078.494 I print_info: n_rot            = 32
0.00.078.494 I print_info: n_swa            = 0
0.00.078.494 I print_info: n_embd_head_k    = 128
0.00.078.494 I print_info: n_embd_head_v    = 128
0.00.078.495 I print_info: n_gqa            = 1
0.00.078.495 I print_info: n_embd_k_gqa     = 2048
0.00.078.496 I print_info: n_embd_v_gqa     = 2048
0.00.078.496 I print_info: f_norm_eps       = 1.0e-05
0.00.078.497 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.497 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.497 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.497 I print_info: f_logit_scale    = 0.0e+00
0.00.078.498 I print_info: n_ff             = 8192
0.00.078.498 I print_info: n_expert         = 0
0.00.078.498 I print_info: n_expert_used    = 0
0.00.078.498 I print_info: causal attn      = 1
0.00.078.498 I print_info: pooling type     = 0
0.00.078.498 I print_info: rope type        = 2
0.00.078.499 I print_info: rope scaling     = linear
0.00.078.499 I print_info: freq_base_train  = 10000.0
0.00.078.499 I print_info: freq_scale_train = 1
0.00.078.499 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.500 I print_info: rope_finetuned   = unknown
0.00.078.500 I print_info: ssm_d_conv       = 0
0.00.078.500 I print_info: ssm_d_inner      = 0
0.00.078.500 I print_info: ssm_d_state      = 0
0.00.078.500 I print_info: ssm_dt_rank      = 0
0.00.078.500 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.500 I print_info: model type       = 1.4B
0.00.078.501 I print_info: model params     = 1.41 B
0.00.078.501 I print_info: general.name     = 1.4B
0.00.078.501 I print_info: vocab type       = BPE
0.00.078.502 I print_info: n_vocab          = 50304
0.00.078.502 I print_info: n_merges         = 50009
0.00.078.502 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.502 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.503 I print_info: LF token         = 128 'Ä'
0.00.078.503 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.503 I print_info: max token length = 1024
0.00.080.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.080.875 I load_tensors: offloading output layer to GPU
0.00.080.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.080.894 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.080.895 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.081.197 I llama_init_from_model: n_seq_max     = 1
0.00.081.198 I llama_init_from_model: n_ctx         = 2048
0.00.081.198 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.081.198 I llama_init_from_model: n_batch       = 2048
0.00.081.198 I llama_init_from_model: n_ubatch      = 512
0.00.081.199 I llama_init_from_model: flash_attn    = 0
0.00.081.199 I llama_init_from_model: freq_base     = 10000.0
0.00.081.199 I llama_init_from_model: freq_scale    = 1
0.00.081.200 I ggml_metal_init: allocating
0.00.081.203 I ggml_metal_init: found device: Apple M4
0.00.081.205 I ggml_metal_init: picking default device: Apple M4
0.00.081.877 I ggml_metal_init: using embedded metal library
0.00.132.098 I ggml_metal_init: GPU name:   Apple M4
0.00.132.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.132.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.132.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.132.103 I ggml_metal_init: simdgroup reduction   = true
0.00.132.104 I ggml_metal_init: simdgroup matrix mul. = true
0.00.132.104 I ggml_metal_init: has bfloat            = true
0.00.132.104 I ggml_metal_init: use bfloat            = true
0.00.132.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.132.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.213.681 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.240.402 I init:      Metal KV buffer size =   384.00 MiB
0.00.240.417 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.240.468 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.241.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.241.463 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.241.463 I llama_init_from_model: graph nodes  = 967
0.00.241.463 I llama_init_from_model: graph splits = 2
0.00.241.466 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.241.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.241.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.321.514 I main: llama threadpool init, n_threads = 4
0.00.321.548 I 
0.00.321.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.321.570 I 
0.00.321.646 I sampler seed: 1234
0.00.321.651 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.321.682 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.321.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.321.684 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.196.173 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48696.84 tokens per second)
0.02.196.174 I llama_perf_context_print:        load time =     296.46 ms
0.02.196.175 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.37 tokens per second)
0.02.196.175 I llama_perf_context_print:        eval time =    1828.04 ms /    63 runs   (   29.02 ms per token,    34.46 tokens per second)
0.02.196.176 I llama_perf_context_print:       total time =    1874.66 ms /    70 tokens
0.02.196.399 I ggml_metal_free: deallocating

real	0m2.506s
user	0m0.140s
sys	0m0.102s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.830 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.962 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.962 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.184 I llama_model_loader: - type  f32:  194 tensors
0.00.037.184 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.185 I print_info: file format = GGUF V3 (latest)
0.00.037.185 I print_info: file type   = Q8_0
0.00.037.186 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.059.286 I load: special tokens cache size = 25
0.00.065.212 I load: token to piece cache size = 0.2984 MB
0.00.065.217 I print_info: arch             = gptneox
0.00.065.217 I print_info: vocab_only       = 0
0.00.065.218 I print_info: n_ctx_train      = 2048
0.00.065.218 I print_info: n_embd           = 2048
0.00.065.218 I print_info: n_layer          = 24
0.00.065.224 I print_info: n_head           = 16
0.00.065.225 I print_info: n_head_kv        = 16
0.00.065.225 I print_info: n_rot            = 32
0.00.065.225 I print_info: n_swa            = 0
0.00.065.225 I print_info: n_embd_head_k    = 128
0.00.065.227 I print_info: n_embd_head_v    = 128
0.00.065.227 I print_info: n_gqa            = 1
0.00.065.228 I print_info: n_embd_k_gqa     = 2048
0.00.065.228 I print_info: n_embd_v_gqa     = 2048
0.00.065.229 I print_info: f_norm_eps       = 1.0e-05
0.00.065.230 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.230 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.230 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.230 I print_info: f_logit_scale    = 0.0e+00
0.00.065.231 I print_info: n_ff             = 8192
0.00.065.231 I print_info: n_expert         = 0
0.00.065.231 I print_info: n_expert_used    = 0
0.00.065.231 I print_info: causal attn      = 1
0.00.065.232 I print_info: pooling type     = 0
0.00.065.232 I print_info: rope type        = 2
0.00.065.232 I print_info: rope scaling     = linear
0.00.065.232 I print_info: freq_base_train  = 10000.0
0.00.065.233 I print_info: freq_scale_train = 1
0.00.065.233 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.233 I print_info: rope_finetuned   = unknown
0.00.065.233 I print_info: ssm_d_conv       = 0
0.00.065.233 I print_info: ssm_d_inner      = 0
0.00.065.233 I print_info: ssm_d_state      = 0
0.00.065.233 I print_info: ssm_dt_rank      = 0
0.00.065.233 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.234 I print_info: model type       = 1.4B
0.00.065.234 I print_info: model params     = 1.41 B
0.00.065.234 I print_info: general.name     = 1.4B
0.00.065.235 I print_info: vocab type       = BPE
0.00.065.235 I print_info: n_vocab          = 50304
0.00.065.235 I print_info: n_merges         = 50009
0.00.065.235 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.235 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.236 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.236 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.236 I print_info: LF token         = 128 'Ä'
0.00.065.236 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.236 I print_info: max token length = 1024
0.00.067.725 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.725 I load_tensors: offloading output layer to GPU
0.00.067.725 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.736 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.737 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.078 I llama_init_from_model: n_seq_max     = 1
0.00.068.079 I llama_init_from_model: n_ctx         = 2048
0.00.068.080 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.068.080 I llama_init_from_model: n_batch       = 2048
0.00.068.080 I llama_init_from_model: n_ubatch      = 512
0.00.068.080 I llama_init_from_model: flash_attn    = 0
0.00.068.081 I llama_init_from_model: freq_base     = 10000.0
0.00.068.081 I llama_init_from_model: freq_scale    = 1
0.00.068.081 I ggml_metal_init: allocating
0.00.068.085 I ggml_metal_init: found device: Apple M4
0.00.068.087 I ggml_metal_init: picking default device: Apple M4
0.00.068.874 I ggml_metal_init: using embedded metal library
0.00.071.639 I ggml_metal_init: GPU name:   Apple M4
0.00.071.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.641 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.641 I ggml_metal_init: simdgroup reduction   = true
0.00.071.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.642 I ggml_metal_init: has bfloat            = true
0.00.071.642 I ggml_metal_init: use bfloat            = true
0.00.071.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.028 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.812 I init:      Metal KV buffer size =   384.00 MiB
0.00.107.821 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.176 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.109.179 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.109.179 I llama_init_from_model: graph nodes  = 967
0.00.109.179 I llama_init_from_model: graph splits = 2
0.00.109.184 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.109.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.092.045 I main: llama threadpool init, n_threads = 4
0.01.092.077 I 
0.01.092.103 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.092.103 I 
0.01.092.323 I sampler seed: 1234
0.01.092.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.092.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.092.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.092.349 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.182.314 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.02.182.315 I llama_perf_context_print:        load time =    1082.21 ms
0.02.182.316 I llama_perf_context_print: prompt eval time =      44.17 ms /     7 tokens (    6.31 ms per token,   158.49 tokens per second)
0.02.182.316 I llama_perf_context_print:        eval time =    1042.81 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.182.317 I llama_perf_context_print:       total time =    1090.27 ms /    70 tokens
0.02.182.534 I ggml_metal_free: deallocating

real	0m2.201s
user	0m0.116s
sys	0m0.206s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.048 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.033 I llama_model_loader: - type  f32:  194 tensors
0.00.034.033 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.034 I print_info: file format = GGUF V3 (latest)
0.00.034.035 I print_info: file type   = Q4_0
0.00.034.036 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.053.463 I load: special tokens cache size = 25
0.00.059.542 I load: token to piece cache size = 0.2984 MB
0.00.059.546 I print_info: arch             = gptneox
0.00.059.546 I print_info: vocab_only       = 0
0.00.059.546 I print_info: n_ctx_train      = 2048
0.00.059.546 I print_info: n_embd           = 2048
0.00.059.547 I print_info: n_layer          = 24
0.00.059.551 I print_info: n_head           = 16
0.00.059.552 I print_info: n_head_kv        = 16
0.00.059.552 I print_info: n_rot            = 32
0.00.059.553 I print_info: n_swa            = 0
0.00.059.553 I print_info: n_embd_head_k    = 128
0.00.059.553 I print_info: n_embd_head_v    = 128
0.00.059.554 I print_info: n_gqa            = 1
0.00.059.554 I print_info: n_embd_k_gqa     = 2048
0.00.059.555 I print_info: n_embd_v_gqa     = 2048
0.00.059.556 I print_info: f_norm_eps       = 1.0e-05
0.00.059.556 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.557 I print_info: f_logit_scale    = 0.0e+00
0.00.059.558 I print_info: n_ff             = 8192
0.00.059.558 I print_info: n_expert         = 0
0.00.059.558 I print_info: n_expert_used    = 0
0.00.059.558 I print_info: causal attn      = 1
0.00.059.558 I print_info: pooling type     = 0
0.00.059.559 I print_info: rope type        = 2
0.00.059.559 I print_info: rope scaling     = linear
0.00.059.559 I print_info: freq_base_train  = 10000.0
0.00.059.560 I print_info: freq_scale_train = 1
0.00.059.560 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.560 I print_info: rope_finetuned   = unknown
0.00.059.560 I print_info: ssm_d_conv       = 0
0.00.059.560 I print_info: ssm_d_inner      = 0
0.00.059.561 I print_info: ssm_d_state      = 0
0.00.059.561 I print_info: ssm_dt_rank      = 0
0.00.059.561 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.561 I print_info: model type       = 1.4B
0.00.059.561 I print_info: model params     = 1.41 B
0.00.059.562 I print_info: general.name     = 1.4B
0.00.059.562 I print_info: vocab type       = BPE
0.00.059.562 I print_info: n_vocab          = 50304
0.00.059.563 I print_info: n_merges         = 50009
0.00.059.563 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.563 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.563 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.563 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.564 I print_info: LF token         = 128 'Ä'
0.00.059.564 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.564 I print_info: max token length = 1024
0.00.061.800 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.800 I load_tensors: offloading output layer to GPU
0.00.061.801 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.811 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.061.813 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.062.178 I llama_init_from_model: n_seq_max     = 1
0.00.062.178 I llama_init_from_model: n_ctx         = 2048
0.00.062.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.179 I llama_init_from_model: n_batch       = 2048
0.00.062.179 I llama_init_from_model: n_ubatch      = 512
0.00.062.179 I llama_init_from_model: flash_attn    = 0
0.00.062.180 I llama_init_from_model: freq_base     = 10000.0
0.00.062.180 I llama_init_from_model: freq_scale    = 1
0.00.062.180 I ggml_metal_init: allocating
0.00.062.183 I ggml_metal_init: found device: Apple M4
0.00.062.185 I ggml_metal_init: picking default device: Apple M4
0.00.062.954 I ggml_metal_init: using embedded metal library
0.00.065.577 I ggml_metal_init: GPU name:   Apple M4
0.00.065.579 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.580 I ggml_metal_init: simdgroup reduction   = true
0.00.065.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.581 I ggml_metal_init: has bfloat            = true
0.00.065.581 I ggml_metal_init: use bfloat            = true
0.00.065.581 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.116 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.182 I init:      Metal KV buffer size =   384.00 MiB
0.00.101.195 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.517 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.519 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.520 I llama_init_from_model: graph nodes  = 967
0.00.102.520 I llama_init_from_model: graph splits = 2
0.00.102.524 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.641 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.680 I main: llama threadpool init, n_threads = 4
0.00.711.719 I 
0.00.711.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.747 I 
0.00.712.039 I sampler seed: 1234
0.00.712.042 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.054 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.054 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.054 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.389.494 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.389.495 I llama_perf_context_print:        load time =     698.63 ms
0.01.389.496 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.01.389.496 I llama_perf_context_print:        eval time =     630.63 ms /    63 runs   (   10.01 ms per token,    99.90 tokens per second)
0.01.389.497 I llama_perf_context_print:       total time =     677.82 ms /    70 tokens
0.01.389.717 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.114s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.848 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.357 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.920 I llama_model_loader: - type  f32:  194 tensors
0.00.026.920 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.921 I print_info: file format = GGUF V3 (latest)
0.00.026.921 I print_info: file type   = Q4_1
0.00.026.922 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.637 I load: special tokens cache size = 25
0.00.052.702 I load: token to piece cache size = 0.2984 MB
0.00.052.705 I print_info: arch             = gptneox
0.00.052.705 I print_info: vocab_only       = 0
0.00.052.705 I print_info: n_ctx_train      = 2048
0.00.052.705 I print_info: n_embd           = 2048
0.00.052.706 I print_info: n_layer          = 24
0.00.052.709 I print_info: n_head           = 16
0.00.052.710 I print_info: n_head_kv        = 16
0.00.052.710 I print_info: n_rot            = 32
0.00.052.710 I print_info: n_swa            = 0
0.00.052.710 I print_info: n_embd_head_k    = 128
0.00.052.710 I print_info: n_embd_head_v    = 128
0.00.052.711 I print_info: n_gqa            = 1
0.00.052.712 I print_info: n_embd_k_gqa     = 2048
0.00.052.713 I print_info: n_embd_v_gqa     = 2048
0.00.052.713 I print_info: f_norm_eps       = 1.0e-05
0.00.052.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.716 I print_info: f_logit_scale    = 0.0e+00
0.00.052.716 I print_info: n_ff             = 8192
0.00.052.716 I print_info: n_expert         = 0
0.00.052.717 I print_info: n_expert_used    = 0
0.00.052.718 I print_info: causal attn      = 1
0.00.052.720 I print_info: pooling type     = 0
0.00.052.720 I print_info: rope type        = 2
0.00.052.720 I print_info: rope scaling     = linear
0.00.052.721 I print_info: freq_base_train  = 10000.0
0.00.052.721 I print_info: freq_scale_train = 1
0.00.052.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.721 I print_info: rope_finetuned   = unknown
0.00.052.722 I print_info: ssm_d_conv       = 0
0.00.052.722 I print_info: ssm_d_inner      = 0
0.00.052.722 I print_info: ssm_d_state      = 0
0.00.052.722 I print_info: ssm_dt_rank      = 0
0.00.052.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.724 I print_info: model type       = 1.4B
0.00.052.724 I print_info: model params     = 1.41 B
0.00.052.724 I print_info: general.name     = 1.4B
0.00.052.725 I print_info: vocab type       = BPE
0.00.052.725 I print_info: n_vocab          = 50304
0.00.052.725 I print_info: n_merges         = 50009
0.00.052.725 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.726 I print_info: LF token         = 128 'Ä'
0.00.052.727 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.727 I print_info: max token length = 1024
0.00.054.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.766 I load_tensors: offloading output layer to GPU
0.00.054.766 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.776 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.778 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.063 I llama_init_from_model: n_seq_max     = 1
0.00.055.063 I llama_init_from_model: n_ctx         = 2048
0.00.055.064 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.064 I llama_init_from_model: n_batch       = 2048
0.00.055.064 I llama_init_from_model: n_ubatch      = 512
0.00.055.064 I llama_init_from_model: flash_attn    = 0
0.00.055.065 I llama_init_from_model: freq_base     = 10000.0
0.00.055.065 I llama_init_from_model: freq_scale    = 1
0.00.055.065 I ggml_metal_init: allocating
0.00.055.069 I ggml_metal_init: found device: Apple M4
0.00.055.071 I ggml_metal_init: picking default device: Apple M4
0.00.055.716 I ggml_metal_init: using embedded metal library
0.00.058.194 I ggml_metal_init: GPU name:   Apple M4
0.00.058.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.196 I ggml_metal_init: simdgroup reduction   = true
0.00.058.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.197 I ggml_metal_init: has bfloat            = true
0.00.058.197 I ggml_metal_init: use bfloat            = true
0.00.058.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.333 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.067 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.076 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.109 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.128 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.129 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.129 I llama_init_from_model: graph nodes  = 967
0.00.089.130 I llama_init_from_model: graph splits = 2
0.00.089.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.063 I main: llama threadpool init, n_threads = 4
0.00.774.105 I 
0.00.774.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.126 I 
0.00.774.357 I sampler seed: 1234
0.00.774.362 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.373 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.374 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.498.210 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61685.49 tokens per second)
0.01.498.211 I llama_perf_context_print:        load time =     765.21 ms
0.01.498.212 I llama_perf_context_print: prompt eval time =      43.56 ms /     7 tokens (    6.22 ms per token,   160.69 tokens per second)
0.01.498.213 I llama_perf_context_print:        eval time =     677.36 ms /    63 runs   (   10.75 ms per token,    93.01 tokens per second)
0.01.498.213 I llama_perf_context_print:       total time =     724.15 ms /    70 tokens
0.01.498.401 I ggml_metal_free: deallocating

real	0m1.516s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.015.958 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.033.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.673 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.675 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.508 I llama_model_loader: - type  f32:  194 tensors
0.00.043.509 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.509 I print_info: file format = GGUF V3 (latest)
0.00.043.510 I print_info: file type   = Q5_0
0.00.043.511 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.067.316 I load: special tokens cache size = 25
0.00.075.436 I load: token to piece cache size = 0.2984 MB
0.00.075.439 I print_info: arch             = gptneox
0.00.075.439 I print_info: vocab_only       = 0
0.00.075.440 I print_info: n_ctx_train      = 2048
0.00.075.440 I print_info: n_embd           = 2048
0.00.075.440 I print_info: n_layer          = 24
0.00.075.443 I print_info: n_head           = 16
0.00.075.444 I print_info: n_head_kv        = 16
0.00.075.445 I print_info: n_rot            = 32
0.00.075.445 I print_info: n_swa            = 0
0.00.075.445 I print_info: n_embd_head_k    = 128
0.00.075.445 I print_info: n_embd_head_v    = 128
0.00.075.446 I print_info: n_gqa            = 1
0.00.075.446 I print_info: n_embd_k_gqa     = 2048
0.00.075.447 I print_info: n_embd_v_gqa     = 2048
0.00.075.448 I print_info: f_norm_eps       = 1.0e-05
0.00.075.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.448 I print_info: f_logit_scale    = 0.0e+00
0.00.075.449 I print_info: n_ff             = 8192
0.00.075.449 I print_info: n_expert         = 0
0.00.075.449 I print_info: n_expert_used    = 0
0.00.075.451 I print_info: causal attn      = 1
0.00.075.454 I print_info: pooling type     = 0
0.00.075.454 I print_info: rope type        = 2
0.00.075.454 I print_info: rope scaling     = linear
0.00.075.455 I print_info: freq_base_train  = 10000.0
0.00.075.455 I print_info: freq_scale_train = 1
0.00.075.455 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.455 I print_info: rope_finetuned   = unknown
0.00.075.455 I print_info: ssm_d_conv       = 0
0.00.075.456 I print_info: ssm_d_inner      = 0
0.00.075.456 I print_info: ssm_d_state      = 0
0.00.075.456 I print_info: ssm_dt_rank      = 0
0.00.075.456 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.456 I print_info: model type       = 1.4B
0.00.075.457 I print_info: model params     = 1.41 B
0.00.075.457 I print_info: general.name     = 1.4B
0.00.075.457 I print_info: vocab type       = BPE
0.00.075.457 I print_info: n_vocab          = 50304
0.00.075.458 I print_info: n_merges         = 50009
0.00.075.458 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.458 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.458 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.459 I print_info: LF token         = 128 'Ä'
0.00.075.459 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.460 I print_info: max token length = 1024
0.00.077.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.077.804 I load_tensors: offloading output layer to GPU
0.00.077.804 I load_tensors: offloaded 25/25 layers to GPU
0.00.077.815 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.077.816 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.078.251 I llama_init_from_model: n_seq_max     = 1
0.00.078.252 I llama_init_from_model: n_ctx         = 2048
0.00.078.252 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.078.252 I llama_init_from_model: n_batch       = 2048
0.00.078.252 I llama_init_from_model: n_ubatch      = 512
0.00.078.253 I llama_init_from_model: flash_attn    = 0
0.00.078.253 I llama_init_from_model: freq_base     = 10000.0
0.00.078.253 I llama_init_from_model: freq_scale    = 1
0.00.078.254 I ggml_metal_init: allocating
0.00.078.257 I ggml_metal_init: found device: Apple M4
0.00.078.259 I ggml_metal_init: picking default device: Apple M4
0.00.078.962 I ggml_metal_init: using embedded metal library
0.00.081.956 I ggml_metal_init: GPU name:   Apple M4
0.00.081.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.959 I ggml_metal_init: simdgroup reduction   = true
0.00.081.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.960 I ggml_metal_init: has bfloat            = true
0.00.081.960 I ggml_metal_init: use bfloat            = true
0.00.081.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.121 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.116.342 I init:      Metal KV buffer size =   384.00 MiB
0.00.116.359 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.398 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.117.361 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.117.362 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.117.362 I llama_init_from_model: graph nodes  = 967
0.00.117.363 I llama_init_from_model: graph splits = 2
0.00.117.366 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.913 I main: llama threadpool init, n_threads = 4
0.00.781.954 I 
0.00.781.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.974 I 
0.00.782.207 I sampler seed: 1234
0.00.782.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.250 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.251 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.251 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.572.281 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.572.281 I llama_perf_context_print:        load time =     765.95 ms
0.01.572.282 I llama_perf_context_print: prompt eval time =      43.19 ms /     7 tokens (    6.17 ms per token,   162.07 tokens per second)
0.01.572.283 I llama_perf_context_print:        eval time =     743.76 ms /    63 runs   (   11.81 ms per token,    84.70 tokens per second)
0.01.572.283 I llama_perf_context_print:       total time =     790.37 ms /    70 tokens
0.01.572.487 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.121s
sys	0m0.169s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.042 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.043 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.034.920 I llama_model_loader: - type  f32:  194 tensors
0.00.034.920 I llama_model_loader: - type q5_1:   97 tensors
0.00.034.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.921 I print_info: file format = GGUF V3 (latest)
0.00.034.921 I print_info: file type   = Q5_1
0.00.034.922 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.056.048 I load: special tokens cache size = 25
0.00.061.984 I load: token to piece cache size = 0.2984 MB
0.00.061.987 I print_info: arch             = gptneox
0.00.061.987 I print_info: vocab_only       = 0
0.00.061.987 I print_info: n_ctx_train      = 2048
0.00.061.987 I print_info: n_embd           = 2048
0.00.061.988 I print_info: n_layer          = 24
0.00.061.991 I print_info: n_head           = 16
0.00.061.992 I print_info: n_head_kv        = 16
0.00.061.992 I print_info: n_rot            = 32
0.00.061.993 I print_info: n_swa            = 0
0.00.061.993 I print_info: n_embd_head_k    = 128
0.00.061.993 I print_info: n_embd_head_v    = 128
0.00.061.994 I print_info: n_gqa            = 1
0.00.061.997 I print_info: n_embd_k_gqa     = 2048
0.00.061.997 I print_info: n_embd_v_gqa     = 2048
0.00.061.998 I print_info: f_norm_eps       = 1.0e-05
0.00.061.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.998 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.999 I print_info: f_logit_scale    = 0.0e+00
0.00.061.999 I print_info: n_ff             = 8192
0.00.062.000 I print_info: n_expert         = 0
0.00.062.000 I print_info: n_expert_used    = 0
0.00.062.000 I print_info: causal attn      = 1
0.00.062.000 I print_info: pooling type     = 0
0.00.062.000 I print_info: rope type        = 2
0.00.062.001 I print_info: rope scaling     = linear
0.00.062.001 I print_info: freq_base_train  = 10000.0
0.00.062.001 I print_info: freq_scale_train = 1
0.00.062.002 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.002 I print_info: rope_finetuned   = unknown
0.00.062.002 I print_info: ssm_d_conv       = 0
0.00.062.002 I print_info: ssm_d_inner      = 0
0.00.062.002 I print_info: ssm_d_state      = 0
0.00.062.002 I print_info: ssm_dt_rank      = 0
0.00.062.002 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.003 I print_info: model type       = 1.4B
0.00.062.003 I print_info: model params     = 1.41 B
0.00.062.003 I print_info: general.name     = 1.4B
0.00.062.004 I print_info: vocab type       = BPE
0.00.062.004 I print_info: n_vocab          = 50304
0.00.062.004 I print_info: n_merges         = 50009
0.00.062.005 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.005 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.006 I print_info: LF token         = 128 'Ä'
0.00.062.006 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.006 I print_info: max token length = 1024
0.00.064.033 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.034 I load_tensors: offloading output layer to GPU
0.00.064.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.045 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.064.046 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.064.330 I llama_init_from_model: n_seq_max     = 1
0.00.064.331 I llama_init_from_model: n_ctx         = 2048
0.00.064.331 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.331 I llama_init_from_model: n_batch       = 2048
0.00.064.332 I llama_init_from_model: n_ubatch      = 512
0.00.064.332 I llama_init_from_model: flash_attn    = 0
0.00.064.332 I llama_init_from_model: freq_base     = 10000.0
0.00.064.332 I llama_init_from_model: freq_scale    = 1
0.00.064.333 I ggml_metal_init: allocating
0.00.064.336 I ggml_metal_init: found device: Apple M4
0.00.064.338 I ggml_metal_init: picking default device: Apple M4
0.00.064.938 I ggml_metal_init: using embedded metal library
0.00.067.404 I ggml_metal_init: GPU name:   Apple M4
0.00.067.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.407 I ggml_metal_init: simdgroup reduction   = true
0.00.067.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.407 I ggml_metal_init: has bfloat            = true
0.00.067.407 I ggml_metal_init: use bfloat            = true
0.00.067.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.385 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.541 I init:      Metal KV buffer size =   384.00 MiB
0.00.098.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.583 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.717 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.718 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.719 I llama_init_from_model: graph nodes  = 967
0.00.099.719 I llama_init_from_model: graph splits = 2
0.00.099.722 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.857 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.278.550 I main: llama threadpool init, n_threads = 4
0.01.278.595 I 
0.01.278.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.278.622 I 
0.01.278.859 I sampler seed: 1234
0.01.278.867 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.278.887 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.278.887 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.278.887 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.115.681 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.02.115.681 I llama_perf_context_print:        load time =    1269.84 ms
0.02.115.682 I llama_perf_context_print: prompt eval time =      46.26 ms /     7 tokens (    6.61 ms per token,   151.33 tokens per second)
0.02.115.683 I llama_perf_context_print:        eval time =     787.57 ms /    63 runs   (   12.50 ms per token,    79.99 tokens per second)
0.02.115.684 I llama_perf_context_print:       total time =     837.14 ms /    70 tokens
0.02.115.899 I ggml_metal_free: deallocating

real	0m2.133s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.021.810 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.029.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.817 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.817 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.038.749 I llama_model_loader: - type  f32:  194 tensors
0.00.038.749 I llama_model_loader: - type q2_K:   49 tensors
0.00.038.749 I llama_model_loader: - type q3_K:   48 tensors
0.00.038.749 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.750 I print_info: file format = GGUF V3 (latest)
0.00.038.750 I print_info: file type   = Q2_K - Medium
0.00.038.751 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.062.322 I load: special tokens cache size = 25
0.00.069.870 I load: token to piece cache size = 0.2984 MB
0.00.069.873 I print_info: arch             = gptneox
0.00.069.874 I print_info: vocab_only       = 0
0.00.069.874 I print_info: n_ctx_train      = 2048
0.00.069.874 I print_info: n_embd           = 2048
0.00.069.874 I print_info: n_layer          = 24
0.00.069.877 I print_info: n_head           = 16
0.00.069.877 I print_info: n_head_kv        = 16
0.00.069.878 I print_info: n_rot            = 32
0.00.069.879 I print_info: n_swa            = 0
0.00.069.880 I print_info: n_embd_head_k    = 128
0.00.069.880 I print_info: n_embd_head_v    = 128
0.00.069.881 I print_info: n_gqa            = 1
0.00.069.881 I print_info: n_embd_k_gqa     = 2048
0.00.069.882 I print_info: n_embd_v_gqa     = 2048
0.00.069.882 I print_info: f_norm_eps       = 1.0e-05
0.00.069.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.883 I print_info: f_logit_scale    = 0.0e+00
0.00.069.883 I print_info: n_ff             = 8192
0.00.069.883 I print_info: n_expert         = 0
0.00.069.885 I print_info: n_expert_used    = 0
0.00.069.886 I print_info: causal attn      = 1
0.00.069.886 I print_info: pooling type     = 0
0.00.069.886 I print_info: rope type        = 2
0.00.069.886 I print_info: rope scaling     = linear
0.00.069.887 I print_info: freq_base_train  = 10000.0
0.00.069.887 I print_info: freq_scale_train = 1
0.00.069.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.891 I print_info: rope_finetuned   = unknown
0.00.069.891 I print_info: ssm_d_conv       = 0
0.00.069.891 I print_info: ssm_d_inner      = 0
0.00.069.891 I print_info: ssm_d_state      = 0
0.00.069.891 I print_info: ssm_dt_rank      = 0
0.00.069.891 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.893 I print_info: model type       = 1.4B
0.00.069.893 I print_info: model params     = 1.41 B
0.00.069.893 I print_info: general.name     = 1.4B
0.00.069.894 I print_info: vocab type       = BPE
0.00.069.894 I print_info: n_vocab          = 50304
0.00.069.895 I print_info: n_merges         = 50009
0.00.069.896 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.896 I print_info: LF token         = 128 'Ä'
0.00.069.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.897 I print_info: max token length = 1024
0.00.071.977 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.977 I load_tensors: offloading output layer to GPU
0.00.071.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.988 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.071.989 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.072.326 I llama_init_from_model: n_seq_max     = 1
0.00.072.326 I llama_init_from_model: n_ctx         = 2048
0.00.072.327 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.327 I llama_init_from_model: n_batch       = 2048
0.00.072.327 I llama_init_from_model: n_ubatch      = 512
0.00.072.327 I llama_init_from_model: flash_attn    = 0
0.00.072.328 I llama_init_from_model: freq_base     = 10000.0
0.00.072.328 I llama_init_from_model: freq_scale    = 1
0.00.072.328 I ggml_metal_init: allocating
0.00.072.331 I ggml_metal_init: found device: Apple M4
0.00.072.333 I ggml_metal_init: picking default device: Apple M4
0.00.073.009 I ggml_metal_init: using embedded metal library
0.00.075.680 I ggml_metal_init: GPU name:   Apple M4
0.00.075.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.682 I ggml_metal_init: simdgroup reduction   = true
0.00.075.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.683 I ggml_metal_init: has bfloat            = true
0.00.075.683 I ggml_metal_init: use bfloat            = true
0.00.075.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.482 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.515 I init:      Metal KV buffer size =   384.00 MiB
0.00.107.526 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.552 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.108.553 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.108.554 I llama_init_from_model: graph nodes  = 967
0.00.108.554 I llama_init_from_model: graph splits = 2
0.00.108.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.620 I main: llama threadpool init, n_threads = 4
0.00.478.655 I 
0.00.478.691 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.691 I 
0.00.478.911 I sampler seed: 1234
0.00.478.915 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.478.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.478.927 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.478.927 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.161.564 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.161.564 I llama_perf_context_print:        load time =     456.81 ms
0.01.161.565 I llama_perf_context_print: prompt eval time =      40.35 ms /     7 tokens (    5.76 ms per token,   173.47 tokens per second)
0.01.161.566 I llama_perf_context_print:        eval time =     639.24 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.161.566 I llama_perf_context_print:       total time =     682.95 ms /    70 tokens
0.01.161.766 I ggml_metal_free: deallocating

real	0m1.179s
user	0m0.115s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.504 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.028.192 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.198 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.198 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.199 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.333 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.037.393 I llama_model_loader: - type  f32:  194 tensors
0.00.037.393 I llama_model_loader: - type q3_K:   25 tensors
0.00.037.394 I llama_model_loader: - type q4_K:   71 tensors
0.00.037.394 I llama_model_loader: - type q5_K:    1 tensors
0.00.037.394 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.394 I print_info: file format = GGUF V3 (latest)
0.00.037.395 I print_info: file type   = Q3_K - Medium
0.00.037.396 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.058.997 I load: special tokens cache size = 25
0.00.065.211 I load: token to piece cache size = 0.2984 MB
0.00.065.214 I print_info: arch             = gptneox
0.00.065.214 I print_info: vocab_only       = 0
0.00.065.214 I print_info: n_ctx_train      = 2048
0.00.065.214 I print_info: n_embd           = 2048
0.00.065.214 I print_info: n_layer          = 24
0.00.065.218 I print_info: n_head           = 16
0.00.065.219 I print_info: n_head_kv        = 16
0.00.065.219 I print_info: n_rot            = 32
0.00.065.219 I print_info: n_swa            = 0
0.00.065.219 I print_info: n_embd_head_k    = 128
0.00.065.219 I print_info: n_embd_head_v    = 128
0.00.065.220 I print_info: n_gqa            = 1
0.00.065.220 I print_info: n_embd_k_gqa     = 2048
0.00.065.221 I print_info: n_embd_v_gqa     = 2048
0.00.065.221 I print_info: f_norm_eps       = 1.0e-05
0.00.065.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.222 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.222 I print_info: f_logit_scale    = 0.0e+00
0.00.065.223 I print_info: n_ff             = 8192
0.00.065.224 I print_info: n_expert         = 0
0.00.065.226 I print_info: n_expert_used    = 0
0.00.065.226 I print_info: causal attn      = 1
0.00.065.226 I print_info: pooling type     = 0
0.00.065.226 I print_info: rope type        = 2
0.00.065.226 I print_info: rope scaling     = linear
0.00.065.227 I print_info: freq_base_train  = 10000.0
0.00.065.227 I print_info: freq_scale_train = 1
0.00.065.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.229 I print_info: rope_finetuned   = unknown
0.00.065.229 I print_info: ssm_d_conv       = 0
0.00.065.229 I print_info: ssm_d_inner      = 0
0.00.065.229 I print_info: ssm_d_state      = 0
0.00.065.229 I print_info: ssm_dt_rank      = 0
0.00.065.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.230 I print_info: model type       = 1.4B
0.00.065.230 I print_info: model params     = 1.41 B
0.00.065.230 I print_info: general.name     = 1.4B
0.00.065.231 I print_info: vocab type       = BPE
0.00.065.232 I print_info: n_vocab          = 50304
0.00.065.232 I print_info: n_merges         = 50009
0.00.065.232 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.232 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.233 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.233 I print_info: LF token         = 128 'Ä'
0.00.065.233 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.233 I print_info: max token length = 1024
0.00.067.249 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.249 I load_tensors: offloading output layer to GPU
0.00.067.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.259 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.067.260 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.067.587 I llama_init_from_model: n_seq_max     = 1
0.00.067.588 I llama_init_from_model: n_ctx         = 2048
0.00.067.588 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.588 I llama_init_from_model: n_batch       = 2048
0.00.067.588 I llama_init_from_model: n_ubatch      = 512
0.00.067.588 I llama_init_from_model: flash_attn    = 0
0.00.067.589 I llama_init_from_model: freq_base     = 10000.0
0.00.067.589 I llama_init_from_model: freq_scale    = 1
0.00.067.589 I ggml_metal_init: allocating
0.00.067.592 I ggml_metal_init: found device: Apple M4
0.00.067.595 I ggml_metal_init: picking default device: Apple M4
0.00.068.217 I ggml_metal_init: using embedded metal library
0.00.070.640 I ggml_metal_init: GPU name:   Apple M4
0.00.070.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.642 I ggml_metal_init: simdgroup reduction   = true
0.00.070.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.642 I ggml_metal_init: has bfloat            = true
0.00.070.643 I ggml_metal_init: use bfloat            = true
0.00.070.643 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.364 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.546 I init:      Metal KV buffer size =   384.00 MiB
0.00.102.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.716 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.103.718 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.103.718 I llama_init_from_model: graph nodes  = 967
0.00.103.718 I llama_init_from_model: graph splits = 2
0.00.103.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.369 I main: llama threadpool init, n_threads = 4
0.00.613.413 I 
0.00.613.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.438 I 
0.00.613.675 I sampler seed: 1234
0.00.613.680 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.731 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.733 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.733 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.363.719 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.01.363.719 I llama_perf_context_print:        load time =     601.86 ms
0.01.363.720 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.01.363.721 I llama_perf_context_print:        eval time =     703.47 ms /    63 runs   (   11.17 ms per token,    89.56 tokens per second)
0.01.363.721 I llama_perf_context_print:       total time =     750.36 ms /    70 tokens
0.01.363.922 I ggml_metal_free: deallocating

real	0m1.380s
user	0m0.113s
sys	0m0.128s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.795 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.264 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.863 I llama_model_loader: - type  f32:  194 tensors
0.00.032.863 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.863 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.864 I llama_model_loader: - type q6_K:   13 tensors
0.00.032.864 I print_info: file format = GGUF V3 (latest)
0.00.032.865 I print_info: file type   = Q4_K - Medium
0.00.032.865 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.053.706 I load: special tokens cache size = 25
0.00.059.681 I load: token to piece cache size = 0.2984 MB
0.00.059.683 I print_info: arch             = gptneox
0.00.059.684 I print_info: vocab_only       = 0
0.00.059.684 I print_info: n_ctx_train      = 2048
0.00.059.684 I print_info: n_embd           = 2048
0.00.059.684 I print_info: n_layer          = 24
0.00.059.687 I print_info: n_head           = 16
0.00.059.688 I print_info: n_head_kv        = 16
0.00.059.688 I print_info: n_rot            = 32
0.00.059.688 I print_info: n_swa            = 0
0.00.059.689 I print_info: n_embd_head_k    = 128
0.00.059.689 I print_info: n_embd_head_v    = 128
0.00.059.689 I print_info: n_gqa            = 1
0.00.059.690 I print_info: n_embd_k_gqa     = 2048
0.00.059.691 I print_info: n_embd_v_gqa     = 2048
0.00.059.691 I print_info: f_norm_eps       = 1.0e-05
0.00.059.692 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.693 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.693 I print_info: f_logit_scale    = 0.0e+00
0.00.059.694 I print_info: n_ff             = 8192
0.00.059.694 I print_info: n_expert         = 0
0.00.059.694 I print_info: n_expert_used    = 0
0.00.059.694 I print_info: causal attn      = 1
0.00.059.695 I print_info: pooling type     = 0
0.00.059.695 I print_info: rope type        = 2
0.00.059.695 I print_info: rope scaling     = linear
0.00.059.695 I print_info: freq_base_train  = 10000.0
0.00.059.696 I print_info: freq_scale_train = 1
0.00.059.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.696 I print_info: rope_finetuned   = unknown
0.00.059.696 I print_info: ssm_d_conv       = 0
0.00.059.696 I print_info: ssm_d_inner      = 0
0.00.059.697 I print_info: ssm_d_state      = 0
0.00.059.697 I print_info: ssm_dt_rank      = 0
0.00.059.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.697 I print_info: model type       = 1.4B
0.00.059.698 I print_info: model params     = 1.41 B
0.00.059.698 I print_info: general.name     = 1.4B
0.00.059.698 I print_info: vocab type       = BPE
0.00.059.699 I print_info: n_vocab          = 50304
0.00.059.699 I print_info: n_merges         = 50009
0.00.059.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.700 I print_info: LF token         = 128 'Ä'
0.00.059.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.700 I print_info: max token length = 1024
0.00.061.671 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.671 I load_tensors: offloading output layer to GPU
0.00.061.671 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.682 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.061.683 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.062.038 I llama_init_from_model: n_seq_max     = 1
0.00.062.039 I llama_init_from_model: n_ctx         = 2048
0.00.062.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.039 I llama_init_from_model: n_batch       = 2048
0.00.062.039 I llama_init_from_model: n_ubatch      = 512
0.00.062.039 I llama_init_from_model: flash_attn    = 0
0.00.062.040 I llama_init_from_model: freq_base     = 10000.0
0.00.062.040 I llama_init_from_model: freq_scale    = 1
0.00.062.040 I ggml_metal_init: allocating
0.00.062.045 I ggml_metal_init: found device: Apple M4
0.00.062.047 I ggml_metal_init: picking default device: Apple M4
0.00.062.665 I ggml_metal_init: using embedded metal library
0.00.065.178 I ggml_metal_init: GPU name:   Apple M4
0.00.065.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.180 I ggml_metal_init: simdgroup reduction   = true
0.00.065.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.181 I ggml_metal_init: has bfloat            = true
0.00.065.181 I ggml_metal_init: use bfloat            = true
0.00.065.181 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.182 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.346 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.781 I init:      Metal KV buffer size =   384.00 MiB
0.00.095.786 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.811 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.852 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.852 I llama_init_from_model: graph nodes  = 967
0.00.096.852 I llama_init_from_model: graph splits = 2
0.00.096.855 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.985 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.891 I main: llama threadpool init, n_threads = 4
0.00.686.932 I 
0.00.686.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.963 I 
0.00.687.184 I sampler seed: 1234
0.00.687.188 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.226 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.227 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.452.751 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.452.752 I llama_perf_context_print:        load time =     678.09 ms
0.01.452.752 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.41 tokens per second)
0.01.452.754 I llama_perf_context_print:        eval time =     715.26 ms /    63 runs   (   11.35 ms per token,    88.08 tokens per second)
0.01.452.754 I llama_perf_context_print:       total time =     765.86 ms /    70 tokens
0.01.452.987 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.112s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.549 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.411 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.411 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.413 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.413 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.419 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.256 I llama_model_loader: - type  f32:  194 tensors
0.00.028.256 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.256 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.257 I print_info: file format = GGUF V3 (latest)
0.00.028.257 I print_info: file type   = Q5_K - Medium
0.00.028.258 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.378 I load: special tokens cache size = 25
0.00.053.336 I load: token to piece cache size = 0.2984 MB
0.00.053.339 I print_info: arch             = gptneox
0.00.053.339 I print_info: vocab_only       = 0
0.00.053.339 I print_info: n_ctx_train      = 2048
0.00.053.339 I print_info: n_embd           = 2048
0.00.053.339 I print_info: n_layer          = 24
0.00.053.342 I print_info: n_head           = 16
0.00.053.343 I print_info: n_head_kv        = 16
0.00.053.343 I print_info: n_rot            = 32
0.00.053.343 I print_info: n_swa            = 0
0.00.053.343 I print_info: n_embd_head_k    = 128
0.00.053.344 I print_info: n_embd_head_v    = 128
0.00.053.344 I print_info: n_gqa            = 1
0.00.053.345 I print_info: n_embd_k_gqa     = 2048
0.00.053.346 I print_info: n_embd_v_gqa     = 2048
0.00.053.346 I print_info: f_norm_eps       = 1.0e-05
0.00.053.347 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.347 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.347 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.347 I print_info: f_logit_scale    = 0.0e+00
0.00.053.348 I print_info: n_ff             = 8192
0.00.053.348 I print_info: n_expert         = 0
0.00.053.348 I print_info: n_expert_used    = 0
0.00.053.348 I print_info: causal attn      = 1
0.00.053.349 I print_info: pooling type     = 0
0.00.053.349 I print_info: rope type        = 2
0.00.053.349 I print_info: rope scaling     = linear
0.00.053.349 I print_info: freq_base_train  = 10000.0
0.00.053.350 I print_info: freq_scale_train = 1
0.00.053.350 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.350 I print_info: rope_finetuned   = unknown
0.00.053.350 I print_info: ssm_d_conv       = 0
0.00.053.351 I print_info: ssm_d_inner      = 0
0.00.053.351 I print_info: ssm_d_state      = 0
0.00.053.351 I print_info: ssm_dt_rank      = 0
0.00.053.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.351 I print_info: model type       = 1.4B
0.00.053.352 I print_info: model params     = 1.41 B
0.00.053.352 I print_info: general.name     = 1.4B
0.00.053.353 I print_info: vocab type       = BPE
0.00.053.353 I print_info: n_vocab          = 50304
0.00.053.354 I print_info: n_merges         = 50009
0.00.053.354 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.354 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.354 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.355 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.355 I print_info: LF token         = 128 'Ä'
0.00.053.355 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.355 I print_info: max token length = 1024
0.00.055.341 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.341 I load_tensors: offloading output layer to GPU
0.00.055.341 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.351 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.353 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.635 I llama_init_from_model: n_seq_max     = 1
0.00.055.636 I llama_init_from_model: n_ctx         = 2048
0.00.055.636 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.636 I llama_init_from_model: n_batch       = 2048
0.00.055.636 I llama_init_from_model: n_ubatch      = 512
0.00.055.637 I llama_init_from_model: flash_attn    = 0
0.00.055.637 I llama_init_from_model: freq_base     = 10000.0
0.00.055.637 I llama_init_from_model: freq_scale    = 1
0.00.055.638 I ggml_metal_init: allocating
0.00.055.641 I ggml_metal_init: found device: Apple M4
0.00.055.643 I ggml_metal_init: picking default device: Apple M4
0.00.056.234 I ggml_metal_init: using embedded metal library
0.00.058.606 I ggml_metal_init: GPU name:   Apple M4
0.00.058.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.609 I ggml_metal_init: simdgroup reduction   = true
0.00.058.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.609 I ggml_metal_init: has bfloat            = true
0.00.058.609 I ggml_metal_init: use bfloat            = true
0.00.058.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.501 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.681 I init:      Metal KV buffer size =   384.00 MiB
0.00.089.690 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.734 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.752 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.754 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.754 I llama_init_from_model: graph nodes  = 967
0.00.090.754 I llama_init_from_model: graph splits = 2
0.00.090.757 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.625 I main: llama threadpool init, n_threads = 4
0.00.784.657 I 
0.00.784.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.679 I 
0.00.784.909 I sampler seed: 1234
0.00.784.914 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.964 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.966 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.641.454 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.641.454 I llama_perf_context_print:        load time =     772.07 ms
0.01.641.455 I llama_perf_context_print: prompt eval time =      51.52 ms /     7 tokens (    7.36 ms per token,   135.86 tokens per second)
0.01.641.456 I llama_perf_context_print:        eval time =     802.13 ms /    63 runs   (   12.73 ms per token,    78.54 tokens per second)
0.01.641.456 I llama_perf_context_print:       total time =     856.83 ms /    70 tokens
0.01.641.767 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.110s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.174 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.948 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.939 I llama_model_loader: - type  f32:  194 tensors
0.00.025.939 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.940 I print_info: file format = GGUF V3 (latest)
0.00.025.940 I print_info: file type   = Q6_K
0.00.025.941 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.053 I load: special tokens cache size = 25
0.00.052.080 I load: token to piece cache size = 0.2984 MB
0.00.052.084 I print_info: arch             = gptneox
0.00.052.084 I print_info: vocab_only       = 0
0.00.052.085 I print_info: n_ctx_train      = 2048
0.00.052.085 I print_info: n_embd           = 2048
0.00.052.085 I print_info: n_layer          = 24
0.00.052.090 I print_info: n_head           = 16
0.00.052.093 I print_info: n_head_kv        = 16
0.00.052.094 I print_info: n_rot            = 32
0.00.052.094 I print_info: n_swa            = 0
0.00.052.094 I print_info: n_embd_head_k    = 128
0.00.052.094 I print_info: n_embd_head_v    = 128
0.00.052.095 I print_info: n_gqa            = 1
0.00.052.095 I print_info: n_embd_k_gqa     = 2048
0.00.052.096 I print_info: n_embd_v_gqa     = 2048
0.00.052.097 I print_info: f_norm_eps       = 1.0e-05
0.00.052.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.099 I print_info: f_logit_scale    = 0.0e+00
0.00.052.103 I print_info: n_ff             = 8192
0.00.052.103 I print_info: n_expert         = 0
0.00.052.104 I print_info: n_expert_used    = 0
0.00.052.105 I print_info: causal attn      = 1
0.00.052.105 I print_info: pooling type     = 0
0.00.052.105 I print_info: rope type        = 2
0.00.052.105 I print_info: rope scaling     = linear
0.00.052.105 I print_info: freq_base_train  = 10000.0
0.00.052.106 I print_info: freq_scale_train = 1
0.00.052.106 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.106 I print_info: rope_finetuned   = unknown
0.00.052.106 I print_info: ssm_d_conv       = 0
0.00.052.107 I print_info: ssm_d_inner      = 0
0.00.052.107 I print_info: ssm_d_state      = 0
0.00.052.107 I print_info: ssm_dt_rank      = 0
0.00.052.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.107 I print_info: model type       = 1.4B
0.00.052.108 I print_info: model params     = 1.41 B
0.00.052.108 I print_info: general.name     = 1.4B
0.00.052.108 I print_info: vocab type       = BPE
0.00.052.109 I print_info: n_vocab          = 50304
0.00.052.109 I print_info: n_merges         = 50009
0.00.052.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.109 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.109 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.109 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.110 I print_info: LF token         = 128 'Ä'
0.00.052.110 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.110 I print_info: max token length = 1024
0.00.054.148 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.148 I load_tensors: offloading output layer to GPU
0.00.054.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.159 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.160 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.458 I llama_init_from_model: n_seq_max     = 1
0.00.054.459 I llama_init_from_model: n_ctx         = 2048
0.00.054.459 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.460 I llama_init_from_model: n_batch       = 2048
0.00.054.460 I llama_init_from_model: n_ubatch      = 512
0.00.054.460 I llama_init_from_model: flash_attn    = 0
0.00.054.460 I llama_init_from_model: freq_base     = 10000.0
0.00.054.461 I llama_init_from_model: freq_scale    = 1
0.00.054.461 I ggml_metal_init: allocating
0.00.054.465 I ggml_metal_init: found device: Apple M4
0.00.054.467 I ggml_metal_init: picking default device: Apple M4
0.00.055.097 I ggml_metal_init: using embedded metal library
0.00.057.646 I ggml_metal_init: GPU name:   Apple M4
0.00.057.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.649 I ggml_metal_init: simdgroup reduction   = true
0.00.057.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.649 I ggml_metal_init: has bfloat            = true
0.00.057.649 I ggml_metal_init: use bfloat            = true
0.00.057.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.984 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.813 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.824 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.880 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.881 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.881 I llama_init_from_model: graph nodes  = 967
0.00.089.882 I llama_init_from_model: graph splits = 2
0.00.089.885 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.015 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.213.233 I main: llama threadpool init, n_threads = 4
0.01.213.284 I 
0.01.213.304 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.213.304 I 
0.01.213.553 I sampler seed: 1234
0.01.213.560 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.213.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.213.571 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.213.571 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.02.108.823 I llama_perf_sampler_print:    sampling time =       1.78 ms /    71 runs   (    0.03 ms per token, 39820.53 tokens per second)
0.02.108.824 I llama_perf_context_print:        load time =    1204.05 ms
0.02.108.825 I llama_perf_context_print: prompt eval time =      54.30 ms /     7 tokens (    7.76 ms per token,   128.92 tokens per second)
0.02.108.827 I llama_perf_context_print:        eval time =     837.20 ms /    63 runs   (   13.29 ms per token,    75.25 tokens per second)
0.02.108.827 I llama_perf_context_print:       total time =     895.60 ms /    70 tokens
0.02.109.071 I ggml_metal_free: deallocating

real	0m2.128s
user	0m0.124s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.541 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.076 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.921 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.939 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.573 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.575 I llama_model_loader: - type  f32:  194 tensors
0.00.053.576 I llama_model_loader: - type  f16:   98 tensors
0.00.053.576 I print_info: file format = GGUF V3 (latest)
0.00.053.577 I print_info: file type   = all F32 (guessed)
0.00.053.578 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.021 I load: special tokens cache size = 25
0.00.085.597 I load: token to piece cache size = 0.2984 MB
0.00.085.600 I print_info: arch             = gptneox
0.00.085.600 I print_info: vocab_only       = 0
0.00.085.601 I print_info: n_ctx_train      = 2048
0.00.085.601 I print_info: n_embd           = 2048
0.00.085.601 I print_info: n_layer          = 24
0.00.085.604 I print_info: n_head           = 16
0.00.085.605 I print_info: n_head_kv        = 16
0.00.085.607 I print_info: n_rot            = 32
0.00.085.607 I print_info: n_swa            = 0
0.00.085.607 I print_info: n_embd_head_k    = 128
0.00.085.607 I print_info: n_embd_head_v    = 128
0.00.085.608 I print_info: n_gqa            = 1
0.00.085.609 I print_info: n_embd_k_gqa     = 2048
0.00.085.609 I print_info: n_embd_v_gqa     = 2048
0.00.085.610 I print_info: f_norm_eps       = 1.0e-05
0.00.085.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.610 I print_info: f_logit_scale    = 0.0e+00
0.00.085.611 I print_info: n_ff             = 8192
0.00.085.611 I print_info: n_expert         = 0
0.00.085.611 I print_info: n_expert_used    = 0
0.00.085.611 I print_info: causal attn      = 1
0.00.085.612 I print_info: pooling type     = 0
0.00.085.612 I print_info: rope type        = 2
0.00.085.612 I print_info: rope scaling     = linear
0.00.085.612 I print_info: freq_base_train  = 10000.0
0.00.085.613 I print_info: freq_scale_train = 1
0.00.085.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.613 I print_info: rope_finetuned   = unknown
0.00.085.613 I print_info: ssm_d_conv       = 0
0.00.085.613 I print_info: ssm_d_inner      = 0
0.00.085.613 I print_info: ssm_d_state      = 0
0.00.085.614 I print_info: ssm_dt_rank      = 0
0.00.085.614 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.614 I print_info: model type       = 1.4B
0.00.085.614 I print_info: model params     = 1.41 B
0.00.085.614 I print_info: general.name     = 1.4B
0.00.085.615 I print_info: vocab type       = BPE
0.00.085.615 I print_info: n_vocab          = 50304
0.00.085.615 I print_info: n_merges         = 50009
0.00.085.616 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.616 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.616 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.616 I print_info: LF token         = 128 'Ä'
0.00.085.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.617 I print_info: max token length = 1024
0.00.088.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.133 I load_tensors: offloading output layer to GPU
0.00.088.133 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.143 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.145 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.439 I llama_init_from_model: n_seq_max     = 1
0.00.088.440 I llama_init_from_model: n_ctx         = 128
0.00.088.440 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.440 I llama_init_from_model: n_batch       = 128
0.00.088.440 I llama_init_from_model: n_ubatch      = 128
0.00.088.441 I llama_init_from_model: flash_attn    = 0
0.00.088.441 I llama_init_from_model: freq_base     = 10000.0
0.00.088.441 I llama_init_from_model: freq_scale    = 1
0.00.088.442 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.442 I ggml_metal_init: allocating
0.00.088.444 I ggml_metal_init: found device: Apple M4
0.00.088.447 I ggml_metal_init: picking default device: Apple M4
0.00.089.046 I ggml_metal_init: using embedded metal library
0.00.091.591 I ggml_metal_init: GPU name:   Apple M4
0.00.091.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.593 I ggml_metal_init: simdgroup reduction   = true
0.00.091.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.594 I ggml_metal_init: has bfloat            = true
0.00.091.594 I ggml_metal_init: use bfloat            = true
0.00.091.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.226 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.527 I init:      Metal KV buffer size =    24.00 MiB
0.00.102.529 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.555 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.421 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.423 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.423 I llama_init_from_model: graph nodes  = 967
0.00.103.423 I llama_init_from_model: graph splits = 2
0.00.103.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.024.148 I 
0.01.024.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.024.243 I perplexity: tokenizing the input ..
0.01.037.664 I perplexity: tokenization took 13.418 ms
0.01.037.672 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.173.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.174.983 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.175.038 I llama_perf_context_print:        load time =    1001.06 ms
0.01.175.039 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.39 tokens per second)
0.01.175.041 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.175.041 I llama_perf_context_print:       total time =     150.89 ms /   129 tokens
0.01.175.816 I ggml_metal_free: deallocating

real	0m1.370s
user	0m0.120s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.116 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.294 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.297 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.298 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.940 I llama_model_loader: - type  f32:  194 tensors
0.00.034.941 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.942 I print_info: file format = GGUF V3 (latest)
0.00.034.942 I print_info: file type   = Q8_0
0.00.034.943 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.651 I load: special tokens cache size = 25
0.00.063.994 I load: token to piece cache size = 0.2984 MB
0.00.063.997 I print_info: arch             = gptneox
0.00.063.997 I print_info: vocab_only       = 0
0.00.063.997 I print_info: n_ctx_train      = 2048
0.00.063.997 I print_info: n_embd           = 2048
0.00.063.997 I print_info: n_layer          = 24
0.00.064.001 I print_info: n_head           = 16
0.00.064.003 I print_info: n_head_kv        = 16
0.00.064.004 I print_info: n_rot            = 32
0.00.064.004 I print_info: n_swa            = 0
0.00.064.004 I print_info: n_embd_head_k    = 128
0.00.064.004 I print_info: n_embd_head_v    = 128
0.00.064.005 I print_info: n_gqa            = 1
0.00.064.006 I print_info: n_embd_k_gqa     = 2048
0.00.064.006 I print_info: n_embd_v_gqa     = 2048
0.00.064.007 I print_info: f_norm_eps       = 1.0e-05
0.00.064.007 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.007 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.007 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.008 I print_info: f_logit_scale    = 0.0e+00
0.00.064.008 I print_info: n_ff             = 8192
0.00.064.008 I print_info: n_expert         = 0
0.00.064.009 I print_info: n_expert_used    = 0
0.00.064.009 I print_info: causal attn      = 1
0.00.064.009 I print_info: pooling type     = 0
0.00.064.009 I print_info: rope type        = 2
0.00.064.009 I print_info: rope scaling     = linear
0.00.064.010 I print_info: freq_base_train  = 10000.0
0.00.064.010 I print_info: freq_scale_train = 1
0.00.064.010 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.011 I print_info: rope_finetuned   = unknown
0.00.064.011 I print_info: ssm_d_conv       = 0
0.00.064.011 I print_info: ssm_d_inner      = 0
0.00.064.011 I print_info: ssm_d_state      = 0
0.00.064.011 I print_info: ssm_dt_rank      = 0
0.00.064.011 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.013 I print_info: model type       = 1.4B
0.00.064.014 I print_info: model params     = 1.41 B
0.00.064.014 I print_info: general.name     = 1.4B
0.00.064.014 I print_info: vocab type       = BPE
0.00.064.015 I print_info: n_vocab          = 50304
0.00.064.015 I print_info: n_merges         = 50009
0.00.064.015 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.015 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.015 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.017 I print_info: LF token         = 128 'Ä'
0.00.064.018 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.018 I print_info: max token length = 1024
0.00.066.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.339 I load_tensors: offloading output layer to GPU
0.00.066.339 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.350 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.352 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.748 I llama_init_from_model: n_seq_max     = 1
0.00.066.749 I llama_init_from_model: n_ctx         = 128
0.00.066.749 I llama_init_from_model: n_ctx_per_seq = 128
0.00.066.749 I llama_init_from_model: n_batch       = 128
0.00.066.749 I llama_init_from_model: n_ubatch      = 128
0.00.066.750 I llama_init_from_model: flash_attn    = 0
0.00.066.750 I llama_init_from_model: freq_base     = 10000.0
0.00.066.750 I llama_init_from_model: freq_scale    = 1
0.00.066.751 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.751 I ggml_metal_init: allocating
0.00.066.754 I ggml_metal_init: found device: Apple M4
0.00.066.756 I ggml_metal_init: picking default device: Apple M4
0.00.067.425 I ggml_metal_init: using embedded metal library
0.00.070.051 I ggml_metal_init: GPU name:   Apple M4
0.00.070.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.054 I ggml_metal_init: simdgroup reduction   = true
0.00.070.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.054 I ggml_metal_init: has bfloat            = true
0.00.070.054 I ggml_metal_init: use bfloat            = true
0.00.070.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.898 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.337 I init:      Metal KV buffer size =    24.00 MiB
0.00.081.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.479 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.082.480 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.082.480 I llama_init_from_model: graph nodes  = 967
0.00.082.480 I llama_init_from_model: graph splits = 2
0.00.082.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.449 I 
0.00.805.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.502 I perplexity: tokenizing the input ..
0.00.813.612 I perplexity: tokenization took 8.109 ms
0.00.813.615 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.937.766 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.938.907 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.938.929 I llama_perf_context_print:        load time =     793.33 ms
0.00.938.930 I llama_perf_context_print: prompt eval time =     123.93 ms /   128 tokens (    0.97 ms per token,  1032.87 tokens per second)
0.00.938.931 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.938.931 I llama_perf_context_print:       total time =     133.48 ms /   129 tokens
0.00.939.287 I ggml_metal_free: deallocating

real	0m0.958s
user	0m0.092s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.923 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.925 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.925 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.926 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.926 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.926 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.928 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.931 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.931 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.932 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.861 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.789 I llama_model_loader: - type  f32:  194 tensors
0.00.024.789 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.789 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.790 I print_info: file format = GGUF V3 (latest)
0.00.024.790 I print_info: file type   = Q4_0
0.00.024.791 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.099 I load: special tokens cache size = 25
0.00.050.079 I load: token to piece cache size = 0.2984 MB
0.00.050.084 I print_info: arch             = gptneox
0.00.050.084 I print_info: vocab_only       = 0
0.00.050.084 I print_info: n_ctx_train      = 2048
0.00.050.084 I print_info: n_embd           = 2048
0.00.050.084 I print_info: n_layer          = 24
0.00.050.088 I print_info: n_head           = 16
0.00.050.088 I print_info: n_head_kv        = 16
0.00.050.089 I print_info: n_rot            = 32
0.00.050.089 I print_info: n_swa            = 0
0.00.050.089 I print_info: n_embd_head_k    = 128
0.00.050.089 I print_info: n_embd_head_v    = 128
0.00.050.090 I print_info: n_gqa            = 1
0.00.050.090 I print_info: n_embd_k_gqa     = 2048
0.00.050.091 I print_info: n_embd_v_gqa     = 2048
0.00.050.092 I print_info: f_norm_eps       = 1.0e-05
0.00.050.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.092 I print_info: f_logit_scale    = 0.0e+00
0.00.050.093 I print_info: n_ff             = 8192
0.00.050.095 I print_info: n_expert         = 0
0.00.050.095 I print_info: n_expert_used    = 0
0.00.050.095 I print_info: causal attn      = 1
0.00.050.095 I print_info: pooling type     = 0
0.00.050.096 I print_info: rope type        = 2
0.00.050.096 I print_info: rope scaling     = linear
0.00.050.096 I print_info: freq_base_train  = 10000.0
0.00.050.097 I print_info: freq_scale_train = 1
0.00.050.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.097 I print_info: rope_finetuned   = unknown
0.00.050.097 I print_info: ssm_d_conv       = 0
0.00.050.097 I print_info: ssm_d_inner      = 0
0.00.050.098 I print_info: ssm_d_state      = 0
0.00.050.098 I print_info: ssm_dt_rank      = 0
0.00.050.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.098 I print_info: model type       = 1.4B
0.00.050.098 I print_info: model params     = 1.41 B
0.00.050.099 I print_info: general.name     = 1.4B
0.00.050.099 I print_info: vocab type       = BPE
0.00.050.099 I print_info: n_vocab          = 50304
0.00.050.100 I print_info: n_merges         = 50009
0.00.050.100 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: LF token         = 128 'Ä'
0.00.050.101 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.101 I print_info: max token length = 1024
0.00.051.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.881 I load_tensors: offloading output layer to GPU
0.00.051.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.892 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.893 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.176 I llama_init_from_model: n_seq_max     = 1
0.00.052.177 I llama_init_from_model: n_ctx         = 128
0.00.052.177 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.177 I llama_init_from_model: n_batch       = 128
0.00.052.177 I llama_init_from_model: n_ubatch      = 128
0.00.052.178 I llama_init_from_model: flash_attn    = 0
0.00.052.178 I llama_init_from_model: freq_base     = 10000.0
0.00.052.178 I llama_init_from_model: freq_scale    = 1
0.00.052.179 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.179 I ggml_metal_init: allocating
0.00.052.183 I ggml_metal_init: found device: Apple M4
0.00.052.185 I ggml_metal_init: picking default device: Apple M4
0.00.052.762 I ggml_metal_init: using embedded metal library
0.00.055.076 I ggml_metal_init: GPU name:   Apple M4
0.00.055.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.078 I ggml_metal_init: simdgroup reduction   = true
0.00.055.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.079 I ggml_metal_init: has bfloat            = true
0.00.055.079 I ggml_metal_init: use bfloat            = true
0.00.055.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.935 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.201 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.204 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.227 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.144 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.145 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.146 I llama_init_from_model: graph nodes  = 967
0.00.067.146 I llama_init_from_model: graph splits = 2
0.00.067.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.016 I 
0.00.601.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.085 I perplexity: tokenizing the input ..
0.00.608.891 I perplexity: tokenization took 7.804 ms
0.00.608.895 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.876 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.129 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.157 I llama_perf_context_print:        load time =     592.15 ms
0.00.733.158 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.02 tokens per second)
0.00.733.159 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.159 I llama_perf_context_print:       total time =     132.15 ms /   129 tokens
0.00.733.671 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.078s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.304 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.249 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.138 I llama_model_loader: - type  f32:  194 tensors
0.00.025.138 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.139 I print_info: file format = GGUF V3 (latest)
0.00.025.140 I print_info: file type   = Q4_1
0.00.025.140 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.211 I load: special tokens cache size = 25
0.00.050.064 I load: token to piece cache size = 0.2984 MB
0.00.050.067 I print_info: arch             = gptneox
0.00.050.067 I print_info: vocab_only       = 0
0.00.050.067 I print_info: n_ctx_train      = 2048
0.00.050.067 I print_info: n_embd           = 2048
0.00.050.067 I print_info: n_layer          = 24
0.00.050.070 I print_info: n_head           = 16
0.00.050.070 I print_info: n_head_kv        = 16
0.00.050.071 I print_info: n_rot            = 32
0.00.050.071 I print_info: n_swa            = 0
0.00.050.072 I print_info: n_embd_head_k    = 128
0.00.050.072 I print_info: n_embd_head_v    = 128
0.00.050.074 I print_info: n_gqa            = 1
0.00.050.075 I print_info: n_embd_k_gqa     = 2048
0.00.050.076 I print_info: n_embd_v_gqa     = 2048
0.00.050.076 I print_info: f_norm_eps       = 1.0e-05
0.00.050.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.081 I print_info: f_logit_scale    = 0.0e+00
0.00.050.082 I print_info: n_ff             = 8192
0.00.050.082 I print_info: n_expert         = 0
0.00.050.083 I print_info: n_expert_used    = 0
0.00.050.083 I print_info: causal attn      = 1
0.00.050.083 I print_info: pooling type     = 0
0.00.050.083 I print_info: rope type        = 2
0.00.050.083 I print_info: rope scaling     = linear
0.00.050.084 I print_info: freq_base_train  = 10000.0
0.00.050.084 I print_info: freq_scale_train = 1
0.00.050.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.085 I print_info: rope_finetuned   = unknown
0.00.050.085 I print_info: ssm_d_conv       = 0
0.00.050.085 I print_info: ssm_d_inner      = 0
0.00.050.085 I print_info: ssm_d_state      = 0
0.00.050.085 I print_info: ssm_dt_rank      = 0
0.00.050.085 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.086 I print_info: model type       = 1.4B
0.00.050.087 I print_info: model params     = 1.41 B
0.00.050.087 I print_info: general.name     = 1.4B
0.00.050.088 I print_info: vocab type       = BPE
0.00.050.088 I print_info: n_vocab          = 50304
0.00.050.088 I print_info: n_merges         = 50009
0.00.050.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.089 I print_info: LF token         = 128 'Ä'
0.00.050.089 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.090 I print_info: max token length = 1024
0.00.052.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.000 I load_tensors: offloading output layer to GPU
0.00.052.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.005 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.006 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.301 I llama_init_from_model: n_seq_max     = 1
0.00.052.302 I llama_init_from_model: n_ctx         = 128
0.00.052.303 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.303 I llama_init_from_model: n_batch       = 128
0.00.052.303 I llama_init_from_model: n_ubatch      = 128
0.00.052.303 I llama_init_from_model: flash_attn    = 0
0.00.052.303 I llama_init_from_model: freq_base     = 10000.0
0.00.052.304 I llama_init_from_model: freq_scale    = 1
0.00.052.304 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.305 I ggml_metal_init: allocating
0.00.052.308 I ggml_metal_init: found device: Apple M4
0.00.052.310 I ggml_metal_init: picking default device: Apple M4
0.00.052.883 I ggml_metal_init: using embedded metal library
0.00.055.249 I ggml_metal_init: GPU name:   Apple M4
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.251 I ggml_metal_init: simdgroup reduction   = true
0.00.055.251 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.251 I ggml_metal_init: has bfloat            = true
0.00.055.252 I ggml_metal_init: use bfloat            = true
0.00.055.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.038 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.364 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.368 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.388 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.314 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.315 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.315 I llama_init_from_model: graph nodes  = 967
0.00.067.316 I llama_init_from_model: graph splits = 2
0.00.067.317 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.834 I 
0.00.663.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.934 I perplexity: tokenizing the input ..
0.00.671.661 I perplexity: tokenization took 7.724 ms
0.00.671.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.614 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.795.830 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.795.863 I llama_perf_context_print:        load time =     654.93 ms
0.00.795.864 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.02 tokens per second)
0.00.795.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.865 I llama_perf_context_print:       total time =     132.04 ms /   129 tokens
0.00.796.315 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.078s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.051 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.189 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.199 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.199 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.001 I llama_model_loader: - type  f32:  194 tensors
0.00.026.002 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.002 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.002 I print_info: file format = GGUF V3 (latest)
0.00.026.003 I print_info: file type   = Q5_0
0.00.026.004 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.718 I load: special tokens cache size = 25
0.00.051.960 I load: token to piece cache size = 0.2984 MB
0.00.051.963 I print_info: arch             = gptneox
0.00.051.963 I print_info: vocab_only       = 0
0.00.051.964 I print_info: n_ctx_train      = 2048
0.00.051.964 I print_info: n_embd           = 2048
0.00.051.964 I print_info: n_layer          = 24
0.00.051.967 I print_info: n_head           = 16
0.00.051.968 I print_info: n_head_kv        = 16
0.00.051.968 I print_info: n_rot            = 32
0.00.051.968 I print_info: n_swa            = 0
0.00.051.969 I print_info: n_embd_head_k    = 128
0.00.051.969 I print_info: n_embd_head_v    = 128
0.00.051.972 I print_info: n_gqa            = 1
0.00.051.973 I print_info: n_embd_k_gqa     = 2048
0.00.051.974 I print_info: n_embd_v_gqa     = 2048
0.00.051.974 I print_info: f_norm_eps       = 1.0e-05
0.00.051.975 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.975 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.975 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.975 I print_info: f_logit_scale    = 0.0e+00
0.00.051.976 I print_info: n_ff             = 8192
0.00.051.976 I print_info: n_expert         = 0
0.00.051.977 I print_info: n_expert_used    = 0
0.00.051.977 I print_info: causal attn      = 1
0.00.051.977 I print_info: pooling type     = 0
0.00.051.977 I print_info: rope type        = 2
0.00.051.977 I print_info: rope scaling     = linear
0.00.051.978 I print_info: freq_base_train  = 10000.0
0.00.051.979 I print_info: freq_scale_train = 1
0.00.051.980 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.980 I print_info: rope_finetuned   = unknown
0.00.051.980 I print_info: ssm_d_conv       = 0
0.00.051.980 I print_info: ssm_d_inner      = 0
0.00.051.980 I print_info: ssm_d_state      = 0
0.00.051.980 I print_info: ssm_dt_rank      = 0
0.00.051.981 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.981 I print_info: model type       = 1.4B
0.00.051.981 I print_info: model params     = 1.41 B
0.00.051.981 I print_info: general.name     = 1.4B
0.00.051.986 I print_info: vocab type       = BPE
0.00.051.986 I print_info: n_vocab          = 50304
0.00.051.986 I print_info: n_merges         = 50009
0.00.051.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.988 I print_info: LF token         = 128 'Ä'
0.00.051.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.989 I print_info: max token length = 1024
0.00.054.034 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.034 I load_tensors: offloading output layer to GPU
0.00.054.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.045 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.046 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.343 I llama_init_from_model: n_seq_max     = 1
0.00.054.344 I llama_init_from_model: n_ctx         = 128
0.00.054.344 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.344 I llama_init_from_model: n_batch       = 128
0.00.054.344 I llama_init_from_model: n_ubatch      = 128
0.00.054.344 I llama_init_from_model: flash_attn    = 0
0.00.054.345 I llama_init_from_model: freq_base     = 10000.0
0.00.054.345 I llama_init_from_model: freq_scale    = 1
0.00.054.345 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.346 I ggml_metal_init: allocating
0.00.054.349 I ggml_metal_init: found device: Apple M4
0.00.054.351 I ggml_metal_init: picking default device: Apple M4
0.00.054.927 I ggml_metal_init: using embedded metal library
0.00.057.364 I ggml_metal_init: GPU name:   Apple M4
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.367 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.367 I ggml_metal_init: simdgroup reduction   = true
0.00.057.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.367 I ggml_metal_init: has bfloat            = true
0.00.057.367 I ggml_metal_init: use bfloat            = true
0.00.057.368 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.382 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.692 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.694 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.726 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.667 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.668 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.669 I llama_init_from_model: graph nodes  = 967
0.00.069.669 I llama_init_from_model: graph splits = 2
0.00.069.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.792 I 
0.00.709.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.854 I perplexity: tokenizing the input ..
0.00.717.925 I perplexity: tokenization took 8.069 ms
0.00.717.928 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.273 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.853.448 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.853.474 I llama_perf_context_print:        load time =     699.73 ms
0.00.853.475 I llama_perf_context_print: prompt eval time =     134.12 ms /   128 tokens (    1.05 ms per token,   954.39 tokens per second)
0.00.853.476 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.476 I llama_perf_context_print:       total time =     143.69 ms /   129 tokens
0.00.854.008 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.896 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.205 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.205 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.208 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.208 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.167 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.183 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.183 I llama_model_loader: - type  f32:  194 tensors
0.00.025.183 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.184 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.184 I print_info: file format = GGUF V3 (latest)
0.00.025.185 I print_info: file type   = Q5_1
0.00.025.190 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.181 I load: special tokens cache size = 25
0.00.050.174 I load: token to piece cache size = 0.2984 MB
0.00.050.177 I print_info: arch             = gptneox
0.00.050.177 I print_info: vocab_only       = 0
0.00.050.177 I print_info: n_ctx_train      = 2048
0.00.050.177 I print_info: n_embd           = 2048
0.00.050.177 I print_info: n_layer          = 24
0.00.050.180 I print_info: n_head           = 16
0.00.050.181 I print_info: n_head_kv        = 16
0.00.050.181 I print_info: n_rot            = 32
0.00.050.181 I print_info: n_swa            = 0
0.00.050.182 I print_info: n_embd_head_k    = 128
0.00.050.182 I print_info: n_embd_head_v    = 128
0.00.050.182 I print_info: n_gqa            = 1
0.00.050.183 I print_info: n_embd_k_gqa     = 2048
0.00.050.186 I print_info: n_embd_v_gqa     = 2048
0.00.050.187 I print_info: f_norm_eps       = 1.0e-05
0.00.050.187 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.187 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.188 I print_info: f_logit_scale    = 0.0e+00
0.00.050.189 I print_info: n_ff             = 8192
0.00.050.189 I print_info: n_expert         = 0
0.00.050.189 I print_info: n_expert_used    = 0
0.00.050.189 I print_info: causal attn      = 1
0.00.050.189 I print_info: pooling type     = 0
0.00.050.189 I print_info: rope type        = 2
0.00.050.190 I print_info: rope scaling     = linear
0.00.050.190 I print_info: freq_base_train  = 10000.0
0.00.050.190 I print_info: freq_scale_train = 1
0.00.050.190 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.191 I print_info: rope_finetuned   = unknown
0.00.050.191 I print_info: ssm_d_conv       = 0
0.00.050.191 I print_info: ssm_d_inner      = 0
0.00.050.193 I print_info: ssm_d_state      = 0
0.00.050.193 I print_info: ssm_dt_rank      = 0
0.00.050.193 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.193 I print_info: model type       = 1.4B
0.00.050.194 I print_info: model params     = 1.41 B
0.00.050.194 I print_info: general.name     = 1.4B
0.00.050.194 I print_info: vocab type       = BPE
0.00.050.196 I print_info: n_vocab          = 50304
0.00.050.196 I print_info: n_merges         = 50009
0.00.050.196 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.197 I print_info: LF token         = 128 'Ä'
0.00.050.197 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.198 I print_info: max token length = 1024
0.00.052.210 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.210 I load_tensors: offloading output layer to GPU
0.00.052.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.221 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.222 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.501 I llama_init_from_model: n_seq_max     = 1
0.00.052.502 I llama_init_from_model: n_ctx         = 128
0.00.052.502 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.502 I llama_init_from_model: n_batch       = 128
0.00.052.502 I llama_init_from_model: n_ubatch      = 128
0.00.052.503 I llama_init_from_model: flash_attn    = 0
0.00.052.503 I llama_init_from_model: freq_base     = 10000.0
0.00.052.503 I llama_init_from_model: freq_scale    = 1
0.00.052.504 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.504 I ggml_metal_init: allocating
0.00.052.507 I ggml_metal_init: found device: Apple M4
0.00.052.509 I ggml_metal_init: picking default device: Apple M4
0.00.053.090 I ggml_metal_init: using embedded metal library
0.00.055.464 I ggml_metal_init: GPU name:   Apple M4
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.467 I ggml_metal_init: simdgroup reduction   = true
0.00.055.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.467 I ggml_metal_init: has bfloat            = true
0.00.055.467 I ggml_metal_init: use bfloat            = true
0.00.055.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.262 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.638 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.641 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.676 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.621 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.622 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.623 I llama_init_from_model: graph nodes  = 967
0.00.067.623 I llama_init_from_model: graph splits = 2
0.00.067.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.107 I 
0.00.740.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.152 I perplexity: tokenizing the input ..
0.00.748.018 I perplexity: tokenization took 7.864 ms
0.00.748.022 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.882.920 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.884.102 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.884.132 I llama_perf_context_print:        load time =     731.20 ms
0.00.884.133 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.46 tokens per second)
0.00.884.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.884.135 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.00.884.615 I ggml_metal_free: deallocating

real	0m0.900s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.979 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.920 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.921 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.925 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.931 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.931 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.892 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.769 I llama_model_loader: - type  f32:  194 tensors
0.00.025.769 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.769 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.770 I print_info: file format = GGUF V3 (latest)
0.00.025.771 I print_info: file type   = Q2_K - Medium
0.00.025.775 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.770 I load: special tokens cache size = 25
0.00.051.709 I load: token to piece cache size = 0.2984 MB
0.00.051.712 I print_info: arch             = gptneox
0.00.051.712 I print_info: vocab_only       = 0
0.00.051.713 I print_info: n_ctx_train      = 2048
0.00.051.713 I print_info: n_embd           = 2048
0.00.051.713 I print_info: n_layer          = 24
0.00.051.716 I print_info: n_head           = 16
0.00.051.716 I print_info: n_head_kv        = 16
0.00.051.716 I print_info: n_rot            = 32
0.00.051.717 I print_info: n_swa            = 0
0.00.051.717 I print_info: n_embd_head_k    = 128
0.00.051.719 I print_info: n_embd_head_v    = 128
0.00.051.720 I print_info: n_gqa            = 1
0.00.051.720 I print_info: n_embd_k_gqa     = 2048
0.00.051.721 I print_info: n_embd_v_gqa     = 2048
0.00.051.721 I print_info: f_norm_eps       = 1.0e-05
0.00.051.722 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.722 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.722 I print_info: f_logit_scale    = 0.0e+00
0.00.051.723 I print_info: n_ff             = 8192
0.00.051.724 I print_info: n_expert         = 0
0.00.051.724 I print_info: n_expert_used    = 0
0.00.051.724 I print_info: causal attn      = 1
0.00.051.724 I print_info: pooling type     = 0
0.00.051.724 I print_info: rope type        = 2
0.00.051.724 I print_info: rope scaling     = linear
0.00.051.726 I print_info: freq_base_train  = 10000.0
0.00.051.727 I print_info: freq_scale_train = 1
0.00.051.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.727 I print_info: rope_finetuned   = unknown
0.00.051.727 I print_info: ssm_d_conv       = 0
0.00.051.727 I print_info: ssm_d_inner      = 0
0.00.051.727 I print_info: ssm_d_state      = 0
0.00.051.728 I print_info: ssm_dt_rank      = 0
0.00.051.728 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.728 I print_info: model type       = 1.4B
0.00.051.728 I print_info: model params     = 1.41 B
0.00.051.728 I print_info: general.name     = 1.4B
0.00.051.729 I print_info: vocab type       = BPE
0.00.051.730 I print_info: n_vocab          = 50304
0.00.051.730 I print_info: n_merges         = 50009
0.00.051.730 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.730 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.730 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.730 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.731 I print_info: LF token         = 128 'Ä'
0.00.051.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.731 I print_info: max token length = 1024
0.00.053.379 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.379 I load_tensors: offloading output layer to GPU
0.00.053.379 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.389 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.391 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.675 I llama_init_from_model: n_seq_max     = 1
0.00.053.676 I llama_init_from_model: n_ctx         = 128
0.00.053.676 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.676 I llama_init_from_model: n_batch       = 128
0.00.053.676 I llama_init_from_model: n_ubatch      = 128
0.00.053.676 I llama_init_from_model: flash_attn    = 0
0.00.053.677 I llama_init_from_model: freq_base     = 10000.0
0.00.053.677 I llama_init_from_model: freq_scale    = 1
0.00.053.677 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.678 I ggml_metal_init: allocating
0.00.053.681 I ggml_metal_init: found device: Apple M4
0.00.053.683 I ggml_metal_init: picking default device: Apple M4
0.00.054.254 I ggml_metal_init: using embedded metal library
0.00.056.700 I ggml_metal_init: GPU name:   Apple M4
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.703 I ggml_metal_init: simdgroup reduction   = true
0.00.056.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.703 I ggml_metal_init: has bfloat            = true
0.00.056.703 I ggml_metal_init: use bfloat            = true
0.00.056.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.758 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.014 I init:      Metal KV buffer size =    24.00 MiB
0.00.068.019 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.046 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.889 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.890 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.890 I llama_init_from_model: graph nodes  = 967
0.00.068.890 I llama_init_from_model: graph splits = 2
0.00.068.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.399.386 I 
0.00.399.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.399.421 I perplexity: tokenizing the input ..
0.00.407.609 I perplexity: tokenization took 8.185 ms
0.00.407.613 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.540.421 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.541.603 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.541.633 I llama_perf_context_print:        load time =     389.40 ms
0.00.541.634 I llama_perf_context_print: prompt eval time =     132.57 ms /   128 tokens (    1.04 ms per token,   965.51 tokens per second)
0.00.541.635 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.541.635 I llama_perf_context_print:       total time =     142.25 ms /   129 tokens
0.00.542.179 I ggml_metal_free: deallocating

real	0m0.557s
user	0m0.079s
sys	0m0.072s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.902 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.143 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.145 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.146 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.146 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.091 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.929 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.930 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.930 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.931 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.931 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.931 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.932 I llama_model_loader: - type  f32:  194 tensors
0.00.024.932 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.932 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.933 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.933 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.933 I print_info: file format = GGUF V3 (latest)
0.00.024.934 I print_info: file type   = Q3_K - Medium
0.00.024.935 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.004 I load: special tokens cache size = 25
0.00.050.098 I load: token to piece cache size = 0.2984 MB
0.00.050.101 I print_info: arch             = gptneox
0.00.050.101 I print_info: vocab_only       = 0
0.00.050.101 I print_info: n_ctx_train      = 2048
0.00.050.101 I print_info: n_embd           = 2048
0.00.050.102 I print_info: n_layer          = 24
0.00.050.104 I print_info: n_head           = 16
0.00.050.105 I print_info: n_head_kv        = 16
0.00.050.105 I print_info: n_rot            = 32
0.00.050.106 I print_info: n_swa            = 0
0.00.050.106 I print_info: n_embd_head_k    = 128
0.00.050.106 I print_info: n_embd_head_v    = 128
0.00.050.107 I print_info: n_gqa            = 1
0.00.050.108 I print_info: n_embd_k_gqa     = 2048
0.00.050.108 I print_info: n_embd_v_gqa     = 2048
0.00.050.109 I print_info: f_norm_eps       = 1.0e-05
0.00.050.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.110 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.110 I print_info: f_logit_scale    = 0.0e+00
0.00.050.111 I print_info: n_ff             = 8192
0.00.050.111 I print_info: n_expert         = 0
0.00.050.111 I print_info: n_expert_used    = 0
0.00.050.111 I print_info: causal attn      = 1
0.00.050.111 I print_info: pooling type     = 0
0.00.050.112 I print_info: rope type        = 2
0.00.050.112 I print_info: rope scaling     = linear
0.00.050.112 I print_info: freq_base_train  = 10000.0
0.00.050.113 I print_info: freq_scale_train = 1
0.00.050.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.116 I print_info: rope_finetuned   = unknown
0.00.050.116 I print_info: ssm_d_conv       = 0
0.00.050.116 I print_info: ssm_d_inner      = 0
0.00.050.116 I print_info: ssm_d_state      = 0
0.00.050.116 I print_info: ssm_dt_rank      = 0
0.00.050.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.117 I print_info: model type       = 1.4B
0.00.050.117 I print_info: model params     = 1.41 B
0.00.050.117 I print_info: general.name     = 1.4B
0.00.050.118 I print_info: vocab type       = BPE
0.00.050.118 I print_info: n_vocab          = 50304
0.00.050.118 I print_info: n_merges         = 50009
0.00.050.118 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.122 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.123 I print_info: LF token         = 128 'Ä'
0.00.050.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.124 I print_info: max token length = 1024
0.00.052.089 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.089 I load_tensors: offloading output layer to GPU
0.00.052.089 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.099 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.101 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.390 I llama_init_from_model: n_seq_max     = 1
0.00.052.391 I llama_init_from_model: n_ctx         = 128
0.00.052.391 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.391 I llama_init_from_model: n_batch       = 128
0.00.052.391 I llama_init_from_model: n_ubatch      = 128
0.00.052.392 I llama_init_from_model: flash_attn    = 0
0.00.052.392 I llama_init_from_model: freq_base     = 10000.0
0.00.052.392 I llama_init_from_model: freq_scale    = 1
0.00.052.393 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.393 I ggml_metal_init: allocating
0.00.052.396 I ggml_metal_init: found device: Apple M4
0.00.052.397 I ggml_metal_init: picking default device: Apple M4
0.00.052.972 I ggml_metal_init: using embedded metal library
0.00.055.324 I ggml_metal_init: GPU name:   Apple M4
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.326 I ggml_metal_init: simdgroup reduction   = true
0.00.055.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.326 I ggml_metal_init: has bfloat            = true
0.00.055.326 I ggml_metal_init: use bfloat            = true
0.00.055.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.195 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.472 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.476 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.379 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.380 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.380 I llama_init_from_model: graph nodes  = 967
0.00.067.381 I llama_init_from_model: graph splits = 2
0.00.067.382 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.601 I 
0.00.473.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.649 I perplexity: tokenizing the input ..
0.00.481.675 I perplexity: tokenization took 8.025 ms
0.00.481.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.990 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.134 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.159 I llama_perf_context_print:        load time =     464.69 ms
0.00.615.160 I llama_perf_context_print: prompt eval time =     132.08 ms /   128 tokens (    1.03 ms per token,   969.08 tokens per second)
0.00.615.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.161 I llama_perf_context_print:       total time =     141.56 ms /   129 tokens
0.00.615.632 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.078s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.906 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.560 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.560 I llama_model_loader: - type  f32:  194 tensors
0.00.024.560 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.561 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.561 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.561 I print_info: file format = GGUF V3 (latest)
0.00.024.562 I print_info: file type   = Q4_K - Medium
0.00.024.563 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.884 I load: special tokens cache size = 25
0.00.049.923 I load: token to piece cache size = 0.2984 MB
0.00.049.926 I print_info: arch             = gptneox
0.00.049.927 I print_info: vocab_only       = 0
0.00.049.927 I print_info: n_ctx_train      = 2048
0.00.049.927 I print_info: n_embd           = 2048
0.00.049.927 I print_info: n_layer          = 24
0.00.049.930 I print_info: n_head           = 16
0.00.049.931 I print_info: n_head_kv        = 16
0.00.049.931 I print_info: n_rot            = 32
0.00.049.932 I print_info: n_swa            = 0
0.00.049.932 I print_info: n_embd_head_k    = 128
0.00.049.933 I print_info: n_embd_head_v    = 128
0.00.049.934 I print_info: n_gqa            = 1
0.00.049.935 I print_info: n_embd_k_gqa     = 2048
0.00.049.936 I print_info: n_embd_v_gqa     = 2048
0.00.049.936 I print_info: f_norm_eps       = 1.0e-05
0.00.049.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.938 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.939 I print_info: f_logit_scale    = 0.0e+00
0.00.049.939 I print_info: n_ff             = 8192
0.00.049.939 I print_info: n_expert         = 0
0.00.049.940 I print_info: n_expert_used    = 0
0.00.049.940 I print_info: causal attn      = 1
0.00.049.940 I print_info: pooling type     = 0
0.00.049.940 I print_info: rope type        = 2
0.00.049.940 I print_info: rope scaling     = linear
0.00.049.941 I print_info: freq_base_train  = 10000.0
0.00.049.941 I print_info: freq_scale_train = 1
0.00.049.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.941 I print_info: rope_finetuned   = unknown
0.00.049.942 I print_info: ssm_d_conv       = 0
0.00.049.942 I print_info: ssm_d_inner      = 0
0.00.049.942 I print_info: ssm_d_state      = 0
0.00.049.942 I print_info: ssm_dt_rank      = 0
0.00.049.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.942 I print_info: model type       = 1.4B
0.00.049.947 I print_info: model params     = 1.41 B
0.00.049.947 I print_info: general.name     = 1.4B
0.00.049.948 I print_info: vocab type       = BPE
0.00.049.948 I print_info: n_vocab          = 50304
0.00.049.948 I print_info: n_merges         = 50009
0.00.049.948 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.948 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.948 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.949 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.949 I print_info: LF token         = 128 'Ä'
0.00.049.949 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.949 I print_info: max token length = 1024
0.00.051.938 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.938 I load_tensors: offloading output layer to GPU
0.00.051.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.949 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.950 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.239 I llama_init_from_model: n_seq_max     = 1
0.00.052.240 I llama_init_from_model: n_ctx         = 128
0.00.052.240 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.240 I llama_init_from_model: n_batch       = 128
0.00.052.240 I llama_init_from_model: n_ubatch      = 128
0.00.052.241 I llama_init_from_model: flash_attn    = 0
0.00.052.241 I llama_init_from_model: freq_base     = 10000.0
0.00.052.241 I llama_init_from_model: freq_scale    = 1
0.00.052.242 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.242 I ggml_metal_init: allocating
0.00.052.245 I ggml_metal_init: found device: Apple M4
0.00.052.247 I ggml_metal_init: picking default device: Apple M4
0.00.052.818 I ggml_metal_init: using embedded metal library
0.00.055.191 I ggml_metal_init: GPU name:   Apple M4
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.194 I ggml_metal_init: simdgroup reduction   = true
0.00.055.194 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.194 I ggml_metal_init: has bfloat            = true
0.00.055.194 I ggml_metal_init: use bfloat            = true
0.00.055.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.195 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.001 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.253 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.258 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.286 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.175 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.176 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.176 I llama_init_from_model: graph nodes  = 967
0.00.067.177 I llama_init_from_model: graph splits = 2
0.00.067.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.188 I 
0.00.568.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.230 I perplexity: tokenizing the input ..
0.00.576.158 I perplexity: tokenization took 7.926 ms
0.00.576.161 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.776 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.711.183 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.711.203 I llama_perf_context_print:        load time =     559.34 ms
0.00.711.204 I llama_perf_context_print: prompt eval time =     133.37 ms /   128 tokens (    1.04 ms per token,   959.74 tokens per second)
0.00.711.205 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.205 I llama_perf_context_print:       total time =     143.02 ms /   129 tokens
0.00.711.582 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.850 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.705 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.716 I llama_model_loader: - type  f32:  194 tensors
0.00.027.717 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.717 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.718 I print_info: file format = GGUF V3 (latest)
0.00.027.718 I print_info: file type   = Q5_K - Medium
0.00.027.722 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.048.150 I load: special tokens cache size = 25
0.00.054.196 I load: token to piece cache size = 0.2984 MB
0.00.054.200 I print_info: arch             = gptneox
0.00.054.201 I print_info: vocab_only       = 0
0.00.054.201 I print_info: n_ctx_train      = 2048
0.00.054.201 I print_info: n_embd           = 2048
0.00.054.201 I print_info: n_layer          = 24
0.00.054.205 I print_info: n_head           = 16
0.00.054.206 I print_info: n_head_kv        = 16
0.00.054.206 I print_info: n_rot            = 32
0.00.054.206 I print_info: n_swa            = 0
0.00.054.206 I print_info: n_embd_head_k    = 128
0.00.054.207 I print_info: n_embd_head_v    = 128
0.00.054.207 I print_info: n_gqa            = 1
0.00.054.208 I print_info: n_embd_k_gqa     = 2048
0.00.054.209 I print_info: n_embd_v_gqa     = 2048
0.00.054.209 I print_info: f_norm_eps       = 1.0e-05
0.00.054.210 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.210 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.210 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.210 I print_info: f_logit_scale    = 0.0e+00
0.00.054.210 I print_info: n_ff             = 8192
0.00.054.211 I print_info: n_expert         = 0
0.00.054.211 I print_info: n_expert_used    = 0
0.00.054.211 I print_info: causal attn      = 1
0.00.054.211 I print_info: pooling type     = 0
0.00.054.211 I print_info: rope type        = 2
0.00.054.211 I print_info: rope scaling     = linear
0.00.054.214 I print_info: freq_base_train  = 10000.0
0.00.054.216 I print_info: freq_scale_train = 1
0.00.054.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.216 I print_info: rope_finetuned   = unknown
0.00.054.216 I print_info: ssm_d_conv       = 0
0.00.054.216 I print_info: ssm_d_inner      = 0
0.00.054.216 I print_info: ssm_d_state      = 0
0.00.054.216 I print_info: ssm_dt_rank      = 0
0.00.054.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.218 I print_info: model type       = 1.4B
0.00.054.218 I print_info: model params     = 1.41 B
0.00.054.218 I print_info: general.name     = 1.4B
0.00.054.219 I print_info: vocab type       = BPE
0.00.054.219 I print_info: n_vocab          = 50304
0.00.054.219 I print_info: n_merges         = 50009
0.00.054.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.220 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.220 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.220 I print_info: LF token         = 128 'Ä'
0.00.054.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.221 I print_info: max token length = 1024
0.00.056.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.350 I load_tensors: offloading output layer to GPU
0.00.056.351 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.362 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.056.363 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.693 I llama_init_from_model: n_seq_max     = 1
0.00.056.694 I llama_init_from_model: n_ctx         = 128
0.00.056.694 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.695 I llama_init_from_model: n_batch       = 128
0.00.056.695 I llama_init_from_model: n_ubatch      = 128
0.00.056.695 I llama_init_from_model: flash_attn    = 0
0.00.056.695 I llama_init_from_model: freq_base     = 10000.0
0.00.056.696 I llama_init_from_model: freq_scale    = 1
0.00.056.696 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.696 I ggml_metal_init: allocating
0.00.056.700 I ggml_metal_init: found device: Apple M4
0.00.056.703 I ggml_metal_init: picking default device: Apple M4
0.00.057.326 I ggml_metal_init: using embedded metal library
0.00.059.797 I ggml_metal_init: GPU name:   Apple M4
0.00.059.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.799 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.799 I ggml_metal_init: simdgroup reduction   = true
0.00.059.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.800 I ggml_metal_init: has bfloat            = true
0.00.059.800 I ggml_metal_init: use bfloat            = true
0.00.059.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.046 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.071.497 I init:      Metal KV buffer size =    24.00 MiB
0.00.071.500 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.527 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.072.458 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.072.459 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.072.459 I llama_init_from_model: graph nodes  = 967
0.00.072.459 I llama_init_from_model: graph splits = 2
0.00.072.461 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.965 I 
0.00.628.997 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.007 I perplexity: tokenizing the input ..
0.00.636.811 I perplexity: tokenization took 7.802 ms
0.00.636.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.812 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.779.103 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.779.131 I llama_perf_context_print:        load time =     617.11 ms
0.00.779.132 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.30 tokens per second)
0.00.779.133 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.133 I llama_perf_context_print:       total time =     150.17 ms /   129 tokens
0.00.779.620 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.080s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.694 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.372 I llama_model_loader: - type  f32:  194 tensors
0.00.025.372 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.373 I print_info: file format = GGUF V3 (latest)
0.00.025.373 I print_info: file type   = Q6_K
0.00.025.374 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.573 I load: special tokens cache size = 25
0.00.050.528 I load: token to piece cache size = 0.2984 MB
0.00.050.531 I print_info: arch             = gptneox
0.00.050.531 I print_info: vocab_only       = 0
0.00.050.531 I print_info: n_ctx_train      = 2048
0.00.050.532 I print_info: n_embd           = 2048
0.00.050.532 I print_info: n_layer          = 24
0.00.050.535 I print_info: n_head           = 16
0.00.050.536 I print_info: n_head_kv        = 16
0.00.050.536 I print_info: n_rot            = 32
0.00.050.536 I print_info: n_swa            = 0
0.00.050.536 I print_info: n_embd_head_k    = 128
0.00.050.536 I print_info: n_embd_head_v    = 128
0.00.050.537 I print_info: n_gqa            = 1
0.00.050.538 I print_info: n_embd_k_gqa     = 2048
0.00.050.539 I print_info: n_embd_v_gqa     = 2048
0.00.050.539 I print_info: f_norm_eps       = 1.0e-05
0.00.050.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.540 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.543 I print_info: f_logit_scale    = 0.0e+00
0.00.050.543 I print_info: n_ff             = 8192
0.00.050.543 I print_info: n_expert         = 0
0.00.050.544 I print_info: n_expert_used    = 0
0.00.050.544 I print_info: causal attn      = 1
0.00.050.550 I print_info: pooling type     = 0
0.00.050.552 I print_info: rope type        = 2
0.00.050.552 I print_info: rope scaling     = linear
0.00.050.553 I print_info: freq_base_train  = 10000.0
0.00.050.553 I print_info: freq_scale_train = 1
0.00.050.553 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.553 I print_info: rope_finetuned   = unknown
0.00.050.554 I print_info: ssm_d_conv       = 0
0.00.050.554 I print_info: ssm_d_inner      = 0
0.00.050.554 I print_info: ssm_d_state      = 0
0.00.050.554 I print_info: ssm_dt_rank      = 0
0.00.050.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.554 I print_info: model type       = 1.4B
0.00.050.555 I print_info: model params     = 1.41 B
0.00.050.555 I print_info: general.name     = 1.4B
0.00.050.555 I print_info: vocab type       = BPE
0.00.050.556 I print_info: n_vocab          = 50304
0.00.050.556 I print_info: n_merges         = 50009
0.00.050.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: LF token         = 128 'Ä'
0.00.050.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: max token length = 1024
0.00.052.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.565 I load_tensors: offloading output layer to GPU
0.00.052.565 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.576 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.577 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.892 I llama_init_from_model: n_seq_max     = 1
0.00.052.893 I llama_init_from_model: n_ctx         = 128
0.00.052.893 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.893 I llama_init_from_model: n_batch       = 128
0.00.052.893 I llama_init_from_model: n_ubatch      = 128
0.00.052.894 I llama_init_from_model: flash_attn    = 0
0.00.052.894 I llama_init_from_model: freq_base     = 10000.0
0.00.052.894 I llama_init_from_model: freq_scale    = 1
0.00.052.895 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.895 I ggml_metal_init: allocating
0.00.052.898 I ggml_metal_init: found device: Apple M4
0.00.052.900 I ggml_metal_init: picking default device: Apple M4
0.00.053.476 I ggml_metal_init: using embedded metal library
0.00.055.854 I ggml_metal_init: GPU name:   Apple M4
0.00.055.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.856 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.856 I ggml_metal_init: simdgroup reduction   = true
0.00.055.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.857 I ggml_metal_init: has bfloat            = true
0.00.055.857 I ggml_metal_init: use bfloat            = true
0.00.055.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.660 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.987 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.991 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.017 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.865 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.866 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.866 I llama_init_from_model: graph nodes  = 967
0.00.067.866 I llama_init_from_model: graph splits = 2
0.00.067.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.335 I 
0.00.364.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.379 I perplexity: tokenizing the input ..
0.00.372.064 I perplexity: tokenization took 7.684 ms
0.00.372.070 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.512.016 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.513.197 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.513.221 I llama_perf_context_print:        load time =     355.63 ms
0.00.513.222 I llama_perf_context_print: prompt eval time =     139.72 ms /   128 tokens (    1.09 ms per token,   916.12 tokens per second)
0.00.513.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.513.223 I llama_perf_context_print:       total time =     148.89 ms /   129 tokens
0.00.513.643 I ggml_metal_free: deallocating

real	0m0.527s
user	0m0.078s
sys	0m0.069s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.252 I build: 4473 (fb740247) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.447 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.197 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.210 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.213 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.215 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.219 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.886 I llama_model_loader: - type  f32:  194 tensors
0.00.052.886 I llama_model_loader: - type  f16:   98 tensors
0.00.052.887 I print_info: file format = GGUF V3 (latest)
0.00.052.888 I print_info: file type   = all F32 (guessed)
0.00.052.889 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.412 I load: special tokens cache size = 25
0.00.086.019 I load: token to piece cache size = 0.2984 MB
0.00.086.022 I print_info: arch             = gptneox
0.00.086.022 I print_info: vocab_only       = 0
0.00.086.022 I print_info: n_ctx_train      = 2048
0.00.086.022 I print_info: n_embd           = 2048
0.00.086.022 I print_info: n_layer          = 24
0.00.086.025 I print_info: n_head           = 16
0.00.086.026 I print_info: n_head_kv        = 16
0.00.086.026 I print_info: n_rot            = 32
0.00.086.026 I print_info: n_swa            = 0
0.00.086.027 I print_info: n_embd_head_k    = 128
0.00.086.027 I print_info: n_embd_head_v    = 128
0.00.086.027 I print_info: n_gqa            = 1
0.00.086.028 I print_info: n_embd_k_gqa     = 2048
0.00.086.029 I print_info: n_embd_v_gqa     = 2048
0.00.086.029 I print_info: f_norm_eps       = 1.0e-05
0.00.086.030 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.030 I print_info: f_logit_scale    = 0.0e+00
0.00.086.031 I print_info: n_ff             = 8192
0.00.086.031 I print_info: n_expert         = 0
0.00.086.031 I print_info: n_expert_used    = 0
0.00.086.031 I print_info: causal attn      = 1
0.00.086.031 I print_info: pooling type     = 0
0.00.086.032 I print_info: rope type        = 2
0.00.086.032 I print_info: rope scaling     = linear
0.00.086.034 I print_info: freq_base_train  = 10000.0
0.00.086.035 I print_info: freq_scale_train = 1
0.00.086.035 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.035 I print_info: rope_finetuned   = unknown
0.00.086.035 I print_info: ssm_d_conv       = 0
0.00.086.035 I print_info: ssm_d_inner      = 0
0.00.086.035 I print_info: ssm_d_state      = 0
0.00.086.035 I print_info: ssm_dt_rank      = 0
0.00.086.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.036 I print_info: model type       = 1.4B
0.00.086.036 I print_info: model params     = 1.41 B
0.00.086.036 I print_info: general.name     = 1.4B
0.00.086.037 I print_info: vocab type       = BPE
0.00.086.037 I print_info: n_vocab          = 50304
0.00.086.037 I print_info: n_merges         = 50009
0.00.086.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.038 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.041 I print_info: LF token         = 128 'Ä'
0.00.086.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.042 I print_info: max token length = 1024
0.00.088.538 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.538 I load_tensors: offloading output layer to GPU
0.00.088.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.548 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.550 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.822 I llama_init_from_model: n_seq_max     = 1
0.00.088.823 I llama_init_from_model: n_ctx         = 128
0.00.088.823 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.823 I llama_init_from_model: n_batch       = 128
0.00.088.823 I llama_init_from_model: n_ubatch      = 128
0.00.088.824 I llama_init_from_model: flash_attn    = 0
0.00.088.824 I llama_init_from_model: freq_base     = 10000.0
0.00.088.824 I llama_init_from_model: freq_scale    = 1
0.00.088.824 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.825 I ggml_metal_init: allocating
0.00.088.827 I ggml_metal_init: found device: Apple M4
0.00.088.830 I ggml_metal_init: picking default device: Apple M4
0.00.089.453 I ggml_metal_init: using embedded metal library
0.00.092.031 I ggml_metal_init: GPU name:   Apple M4
0.00.092.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.034 I ggml_metal_init: simdgroup reduction   = true
0.00.092.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.034 I ggml_metal_init: has bfloat            = true
0.00.092.034 I ggml_metal_init: use bfloat            = true
0.00.092.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.375 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.664 I init:      Metal KV buffer size =    24.00 MiB
0.00.102.668 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.565 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.566 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.566 I llama_init_from_model: graph nodes  = 967
0.00.103.566 I llama_init_from_model: graph splits = 2
0.00.103.568 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.568 I 
0.00.103.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.596 I compute_imatrix: tokenizing the input ..
0.00.110.438 I compute_imatrix: tokenization took 6.842 ms
0.00.110.440 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.725.061 I compute_imatrix: 1.61 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.728.236 I llama_perf_context_print:        load time =    1703.61 ms
0.01.728.237 I llama_perf_context_print: prompt eval time =    1613.98 ms /   128 tokens (   12.61 ms per token,    79.31 tokens per second)
0.01.728.238 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.728.238 I llama_perf_context_print:       total time =    1706.78 ms /   129 tokens
0.01.729.295 I ggml_metal_free: deallocating

real	0m1.922s
user	0m0.176s
sys	0m0.264s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4473 (fb740247)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12160a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12160aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12160aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12160b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12160bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12160c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12160c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12160cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12160d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12160d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12160dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12160e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12160ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12160f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12160fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1216135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1216162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1216177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1216183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1216191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12161a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12161a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12161ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12161b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12161bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12161c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12161c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12161ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12161d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12161da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12161e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12161e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12161ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12161f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12161f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12161fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1216209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1216217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1216225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1216261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1216271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1216281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1216291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12162a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12162a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12162ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12162b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12162b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12162bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12161b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12162c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12162c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12162cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12162d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12162d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12162dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12162e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12162e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12162ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12162f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12162f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12162fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1216302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1216316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1216344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1216352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1216360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1216369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1216377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1216385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12163a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12163a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12163aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12163af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12163b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12163b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12163bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12163c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12163c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12163cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12163cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12163d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12163d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12163dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12163e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12163e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12163eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12163f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12163f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12163f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12163fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1216419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1216422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1216430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1216447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1216455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1216463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1216484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1216494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12164a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12164a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12164b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12164b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12164b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12164bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12164c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12164cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12164d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12164d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12164dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12164e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12164e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12164ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12164f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12164f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12164fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1216507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1216517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1216527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1216581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1216591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12165a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12165a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12165ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12165b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12165b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12165bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12165c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12165c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12165cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12165d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12165d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12165dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12165e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12165e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12165ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12165f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12165f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12165fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1216606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1216610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1216619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1216627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1216635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1216656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1216680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1216686c0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.152.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.152.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124d04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124d04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124d053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124d05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124d05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124d06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124d06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124d069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124d06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124d073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124d07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124d07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124d089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124d091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124d099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124d0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124d0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124d0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124d0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124d0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124d0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124d0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124d0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124d0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124d0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124d0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124d0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124d0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124d0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124d0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124d0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124d0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124d10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124d10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124d109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124d10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124d11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124d11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124d11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124d11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124d12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124d128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124d12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124d131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124d13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124d13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124d13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124d14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124d147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124d14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124d150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124d15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124d15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124d15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124d16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124d166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124d16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124d17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124d175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124d17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124d17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124d18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124d18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124d18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124d19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124d194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124d19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124d19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124d1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124d1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124d1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124d1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124d1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124d1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124d1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124d1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124d1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124d1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124d1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124d1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124d1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124d1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124d1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124d1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124d1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124d1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124d1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124d1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124d1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124d1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124d203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124d20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124d20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124d21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124d21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124d219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124d21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124d222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124d22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124d22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124d23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124d23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124d23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124d23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124d241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124d24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124d24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124d24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124d253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124d25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124d25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124d260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124d26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124d269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124d26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124d272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124d27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124d27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124d28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124d28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124d288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124d28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124d291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124d29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124d29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124d29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124d2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124d2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124d2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124d2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124d2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124d2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124d2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124d2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124d2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124d2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124d2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124d2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124d2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124d2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124d2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124d2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124d2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124d2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124d2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124d2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124d2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124d300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124d30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124d30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124d30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124d31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124d316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124d31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124d31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124d32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124d328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124d32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124d33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124d335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124d33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124d33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124d34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124d347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124d34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124d35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124d35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124d35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124d36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124d366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124d36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124d36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124d37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124d37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124d37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124d38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124d385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124d38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124d38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124d39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124d39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124d39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124d3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124d3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124d3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124d3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124d3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124d3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124d3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124d3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124d3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124d3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124d3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124d3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124d3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124d3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124d3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124d3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124d3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124d3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124d3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124d3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124d3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124d3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124d40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124d40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124d40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124d410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124d41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124d41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124d42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124d42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124d42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124d434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124d43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124d44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124d44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124d44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124d45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124d45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124d45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124d462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124d46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124d46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124d47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124d479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124d47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124d48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124d48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124d490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124d49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124d49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124d4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124d4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124d4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124d4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124d4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124d4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124d4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124d4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124d4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124d4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124d4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124d4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124d4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124d4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124d4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124d4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124d4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124d503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124d50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124d50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124d51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124d51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124d52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124d52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124d52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124d531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124d53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124d53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124d54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124d548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124d54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124d55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124d55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124d55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124d56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124d56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124d57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124d57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124d57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124d57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124d58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124d58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124d58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124d59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124d59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124d59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124d5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124d5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124d5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124d5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124d5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124d5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124d5c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124d5ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124d5d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124d5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124d5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124d5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124d5e930 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124c046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124c04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124c04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124c05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124c058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124c05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124c06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124c065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124c06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124c06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124c07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124c079c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124c084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124c08c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124c094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124c09bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124c0a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124c0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124c0b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124c0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124c0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124c0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124c0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124c0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124c0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124c0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124c0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124c0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124c0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124c0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124c0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124c0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124c0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124c10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124c104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124c10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124c10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124c111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124c11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124c11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124c11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124c123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124c12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124c12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124c13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124c13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124c139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124c13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124c142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124c14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124c14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124c15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124c15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124c158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124c15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124c161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124c16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124c16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124c170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124c17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124c17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124c18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124c186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124c18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124c18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124c19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124c198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124c19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124c1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124c1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124c1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124c1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124c1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124c1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124c1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124c1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124c1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124c1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124c1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124c1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124c1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124c1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124c1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124c1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124c1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124c1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124c1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124c1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124c1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124c20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124c20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124c20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124c21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124c214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124c21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124c21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124c22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124c226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124c22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124c22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124c233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124c23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124c23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124c243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124c24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124c24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124c25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124c25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124c259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124c25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124c262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124c26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124c26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124c27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124c27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124c278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124c27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124c281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124c28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124c28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124c28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124c29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124c29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124c29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124c2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124c2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124c2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124c2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124c2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124c2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124c2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124c2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124c2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124c2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124c2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124c2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124c2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124c2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124c2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124c2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124c2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124c2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124c2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124c2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124c2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124c2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124c30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124c306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124c30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124c30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124c31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124c318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124c31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124c32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124c32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124c32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124c32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124c33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124c337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124c33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124c340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124c34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124c34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124c34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124c35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124c356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124c35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124c35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124c36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124c36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124c36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124c37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124c375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124c37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124c37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124c38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124c387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124c38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124c39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124c394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124c39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124c39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124c3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124c3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124c3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124c3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124c3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124c3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124c3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124c3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124c3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124c3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124c3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124c3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124c3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124c3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124c3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124c3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124c3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124c3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124c3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124c3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124c3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124c3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124c403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124c40850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124c40cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124c41130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124c41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124c41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124c42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124c426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124c42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124c42f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124c433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124c43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124c43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124c44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124c44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124c44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124c45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124c45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124c45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124c46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124c464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124c46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124c46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124c47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124c47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124c47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124c47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124c483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124c48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124c48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124c49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124c49590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124c49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124c4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124c4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124c4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124c4b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124c4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124c4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124c4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124c4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124c4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124c4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124c4cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124c4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124c4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124c4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124c4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124c4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124c4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124c4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124c4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124c4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124c4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124c50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124c50480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124c508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124c50d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124c511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124c51640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124c51ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124c51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124c52390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124c52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124c52c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124c530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124c53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124c539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124c53e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124c542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124c54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124c54b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124c54ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124c55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124c558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124c56340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124c56a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124c57180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124c578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124c57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124c57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124c585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124c58be0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.015s
user	0m0.297s
sys	0m0.333s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4473 (fb740247)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14470d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14470dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14470e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14470e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14470ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14470f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14470f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14470fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1447125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144712df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144715240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1447167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144717040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144717760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1447191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1447194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14471a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14471a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14471ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14471b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14471b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14471ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14471bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14471c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14471c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14471ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14471d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14471d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14471d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14471df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14471e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14471ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14471f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14471fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144721290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1447223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144723480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1447249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1447257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1447260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1447293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14472a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14472a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14472ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14472b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14472b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14472be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14472c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14472c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14472ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14472d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14472d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14472de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14472e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14472e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14472ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14471eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14472f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14472fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14472ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1447314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1447324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144732f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1447334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144733f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1447348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1447351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144736470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144736910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144736db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1447376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1447384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1447392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144739750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14473a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14473a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14473a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14473ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14473b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14473b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14473bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14473c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14473c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14473ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14473ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14473d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14473d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14473dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14473e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14473e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14473ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14473ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14473f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14473f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14473fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1447401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144740af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144740f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1447418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144742210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1447426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144742b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144744270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144744710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144745050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1447454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1447462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1447470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1447479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144748330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1447487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1447495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14474a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14474a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14474acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14474b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14474b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14474bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14474c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14474c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14474c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14474cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14474d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14474dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14474e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14474e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14474eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14474f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14474f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14474ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1447503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1447519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1447529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1447539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1447549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1447559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14475a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14475a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14475aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14475b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14475b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14475be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14475c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14475c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14475ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14475d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14475d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14475de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14475e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14475e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14475ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14475f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14475f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14475fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1447603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1447608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1447618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1447628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144762e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1447638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144763e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1447642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144764750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1447659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1447667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1447670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1447688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144769e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14476a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14476a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14476aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14476b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14476b8c0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145804d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1458051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145805660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145805ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145805f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1458063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145806820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145806c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145807100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145807570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1458079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1458080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145808bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1458093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145809bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14580a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14580a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14580b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14580b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14580bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14580c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14580cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14580d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14580dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14580e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14580e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14580e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14580ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14580f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14580f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14580fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14580ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1458103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1458106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145810b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145810f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1458113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145811860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145811cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145812140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1458125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145812a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145812e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145813300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145813770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145813be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145814050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1458144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145814930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145814da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145815680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145815af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145815f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1458163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145816840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145816db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1458172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145817b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145818470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1458188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145818d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1458191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145819630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145819aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145819f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14581a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14581a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14581ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14581b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14581b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14581b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14581be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14581c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14581c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14581cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14581cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14581d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14581d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14581dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14581e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14581e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14581ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14581eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14581f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14581f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14581fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1458200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145820520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145820990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145820e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145821270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1458216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145821b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145821fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145822430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1458228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145822d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145823180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1458235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145823a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145823ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145824340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1458247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145824c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145825090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145825500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145825970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145825de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145826250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1458266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145826fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145827410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145827cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145828160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1458285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145828a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145828eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145829320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145829790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145829c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14582a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14582a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14582a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14582adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14582b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14582b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14582bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14582bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14582c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14582c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14582ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14582d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14582d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14582da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14582de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14582e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14582e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14582ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14582f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14582f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14582f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14582fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145830210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145830680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145830af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145830f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1458313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145831cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145832120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145832590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145832a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145832e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1458332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145833750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145833bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145834030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1458344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145834910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145834d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1458351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1458360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1458363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1458370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1458379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1458382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1458398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14583a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14583a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14583aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14583af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14583b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14583b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14583bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14583c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14583c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14583c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14583ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14583d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14583d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14583db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14583dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14583e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14583e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14583ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14583f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14583f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14583fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145840080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1458404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145840960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145841760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145841c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1458427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145842aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145843060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145843be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1458441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145844760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1458452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1458458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145845e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145846420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1458469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145847560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145847b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1458480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1458486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145848c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145849220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1458497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145849da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14584a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14584a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14584aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14584b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14584ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14584c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14584c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14584cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14584d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14584d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14584dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14584e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14584e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14584ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14584f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14584f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14584ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145850520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145850ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1458510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145851c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1458521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1458527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145852d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145853320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1458538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145854460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145854a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1458555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1458566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145856ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1458571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1458576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145857ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1458580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1458585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145858aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145858fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1458594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1458599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145859ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14585a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14585a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14585ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14585b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14585b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14585c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14585c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14585cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14585d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14585d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14585e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14585e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14585ea90 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14585ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14584c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14584b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1458483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145845b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1458552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145852a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1458507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14584e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1458466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145843ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145848f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14584a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14584f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14584c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145854160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145847de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145851360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14584abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14584ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145847820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145855860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145844a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145843320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1458455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145855e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14584b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1458535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1458494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14584bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14584fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145847260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145850220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145851920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145846120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145854720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145851ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14584d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1458569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145844fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1458563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145844460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145854ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14584eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145850da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145853ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1458524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14584a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145841f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145804880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14585dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14580baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14585ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14585f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14585f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14585f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14585f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14585feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145860170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145860430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1458606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1458609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145860c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145860f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1458611f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1458614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145861770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145861a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145861cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145861fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145862270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145862530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1458627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145862d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145863000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1458632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145863580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145863840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145863b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145863dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145864080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145864340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145864600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1458648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145864b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145864e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145865100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1458653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145865680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145865940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145865c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145865ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145866180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145866440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145866700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1458669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145866c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145866f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145867200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1458674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145867780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145867a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145867d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145867fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145868280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145868540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145868800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145868ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145868d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145869040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145869300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1458695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145869880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145869b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145869e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14586a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14586a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14586a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14586a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14586abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14586ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14586b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14586b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14586b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14586b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14586bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14586bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14586c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14586c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14586c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14586ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14586ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14586cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14586d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14586d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14586d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14586da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14586dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14586e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14586e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14586e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14586e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14586eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14586edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14586f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14586f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14586f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14586f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14586fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14586fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145870100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1458703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145870680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145870940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145870c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145870ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145871180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145871440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145871700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1458719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145871c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145871f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145872200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1458724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145872780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145872a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145872d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145872fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145873280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145873540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145873800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145873ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145873d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145874040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145874300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1458745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145874880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145874b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145874e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1458750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145875380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145875640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145875900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145875bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145875e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145876140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145876400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1458766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145876980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145876c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145876f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1458771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145877480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145877740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145877a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145877cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145877f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145878240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145878500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1458787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145878a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145878d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145879000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1458792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145879580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145879840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145879b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145879dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14587a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14587a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14587a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14587abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14587ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14587b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14587b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14587b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14587b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14587bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14587bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14587c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14587c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14587c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14587cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14587d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14587d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14587dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14587e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14587e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14587ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14587f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14587f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14587fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1458801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145880710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145880c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1458811b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145881700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145881c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1458821a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1458826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145882c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145883190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1458836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145883c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145884180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1458846d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145884c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145885170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1458856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145885c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145886160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1458866b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145886c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145887150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1458876a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145887bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145888140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145888690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145888be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145889130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145889680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145889bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14588a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14588a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14588abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14588b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14588b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14588b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14588bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14588bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14588c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14588c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14588cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14588d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14588d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14588d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14588ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14588e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14588e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14588eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14588ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14588f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14588f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14588fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1458909b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1458910d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1458917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145891ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145891f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145892520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145892b30 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.243s
sys	0m0.145s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
