### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.50 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.80 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.47 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.00 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.26 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.26 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  179.87 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    1.07 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.15 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 223.01 sec*proc (27 tests)

Total Test time (real) = 223.02 sec

real	3m43.169s
user	7m40.372s
sys	0m5.882s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.32 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.21 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.22 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.16 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.88 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.08 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.46 sec*proc (27 tests)

Total Test time (real) =  50.47 sec

real	0m50.483s
user	1m11.115s
sys	0m5.227s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.144 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.779 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.086 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.096 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.097 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.098 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.098 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.099 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.100 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.101 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.101 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.102 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.106 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.110 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.110 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.111 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.112 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.112 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.113 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.114 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.898 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.900 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.901 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.902 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.902 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.903 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.903 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.904 I llama_model_loader: - type  f32:  124 tensors
0.00.029.905 I llama_model_loader: - type  f16:   73 tensors
0.00.034.447 I llm_load_vocab: special tokens cache size = 5
0.00.036.897 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.901 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.901 I llm_load_print_meta: arch             = bert
0.00.036.902 I llm_load_print_meta: vocab type       = WPM
0.00.036.902 I llm_load_print_meta: n_vocab          = 30522
0.00.036.903 I llm_load_print_meta: n_merges         = 0
0.00.036.903 I llm_load_print_meta: vocab_only       = 0
0.00.036.904 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.904 I llm_load_print_meta: n_embd           = 384
0.00.036.904 I llm_load_print_meta: n_layer          = 12
0.00.036.909 I llm_load_print_meta: n_head           = 12
0.00.036.910 I llm_load_print_meta: n_head_kv        = 12
0.00.036.911 I llm_load_print_meta: n_rot            = 32
0.00.036.911 I llm_load_print_meta: n_swa            = 0
0.00.036.911 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.912 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.913 I llm_load_print_meta: n_gqa            = 1
0.00.036.914 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.914 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.915 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.918 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.918 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.918 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.919 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.920 I llm_load_print_meta: n_ff             = 1536
0.00.036.920 I llm_load_print_meta: n_expert         = 0
0.00.036.920 I llm_load_print_meta: n_expert_used    = 0
0.00.036.920 I llm_load_print_meta: causal attn      = 0
0.00.036.921 I llm_load_print_meta: pooling type     = 2
0.00.036.921 I llm_load_print_meta: rope type        = 2
0.00.036.921 I llm_load_print_meta: rope scaling     = linear
0.00.036.922 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.922 I llm_load_print_meta: freq_scale_train = 1
0.00.036.923 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.923 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.923 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.924 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.924 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.926 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.940 I llm_load_print_meta: model type       = 33M
0.00.036.940 I llm_load_print_meta: model ftype      = F16
0.00.036.941 I llm_load_print_meta: model params     = 33.21 M
0.00.036.943 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.944 I llm_load_print_meta: general.name     = Bge Small
0.00.036.944 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.944 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.945 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.945 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.945 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.947 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.948 I llm_load_print_meta: max token length = 21
0.00.039.136 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.136 I llm_load_tensors: offloading output layer to GPU
0.00.039.137 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.164 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.166 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.811 I llama_new_context_with_model: n_seq_max     = 1
0.00.039.813 I llama_new_context_with_model: n_ctx         = 512
0.00.039.813 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.039.813 I llama_new_context_with_model: n_batch       = 2048
0.00.039.814 I llama_new_context_with_model: n_ubatch      = 2048
0.00.039.814 I llama_new_context_with_model: flash_attn    = 0
0.00.039.815 I llama_new_context_with_model: freq_base     = 10000.0
0.00.039.815 I llama_new_context_with_model: freq_scale    = 1
0.00.039.816 I ggml_metal_init: allocating
0.00.039.820 I ggml_metal_init: found device: Apple M4
0.00.039.823 I ggml_metal_init: picking default device: Apple M4
0.00.040.658 I ggml_metal_init: using embedded metal library
0.00.044.273 I ggml_metal_init: GPU name:   Apple M4
0.00.044.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.277 I ggml_metal_init: simdgroup reduction   = true
0.00.044.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.277 I ggml_metal_init: has bfloat            = true
0.00.044.278 I ggml_metal_init: use bfloat            = true
0.00.044.278 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.055.520 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.522 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.523 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.056.346 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.056.348 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.056.348 I llama_new_context_with_model: graph nodes  = 429
0.00.056.348 I llama_new_context_with_model: graph splits = 2
0.00.056.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.837 I 
0.00.062.862 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.063.554 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.327 I llama_perf_context_print:        load time =      44.05 ms
0.00.068.328 I llama_perf_context_print: prompt eval time =       4.63 ms /     9 tokens (    0.51 ms per token,  1942.17 tokens per second)
0.00.068.329 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.329 I llama_perf_context_print:       total time =       5.49 ms /    10 tokens
0.00.068.454 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.048s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.025 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.121 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.126 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.129 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.129 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.129 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.130 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.131 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.131 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.131 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.132 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.134 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.134 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.134 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.135 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.135 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.135 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.136 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.672 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.348 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.349 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.349 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.350 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.350 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.350 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.350 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.351 I llama_model_loader: - type  f32:  124 tensors
0.00.014.351 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.724 I llm_load_vocab: special tokens cache size = 5
0.00.018.046 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.048 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.048 I llm_load_print_meta: arch             = bert
0.00.018.048 I llm_load_print_meta: vocab type       = WPM
0.00.018.049 I llm_load_print_meta: n_vocab          = 30522
0.00.018.049 I llm_load_print_meta: n_merges         = 0
0.00.018.049 I llm_load_print_meta: vocab_only       = 0
0.00.018.049 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.049 I llm_load_print_meta: n_embd           = 384
0.00.018.049 I llm_load_print_meta: n_layer          = 12
0.00.018.051 I llm_load_print_meta: n_head           = 12
0.00.018.052 I llm_load_print_meta: n_head_kv        = 12
0.00.018.052 I llm_load_print_meta: n_rot            = 32
0.00.018.052 I llm_load_print_meta: n_swa            = 0
0.00.018.052 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.053 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.053 I llm_load_print_meta: n_gqa            = 1
0.00.018.055 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.055 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.056 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.056 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.057 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.057 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.057 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.058 I llm_load_print_meta: n_ff             = 1536
0.00.018.058 I llm_load_print_meta: n_expert         = 0
0.00.018.058 I llm_load_print_meta: n_expert_used    = 0
0.00.018.058 I llm_load_print_meta: causal attn      = 0
0.00.018.058 I llm_load_print_meta: pooling type     = 2
0.00.018.059 I llm_load_print_meta: rope type        = 2
0.00.018.059 I llm_load_print_meta: rope scaling     = linear
0.00.018.059 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.059 I llm_load_print_meta: freq_scale_train = 1
0.00.018.060 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.060 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.060 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.060 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.060 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.060 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.060 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.067 I llm_load_print_meta: model type       = 33M
0.00.018.067 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.068 I llm_load_print_meta: model params     = 33.21 M
0.00.018.068 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.068 I llm_load_print_meta: general.name     = Bge Small
0.00.018.069 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.069 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.069 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.069 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.069 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.069 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.070 I llm_load_print_meta: max token length = 21
0.00.019.320 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.320 I llm_load_tensors: offloading output layer to GPU
0.00.019.320 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.327 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.328 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.678 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.678 I llama_new_context_with_model: n_ctx         = 512
0.00.019.678 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.679 I llama_new_context_with_model: n_batch       = 2048
0.00.019.679 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.679 I llama_new_context_with_model: flash_attn    = 0
0.00.019.679 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.680 I llama_new_context_with_model: freq_scale    = 1
0.00.019.680 I ggml_metal_init: allocating
0.00.019.683 I ggml_metal_init: found device: Apple M4
0.00.019.685 I ggml_metal_init: picking default device: Apple M4
0.00.020.201 I ggml_metal_init: using embedded metal library
0.00.022.231 I ggml_metal_init: GPU name:   Apple M4
0.00.022.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.234 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.234 I ggml_metal_init: simdgroup reduction   = true
0.00.022.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.234 I ggml_metal_init: has bfloat            = true
0.00.022.234 I ggml_metal_init: use bfloat            = true
0.00.022.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.236 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.199 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.201 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.204 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.031.818 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.031.819 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.031.819 I llama_new_context_with_model: graph nodes  = 429
0.00.031.820 I llama_new_context_with_model: graph splits = 2
0.00.031.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.117 I 
0.00.036.139 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.036.664 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.038 I llama_perf_context_print:        load time =      27.09 ms
0.00.041.039 I llama_perf_context_print: prompt eval time =       4.25 ms /     9 tokens (    0.47 ms per token,  2117.65 tokens per second)
0.00.041.040 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.041 I llama_perf_context_print:       total time =       4.92 ms /    10 tokens
0.00.041.202 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.176 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.986 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.975 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.983 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.985 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.986 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.987 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.988 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.989 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.989 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.990 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.991 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.994 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.995 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.996 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.798 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.798 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.798 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.799 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.799 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.800 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.800 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.800 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.801 I llama_model_loader: - type  f32:   41 tensors
0.00.048.803 I llama_model_loader: - type  f16:   29 tensors
0.00.066.834 W llm_load_vocab: empty token at index 5
0.00.071.297 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.635 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.661 I llm_load_vocab: special tokens cache size = 5
0.00.339.854 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.861 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.862 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.862 I llm_load_print_meta: vocab type       = BPE
0.00.339.862 I llm_load_print_meta: n_vocab          = 61056
0.00.339.862 I llm_load_print_meta: n_merges         = 39382
0.00.339.863 I llm_load_print_meta: vocab_only       = 0
0.00.339.863 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.865 I llm_load_print_meta: n_embd           = 384
0.00.339.865 I llm_load_print_meta: n_layer          = 4
0.00.339.873 I llm_load_print_meta: n_head           = 12
0.00.339.874 I llm_load_print_meta: n_head_kv        = 12
0.00.339.874 I llm_load_print_meta: n_rot            = 32
0.00.339.874 I llm_load_print_meta: n_swa            = 0
0.00.339.874 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.875 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.875 I llm_load_print_meta: n_gqa            = 1
0.00.339.876 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.876 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.877 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.878 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.879 I llm_load_print_meta: n_ff             = 1536
0.00.339.879 I llm_load_print_meta: n_expert         = 0
0.00.339.879 I llm_load_print_meta: n_expert_used    = 0
0.00.339.879 I llm_load_print_meta: causal attn      = 0
0.00.339.879 I llm_load_print_meta: pooling type     = -1
0.00.339.879 I llm_load_print_meta: rope type        = -1
0.00.339.880 I llm_load_print_meta: rope scaling     = linear
0.00.339.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.880 I llm_load_print_meta: freq_scale_train = 1
0.00.339.881 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.886 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.888 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.888 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.888 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.888 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.888 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.923 I llm_load_print_meta: model type       = 33M
0.00.339.924 I llm_load_print_meta: model ftype      = F16
0.00.339.924 I llm_load_print_meta: model params     = 32.90 M
0.00.339.925 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.927 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.927 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.927 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.927 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.927 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.928 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.928 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.928 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.928 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.928 I llm_load_print_meta: max token length = 45
0.00.341.098 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.098 I llm_load_tensors: offloading output layer to GPU
0.00.341.098 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.123 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.124 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.125 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.125 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.125 I llama_new_context_with_model: n_batch       = 2048
0.00.342.126 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.126 I llama_new_context_with_model: flash_attn    = 0
0.00.342.126 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.127 I llama_new_context_with_model: freq_scale    = 1
0.00.342.127 I ggml_metal_init: allocating
0.00.342.138 I ggml_metal_init: found device: Apple M4
0.00.342.141 I ggml_metal_init: picking default device: Apple M4
0.00.343.162 I ggml_metal_init: using embedded metal library
0.00.345.725 I ggml_metal_init: GPU name:   Apple M4
0.00.345.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.727 I ggml_metal_init: simdgroup reduction   = true
0.00.345.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.728 I ggml_metal_init: has bfloat            = true
0.00.345.728 I ggml_metal_init: use bfloat            = true
0.00.345.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.356.026 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.356.028 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.356.029 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.356.639 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.356.640 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.356.640 I llama_new_context_with_model: graph nodes  = 154
0.00.356.640 I llama_new_context_with_model: graph splits = 2
0.00.356.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.526 I 
0.00.368.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.368.715 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.715 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.718 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.718 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.722 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.722 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.369.274 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.071 I llama_perf_context_print:        load time =     346.53 ms
0.00.372.073 I llama_perf_context_print: prompt eval time =       2.79 ms /    62 tokens (    0.04 ms per token, 22222.22 tokens per second)
0.00.372.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.073 I llama_perf_context_print:       total time =       3.55 ms /    63 tokens
0.00.372.246 I ggml_metal_free: deallocating

real	0m1.071s
user	0m0.347s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.132 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.263 I main: llama backend init
0.00.000.272 I main: load the model and apply lora adapter, if any
0.00.036.113 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.612 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.655 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.660 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.671 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.882 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.066.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.478 I llama_model_loader: - type  f32:  194 tensors
0.00.066.478 I llama_model_loader: - type  f16:   98 tensors
0.00.102.598 I llm_load_vocab: special tokens cache size = 25
0.00.109.864 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.109.867 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.109.867 I llm_load_print_meta: arch             = gptneox
0.00.109.868 I llm_load_print_meta: vocab type       = BPE
0.00.109.868 I llm_load_print_meta: n_vocab          = 50304
0.00.109.868 I llm_load_print_meta: n_merges         = 50009
0.00.109.868 I llm_load_print_meta: vocab_only       = 0
0.00.109.869 I llm_load_print_meta: n_ctx_train      = 2048
0.00.109.869 I llm_load_print_meta: n_embd           = 2048
0.00.109.869 I llm_load_print_meta: n_layer          = 24
0.00.109.872 I llm_load_print_meta: n_head           = 16
0.00.109.873 I llm_load_print_meta: n_head_kv        = 16
0.00.109.873 I llm_load_print_meta: n_rot            = 32
0.00.109.874 I llm_load_print_meta: n_swa            = 0
0.00.109.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.109.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.109.875 I llm_load_print_meta: n_gqa            = 1
0.00.109.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.109.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.109.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.109.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.109.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.109.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.109.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.109.878 I llm_load_print_meta: n_ff             = 8192
0.00.109.878 I llm_load_print_meta: n_expert         = 0
0.00.109.879 I llm_load_print_meta: n_expert_used    = 0
0.00.109.879 I llm_load_print_meta: causal attn      = 1
0.00.109.879 I llm_load_print_meta: pooling type     = 0
0.00.109.879 I llm_load_print_meta: rope type        = 2
0.00.109.879 I llm_load_print_meta: rope scaling     = linear
0.00.109.881 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.109.881 I llm_load_print_meta: freq_scale_train = 1
0.00.109.882 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.109.882 I llm_load_print_meta: rope_finetuned   = unknown
0.00.109.882 I llm_load_print_meta: ssm_d_conv       = 0
0.00.109.882 I llm_load_print_meta: ssm_d_inner      = 0
0.00.109.882 I llm_load_print_meta: ssm_d_state      = 0
0.00.109.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.109.883 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.109.897 I llm_load_print_meta: model type       = 1.4B
0.00.109.897 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.109.898 I llm_load_print_meta: model params     = 1.41 B
0.00.109.898 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.109.899 I llm_load_print_meta: general.name     = 1.4B
0.00.109.899 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.109.899 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.109.899 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.109.899 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.109.900 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.109.900 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.109.900 I llm_load_print_meta: max token length = 1024
0.00.112.556 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.112.556 I llm_load_tensors: offloading output layer to GPU
0.00.112.557 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.112.574 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.112.575 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.113.584 I llama_new_context_with_model: n_seq_max     = 1
0.00.113.585 I llama_new_context_with_model: n_ctx         = 2048
0.00.113.585 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.113.585 I llama_new_context_with_model: n_batch       = 2048
0.00.113.585 I llama_new_context_with_model: n_ubatch      = 512
0.00.113.586 I llama_new_context_with_model: flash_attn    = 0
0.00.113.586 I llama_new_context_with_model: freq_base     = 10000.0
0.00.113.586 I llama_new_context_with_model: freq_scale    = 1
0.00.113.587 I ggml_metal_init: allocating
0.00.113.590 I ggml_metal_init: found device: Apple M4
0.00.113.592 I ggml_metal_init: picking default device: Apple M4
0.00.114.273 I ggml_metal_init: using embedded metal library
0.00.121.909 I ggml_metal_init: GPU name:   Apple M4
0.00.121.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.912 I ggml_metal_init: simdgroup reduction   = true
0.00.121.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.913 I ggml_metal_init: has bfloat            = true
0.00.121.913 I ggml_metal_init: use bfloat            = true
0.00.121.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.157.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.183 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.203 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.145 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.146 I llama_new_context_with_model: graph nodes  = 967
0.00.158.146 I llama_new_context_with_model: graph splits = 2
0.00.158.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.412 I main: llama threadpool init, n_threads = 4
0.00.237.444 I 
0.00.237.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.237.483 I 
0.00.237.562 I sampler seed: 1234
0.00.237.567 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.237.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.237.604 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.237.604 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.085.368 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.02.085.369 I llama_perf_context_print:        load time =     201.29 ms
0.02.085.370 I llama_perf_context_print: prompt eval time =      37.55 ms /     7 tokens (    5.36 ms per token,   186.40 tokens per second)
0.02.085.374 I llama_perf_context_print:        eval time =    1807.33 ms /    63 runs   (   28.69 ms per token,    34.86 tokens per second)
0.02.085.375 I llama_perf_context_print:       total time =    1847.96 ms /    70 tokens
0.02.085.561 I ggml_metal_free: deallocating

real	0m2.452s
user	0m0.148s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.628 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.973 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.620 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.633 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.634 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.671 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.280 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.283 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.285 I llama_model_loader: - type  f32:  194 tensors
0.00.057.285 I llama_model_loader: - type  f16:   98 tensors
0.00.086.673 I llm_load_vocab: special tokens cache size = 25
0.00.093.290 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.294 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.296 I llm_load_print_meta: arch             = gptneox
0.00.093.296 I llm_load_print_meta: vocab type       = BPE
0.00.093.296 I llm_load_print_meta: n_vocab          = 50304
0.00.093.297 I llm_load_print_meta: n_merges         = 50009
0.00.093.297 I llm_load_print_meta: vocab_only       = 0
0.00.093.297 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.297 I llm_load_print_meta: n_embd           = 2048
0.00.093.298 I llm_load_print_meta: n_layer          = 24
0.00.093.301 I llm_load_print_meta: n_head           = 16
0.00.093.302 I llm_load_print_meta: n_head_kv        = 16
0.00.093.303 I llm_load_print_meta: n_rot            = 32
0.00.093.303 I llm_load_print_meta: n_swa            = 0
0.00.093.304 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.305 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.305 I llm_load_print_meta: n_gqa            = 1
0.00.093.306 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.306 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.307 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.307 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.307 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.307 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.307 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.308 I llm_load_print_meta: n_ff             = 8192
0.00.093.308 I llm_load_print_meta: n_expert         = 0
0.00.093.308 I llm_load_print_meta: n_expert_used    = 0
0.00.093.308 I llm_load_print_meta: causal attn      = 1
0.00.093.310 I llm_load_print_meta: pooling type     = 0
0.00.093.311 I llm_load_print_meta: rope type        = 2
0.00.093.311 I llm_load_print_meta: rope scaling     = linear
0.00.093.311 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.312 I llm_load_print_meta: freq_scale_train = 1
0.00.093.312 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.312 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.316 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.320 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.324 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.324 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.325 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.338 I llm_load_print_meta: model type       = 1.4B
0.00.093.338 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.093.340 I llm_load_print_meta: model params     = 1.41 B
0.00.093.341 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.093.341 I llm_load_print_meta: general.name     = 1.4B
0.00.093.341 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.341 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.342 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.342 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.342 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.344 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.344 I llm_load_print_meta: max token length = 1024
0.00.095.990 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.991 I llm_load_tensors: offloading output layer to GPU
0.00.095.991 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.001 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.002 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.952 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.953 I llama_new_context_with_model: n_ctx         = 128
0.00.096.954 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.954 I llama_new_context_with_model: n_batch       = 128
0.00.096.954 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.954 I llama_new_context_with_model: flash_attn    = 0
0.00.096.954 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.955 I llama_new_context_with_model: freq_scale    = 1
0.00.096.955 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.955 I ggml_metal_init: allocating
0.00.096.962 I ggml_metal_init: found device: Apple M4
0.00.096.966 I ggml_metal_init: picking default device: Apple M4
0.00.097.550 I ggml_metal_init: using embedded metal library
0.00.099.689 I ggml_metal_init: GPU name:   Apple M4
0.00.099.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.692 I ggml_metal_init: simdgroup reduction   = true
0.00.099.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.692 I ggml_metal_init: has bfloat            = true
0.00.099.692 I ggml_metal_init: use bfloat            = true
0.00.099.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.637 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.639 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.652 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.508 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.510 I llama_new_context_with_model: graph nodes  = 967
0.00.110.510 I llama_new_context_with_model: graph splits = 2
0.00.110.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.954.653 I 
0.00.954.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.954.728 I perplexity: tokenizing the input ..
0.00.967.652 I perplexity: tokenization took 12.923 ms
0.00.967.658 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.088.725 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.090.563 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.090.620 I llama_perf_context_print:        load time =     928.66 ms
0.01.090.622 I llama_perf_context_print: prompt eval time =     120.06 ms /   128 tokens (    0.94 ms per token,  1066.12 tokens per second)
0.01.090.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.624 I llama_perf_context_print:       total time =     135.97 ms /   129 tokens
0.01.091.274 I ggml_metal_free: deallocating

real	0m1.284s
user	0m0.125s
sys	0m0.204s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.150 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.033.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.088 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.258 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.042.967 I llama_model_loader: - type  f32:  194 tensors
0.00.042.968 I llama_model_loader: - type q8_0:   98 tensors
0.00.069.219 I llm_load_vocab: special tokens cache size = 25
0.00.078.091 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.096 I llm_load_print_meta: arch             = gptneox
0.00.078.097 I llm_load_print_meta: vocab type       = BPE
0.00.078.097 I llm_load_print_meta: n_vocab          = 50304
0.00.078.097 I llm_load_print_meta: n_merges         = 50009
0.00.078.097 I llm_load_print_meta: vocab_only       = 0
0.00.078.098 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.104 I llm_load_print_meta: n_embd           = 2048
0.00.078.105 I llm_load_print_meta: n_layer          = 24
0.00.078.111 I llm_load_print_meta: n_head           = 16
0.00.078.112 I llm_load_print_meta: n_head_kv        = 16
0.00.078.112 I llm_load_print_meta: n_rot            = 32
0.00.078.112 I llm_load_print_meta: n_swa            = 0
0.00.078.112 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.113 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.114 I llm_load_print_meta: n_gqa            = 1
0.00.078.115 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.115 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.116 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.117 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.117 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.117 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.117 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.118 I llm_load_print_meta: n_ff             = 8192
0.00.078.119 I llm_load_print_meta: n_expert         = 0
0.00.078.119 I llm_load_print_meta: n_expert_used    = 0
0.00.078.119 I llm_load_print_meta: causal attn      = 1
0.00.078.119 I llm_load_print_meta: pooling type     = 0
0.00.078.119 I llm_load_print_meta: rope type        = 2
0.00.078.120 I llm_load_print_meta: rope scaling     = linear
0.00.078.120 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.120 I llm_load_print_meta: freq_scale_train = 1
0.00.078.121 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.122 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.123 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.123 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.123 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.123 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.123 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.137 I llm_load_print_meta: model type       = 1.4B
0.00.078.138 I llm_load_print_meta: model ftype      = Q8_0
0.00.078.138 I llm_load_print_meta: model params     = 1.41 B
0.00.078.139 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.078.139 I llm_load_print_meta: general.name     = 1.4B
0.00.078.140 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.140 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.140 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.140 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.141 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.141 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.141 I llm_load_print_meta: max token length = 1024
0.00.081.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.215 I llm_load_tensors: offloading output layer to GPU
0.00.081.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.226 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.081.228 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.082.687 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.688 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.689 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.689 I llama_new_context_with_model: n_batch       = 2048
0.00.082.689 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.690 I llama_new_context_with_model: flash_attn    = 0
0.00.082.690 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.691 I llama_new_context_with_model: freq_scale    = 1
0.00.082.691 I ggml_metal_init: allocating
0.00.082.699 I ggml_metal_init: found device: Apple M4
0.00.082.702 I ggml_metal_init: picking default device: Apple M4
0.00.083.555 I ggml_metal_init: using embedded metal library
0.00.086.815 I ggml_metal_init: GPU name:   Apple M4
0.00.086.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.818 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.818 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.818 I ggml_metal_init: simdgroup reduction   = true
0.00.086.819 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.819 I ggml_metal_init: has bfloat            = true
0.00.086.819 I ggml_metal_init: use bfloat            = true
0.00.086.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.251 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.275 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.414 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.415 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.415 I llama_new_context_with_model: graph nodes  = 967
0.00.124.416 I llama_new_context_with_model: graph splits = 2
0.00.124.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.187.662 I main: llama threadpool init, n_threads = 4
0.01.187.697 I 
0.01.187.724 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.187.725 I 
0.01.187.977 I sampler seed: 1234
0.01.187.982 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.188.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.188.023 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.188.023 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.276.854 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.02.276.855 I llama_perf_context_print:        load time =    1177.51 ms
0.02.276.856 I llama_perf_context_print: prompt eval time =      41.26 ms /     7 tokens (    5.89 ms per token,   169.66 tokens per second)
0.02.276.856 I llama_perf_context_print:        eval time =    1044.63 ms /    63 runs   (   16.58 ms per token,    60.31 tokens per second)
0.02.276.857 I llama_perf_context_print:       total time =    1089.20 ms /    70 tokens
0.02.277.032 I ggml_metal_free: deallocating

real	0m2.295s
user	0m0.126s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.599 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.450 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.107 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.108 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.108 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.109 I llama_model_loader: - type  f32:  194 tensors
0.00.030.109 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.096 I llm_load_vocab: special tokens cache size = 25
0.00.060.145 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.148 I llm_load_print_meta: arch             = gptneox
0.00.060.148 I llm_load_print_meta: vocab type       = BPE
0.00.060.148 I llm_load_print_meta: n_vocab          = 50304
0.00.060.149 I llm_load_print_meta: n_merges         = 50009
0.00.060.149 I llm_load_print_meta: vocab_only       = 0
0.00.060.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.149 I llm_load_print_meta: n_embd           = 2048
0.00.060.149 I llm_load_print_meta: n_layer          = 24
0.00.060.153 I llm_load_print_meta: n_head           = 16
0.00.060.153 I llm_load_print_meta: n_head_kv        = 16
0.00.060.153 I llm_load_print_meta: n_rot            = 32
0.00.060.154 I llm_load_print_meta: n_swa            = 0
0.00.060.154 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.157 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.157 I llm_load_print_meta: n_gqa            = 1
0.00.060.158 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.158 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.160 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.161 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.161 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.161 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.161 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.161 I llm_load_print_meta: n_ff             = 8192
0.00.060.162 I llm_load_print_meta: n_expert         = 0
0.00.060.162 I llm_load_print_meta: n_expert_used    = 0
0.00.060.162 I llm_load_print_meta: causal attn      = 1
0.00.060.162 I llm_load_print_meta: pooling type     = 0
0.00.060.163 I llm_load_print_meta: rope type        = 2
0.00.060.163 I llm_load_print_meta: rope scaling     = linear
0.00.060.164 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.164 I llm_load_print_meta: freq_scale_train = 1
0.00.060.164 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.164 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.164 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.165 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.165 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.165 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.165 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.178 I llm_load_print_meta: model type       = 1.4B
0.00.060.178 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.178 I llm_load_print_meta: model params     = 1.41 B
0.00.060.179 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.179 I llm_load_print_meta: general.name     = 1.4B
0.00.060.179 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.179 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.180 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.180 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.180 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.180 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.181 I llm_load_print_meta: max token length = 1024
0.00.062.349 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.349 I llm_load_tensors: offloading output layer to GPU
0.00.062.349 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.359 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.360 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.280 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.281 I llama_new_context_with_model: n_ctx         = 128
0.00.063.281 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.281 I llama_new_context_with_model: n_batch       = 128
0.00.063.281 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.281 I llama_new_context_with_model: flash_attn    = 0
0.00.063.282 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.282 I llama_new_context_with_model: freq_scale    = 1
0.00.063.282 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.283 I ggml_metal_init: allocating
0.00.063.286 I ggml_metal_init: found device: Apple M4
0.00.063.288 I ggml_metal_init: picking default device: Apple M4
0.00.063.847 I ggml_metal_init: using embedded metal library
0.00.065.777 I ggml_metal_init: GPU name:   Apple M4
0.00.065.778 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.779 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.779 I ggml_metal_init: simdgroup reduction   = true
0.00.065.779 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.779 I ggml_metal_init: has bfloat            = true
0.00.065.779 I ggml_metal_init: use bfloat            = true
0.00.065.780 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.439 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.443 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.458 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.379 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.380 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.380 I llama_new_context_with_model: graph nodes  = 967
0.00.075.380 I llama_new_context_with_model: graph splits = 2
0.00.075.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.146 I 
0.00.959.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.959.174 I perplexity: tokenizing the input ..
0.00.967.186 I perplexity: tokenization took 8.011 ms
0.00.967.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.088.962 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.090.149 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.090.177 I llama_perf_context_print:        load time =     948.54 ms
0.01.090.178 I llama_perf_context_print: prompt eval time =     121.55 ms /   128 tokens (    0.95 ms per token,  1053.10 tokens per second)
0.01.090.179 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.179 I llama_perf_context_print:       total time =     131.03 ms /   129 tokens
0.01.090.624 I ggml_metal_free: deallocating

real	0m1.109s
user	0m0.087s
sys	0m0.178s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.709 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.319 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.707 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.963 I llama_model_loader: - type  f32:  194 tensors
0.00.026.964 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.435 I llm_load_vocab: special tokens cache size = 25
0.00.054.340 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.343 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.343 I llm_load_print_meta: arch             = gptneox
0.00.054.344 I llm_load_print_meta: vocab type       = BPE
0.00.054.344 I llm_load_print_meta: n_vocab          = 50304
0.00.054.344 I llm_load_print_meta: n_merges         = 50009
0.00.054.345 I llm_load_print_meta: vocab_only       = 0
0.00.054.345 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.346 I llm_load_print_meta: n_embd           = 2048
0.00.054.347 I llm_load_print_meta: n_layer          = 24
0.00.054.354 I llm_load_print_meta: n_head           = 16
0.00.054.355 I llm_load_print_meta: n_head_kv        = 16
0.00.054.355 I llm_load_print_meta: n_rot            = 32
0.00.054.357 I llm_load_print_meta: n_swa            = 0
0.00.054.357 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.358 I llm_load_print_meta: n_gqa            = 1
0.00.054.360 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.361 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.362 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.362 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.362 I llm_load_print_meta: n_ff             = 8192
0.00.054.363 I llm_load_print_meta: n_expert         = 0
0.00.054.363 I llm_load_print_meta: n_expert_used    = 0
0.00.054.363 I llm_load_print_meta: causal attn      = 1
0.00.054.363 I llm_load_print_meta: pooling type     = 0
0.00.054.365 I llm_load_print_meta: rope type        = 2
0.00.054.365 I llm_load_print_meta: rope scaling     = linear
0.00.054.365 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.365 I llm_load_print_meta: freq_scale_train = 1
0.00.054.365 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.366 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.366 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.366 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.366 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.366 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.366 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.380 I llm_load_print_meta: model type       = 1.4B
0.00.054.380 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.381 I llm_load_print_meta: model params     = 1.41 B
0.00.054.381 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.381 I llm_load_print_meta: general.name     = 1.4B
0.00.054.381 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.382 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.382 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.382 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.382 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.382 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.386 I llm_load_print_meta: max token length = 1024
0.00.056.665 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.665 I llm_load_tensors: offloading output layer to GPU
0.00.056.666 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.676 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.677 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.665 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.666 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.666 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.666 I llama_new_context_with_model: n_batch       = 2048
0.00.057.667 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.667 I llama_new_context_with_model: flash_attn    = 0
0.00.057.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.668 I llama_new_context_with_model: freq_scale    = 1
0.00.057.668 I ggml_metal_init: allocating
0.00.057.674 I ggml_metal_init: found device: Apple M4
0.00.057.677 I ggml_metal_init: picking default device: Apple M4
0.00.058.325 I ggml_metal_init: using embedded metal library
0.00.060.463 I ggml_metal_init: GPU name:   Apple M4
0.00.060.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.465 I ggml_metal_init: simdgroup reduction   = true
0.00.060.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.465 I ggml_metal_init: has bfloat            = true
0.00.060.466 I ggml_metal_init: use bfloat            = true
0.00.060.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.106 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.123 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.156 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.271 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.272 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.273 I llama_new_context_with_model: graph nodes  = 967
0.00.093.273 I llama_new_context_with_model: graph splits = 2
0.00.093.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.440 I main: llama threadpool init, n_threads = 4
0.00.689.485 I 
0.00.689.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.689.519 I 
0.00.689.750 I sampler seed: 1234
0.00.689.754 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.770 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.368.867 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.368.868 I llama_perf_context_print:        load time =     678.72 ms
0.01.368.869 I llama_perf_context_print: prompt eval time =      36.69 ms /     7 tokens (    5.24 ms per token,   190.81 tokens per second)
0.01.368.869 I llama_perf_context_print:        eval time =     639.23 ms /    63 runs   (   10.15 ms per token,    98.56 tokens per second)
0.01.368.870 I llama_perf_context_print:       total time =     679.43 ms /    70 tokens
0.01.369.046 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.940 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.746 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.862 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.917 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.919 I llama_model_loader: - type  f32:  194 tensors
0.00.024.919 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.185 I llm_load_vocab: special tokens cache size = 25
0.00.051.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.046 I llm_load_print_meta: arch             = gptneox
0.00.051.047 I llm_load_print_meta: vocab type       = BPE
0.00.051.047 I llm_load_print_meta: n_vocab          = 50304
0.00.051.047 I llm_load_print_meta: n_merges         = 50009
0.00.051.048 I llm_load_print_meta: vocab_only       = 0
0.00.051.048 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.048 I llm_load_print_meta: n_embd           = 2048
0.00.051.048 I llm_load_print_meta: n_layer          = 24
0.00.051.052 I llm_load_print_meta: n_head           = 16
0.00.051.053 I llm_load_print_meta: n_head_kv        = 16
0.00.051.054 I llm_load_print_meta: n_rot            = 32
0.00.051.055 I llm_load_print_meta: n_swa            = 0
0.00.051.055 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.055 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.056 I llm_load_print_meta: n_gqa            = 1
0.00.051.057 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.057 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.059 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.059 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.059 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.060 I llm_load_print_meta: n_ff             = 8192
0.00.051.060 I llm_load_print_meta: n_expert         = 0
0.00.051.060 I llm_load_print_meta: n_expert_used    = 0
0.00.051.060 I llm_load_print_meta: causal attn      = 1
0.00.051.061 I llm_load_print_meta: pooling type     = 0
0.00.051.061 I llm_load_print_meta: rope type        = 2
0.00.051.062 I llm_load_print_meta: rope scaling     = linear
0.00.051.062 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.062 I llm_load_print_meta: freq_scale_train = 1
0.00.051.063 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.063 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.063 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.065 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.066 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.066 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.077 I llm_load_print_meta: model type       = 1.4B
0.00.051.077 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.077 I llm_load_print_meta: model params     = 1.41 B
0.00.051.078 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.078 I llm_load_print_meta: general.name     = 1.4B
0.00.051.078 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.079 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.079 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.079 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.079 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.079 I llm_load_print_meta: max token length = 1024
0.00.052.627 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.628 I llm_load_tensors: offloading output layer to GPU
0.00.052.628 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.637 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.638 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.502 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.502 I llama_new_context_with_model: n_ctx         = 128
0.00.053.503 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.503 I llama_new_context_with_model: n_batch       = 128
0.00.053.503 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.503 I llama_new_context_with_model: flash_attn    = 0
0.00.053.504 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.504 I llama_new_context_with_model: freq_scale    = 1
0.00.053.504 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.505 I ggml_metal_init: allocating
0.00.053.511 I ggml_metal_init: found device: Apple M4
0.00.053.513 I ggml_metal_init: picking default device: Apple M4
0.00.054.054 I ggml_metal_init: using embedded metal library
0.00.055.983 I ggml_metal_init: GPU name:   Apple M4
0.00.055.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.985 I ggml_metal_init: simdgroup reduction   = true
0.00.055.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.986 I ggml_metal_init: has bfloat            = true
0.00.055.986 I ggml_metal_init: use bfloat            = true
0.00.055.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.228 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.230 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.243 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.125 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.126 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.126 I llama_new_context_with_model: graph nodes  = 967
0.00.066.126 I llama_new_context_with_model: graph splits = 2
0.00.066.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.165 I 
0.00.633.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.633.223 I perplexity: tokenizing the input ..
0.00.641.307 I perplexity: tokenization took 8.083 ms
0.00.641.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.225 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.765.399 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.765.432 I llama_perf_context_print:        load time =     623.22 ms
0.00.765.433 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.31 tokens per second)
0.00.765.434 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.434 I llama_perf_context_print:       total time =     132.27 ms /   129 tokens
0.00.765.943 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.077s
sys	0m0.113s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.616 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.623 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.624 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.792 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.793 I llama_model_loader: - type  f32:  194 tensors
0.00.027.793 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.794 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.202 I llm_load_vocab: special tokens cache size = 25
0.00.054.187 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.190 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.191 I llm_load_print_meta: arch             = gptneox
0.00.054.191 I llm_load_print_meta: vocab type       = BPE
0.00.054.191 I llm_load_print_meta: n_vocab          = 50304
0.00.054.191 I llm_load_print_meta: n_merges         = 50009
0.00.054.192 I llm_load_print_meta: vocab_only       = 0
0.00.054.192 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.192 I llm_load_print_meta: n_embd           = 2048
0.00.054.192 I llm_load_print_meta: n_layer          = 24
0.00.054.195 I llm_load_print_meta: n_head           = 16
0.00.054.196 I llm_load_print_meta: n_head_kv        = 16
0.00.054.196 I llm_load_print_meta: n_rot            = 32
0.00.054.196 I llm_load_print_meta: n_swa            = 0
0.00.054.196 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.197 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.197 I llm_load_print_meta: n_gqa            = 1
0.00.054.198 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.199 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.199 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.200 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.200 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.200 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.200 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.201 I llm_load_print_meta: n_ff             = 8192
0.00.054.201 I llm_load_print_meta: n_expert         = 0
0.00.054.201 I llm_load_print_meta: n_expert_used    = 0
0.00.054.202 I llm_load_print_meta: causal attn      = 1
0.00.054.202 I llm_load_print_meta: pooling type     = 0
0.00.054.202 I llm_load_print_meta: rope type        = 2
0.00.054.202 I llm_load_print_meta: rope scaling     = linear
0.00.054.203 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.203 I llm_load_print_meta: freq_scale_train = 1
0.00.054.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.204 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.204 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.204 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.216 I llm_load_print_meta: model type       = 1.4B
0.00.054.216 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.217 I llm_load_print_meta: model params     = 1.41 B
0.00.054.217 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.217 I llm_load_print_meta: general.name     = 1.4B
0.00.054.218 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.218 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.219 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.219 I llm_load_print_meta: max token length = 1024
0.00.056.199 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.199 I llm_load_tensors: offloading output layer to GPU
0.00.056.200 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.209 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.210 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.202 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.203 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.203 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.203 I llama_new_context_with_model: n_batch       = 2048
0.00.057.204 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.204 I llama_new_context_with_model: flash_attn    = 0
0.00.057.204 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.204 I llama_new_context_with_model: freq_scale    = 1
0.00.057.205 I ggml_metal_init: allocating
0.00.057.208 I ggml_metal_init: found device: Apple M4
0.00.057.210 I ggml_metal_init: picking default device: Apple M4
0.00.057.770 I ggml_metal_init: using embedded metal library
0.00.059.706 I ggml_metal_init: GPU name:   Apple M4
0.00.059.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.708 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.708 I ggml_metal_init: simdgroup reduction   = true
0.00.059.708 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.708 I ggml_metal_init: has bfloat            = true
0.00.059.709 I ggml_metal_init: use bfloat            = true
0.00.059.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.742 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.748 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.893 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.894 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.894 I llama_new_context_with_model: graph nodes  = 967
0.00.088.895 I llama_new_context_with_model: graph splits = 2
0.00.088.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.658 I main: llama threadpool init, n_threads = 4
0.00.727.702 I 
0.00.727.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.727.727 I 
0.00.727.957 I sampler seed: 1234
0.00.727.961 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.976 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.977 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.977 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.453.187 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65317.39 tokens per second)
0.01.453.188 I llama_perf_context_print:        load time =     715.93 ms
0.01.453.189 I llama_perf_context_print: prompt eval time =      39.44 ms /     7 tokens (    5.63 ms per token,   177.47 tokens per second)
0.01.453.189 I llama_perf_context_print:        eval time =     682.92 ms /    63 runs   (   10.84 ms per token,    92.25 tokens per second)
0.01.453.190 I llama_perf_context_print:       total time =     725.53 ms /    70 tokens
0.01.453.361 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.108s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.972 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.031 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.035 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.035 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.036 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.037 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.038 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.040 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.139 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.139 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.141 I llama_model_loader: - type  f32:  194 tensors
0.00.024.141 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.141 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.066 I llm_load_vocab: special tokens cache size = 25
0.00.050.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.925 I llm_load_print_meta: arch             = gptneox
0.00.050.926 I llm_load_print_meta: vocab type       = BPE
0.00.050.926 I llm_load_print_meta: n_vocab          = 50304
0.00.050.926 I llm_load_print_meta: n_merges         = 50009
0.00.050.926 I llm_load_print_meta: vocab_only       = 0
0.00.050.926 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.927 I llm_load_print_meta: n_embd           = 2048
0.00.050.927 I llm_load_print_meta: n_layer          = 24
0.00.050.929 I llm_load_print_meta: n_head           = 16
0.00.050.930 I llm_load_print_meta: n_head_kv        = 16
0.00.050.930 I llm_load_print_meta: n_rot            = 32
0.00.050.931 I llm_load_print_meta: n_swa            = 0
0.00.050.931 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.931 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.934 I llm_load_print_meta: n_gqa            = 1
0.00.050.935 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.935 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.936 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.936 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.937 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.937 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.937 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.938 I llm_load_print_meta: n_ff             = 8192
0.00.050.938 I llm_load_print_meta: n_expert         = 0
0.00.050.938 I llm_load_print_meta: n_expert_used    = 0
0.00.050.938 I llm_load_print_meta: causal attn      = 1
0.00.050.938 I llm_load_print_meta: pooling type     = 0
0.00.050.939 I llm_load_print_meta: rope type        = 2
0.00.050.939 I llm_load_print_meta: rope scaling     = linear
0.00.050.939 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.940 I llm_load_print_meta: freq_scale_train = 1
0.00.050.940 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.940 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.940 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.942 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.942 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.942 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.953 I llm_load_print_meta: model type       = 1.4B
0.00.050.954 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.955 I llm_load_print_meta: model params     = 1.41 B
0.00.050.956 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.956 I llm_load_print_meta: general.name     = 1.4B
0.00.050.956 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.956 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.956 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.956 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.957 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: max token length = 1024
0.00.052.554 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.554 I llm_load_tensors: offloading output layer to GPU
0.00.052.554 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.564 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.565 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.421 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.422 I llama_new_context_with_model: n_ctx         = 128
0.00.053.422 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.422 I llama_new_context_with_model: n_batch       = 128
0.00.053.422 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.422 I llama_new_context_with_model: flash_attn    = 0
0.00.053.423 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.423 I llama_new_context_with_model: freq_scale    = 1
0.00.053.423 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.424 I ggml_metal_init: allocating
0.00.053.429 I ggml_metal_init: found device: Apple M4
0.00.053.431 I ggml_metal_init: picking default device: Apple M4
0.00.053.954 I ggml_metal_init: using embedded metal library
0.00.055.867 I ggml_metal_init: GPU name:   Apple M4
0.00.055.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.869 I ggml_metal_init: simdgroup reduction   = true
0.00.055.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.870 I ggml_metal_init: has bfloat            = true
0.00.055.870 I ggml_metal_init: use bfloat            = true
0.00.055.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.737 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.742 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.629 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.629 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.630 I llama_new_context_with_model: graph nodes  = 967
0.00.065.630 I llama_new_context_with_model: graph splits = 2
0.00.065.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.447 I 
0.00.678.473 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.678.478 I perplexity: tokenizing the input ..
0.00.686.721 I perplexity: tokenization took 8.242 ms
0.00.686.724 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.674 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.810.877 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.810.906 I llama_perf_context_print:        load time =     669.47 ms
0.00.810.907 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.03 tokens per second)
0.00.810.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.909 I llama_perf_context_print:       total time =     132.46 ms /   129 tokens
0.00.811.386 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.078s
sys	0m0.114s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.631 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.528 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.529 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.532 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.604 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.731 I llama_model_loader: - type  f32:  194 tensors
0.00.024.731 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.732 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.793 I llm_load_vocab: special tokens cache size = 25
0.00.051.756 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.759 I llm_load_print_meta: arch             = gptneox
0.00.051.760 I llm_load_print_meta: vocab type       = BPE
0.00.051.760 I llm_load_print_meta: n_vocab          = 50304
0.00.051.760 I llm_load_print_meta: n_merges         = 50009
0.00.051.760 I llm_load_print_meta: vocab_only       = 0
0.00.051.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.760 I llm_load_print_meta: n_embd           = 2048
0.00.051.761 I llm_load_print_meta: n_layer          = 24
0.00.051.763 I llm_load_print_meta: n_head           = 16
0.00.051.764 I llm_load_print_meta: n_head_kv        = 16
0.00.051.764 I llm_load_print_meta: n_rot            = 32
0.00.051.764 I llm_load_print_meta: n_swa            = 0
0.00.051.765 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.766 I llm_load_print_meta: n_gqa            = 1
0.00.051.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.767 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.768 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.768 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.768 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.768 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.769 I llm_load_print_meta: n_ff             = 8192
0.00.051.769 I llm_load_print_meta: n_expert         = 0
0.00.051.769 I llm_load_print_meta: n_expert_used    = 0
0.00.051.773 I llm_load_print_meta: causal attn      = 1
0.00.051.774 I llm_load_print_meta: pooling type     = 0
0.00.051.774 I llm_load_print_meta: rope type        = 2
0.00.051.775 I llm_load_print_meta: rope scaling     = linear
0.00.051.775 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.775 I llm_load_print_meta: freq_scale_train = 1
0.00.051.776 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.776 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.777 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.788 I llm_load_print_meta: model type       = 1.4B
0.00.051.789 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.789 I llm_load_print_meta: model params     = 1.41 B
0.00.051.790 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.790 I llm_load_print_meta: general.name     = 1.4B
0.00.051.790 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.790 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.790 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.791 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.791 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.791 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.791 I llm_load_print_meta: max token length = 1024
0.00.053.819 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.819 I llm_load_tensors: offloading output layer to GPU
0.00.053.819 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.829 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.830 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.736 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.737 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.737 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.738 I llama_new_context_with_model: n_batch       = 2048
0.00.054.738 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.738 I llama_new_context_with_model: flash_attn    = 0
0.00.054.738 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.739 I llama_new_context_with_model: freq_scale    = 1
0.00.054.739 I ggml_metal_init: allocating
0.00.054.742 I ggml_metal_init: found device: Apple M4
0.00.054.744 I ggml_metal_init: picking default device: Apple M4
0.00.055.315 I ggml_metal_init: using embedded metal library
0.00.057.243 I ggml_metal_init: GPU name:   Apple M4
0.00.057.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.245 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.245 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.245 I ggml_metal_init: simdgroup reduction   = true
0.00.057.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.245 I ggml_metal_init: has bfloat            = true
0.00.057.245 I ggml_metal_init: use bfloat            = true
0.00.057.246 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.133 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.141 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.161 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.105 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.107 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.107 I llama_new_context_with_model: graph nodes  = 967
0.00.086.107 I llama_new_context_with_model: graph splits = 2
0.00.086.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.059 I main: llama threadpool init, n_threads = 4
0.00.750.100 I 
0.00.750.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.750.124 I 
0.00.750.361 I sampler seed: 1234
0.00.750.365 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.390 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.390 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.540.021 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.540.022 I llama_perf_context_print:        load time =     741.42 ms
0.01.540.022 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.48 tokens per second)
0.01.540.024 I llama_perf_context_print:        eval time =     749.97 ms /    63 runs   (   11.90 ms per token,    84.00 tokens per second)
0.01.540.025 I llama_perf_context_print:       total time =     789.96 ms /    70 tokens
0.01.540.203 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.868 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.180 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.114 I llm_load_vocab: special tokens cache size = 25
0.00.051.894 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.897 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.898 I llm_load_print_meta: arch             = gptneox
0.00.051.898 I llm_load_print_meta: vocab type       = BPE
0.00.051.898 I llm_load_print_meta: n_vocab          = 50304
0.00.051.898 I llm_load_print_meta: n_merges         = 50009
0.00.051.899 I llm_load_print_meta: vocab_only       = 0
0.00.051.899 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.899 I llm_load_print_meta: n_embd           = 2048
0.00.051.899 I llm_load_print_meta: n_layer          = 24
0.00.051.902 I llm_load_print_meta: n_head           = 16
0.00.051.903 I llm_load_print_meta: n_head_kv        = 16
0.00.051.903 I llm_load_print_meta: n_rot            = 32
0.00.051.903 I llm_load_print_meta: n_swa            = 0
0.00.051.903 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.904 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.904 I llm_load_print_meta: n_gqa            = 1
0.00.051.905 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.906 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.906 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.907 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.907 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.907 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.907 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.908 I llm_load_print_meta: n_ff             = 8192
0.00.051.908 I llm_load_print_meta: n_expert         = 0
0.00.051.908 I llm_load_print_meta: n_expert_used    = 0
0.00.051.908 I llm_load_print_meta: causal attn      = 1
0.00.051.908 I llm_load_print_meta: pooling type     = 0
0.00.051.908 I llm_load_print_meta: rope type        = 2
0.00.051.909 I llm_load_print_meta: rope scaling     = linear
0.00.051.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.911 I llm_load_print_meta: freq_scale_train = 1
0.00.051.911 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.912 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.912 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.912 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.923 I llm_load_print_meta: model type       = 1.4B
0.00.051.923 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.924 I llm_load_print_meta: model params     = 1.41 B
0.00.051.924 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.924 I llm_load_print_meta: general.name     = 1.4B
0.00.051.924 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.925 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.925 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: max token length = 1024
0.00.053.503 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.503 I llm_load_tensors: offloading output layer to GPU
0.00.053.503 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.513 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.514 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.383 I llama_new_context_with_model: n_ctx         = 128
0.00.054.383 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.383 I llama_new_context_with_model: n_batch       = 128
0.00.054.384 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.384 I llama_new_context_with_model: flash_attn    = 0
0.00.054.384 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.384 I llama_new_context_with_model: freq_scale    = 1
0.00.054.385 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.385 I ggml_metal_init: allocating
0.00.054.388 I ggml_metal_init: found device: Apple M4
0.00.054.390 I ggml_metal_init: picking default device: Apple M4
0.00.054.925 I ggml_metal_init: using embedded metal library
0.00.056.893 I ggml_metal_init: GPU name:   Apple M4
0.00.056.895 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.896 I ggml_metal_init: simdgroup reduction   = true
0.00.056.896 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.896 I ggml_metal_init: has bfloat            = true
0.00.056.896 I ggml_metal_init: use bfloat            = true
0.00.056.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.238 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.240 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.159 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.160 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.160 I llama_new_context_with_model: graph nodes  = 967
0.00.067.160 I llama_new_context_with_model: graph splits = 2
0.00.067.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.776 I 
0.00.709.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.709.815 I perplexity: tokenizing the input ..
0.00.717.757 I perplexity: tokenization took 7.941 ms
0.00.717.766 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.852.355 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.853.978 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.004 I llama_perf_context_print:        load time =     699.79 ms
0.00.854.005 I llama_perf_context_print: prompt eval time =     134.36 ms /   128 tokens (    1.05 ms per token,   952.66 tokens per second)
0.00.854.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.006 I llama_perf_context_print:       total time =     144.23 ms /   129 tokens
0.00.854.459 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.075s
sys	0m0.123s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.020 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.026 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.027 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.027 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.028 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.029 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.391 I llama_model_loader: - type  f32:  194 tensors
0.00.025.392 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.392 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.404 I llm_load_vocab: special tokens cache size = 25
0.00.052.331 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.334 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.334 I llm_load_print_meta: arch             = gptneox
0.00.052.335 I llm_load_print_meta: vocab type       = BPE
0.00.052.335 I llm_load_print_meta: n_vocab          = 50304
0.00.052.335 I llm_load_print_meta: n_merges         = 50009
0.00.052.335 I llm_load_print_meta: vocab_only       = 0
0.00.052.335 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.336 I llm_load_print_meta: n_embd           = 2048
0.00.052.336 I llm_load_print_meta: n_layer          = 24
0.00.052.339 I llm_load_print_meta: n_head           = 16
0.00.052.340 I llm_load_print_meta: n_head_kv        = 16
0.00.052.340 I llm_load_print_meta: n_rot            = 32
0.00.052.340 I llm_load_print_meta: n_swa            = 0
0.00.052.340 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.340 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.341 I llm_load_print_meta: n_gqa            = 1
0.00.052.342 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.345 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.346 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.346 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.346 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.346 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.348 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.349 I llm_load_print_meta: n_ff             = 8192
0.00.052.349 I llm_load_print_meta: n_expert         = 0
0.00.052.349 I llm_load_print_meta: n_expert_used    = 0
0.00.052.351 I llm_load_print_meta: causal attn      = 1
0.00.052.352 I llm_load_print_meta: pooling type     = 0
0.00.052.357 I llm_load_print_meta: rope type        = 2
0.00.052.357 I llm_load_print_meta: rope scaling     = linear
0.00.052.358 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.358 I llm_load_print_meta: freq_scale_train = 1
0.00.052.358 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.358 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.358 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.364 I llm_load_print_meta: model type       = 1.4B
0.00.052.364 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.364 I llm_load_print_meta: model params     = 1.41 B
0.00.052.365 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.365 I llm_load_print_meta: general.name     = 1.4B
0.00.052.365 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.366 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.366 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.367 I llm_load_print_meta: max token length = 1024
0.00.054.191 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.191 I llm_load_tensors: offloading output layer to GPU
0.00.054.191 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.196 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.197 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.247 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.248 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.248 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.248 I llama_new_context_with_model: n_batch       = 2048
0.00.055.248 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.248 I llama_new_context_with_model: flash_attn    = 0
0.00.055.249 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.249 I llama_new_context_with_model: freq_scale    = 1
0.00.055.249 I ggml_metal_init: allocating
0.00.055.255 I ggml_metal_init: found device: Apple M4
0.00.055.257 I ggml_metal_init: picking default device: Apple M4
0.00.055.809 I ggml_metal_init: using embedded metal library
0.00.057.742 I ggml_metal_init: GPU name:   Apple M4
0.00.057.743 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.744 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.744 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.744 I ggml_metal_init: simdgroup reduction   = true
0.00.057.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.746 I ggml_metal_init: has bfloat            = true
0.00.057.746 I ggml_metal_init: use bfloat            = true
0.00.057.747 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.575 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.611 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.612 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.613 I llama_new_context_with_model: graph nodes  = 967
0.00.086.613 I llama_new_context_with_model: graph splits = 2
0.00.086.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.731 I main: llama threadpool init, n_threads = 4
0.00.714.765 I 
0.00.714.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.714.790 I 
0.00.714.940 I sampler seed: 1234
0.00.714.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.956 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.956 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.956 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.556.715 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.556.716 I llama_perf_context_print:        load time =     704.84 ms
0.01.556.717 I llama_perf_context_print: prompt eval time =      36.67 ms /     7 tokens (    5.24 ms per token,   190.91 tokens per second)
0.01.556.718 I llama_perf_context_print:        eval time =     801.99 ms /    63 runs   (   12.73 ms per token,    78.56 tokens per second)
0.01.556.718 I llama_perf_context_print:       total time =     841.99 ms /    70 tokens
0.01.556.912 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.645 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.246 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.248 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.249 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.250 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.092 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.094 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.094 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.095 I llama_model_loader: - type  f32:  194 tensors
0.00.023.095 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.096 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.172 I llm_load_vocab: special tokens cache size = 25
0.00.049.045 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.048 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.048 I llm_load_print_meta: arch             = gptneox
0.00.049.048 I llm_load_print_meta: vocab type       = BPE
0.00.049.048 I llm_load_print_meta: n_vocab          = 50304
0.00.049.049 I llm_load_print_meta: n_merges         = 50009
0.00.049.049 I llm_load_print_meta: vocab_only       = 0
0.00.049.049 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.049 I llm_load_print_meta: n_embd           = 2048
0.00.049.049 I llm_load_print_meta: n_layer          = 24
0.00.049.053 I llm_load_print_meta: n_head           = 16
0.00.049.053 I llm_load_print_meta: n_head_kv        = 16
0.00.049.054 I llm_load_print_meta: n_rot            = 32
0.00.049.054 I llm_load_print_meta: n_swa            = 0
0.00.049.054 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.054 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.055 I llm_load_print_meta: n_gqa            = 1
0.00.049.056 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.057 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.057 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.059 I llm_load_print_meta: n_ff             = 8192
0.00.049.059 I llm_load_print_meta: n_expert         = 0
0.00.049.059 I llm_load_print_meta: n_expert_used    = 0
0.00.049.059 I llm_load_print_meta: causal attn      = 1
0.00.049.059 I llm_load_print_meta: pooling type     = 0
0.00.049.059 I llm_load_print_meta: rope type        = 2
0.00.049.060 I llm_load_print_meta: rope scaling     = linear
0.00.049.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.060 I llm_load_print_meta: freq_scale_train = 1
0.00.049.060 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.061 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.061 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.061 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.061 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.061 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.061 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.073 I llm_load_print_meta: model type       = 1.4B
0.00.049.073 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.074 I llm_load_print_meta: model params     = 1.41 B
0.00.049.076 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.076 I llm_load_print_meta: general.name     = 1.4B
0.00.049.076 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.076 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.076 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.077 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.078 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.078 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.078 I llm_load_print_meta: max token length = 1024
0.00.050.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.678 I llm_load_tensors: offloading output layer to GPU
0.00.050.678 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.687 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.688 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.533 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.534 I llama_new_context_with_model: n_ctx         = 128
0.00.051.534 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.534 I llama_new_context_with_model: n_batch       = 128
0.00.051.534 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.534 I llama_new_context_with_model: flash_attn    = 0
0.00.051.535 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.535 I llama_new_context_with_model: freq_scale    = 1
0.00.051.535 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.536 I ggml_metal_init: allocating
0.00.051.539 I ggml_metal_init: found device: Apple M4
0.00.051.541 I ggml_metal_init: picking default device: Apple M4
0.00.052.086 I ggml_metal_init: using embedded metal library
0.00.054.001 I ggml_metal_init: GPU name:   Apple M4
0.00.054.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.003 I ggml_metal_init: simdgroup reduction   = true
0.00.054.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.003 I ggml_metal_init: has bfloat            = true
0.00.054.004 I ggml_metal_init: use bfloat            = true
0.00.054.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.126 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.128 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.141 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.989 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.990 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.990 I llama_new_context_with_model: graph nodes  = 967
0.00.063.991 I llama_new_context_with_model: graph splits = 2
0.00.064.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.962 I 
0.00.666.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.666.015 I perplexity: tokenizing the input ..
0.00.673.646 I perplexity: tokenization took 7.629 ms
0.00.673.650 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.965 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.809.128 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.809.153 I llama_perf_context_print:        load time =     657.31 ms
0.00.809.154 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.58 tokens per second)
0.00.809.156 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.156 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.00.809.589 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.076s
sys	0m0.129s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.269 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.270 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.272 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.272 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.273 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.511 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.692 I llama_model_loader: - type  f32:  194 tensors
0.00.024.692 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.693 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.693 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.006 I llm_load_vocab: special tokens cache size = 25
0.00.050.724 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.726 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.727 I llm_load_print_meta: arch             = gptneox
0.00.050.727 I llm_load_print_meta: vocab type       = BPE
0.00.050.727 I llm_load_print_meta: n_vocab          = 50304
0.00.050.728 I llm_load_print_meta: n_merges         = 50009
0.00.050.728 I llm_load_print_meta: vocab_only       = 0
0.00.050.728 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.728 I llm_load_print_meta: n_embd           = 2048
0.00.050.728 I llm_load_print_meta: n_layer          = 24
0.00.050.731 I llm_load_print_meta: n_head           = 16
0.00.050.737 I llm_load_print_meta: n_head_kv        = 16
0.00.050.738 I llm_load_print_meta: n_rot            = 32
0.00.050.738 I llm_load_print_meta: n_swa            = 0
0.00.050.738 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.739 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.740 I llm_load_print_meta: n_gqa            = 1
0.00.050.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.741 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.742 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.743 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.743 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.743 I llm_load_print_meta: n_ff             = 8192
0.00.050.744 I llm_load_print_meta: n_expert         = 0
0.00.050.745 I llm_load_print_meta: n_expert_used    = 0
0.00.050.747 I llm_load_print_meta: causal attn      = 1
0.00.050.747 I llm_load_print_meta: pooling type     = 0
0.00.050.747 I llm_load_print_meta: rope type        = 2
0.00.050.748 I llm_load_print_meta: rope scaling     = linear
0.00.050.748 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.748 I llm_load_print_meta: freq_scale_train = 1
0.00.050.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.748 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.749 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.749 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.761 I llm_load_print_meta: model type       = 1.4B
0.00.050.762 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.762 I llm_load_print_meta: model params     = 1.41 B
0.00.050.763 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.763 I llm_load_print_meta: general.name     = 1.4B
0.00.050.763 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: max token length = 1024
0.00.052.653 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.653 I llm_load_tensors: offloading output layer to GPU
0.00.052.653 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.664 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.664 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.713 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.714 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.714 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.714 I llama_new_context_with_model: n_batch       = 2048
0.00.053.714 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.714 I llama_new_context_with_model: flash_attn    = 0
0.00.053.715 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.715 I llama_new_context_with_model: freq_scale    = 1
0.00.053.715 I ggml_metal_init: allocating
0.00.053.719 I ggml_metal_init: found device: Apple M4
0.00.053.720 I ggml_metal_init: picking default device: Apple M4
0.00.054.283 I ggml_metal_init: using embedded metal library
0.00.056.211 I ggml_metal_init: GPU name:   Apple M4
0.00.056.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.213 I ggml_metal_init: simdgroup reduction   = true
0.00.056.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.215 I ggml_metal_init: has bfloat            = true
0.00.056.215 I ggml_metal_init: use bfloat            = true
0.00.056.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.724 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.735 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.695 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.696 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.697 I llama_new_context_with_model: graph nodes  = 967
0.00.084.697 I llama_new_context_with_model: graph splits = 2
0.00.084.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.447.476 I main: llama threadpool init, n_threads = 4
0.00.447.518 I 
0.00.447.545 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.447.546 I 
0.00.447.776 I sampler seed: 1234
0.00.447.780 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.447.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.447.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.447.829 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.128.536 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.128.536 I llama_perf_context_print:        load time =     437.73 ms
0.01.128.537 I llama_perf_context_print: prompt eval time =      35.93 ms /     7 tokens (    5.13 ms per token,   194.81 tokens per second)
0.01.128.538 I llama_perf_context_print:        eval time =     641.83 ms /    63 runs   (   10.19 ms per token,    98.16 tokens per second)
0.01.128.538 I llama_perf_context_print:       total time =     681.06 ms /    70 tokens
0.01.128.725 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.107s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.574 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.030 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.886 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.887 I llama_model_loader: - type  f32:  194 tensors
0.00.023.887 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.887 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.706 I llm_load_vocab: special tokens cache size = 25
0.00.050.597 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.600 I llm_load_print_meta: arch             = gptneox
0.00.050.601 I llm_load_print_meta: vocab type       = BPE
0.00.050.601 I llm_load_print_meta: n_vocab          = 50304
0.00.050.601 I llm_load_print_meta: n_merges         = 50009
0.00.050.601 I llm_load_print_meta: vocab_only       = 0
0.00.050.601 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.602 I llm_load_print_meta: n_embd           = 2048
0.00.050.602 I llm_load_print_meta: n_layer          = 24
0.00.050.605 I llm_load_print_meta: n_head           = 16
0.00.050.605 I llm_load_print_meta: n_head_kv        = 16
0.00.050.606 I llm_load_print_meta: n_rot            = 32
0.00.050.606 I llm_load_print_meta: n_swa            = 0
0.00.050.606 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.606 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.607 I llm_load_print_meta: n_gqa            = 1
0.00.050.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.609 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.609 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.609 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.611 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.611 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.612 I llm_load_print_meta: n_ff             = 8192
0.00.050.612 I llm_load_print_meta: n_expert         = 0
0.00.050.612 I llm_load_print_meta: n_expert_used    = 0
0.00.050.612 I llm_load_print_meta: causal attn      = 1
0.00.050.613 I llm_load_print_meta: pooling type     = 0
0.00.050.613 I llm_load_print_meta: rope type        = 2
0.00.050.613 I llm_load_print_meta: rope scaling     = linear
0.00.050.613 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.614 I llm_load_print_meta: freq_scale_train = 1
0.00.050.614 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.614 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.614 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.615 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.615 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.615 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.617 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.628 I llm_load_print_meta: model type       = 1.4B
0.00.050.628 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.629 I llm_load_print_meta: model params     = 1.41 B
0.00.050.629 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.630 I llm_load_print_meta: general.name     = 1.4B
0.00.050.630 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.631 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.632 I llm_load_print_meta: max token length = 1024
0.00.052.321 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.322 I llm_load_tensors: offloading output layer to GPU
0.00.052.322 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.332 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.333 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.282 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.283 I llama_new_context_with_model: n_ctx         = 128
0.00.053.283 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.283 I llama_new_context_with_model: n_batch       = 128
0.00.053.283 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.284 I llama_new_context_with_model: flash_attn    = 0
0.00.053.284 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.284 I llama_new_context_with_model: freq_scale    = 1
0.00.053.285 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.285 I ggml_metal_init: allocating
0.00.053.289 I ggml_metal_init: found device: Apple M4
0.00.053.291 I ggml_metal_init: picking default device: Apple M4
0.00.053.830 I ggml_metal_init: using embedded metal library
0.00.055.803 I ggml_metal_init: GPU name:   Apple M4
0.00.055.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.805 I ggml_metal_init: simdgroup reduction   = true
0.00.055.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.805 I ggml_metal_init: has bfloat            = true
0.00.055.805 I ggml_metal_init: use bfloat            = true
0.00.055.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.147 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.149 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.173 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.131 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.132 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.132 I llama_new_context_with_model: graph nodes  = 967
0.00.066.132 I llama_new_context_with_model: graph splits = 2
0.00.066.145 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.402.676 I 
0.00.402.717 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.402.733 I perplexity: tokenizing the input ..
0.00.410.593 I perplexity: tokenization took 7.859 ms
0.00.410.602 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.543.046 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.197 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.222 I llama_perf_context_print:        load time =     393.10 ms
0.00.544.223 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.16 tokens per second)
0.00.544.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.225 I llama_perf_context_print:       total time =     141.55 ms /   129 tokens
0.00.544.690 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.077s
sys	0m0.078s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.639 I llama_model_loader: - type  f32:  194 tensors
0.00.026.639 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.639 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.640 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.030 I llm_load_vocab: special tokens cache size = 25
0.00.052.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.939 I llm_load_print_meta: arch             = gptneox
0.00.052.940 I llm_load_print_meta: vocab type       = BPE
0.00.052.940 I llm_load_print_meta: n_vocab          = 50304
0.00.052.940 I llm_load_print_meta: n_merges         = 50009
0.00.052.940 I llm_load_print_meta: vocab_only       = 0
0.00.052.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.941 I llm_load_print_meta: n_embd           = 2048
0.00.052.941 I llm_load_print_meta: n_layer          = 24
0.00.052.943 I llm_load_print_meta: n_head           = 16
0.00.052.944 I llm_load_print_meta: n_head_kv        = 16
0.00.052.945 I llm_load_print_meta: n_rot            = 32
0.00.052.945 I llm_load_print_meta: n_swa            = 0
0.00.052.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.945 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.946 I llm_load_print_meta: n_gqa            = 1
0.00.052.947 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.952 I llm_load_print_meta: n_ff             = 8192
0.00.052.953 I llm_load_print_meta: n_expert         = 0
0.00.052.955 I llm_load_print_meta: n_expert_used    = 0
0.00.052.955 I llm_load_print_meta: causal attn      = 1
0.00.052.955 I llm_load_print_meta: pooling type     = 0
0.00.052.955 I llm_load_print_meta: rope type        = 2
0.00.052.955 I llm_load_print_meta: rope scaling     = linear
0.00.052.956 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.956 I llm_load_print_meta: freq_scale_train = 1
0.00.052.956 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.956 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.957 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.957 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.957 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.957 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.965 I llm_load_print_meta: model type       = 1.4B
0.00.052.966 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.966 I llm_load_print_meta: model params     = 1.41 B
0.00.052.967 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.967 I llm_load_print_meta: general.name     = 1.4B
0.00.052.968 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.969 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.970 I llm_load_print_meta: max token length = 1024
0.00.054.759 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.760 I llm_load_tensors: offloading output layer to GPU
0.00.054.760 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.765 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.765 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.689 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.689 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.690 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.690 I llama_new_context_with_model: n_batch       = 2048
0.00.055.690 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.690 I llama_new_context_with_model: flash_attn    = 0
0.00.055.691 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.691 I llama_new_context_with_model: freq_scale    = 1
0.00.055.691 I ggml_metal_init: allocating
0.00.055.695 I ggml_metal_init: found device: Apple M4
0.00.055.696 I ggml_metal_init: picking default device: Apple M4
0.00.056.255 I ggml_metal_init: using embedded metal library
0.00.058.179 I ggml_metal_init: GPU name:   Apple M4
0.00.058.182 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.183 I ggml_metal_init: simdgroup reduction   = true
0.00.058.183 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.183 I ggml_metal_init: has bfloat            = true
0.00.058.183 I ggml_metal_init: use bfloat            = true
0.00.058.184 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.728 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.750 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.742 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.743 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.743 I llama_new_context_with_model: graph nodes  = 967
0.00.086.744 I llama_new_context_with_model: graph splits = 2
0.00.086.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.971 I main: llama threadpool init, n_threads = 4
0.00.554.008 I 
0.00.554.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.554.057 I 
0.00.554.295 I sampler seed: 1234
0.00.554.300 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.554.343 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.554.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.554.347 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.301.741 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.301.741 I llama_perf_context_print:        load time =     543.03 ms
0.01.301.742 I llama_perf_context_print: prompt eval time =      35.70 ms /     7 tokens (    5.10 ms per token,   196.08 tokens per second)
0.01.301.743 I llama_perf_context_print:        eval time =     708.70 ms /    63 runs   (   11.25 ms per token,    88.90 tokens per second)
0.01.301.744 I llama_perf_context_print:       total time =     747.77 ms /    70 tokens
0.01.301.919 I ggml_metal_free: deallocating

real	0m1.317s
user	0m0.108s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.605 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.380 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.386 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.396 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.607 I llama_model_loader: - type  f32:  194 tensors
0.00.023.608 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.608 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.608 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.608 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.825 I llm_load_vocab: special tokens cache size = 25
0.00.049.710 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.713 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.713 I llm_load_print_meta: arch             = gptneox
0.00.049.714 I llm_load_print_meta: vocab type       = BPE
0.00.049.714 I llm_load_print_meta: n_vocab          = 50304
0.00.049.714 I llm_load_print_meta: n_merges         = 50009
0.00.049.714 I llm_load_print_meta: vocab_only       = 0
0.00.049.715 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.715 I llm_load_print_meta: n_embd           = 2048
0.00.049.715 I llm_load_print_meta: n_layer          = 24
0.00.049.718 I llm_load_print_meta: n_head           = 16
0.00.049.719 I llm_load_print_meta: n_head_kv        = 16
0.00.049.719 I llm_load_print_meta: n_rot            = 32
0.00.049.720 I llm_load_print_meta: n_swa            = 0
0.00.049.721 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.721 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.722 I llm_load_print_meta: n_gqa            = 1
0.00.049.723 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.723 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.724 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.724 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.725 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.725 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.725 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.725 I llm_load_print_meta: n_ff             = 8192
0.00.049.726 I llm_load_print_meta: n_expert         = 0
0.00.049.726 I llm_load_print_meta: n_expert_used    = 0
0.00.049.726 I llm_load_print_meta: causal attn      = 1
0.00.049.726 I llm_load_print_meta: pooling type     = 0
0.00.049.726 I llm_load_print_meta: rope type        = 2
0.00.049.726 I llm_load_print_meta: rope scaling     = linear
0.00.049.727 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.727 I llm_load_print_meta: freq_scale_train = 1
0.00.049.727 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.728 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.728 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.728 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.728 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.728 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.728 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.740 I llm_load_print_meta: model type       = 1.4B
0.00.049.740 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.740 I llm_load_print_meta: model params     = 1.41 B
0.00.049.742 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.742 I llm_load_print_meta: general.name     = 1.4B
0.00.049.742 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.742 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.743 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.744 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.744 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.744 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.744 I llm_load_print_meta: max token length = 1024
0.00.051.254 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.254 I llm_load_tensors: offloading output layer to GPU
0.00.051.254 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.264 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.265 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.084 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.085 I llama_new_context_with_model: n_ctx         = 128
0.00.052.085 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.085 I llama_new_context_with_model: n_batch       = 128
0.00.052.085 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.086 I llama_new_context_with_model: flash_attn    = 0
0.00.052.086 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.086 I llama_new_context_with_model: freq_scale    = 1
0.00.052.087 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.087 I ggml_metal_init: allocating
0.00.052.091 I ggml_metal_init: found device: Apple M4
0.00.052.093 I ggml_metal_init: picking default device: Apple M4
0.00.052.618 I ggml_metal_init: using embedded metal library
0.00.054.569 I ggml_metal_init: GPU name:   Apple M4
0.00.054.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.571 I ggml_metal_init: simdgroup reduction   = true
0.00.054.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.572 I ggml_metal_init: has bfloat            = true
0.00.054.572 I ggml_metal_init: use bfloat            = true
0.00.054.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.714 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.718 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.731 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.584 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.585 I llama_new_context_with_model: graph nodes  = 967
0.00.064.585 I llama_new_context_with_model: graph splits = 2
0.00.064.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.614 I 
0.00.504.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.504.645 I perplexity: tokenizing the input ..
0.00.512.588 I perplexity: tokenization took 7.941 ms
0.00.512.590 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.644.085 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.645.260 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.645.278 I llama_perf_context_print:        load time =     496.00 ms
0.00.645.279 I llama_perf_context_print: prompt eval time =     131.27 ms /   128 tokens (    1.03 ms per token,   975.10 tokens per second)
0.00.645.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.282 I llama_perf_context_print:       total time =     140.67 ms /   129 tokens
0.00.645.727 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.076s
sys	0m0.103s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.127 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.277 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.278 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.278 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.279 I llama_model_loader: - type  f32:  194 tensors
0.00.025.279 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.279 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.279 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.623 I llm_load_vocab: special tokens cache size = 25
0.00.051.562 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.565 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.565 I llm_load_print_meta: arch             = gptneox
0.00.051.565 I llm_load_print_meta: vocab type       = BPE
0.00.051.566 I llm_load_print_meta: n_vocab          = 50304
0.00.051.566 I llm_load_print_meta: n_merges         = 50009
0.00.051.566 I llm_load_print_meta: vocab_only       = 0
0.00.051.566 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.566 I llm_load_print_meta: n_embd           = 2048
0.00.051.566 I llm_load_print_meta: n_layer          = 24
0.00.051.569 I llm_load_print_meta: n_head           = 16
0.00.051.570 I llm_load_print_meta: n_head_kv        = 16
0.00.051.570 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.571 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.571 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.572 I llm_load_print_meta: n_gqa            = 1
0.00.051.573 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.573 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.574 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.574 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.576 I llm_load_print_meta: n_ff             = 8192
0.00.051.576 I llm_load_print_meta: n_expert         = 0
0.00.051.577 I llm_load_print_meta: n_expert_used    = 0
0.00.051.577 I llm_load_print_meta: causal attn      = 1
0.00.051.577 I llm_load_print_meta: pooling type     = 0
0.00.051.578 I llm_load_print_meta: rope type        = 2
0.00.051.578 I llm_load_print_meta: rope scaling     = linear
0.00.051.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.581 I llm_load_print_meta: freq_scale_train = 1
0.00.051.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.582 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.593 I llm_load_print_meta: model type       = 1.4B
0.00.051.593 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.594 I llm_load_print_meta: model params     = 1.41 B
0.00.051.594 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.594 I llm_load_print_meta: general.name     = 1.4B
0.00.051.595 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.596 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.596 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.596 I llm_load_print_meta: max token length = 1024
0.00.053.147 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.147 I llm_load_tensors: offloading output layer to GPU
0.00.053.148 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.157 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.158 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.009 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.010 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.010 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.010 I llama_new_context_with_model: n_batch       = 2048
0.00.054.010 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.011 I llama_new_context_with_model: flash_attn    = 0
0.00.054.011 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.011 I llama_new_context_with_model: freq_scale    = 1
0.00.054.012 I ggml_metal_init: allocating
0.00.054.015 I ggml_metal_init: found device: Apple M4
0.00.054.017 I ggml_metal_init: picking default device: Apple M4
0.00.054.556 I ggml_metal_init: using embedded metal library
0.00.056.457 I ggml_metal_init: GPU name:   Apple M4
0.00.056.459 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.460 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.461 I ggml_metal_init: simdgroup reduction   = true
0.00.056.461 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.461 I ggml_metal_init: has bfloat            = true
0.00.056.461 I ggml_metal_init: use bfloat            = true
0.00.056.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.538 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.546 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.620 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.621 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.622 I llama_new_context_with_model: graph nodes  = 967
0.00.085.622 I llama_new_context_with_model: graph splits = 2
0.00.085.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.682 I main: llama threadpool init, n_threads = 4
0.00.624.753 I 
0.00.624.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.624.781 I 
0.00.625.050 I sampler seed: 1234
0.00.625.062 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.078 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.382.948 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.01.382.948 I llama_perf_context_print:        load time =     614.98 ms
0.01.382.949 I llama_perf_context_print: prompt eval time =      40.01 ms /     7 tokens (    5.72 ms per token,   174.95 tokens per second)
0.01.382.950 I llama_perf_context_print:        eval time =     714.76 ms /    63 runs   (   11.35 ms per token,    88.14 tokens per second)
0.01.382.953 I llama_perf_context_print:       total time =     758.27 ms /    70 tokens
0.01.383.118 I ggml_metal_free: deallocating

real	0m1.400s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.466 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.329 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.336 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.337 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.337 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.341 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.342 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.602 I llama_model_loader: - type  f32:  194 tensors
0.00.024.602 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.602 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.603 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.508 I llm_load_vocab: special tokens cache size = 25
0.00.051.393 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.396 I llm_load_print_meta: arch             = gptneox
0.00.051.396 I llm_load_print_meta: vocab type       = BPE
0.00.051.396 I llm_load_print_meta: n_vocab          = 50304
0.00.051.396 I llm_load_print_meta: n_merges         = 50009
0.00.051.397 I llm_load_print_meta: vocab_only       = 0
0.00.051.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.397 I llm_load_print_meta: n_embd           = 2048
0.00.051.397 I llm_load_print_meta: n_layer          = 24
0.00.051.400 I llm_load_print_meta: n_head           = 16
0.00.051.401 I llm_load_print_meta: n_head_kv        = 16
0.00.051.401 I llm_load_print_meta: n_rot            = 32
0.00.051.401 I llm_load_print_meta: n_swa            = 0
0.00.051.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.402 I llm_load_print_meta: n_gqa            = 1
0.00.051.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.406 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.407 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.407 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.408 I llm_load_print_meta: n_ff             = 8192
0.00.051.408 I llm_load_print_meta: n_expert         = 0
0.00.051.408 I llm_load_print_meta: n_expert_used    = 0
0.00.051.408 I llm_load_print_meta: causal attn      = 1
0.00.051.409 I llm_load_print_meta: pooling type     = 0
0.00.051.409 I llm_load_print_meta: rope type        = 2
0.00.051.409 I llm_load_print_meta: rope scaling     = linear
0.00.051.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.410 I llm_load_print_meta: freq_scale_train = 1
0.00.051.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.411 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.423 I llm_load_print_meta: model type       = 1.4B
0.00.051.423 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.424 I llm_load_print_meta: model params     = 1.41 B
0.00.051.424 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.424 I llm_load_print_meta: general.name     = 1.4B
0.00.051.425 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.425 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.425 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.425 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.425 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.425 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.425 I llm_load_print_meta: max token length = 1024
0.00.053.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.393 I llm_load_tensors: offloading output layer to GPU
0.00.053.393 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.403 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.404 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.308 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.308 I llama_new_context_with_model: n_ctx         = 128
0.00.054.309 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.309 I llama_new_context_with_model: n_batch       = 128
0.00.054.309 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.309 I llama_new_context_with_model: flash_attn    = 0
0.00.054.309 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.310 I llama_new_context_with_model: freq_scale    = 1
0.00.054.310 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.310 I ggml_metal_init: allocating
0.00.054.314 I ggml_metal_init: found device: Apple M4
0.00.054.316 I ggml_metal_init: picking default device: Apple M4
0.00.054.856 I ggml_metal_init: using embedded metal library
0.00.056.771 I ggml_metal_init: GPU name:   Apple M4
0.00.056.773 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.774 I ggml_metal_init: simdgroup reduction   = true
0.00.056.774 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.774 I ggml_metal_init: has bfloat            = true
0.00.056.774 I ggml_metal_init: use bfloat            = true
0.00.056.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.325 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.329 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.343 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.251 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.252 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.252 I llama_new_context_with_model: graph nodes  = 967
0.00.067.252 I llama_new_context_with_model: graph splits = 2
0.00.067.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.959 I 
0.00.579.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.579.020 I perplexity: tokenizing the input ..
0.00.586.626 I perplexity: tokenization took 7.605 ms
0.00.586.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.113 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.722.285 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.318 I llama_perf_context_print:        load time =     569.48 ms
0.00.722.319 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.39 tokens per second)
0.00.722.320 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.321 I llama_perf_context_print:       total time =     143.37 ms /   129 tokens
0.00.722.838 I ggml_metal_free: deallocating

real	0m0.739s
user	0m0.077s
sys	0m0.112s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.864 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.548 I llama_model_loader: - type  f32:  194 tensors
0.00.025.548 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.548 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.993 I llm_load_vocab: special tokens cache size = 25
0.00.051.840 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.843 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.843 I llm_load_print_meta: arch             = gptneox
0.00.051.843 I llm_load_print_meta: vocab type       = BPE
0.00.051.844 I llm_load_print_meta: n_vocab          = 50304
0.00.051.844 I llm_load_print_meta: n_merges         = 50009
0.00.051.844 I llm_load_print_meta: vocab_only       = 0
0.00.051.844 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.844 I llm_load_print_meta: n_embd           = 2048
0.00.051.845 I llm_load_print_meta: n_layer          = 24
0.00.051.847 I llm_load_print_meta: n_head           = 16
0.00.051.848 I llm_load_print_meta: n_head_kv        = 16
0.00.051.848 I llm_load_print_meta: n_rot            = 32
0.00.051.848 I llm_load_print_meta: n_swa            = 0
0.00.051.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.849 I llm_load_print_meta: n_gqa            = 1
0.00.051.850 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.851 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.853 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.856 I llm_load_print_meta: n_ff             = 8192
0.00.051.856 I llm_load_print_meta: n_expert         = 0
0.00.051.856 I llm_load_print_meta: n_expert_used    = 0
0.00.051.856 I llm_load_print_meta: causal attn      = 1
0.00.051.856 I llm_load_print_meta: pooling type     = 0
0.00.051.857 I llm_load_print_meta: rope type        = 2
0.00.051.857 I llm_load_print_meta: rope scaling     = linear
0.00.051.857 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.857 I llm_load_print_meta: freq_scale_train = 1
0.00.051.858 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.858 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.858 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.858 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.858 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.858 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.859 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.872 I llm_load_print_meta: model type       = 1.4B
0.00.051.875 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.876 I llm_load_print_meta: model params     = 1.41 B
0.00.051.876 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.876 I llm_load_print_meta: general.name     = 1.4B
0.00.051.877 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.877 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.879 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.879 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.879 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.879 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.879 I llm_load_print_meta: max token length = 1024
0.00.053.839 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.840 I llm_load_tensors: offloading output layer to GPU
0.00.053.840 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.850 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.851 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.803 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.804 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.804 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.804 I llama_new_context_with_model: n_batch       = 2048
0.00.054.805 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.805 I llama_new_context_with_model: flash_attn    = 0
0.00.054.805 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.805 I llama_new_context_with_model: freq_scale    = 1
0.00.054.806 I ggml_metal_init: allocating
0.00.054.809 I ggml_metal_init: found device: Apple M4
0.00.054.811 I ggml_metal_init: picking default device: Apple M4
0.00.055.349 I ggml_metal_init: using embedded metal library
0.00.057.303 I ggml_metal_init: GPU name:   Apple M4
0.00.057.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.305 I ggml_metal_init: simdgroup reduction   = true
0.00.057.306 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.306 I ggml_metal_init: has bfloat            = true
0.00.057.306 I ggml_metal_init: use bfloat            = true
0.00.057.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.449 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.455 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.474 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.428 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.429 I llama_new_context_with_model: graph nodes  = 967
0.00.085.430 I llama_new_context_with_model: graph splits = 2
0.00.085.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.452 I main: llama threadpool init, n_threads = 4
0.00.702.486 I 
0.00.702.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.702.515 I 
0.00.702.739 I sampler seed: 1234
0.00.702.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.780 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.780 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.543.198 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.01.543.199 I llama_perf_context_print:        load time =     692.58 ms
0.01.543.199 I llama_perf_context_print: prompt eval time =      38.69 ms /     7 tokens (    5.53 ms per token,   180.94 tokens per second)
0.01.543.200 I llama_perf_context_print:        eval time =     798.80 ms /    63 runs   (   12.68 ms per token,    78.87 tokens per second)
0.01.543.200 I llama_perf_context_print:       total time =     840.75 ms /    70 tokens
0.01.543.375 I ggml_metal_free: deallocating

real	0m1.561s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.474 I llama_model_loader: - type  f32:  194 tensors
0.00.023.474 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.474 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.454 I llm_load_vocab: special tokens cache size = 25
0.00.049.384 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.387 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.387 I llm_load_print_meta: arch             = gptneox
0.00.049.388 I llm_load_print_meta: vocab type       = BPE
0.00.049.388 I llm_load_print_meta: n_vocab          = 50304
0.00.049.388 I llm_load_print_meta: n_merges         = 50009
0.00.049.388 I llm_load_print_meta: vocab_only       = 0
0.00.049.389 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.389 I llm_load_print_meta: n_embd           = 2048
0.00.049.389 I llm_load_print_meta: n_layer          = 24
0.00.049.391 I llm_load_print_meta: n_head           = 16
0.00.049.392 I llm_load_print_meta: n_head_kv        = 16
0.00.049.393 I llm_load_print_meta: n_rot            = 32
0.00.049.393 I llm_load_print_meta: n_swa            = 0
0.00.049.393 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.393 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.394 I llm_load_print_meta: n_gqa            = 1
0.00.049.395 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.395 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.396 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.396 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.397 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.397 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.397 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.398 I llm_load_print_meta: n_ff             = 8192
0.00.049.398 I llm_load_print_meta: n_expert         = 0
0.00.049.398 I llm_load_print_meta: n_expert_used    = 0
0.00.049.398 I llm_load_print_meta: causal attn      = 1
0.00.049.398 I llm_load_print_meta: pooling type     = 0
0.00.049.398 I llm_load_print_meta: rope type        = 2
0.00.049.399 I llm_load_print_meta: rope scaling     = linear
0.00.049.399 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.400 I llm_load_print_meta: freq_scale_train = 1
0.00.049.400 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.400 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.401 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.401 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.412 I llm_load_print_meta: model type       = 1.4B
0.00.049.413 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.414 I llm_load_print_meta: model params     = 1.41 B
0.00.049.415 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.415 I llm_load_print_meta: general.name     = 1.4B
0.00.049.415 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.415 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.417 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.417 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.417 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.417 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.417 I llm_load_print_meta: max token length = 1024
0.00.051.377 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.378 I llm_load_tensors: offloading output layer to GPU
0.00.051.378 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.388 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.389 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.263 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.264 I llama_new_context_with_model: n_ctx         = 128
0.00.052.264 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.264 I llama_new_context_with_model: n_batch       = 128
0.00.052.265 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.265 I llama_new_context_with_model: flash_attn    = 0
0.00.052.265 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.266 I llama_new_context_with_model: freq_scale    = 1
0.00.052.266 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.266 I ggml_metal_init: allocating
0.00.052.272 I ggml_metal_init: found device: Apple M4
0.00.052.274 I ggml_metal_init: picking default device: Apple M4
0.00.052.801 I ggml_metal_init: using embedded metal library
0.00.054.753 I ggml_metal_init: GPU name:   Apple M4
0.00.054.754 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.755 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.755 I ggml_metal_init: simdgroup reduction   = true
0.00.054.755 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.755 I ggml_metal_init: has bfloat            = true
0.00.054.756 I ggml_metal_init: use bfloat            = true
0.00.054.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.771 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.775 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.788 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.681 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.682 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.682 I llama_new_context_with_model: graph nodes  = 967
0.00.064.682 I llama_new_context_with_model: graph splits = 2
0.00.064.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.720 I 
0.00.652.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.652.761 I perplexity: tokenizing the input ..
0.00.660.965 I perplexity: tokenization took 8.202 ms
0.00.660.970 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.679 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.881 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.912 I llama_perf_context_print:        load time =     643.91 ms
0.00.802.913 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.15 tokens per second)
0.00.802.914 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.914 I llama_perf_context_print:       total time =     150.19 ms /   129 tokens
0.00.803.388 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.076s
sys	0m0.126s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.847 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.356 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.364 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.364 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.707 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.708 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.709 I llama_model_loader: - type  f32:  194 tensors
0.00.025.709 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.970 I llm_load_vocab: special tokens cache size = 25
0.00.052.695 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.698 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.698 I llm_load_print_meta: arch             = gptneox
0.00.052.699 I llm_load_print_meta: vocab type       = BPE
0.00.052.699 I llm_load_print_meta: n_vocab          = 50304
0.00.052.699 I llm_load_print_meta: n_merges         = 50009
0.00.052.699 I llm_load_print_meta: vocab_only       = 0
0.00.052.699 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.700 I llm_load_print_meta: n_embd           = 2048
0.00.052.700 I llm_load_print_meta: n_layer          = 24
0.00.052.703 I llm_load_print_meta: n_head           = 16
0.00.052.704 I llm_load_print_meta: n_head_kv        = 16
0.00.052.704 I llm_load_print_meta: n_rot            = 32
0.00.052.704 I llm_load_print_meta: n_swa            = 0
0.00.052.705 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.705 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.705 I llm_load_print_meta: n_gqa            = 1
0.00.052.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.707 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.708 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.710 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.710 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.710 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.711 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.712 I llm_load_print_meta: n_ff             = 8192
0.00.052.712 I llm_load_print_meta: n_expert         = 0
0.00.052.712 I llm_load_print_meta: n_expert_used    = 0
0.00.052.712 I llm_load_print_meta: causal attn      = 1
0.00.052.714 I llm_load_print_meta: pooling type     = 0
0.00.052.715 I llm_load_print_meta: rope type        = 2
0.00.052.716 I llm_load_print_meta: rope scaling     = linear
0.00.052.716 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.716 I llm_load_print_meta: freq_scale_train = 1
0.00.052.716 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.717 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.717 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.717 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.729 I llm_load_print_meta: model type       = 1.4B
0.00.052.729 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.730 I llm_load_print_meta: model params     = 1.41 B
0.00.052.731 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.732 I llm_load_print_meta: general.name     = 1.4B
0.00.052.732 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.732 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.732 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.732 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.733 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.733 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.733 I llm_load_print_meta: max token length = 1024
0.00.054.838 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.838 I llm_load_tensors: offloading output layer to GPU
0.00.054.839 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.849 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.850 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.794 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.795 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.795 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.795 I llama_new_context_with_model: n_batch       = 2048
0.00.055.795 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.796 I llama_new_context_with_model: flash_attn    = 0
0.00.055.796 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.796 I llama_new_context_with_model: freq_scale    = 1
0.00.055.796 I ggml_metal_init: allocating
0.00.055.800 I ggml_metal_init: found device: Apple M4
0.00.055.802 I ggml_metal_init: picking default device: Apple M4
0.00.056.361 I ggml_metal_init: using embedded metal library
0.00.058.305 I ggml_metal_init: GPU name:   Apple M4
0.00.058.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.307 I ggml_metal_init: simdgroup reduction   = true
0.00.058.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.309 I ggml_metal_init: has bfloat            = true
0.00.058.309 I ggml_metal_init: use bfloat            = true
0.00.058.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.886 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.894 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.913 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.974 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.975 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.976 I llama_new_context_with_model: graph nodes  = 967
0.00.087.976 I llama_new_context_with_model: graph splits = 2
0.00.087.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.769 I main: llama threadpool init, n_threads = 4
0.00.756.801 I 
0.00.756.831 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.756.833 I 
0.00.757.055 I sampler seed: 1234
0.00.757.060 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.075 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.076 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.076 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.626.592 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.626.592 I llama_perf_context_print:        load time =     746.92 ms
0.01.626.593 I llama_perf_context_print: prompt eval time =      38.49 ms /     7 tokens (    5.50 ms per token,   181.85 tokens per second)
0.01.626.594 I llama_perf_context_print:        eval time =     828.01 ms /    63 runs   (   13.14 ms per token,    76.09 tokens per second)
0.01.626.595 I llama_perf_context_print:       total time =     869.83 ms /    70 tokens
0.01.626.764 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4227 (fbc6438a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.924 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.648 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.649 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.651 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.651 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.656 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.656 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.656 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.724 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.692 I llama_model_loader: - type  f32:  194 tensors
0.00.024.693 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.692 I llm_load_vocab: special tokens cache size = 25
0.00.050.577 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.580 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.580 I llm_load_print_meta: arch             = gptneox
0.00.050.580 I llm_load_print_meta: vocab type       = BPE
0.00.050.581 I llm_load_print_meta: n_vocab          = 50304
0.00.050.581 I llm_load_print_meta: n_merges         = 50009
0.00.050.581 I llm_load_print_meta: vocab_only       = 0
0.00.050.581 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.581 I llm_load_print_meta: n_embd           = 2048
0.00.050.581 I llm_load_print_meta: n_layer          = 24
0.00.050.584 I llm_load_print_meta: n_head           = 16
0.00.050.587 I llm_load_print_meta: n_head_kv        = 16
0.00.050.587 I llm_load_print_meta: n_rot            = 32
0.00.050.587 I llm_load_print_meta: n_swa            = 0
0.00.050.588 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.588 I llm_load_print_meta: n_gqa            = 1
0.00.050.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.590 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.591 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.591 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.592 I llm_load_print_meta: n_ff             = 8192
0.00.050.592 I llm_load_print_meta: n_expert         = 0
0.00.050.592 I llm_load_print_meta: n_expert_used    = 0
0.00.050.592 I llm_load_print_meta: causal attn      = 1
0.00.050.593 I llm_load_print_meta: pooling type     = 0
0.00.050.597 I llm_load_print_meta: rope type        = 2
0.00.050.597 I llm_load_print_meta: rope scaling     = linear
0.00.050.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.598 I llm_load_print_meta: freq_scale_train = 1
0.00.050.599 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.600 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.600 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.600 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.600 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.612 I llm_load_print_meta: model type       = 1.4B
0.00.050.612 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.613 I llm_load_print_meta: model params     = 1.41 B
0.00.050.613 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.614 I llm_load_print_meta: general.name     = 1.4B
0.00.050.614 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.615 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.615 I llm_load_print_meta: max token length = 1024
0.00.052.164 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.164 I llm_load_tensors: offloading output layer to GPU
0.00.052.164 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.173 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.174 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.997 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.998 I llama_new_context_with_model: n_ctx         = 128
0.00.052.998 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.998 I llama_new_context_with_model: n_batch       = 128
0.00.052.999 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.999 I llama_new_context_with_model: flash_attn    = 0
0.00.052.999 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.999 I llama_new_context_with_model: freq_scale    = 1
0.00.053.000 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.000 I ggml_metal_init: allocating
0.00.053.003 I ggml_metal_init: found device: Apple M4
0.00.053.005 I ggml_metal_init: picking default device: Apple M4
0.00.053.535 I ggml_metal_init: using embedded metal library
0.00.055.425 I ggml_metal_init: GPU name:   Apple M4
0.00.055.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.427 I ggml_metal_init: simdgroup reduction   = true
0.00.055.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.427 I ggml_metal_init: has bfloat            = true
0.00.055.427 I ggml_metal_init: use bfloat            = true
0.00.055.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.501 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.515 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.425 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.426 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.427 I llama_new_context_with_model: graph nodes  = 967
0.00.065.427 I llama_new_context_with_model: graph splits = 2
0.00.065.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.174.614 I 
0.00.174.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.174.667 I perplexity: tokenizing the input ..
0.00.182.142 I perplexity: tokenization took 7.473 ms
0.00.182.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.323.050 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.324.278 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.324.301 I llama_perf_context_print:        load time =     164.68 ms
0.00.324.302 I llama_perf_context_print: prompt eval time =     140.61 ms /   128 tokens (    1.10 ms per token,   910.32 tokens per second)
0.00.324.303 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.324.304 I llama_perf_context_print:       total time =     149.69 ms /   129 tokens
0.00.324.661 I ggml_metal_free: deallocating

real	0m0.341s
user	0m0.075s
sys	0m0.044s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4227 (fbc6438a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13760d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13760d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13760daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13760dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13760eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13760f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13760fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1376101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1376108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137611700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137611ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1376125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137612d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137613430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1376143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137614cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137615e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137616130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1376165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137617120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137617660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137617920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137617dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137618260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137618700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137618ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137619040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1376194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137619980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137619e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13761f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13761f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1376203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137620870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137620d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1376211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137621650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137621af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137621f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137622430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1376228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137622d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137623210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1376236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137623ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137624490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137624930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137624dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137625710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137625bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137626050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1376264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137626e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1376272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137627770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1376280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137628550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1376289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137629330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1376297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137629c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13762a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13762a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13762aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13761b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13762b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13762b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13762b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13762be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13762c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13762c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13762cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13762d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13762d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13762da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13762dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13762e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13762e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13762ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13762f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13762f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13762faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13762ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1376303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137630880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137630d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1376311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137631660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137631b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137631fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137632440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1376328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137632d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137633220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1376336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137633b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137634000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1376344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137634de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137635720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137636060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137636500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1376369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137636e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1376372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137637780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137637c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1376380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137638560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137638ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1376397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13763a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13763a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13763aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13763afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13763b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13763ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13763bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13763c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13763c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13763ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13763d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13763daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13763e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13763e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13763ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13763f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13763f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13763fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137640380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1376408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137640e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137641370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1376418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137641e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1376428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137643350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1376438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137643df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137644340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137644890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137644de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137645330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137645dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137646dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137647310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137647860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137647db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137648300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1376492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137649840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137649d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13764a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13764a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13764ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13764b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13764b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13764bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13764c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13764c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13764cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13764d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13764d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13764dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13764e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13764e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13764ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13764f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13764f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13764fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137650280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1376507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137650d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137651270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1376517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137651d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137652260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1376527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137652c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1376530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137653590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137653a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137653ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137654810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137654cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137655150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1376555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137655a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137655f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1376563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137656920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137657040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137657760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137657e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1376585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137658860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137658e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137659480 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137610140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1376105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137611000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1376118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137612850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137612f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137613630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137614410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137615480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137615b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137616260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137617040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137617920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137618200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137618670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137618ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137618f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1376193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13761a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13761a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13761acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13761b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13761b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13761ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13761be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1376209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1376212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1376228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1376231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1376250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1376259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1376262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1376278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1376281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13762a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13762a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13762a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13762ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13762b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13762b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13762bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13762bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13762c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13762c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13762cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13762d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13762d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13762da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13762dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13762e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13762e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13762ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13762f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13762f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13762f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13762fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1376306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1376325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1376337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1376344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1376356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137635b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137636870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137636ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1376375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137637a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137637ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137638310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137638780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137638bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137639060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1376394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137639940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13763a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13763a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13763ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13763af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13763b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13763b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13763bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13763c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13763c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13763ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13763ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13763d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13763d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13763dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13763e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13763e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13763ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13763f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13763f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13763f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13763fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1376425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1376437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137643c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1376444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137644960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137644dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1376456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137646400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137647150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1376475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137647ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137648780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137648bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137649060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1376494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137649db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13764a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13764a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13764ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13764af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13764b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13764b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13764bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13764c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13764c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13764ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13764ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13764d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13764d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13764dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13764e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13764e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13764e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13764ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13764f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13764f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13764fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13764ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1376503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137650ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137651580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1376519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137651e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1376522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1376529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1376530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1376537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137653e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137654300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137654be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13760ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137610140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1376105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137611000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1376118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137612850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137612f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137613630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137614410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137614d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137615480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137615b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137616260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137617040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137617920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137618200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137618670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137618ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137618f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1376193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13761a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13761a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13761acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13761b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13761b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13761ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13761be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137620570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1376209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137620e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1376212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137621ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137622010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1376228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1376231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137623ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137624800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137624c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1376250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137625550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1376259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1376262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137626710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137626b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137627460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1376278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137627d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1376281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137628620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137629370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13762a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13762a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13762a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13762ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13762b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13762b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13762bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13762bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13762c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13762c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13762cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13762d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13762d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13762da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13762dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13762e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13762e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13762ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13762f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13762f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13762f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13762fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137630260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1376306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137630b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137630fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137631420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137631d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137632170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1376325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1376337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137633c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1376344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137634960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137634dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1376356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137635b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137636870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137636ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1376375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137637a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137637ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137638310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137638780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137638bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137639060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1376394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137639940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13763a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13763a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13763ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13763af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13763b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13763b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13763bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13763c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13763c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13763ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13763ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13763d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13763d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13763dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13763e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13763e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13763ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13763f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13763f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13763f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13763fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137640260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137640fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137641420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137642170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1376425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137642a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137642ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1376437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137643c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137644080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1376444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137644960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137644dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1376456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137646400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137647150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1376475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137647ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137648780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137648bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137649060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1376494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137649db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13764a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13764a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13764ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13764af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13764b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13764b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13764bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13764c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13764c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13764ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13764ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13764d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13764d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13764dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13764e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13764e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13764e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13764ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13764f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13764f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13764fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13764ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1376503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137650ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137651580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1376519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137651e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1376522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1376529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1376530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1376537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137653e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137654300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137654770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137654be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.798s
user	0m0.288s
sys	0m0.301s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4227 (fbc6438a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d8074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d807c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d808210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d8087c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d808d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d809320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d8098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d809e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d80a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d80a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d80ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d80b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d80be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d80c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d80ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d80d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d80dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d80e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d80ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d80f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d80f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d8100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d8107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d811060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d811780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d811a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d812050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d812cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d813200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d8134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d813960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d813c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d8144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d814cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d815150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d8155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d815a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d815f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d8163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d816870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d816d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d8171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d817650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d817910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d817f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d818e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d819460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d819a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d81a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d81a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d81aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d81b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d81baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d81bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d81c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d81c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d81ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d81d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d81d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d81dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d81e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d81e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d81e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d81ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d81f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d81f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d81fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d820100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d8205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d820a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d820ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d821380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d821820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d821cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d822160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d822600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d822aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d822f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d823880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d823d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d8241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d824660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d824b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d824fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d825440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d8258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d825d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d826220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d8266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d826b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d827000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d8274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d827940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d827de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d828430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d8288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d829210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d8296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d829b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d829ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d82a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d82a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d82add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d82b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d82b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d82bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d82c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d82c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d82c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d82ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d82d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d82d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d82dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d82e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d82e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d82e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d82ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d82f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d82f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d82fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d830110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d8305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d830a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d830ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d831390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d831cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d832170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d832610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d832ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d832f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d8333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d833890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d833d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d8341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d834670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d834b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d834fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d8358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d835d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d836230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d8366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d836b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d837010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d8374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d837950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d837df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d838340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d838890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d838de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d839330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d8395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d839c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d83a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d83a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d83ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d83b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d83bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d83c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d83c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d83ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d83d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d83d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d83dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d83e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d83e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d83ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d83f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d83f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d83fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d840190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d8406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d840c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d8416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d841c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d842170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d8426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d842c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d843160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d8436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d843c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d844150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d8446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d844bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d845140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d845be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d846130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d846680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d846bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d847120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d847670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d847bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d848110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d848660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d848bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d849100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d849650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d849ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d84a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d84a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d84ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d84b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d84b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d84bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d84c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d84c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d84cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d84d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d84d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d84db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d84e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d84e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d84eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d84f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d84f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d84fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d84ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d850480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d850920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d850dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d851260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d851700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d852040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d8524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d852980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d852e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d8532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d853760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d853cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d8543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d854af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d855210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d855930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d855bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d856200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d856810 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.090.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c6059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c605e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c606290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c606700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c606b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c606fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c607450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c6078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c607d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c6081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c608610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c608c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c609790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c609f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c60a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c60ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c60b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c60bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c60c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c60cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c60d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c60d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c60e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c60e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c60ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c60f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c60f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c60f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c60fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c610210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c610680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c610bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c611020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c6112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c611bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c612030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c6124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c612910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c612d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c6131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c613660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c613ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c613f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c6143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c614c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c615100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c615570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c6159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c615e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c6162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c616ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c617010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c617480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c6179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c617ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c618360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c6187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c618c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c6190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c619990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c619e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c61a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c61a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c61ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104004230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1040046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104004b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104004f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1040053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104005860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104005cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104006140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1040065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104006a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104006e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104007300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104007770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104007be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104008050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1040084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104008930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104008da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104009210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104009680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104009af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104009f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10400a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10400a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10400acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10400b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10400b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10400ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10400be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10400c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10400c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10400cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10400d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10400d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10400d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10400dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10400e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10400e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10400ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10400ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10400f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10400f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10400fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104010100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104010570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1040109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104010e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1040112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104011730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104011ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104012010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104012480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1040128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104012d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1040131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104013640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104013ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104013f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104014390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104014800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104014c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1040150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104015550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1040159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104015e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1040162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104016710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104016b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104016ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104017460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1040178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104017d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1040181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104018620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104018a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104018f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104019370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1040197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104019c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10401a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10401a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10401a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10401ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10401b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10401b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10401bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10401bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10401c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10401c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10401cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10401d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10401d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10401da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10401dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10401e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10401e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10401ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10401f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10401f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1040200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104020380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104020640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104020ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104020f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104021390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104021800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104021c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1040220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104022550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1040229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104022e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1040232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104023710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104023b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104023ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104024460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1040248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104024d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1040251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104025620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104025a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104025f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104026370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1040267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104026c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1040270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104027530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1040279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104027e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104028280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1040286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104028b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104028fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104029440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1040298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104029d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10402a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10402a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10402aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10402aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10402b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10402b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10402bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10402c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10402c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10402c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10402cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10402d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10402d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10402db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10402dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10402e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10402e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10402ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10402f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10402f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10402fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10402fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104030330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1040307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104030c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104031080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1040314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104031960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104031dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104032240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1040326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104032b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104032f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104033400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104033f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104034660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104034d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1040354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104035a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104035e90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c6059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c605e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c606290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c606700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c606b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c606fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c607450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c6078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c607d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c6081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c608610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c608bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c6094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c609c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c60a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c60ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c60b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c60b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c60c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c60c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c60d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c60d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c60de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c60e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c60f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c60f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c60f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c60fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c610260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c6106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c610b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c610fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c611270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c6116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c611b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c611fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c612430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c6128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c612d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c613180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c6135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c613a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c613ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c6147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c614c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c615090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c615500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c615970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c615de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c616250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c6166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c616b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c616fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c617410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c617880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c617cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c618160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c6185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c618a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c619320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c619790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c619c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c61a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c61a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c61a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c61adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c61b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c61b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c61bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c61c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c61c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c61cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c61d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c61d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c61dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c61dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c61e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c61e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c61eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c61f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c61f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c61fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c620840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c620d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c621260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c621770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c622190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c6226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c622bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c6230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c6235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c623ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c623ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c624f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c625430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c625940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c625e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c626870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c626d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c6277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c628330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c6285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c628b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c629010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c629520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c629a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c629f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c62a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c62a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c62ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c62b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c62b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c62bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c62c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c62c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c62ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c62d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c62d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c62dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c62e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c62e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c62eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c62f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c62f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c62fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c62ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c630480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c630ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c6313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c6318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c631dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c6322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c6327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c632d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c633210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c633720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c634650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c635070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c635d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c6367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c636d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c636fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c6375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c638200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c638810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c638e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c639610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c639ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c639f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c63a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c63aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c63b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c63b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c63bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c63c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c63c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c63cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c63d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c63d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c63db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c63e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c63e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c63eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c63f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c63f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c63fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c6400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c6405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c641090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c6415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c641b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c642080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c6425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c642b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c6435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c643b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c644060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c6445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c644b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c645050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c6455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c645af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c646040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c646ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c647030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c647580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c648020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c648570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c649010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c649560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c64a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c64a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c64aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c64aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c64b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c64ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c64bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c64c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c64ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c64cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c64de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c64e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c64e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c64ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c64f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c64f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c64fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c64fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c650360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c650800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c650ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c651db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c6524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c652bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c653310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c6535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c653be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c6541f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.943s
user	0m0.239s
sys	0m0.139s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.24 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.25 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
