Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.327s
user	0m0.558s
sys	0m0.793s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama-gguf
[ 21%] Built target llama
[ 21%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 21%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Linking C executable ../bin/test-c
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Built target llava
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking CXX executable ../../bin/llama-run
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking CXX static library libllava_static.a
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-simple-chat
[ 32%] Built target llama-simple
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target test-c
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llama-run
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-chat-template
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-chat-template
[ 48%] Built target test-arg-parser
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 50%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 62%] Built target test-rope
[ 63%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 63%] Built target llama-batched-bench
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 65%] Built target test-json-schema-to-grammar
[ 65%] Built target llama-batched
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-bench
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-infill
[ 73%] Built target llama-lookup
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 76%] Generating loading.html.hpp
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Generating completion.js.hpp
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-cli
[ 84%] Generating deps_daisyui.min.css.hpp
[ 84%] Generating deps_markdown-it.js.hpp
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-parallel
[ 84%] Built target llama-quantize
[ 84%] Built target llama-retrieval
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Generating deps_tailwindcss.js.hpp
[ 87%] Generating deps_vue.esm-browser.js.hpp
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-tokenize
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Generating index.html.hpp
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-cvector-generator
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.495s
user	0m5.988s
sys	0m9.420s

main: quantize time =  4446.02 ms
main:    total time =  4446.02 ms

main: quantize time =  2107.02 ms
main:    total time =  2107.02 ms

main: quantize time =  1847.46 ms
main:    total time =  1847.46 ms

main: quantize time =  2172.61 ms
main:    total time =  2172.61 ms

main: quantize time =  2601.84 ms
main:    total time =  2601.84 ms

main: quantize time =  5120.19 ms
main:    total time =  5120.19 ms

main: quantize time =  5759.96 ms
main:    total time =  5759.96 ms

main: quantize time =  6864.43 ms
main:    total time =  6864.43 ms

main: quantize time =  5902.83 ms
main:    total time =  5902.83 ms

main: quantize time =  4538.58 ms
main:    total time =  4538.58 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.264 I main: llama backend init
0.00.000.285 I main: load the model and apply lora adapter, if any
0.00.029.606 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.500 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.525 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.629 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.404 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.407 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.408 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.408 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.409 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.410 I llama_model_loader: - type  f32:  194 tensors
0.00.060.411 I llama_model_loader: - type  f16:   98 tensors
0.00.091.804 I llm_load_vocab: special tokens cache size = 25
0.00.099.074 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.077 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.078 I llm_load_print_meta: arch             = gptneox
0.00.099.078 I llm_load_print_meta: vocab type       = BPE
0.00.099.078 I llm_load_print_meta: n_vocab          = 50304
0.00.099.078 I llm_load_print_meta: n_merges         = 50009
0.00.099.078 I llm_load_print_meta: vocab_only       = 0
0.00.099.079 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.079 I llm_load_print_meta: n_embd           = 2048
0.00.099.079 I llm_load_print_meta: n_layer          = 24
0.00.099.082 I llm_load_print_meta: n_head           = 16
0.00.099.083 I llm_load_print_meta: n_head_kv        = 16
0.00.099.083 I llm_load_print_meta: n_rot            = 32
0.00.099.083 I llm_load_print_meta: n_swa            = 0
0.00.099.083 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.084 I llm_load_print_meta: n_gqa            = 1
0.00.099.085 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.087 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.088 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.088 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.089 I llm_load_print_meta: n_ff             = 8192
0.00.099.089 I llm_load_print_meta: n_expert         = 0
0.00.099.089 I llm_load_print_meta: n_expert_used    = 0
0.00.099.089 I llm_load_print_meta: causal attn      = 1
0.00.099.090 I llm_load_print_meta: pooling type     = 0
0.00.099.090 I llm_load_print_meta: rope type        = 2
0.00.099.090 I llm_load_print_meta: rope scaling     = linear
0.00.099.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.091 I llm_load_print_meta: freq_scale_train = 1
0.00.099.091 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.091 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.091 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.091 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.092 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.092 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.104 I llm_load_print_meta: model type       = 1.4B
0.00.099.104 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.105 I llm_load_print_meta: model params     = 1.41 B
0.00.099.105 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.105 I llm_load_print_meta: general.name     = 1.4B
0.00.099.106 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.106 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.106 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.107 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.099.107 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.107 I llm_load_print_meta: max token length = 1024
0.00.101.641 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.641 I llm_load_tensors: offloading output layer to GPU
0.00.101.641 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.658 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.659 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.629 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.630 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.630 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.630 I llama_new_context_with_model: n_batch       = 2048
0.00.102.630 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.631 I llama_new_context_with_model: flash_attn    = 0
0.00.102.631 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.631 I llama_new_context_with_model: freq_scale    = 1
0.00.102.632 I ggml_metal_init: allocating
0.00.102.639 I ggml_metal_init: found device: Apple M4
0.00.102.643 I ggml_metal_init: picking default device: Apple M4
0.00.103.300 I ggml_metal_init: using embedded metal library
0.00.110.469 I ggml_metal_init: GPU name:   Apple M4
0.00.110.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.472 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.472 I ggml_metal_init: simdgroup reduction   = true
0.00.110.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.472 I ggml_metal_init: has bfloat            = true
0.00.110.473 I ggml_metal_init: use bfloat            = true
0.00.110.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.145.209 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.145.215 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.145.234 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.146.144 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.146.145 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.146.145 I llama_new_context_with_model: graph nodes  = 967
0.00.146.145 I llama_new_context_with_model: graph splits = 2
0.00.146.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.215.250 I main: llama threadpool init, n_threads = 4
0.00.215.290 I 
0.00.215.309 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.215.309 I 
0.00.215.384 I sampler seed: 1234
0.00.215.389 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.215.411 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.215.413 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.215.413 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.075.648 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.02.075.648 I llama_perf_context_print:        load time =     185.63 ms
0.02.075.649 I llama_perf_context_print: prompt eval time =      37.73 ms /     7 tokens (    5.39 ms per token,   185.50 tokens per second)
0.02.075.650 I llama_perf_context_print:        eval time =    1819.66 ms /    63 runs   (   28.88 ms per token,    34.62 tokens per second)
0.02.075.651 I llama_perf_context_print:       total time =    1860.40 ms /    70 tokens
0.02.075.819 I ggml_metal_free: deallocating

real	0m2.381s
user	0m0.145s
sys	0m0.086s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.759 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.619 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.620 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.622 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.622 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.625 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.152 I llama_model_loader: - type  f32:  194 tensors
0.00.038.153 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.712 I llm_load_vocab: special tokens cache size = 25
0.00.069.411 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.414 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.415 I llm_load_print_meta: arch             = gptneox
0.00.069.415 I llm_load_print_meta: vocab type       = BPE
0.00.069.415 I llm_load_print_meta: n_vocab          = 50304
0.00.069.415 I llm_load_print_meta: n_merges         = 50009
0.00.069.416 I llm_load_print_meta: vocab_only       = 0
0.00.069.417 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.418 I llm_load_print_meta: n_embd           = 2048
0.00.069.418 I llm_load_print_meta: n_layer          = 24
0.00.069.421 I llm_load_print_meta: n_head           = 16
0.00.069.422 I llm_load_print_meta: n_head_kv        = 16
0.00.069.422 I llm_load_print_meta: n_rot            = 32
0.00.069.422 I llm_load_print_meta: n_swa            = 0
0.00.069.423 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.423 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.423 I llm_load_print_meta: n_gqa            = 1
0.00.069.424 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.425 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.425 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.425 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.425 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.427 I llm_load_print_meta: n_ff             = 8192
0.00.069.427 I llm_load_print_meta: n_expert         = 0
0.00.069.427 I llm_load_print_meta: n_expert_used    = 0
0.00.069.427 I llm_load_print_meta: causal attn      = 1
0.00.069.427 I llm_load_print_meta: pooling type     = 0
0.00.069.427 I llm_load_print_meta: rope type        = 2
0.00.069.428 I llm_load_print_meta: rope scaling     = linear
0.00.069.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.428 I llm_load_print_meta: freq_scale_train = 1
0.00.069.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.431 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.431 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.431 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.432 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.445 I llm_load_print_meta: model type       = 1.4B
0.00.069.445 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.446 I llm_load_print_meta: model params     = 1.41 B
0.00.069.446 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.446 I llm_load_print_meta: general.name     = 1.4B
0.00.069.447 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.447 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.447 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.447 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.448 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.448 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.448 I llm_load_print_meta: max token length = 1024
0.00.071.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.679 I llm_load_tensors: offloading output layer to GPU
0.00.071.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.684 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.685 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.710 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.711 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.711 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.712 I llama_new_context_with_model: n_batch       = 2048
0.00.072.712 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.712 I llama_new_context_with_model: flash_attn    = 0
0.00.072.712 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.713 I llama_new_context_with_model: freq_scale    = 1
0.00.072.713 I ggml_metal_init: allocating
0.00.072.720 I ggml_metal_init: found device: Apple M4
0.00.072.722 I ggml_metal_init: picking default device: Apple M4
0.00.073.452 I ggml_metal_init: using embedded metal library
0.00.075.818 I ggml_metal_init: GPU name:   Apple M4
0.00.075.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.821 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.821 I ggml_metal_init: simdgroup reduction   = true
0.00.075.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.822 I ggml_metal_init: has bfloat            = true
0.00.075.822 I ggml_metal_init: use bfloat            = true
0.00.075.822 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.858 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.869 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.897 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.936 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.937 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.938 I llama_new_context_with_model: graph nodes  = 967
0.00.111.938 I llama_new_context_with_model: graph splits = 2
0.00.111.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.216.087 I main: llama threadpool init, n_threads = 4
0.01.216.122 I 
0.01.216.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.216.142 I 
0.01.216.371 I sampler seed: 1234
0.01.216.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.216.427 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.216.445 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.216.445 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.307.251 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.307.252 I llama_perf_context_print:        load time =    1206.32 ms
0.02.307.253 I llama_perf_context_print: prompt eval time =      36.19 ms /     7 tokens (    5.17 ms per token,   193.43 tokens per second)
0.02.307.253 I llama_perf_context_print:        eval time =    1051.68 ms /    63 runs   (   16.69 ms per token,    59.90 tokens per second)
0.02.307.257 I llama_perf_context_print:       total time =    1091.17 ms /    70 tokens
0.02.307.421 I ggml_metal_free: deallocating

real	0m2.324s
user	0m0.117s
sys	0m0.226s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.016.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.860 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.870 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.874 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.874 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.076 I llama_model_loader: - type  f32:  194 tensors
0.00.042.076 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.077 I llama_model_loader: - type q6_K:    1 tensors
0.00.068.677 I llm_load_vocab: special tokens cache size = 25
0.00.078.145 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.149 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.149 I llm_load_print_meta: arch             = gptneox
0.00.078.149 I llm_load_print_meta: vocab type       = BPE
0.00.078.150 I llm_load_print_meta: n_vocab          = 50304
0.00.078.150 I llm_load_print_meta: n_merges         = 50009
0.00.078.150 I llm_load_print_meta: vocab_only       = 0
0.00.078.150 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.151 I llm_load_print_meta: n_embd           = 2048
0.00.078.151 I llm_load_print_meta: n_layer          = 24
0.00.078.154 I llm_load_print_meta: n_head           = 16
0.00.078.155 I llm_load_print_meta: n_head_kv        = 16
0.00.078.158 I llm_load_print_meta: n_rot            = 32
0.00.078.158 I llm_load_print_meta: n_swa            = 0
0.00.078.158 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.158 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.159 I llm_load_print_meta: n_gqa            = 1
0.00.078.161 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.162 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.163 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.164 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.167 I llm_load_print_meta: n_ff             = 8192
0.00.078.168 I llm_load_print_meta: n_expert         = 0
0.00.078.168 I llm_load_print_meta: n_expert_used    = 0
0.00.078.168 I llm_load_print_meta: causal attn      = 1
0.00.078.168 I llm_load_print_meta: pooling type     = 0
0.00.078.168 I llm_load_print_meta: rope type        = 2
0.00.078.169 I llm_load_print_meta: rope scaling     = linear
0.00.078.169 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.169 I llm_load_print_meta: freq_scale_train = 1
0.00.078.170 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.175 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.175 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.175 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.176 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.176 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.176 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.189 I llm_load_print_meta: model type       = 1.4B
0.00.078.190 I llm_load_print_meta: model ftype      = Q4_0
0.00.078.190 I llm_load_print_meta: model params     = 1.41 B
0.00.078.191 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.078.191 I llm_load_print_meta: general.name     = 1.4B
0.00.078.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.192 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.192 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.192 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.193 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.193 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.193 I llm_load_print_meta: max token length = 1024
0.00.081.081 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.081 I llm_load_tensors: offloading output layer to GPU
0.00.081.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.093 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.095 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.082.654 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.656 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.656 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.657 I llama_new_context_with_model: n_batch       = 2048
0.00.082.657 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.657 I llama_new_context_with_model: flash_attn    = 0
0.00.082.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.659 I llama_new_context_with_model: freq_scale    = 1
0.00.082.659 I ggml_metal_init: allocating
0.00.082.669 I ggml_metal_init: found device: Apple M4
0.00.082.672 I ggml_metal_init: picking default device: Apple M4
0.00.083.661 I ggml_metal_init: using embedded metal library
0.00.087.166 I ggml_metal_init: GPU name:   Apple M4
0.00.087.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.170 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.170 I ggml_metal_init: simdgroup reduction   = true
0.00.087.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.171 I ggml_metal_init: has bfloat            = true
0.00.087.171 I ggml_metal_init: use bfloat            = true
0.00.087.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.458 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.470 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.494 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.528 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.529 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.530 I llama_new_context_with_model: graph nodes  = 967
0.00.125.530 I llama_new_context_with_model: graph splits = 2
0.00.125.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.930.328 I main: llama threadpool init, n_threads = 4
0.00.930.365 I 
0.00.930.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.930.386 I 
0.00.930.623 I sampler seed: 1234
0.00.930.628 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.930.639 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.930.639 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.930.639 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.614.742 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.614.743 I llama_perf_context_print:        load time =     913.69 ms
0.01.614.744 I llama_perf_context_print: prompt eval time =      37.06 ms /     7 tokens (    5.29 ms per token,   188.87 tokens per second)
0.01.614.745 I llama_perf_context_print:        eval time =     643.94 ms /    63 runs   (   10.22 ms per token,    97.84 tokens per second)
0.01.614.746 I llama_perf_context_print:       total time =     684.41 ms /    70 tokens
0.01.614.935 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.126s
sys	0m0.180s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.342 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.089 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.091 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.092 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.093 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.771 I llama_model_loader: - type  f32:  194 tensors
0.00.026.771 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.036 I llm_load_vocab: special tokens cache size = 25
0.00.053.248 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.250 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.251 I llm_load_print_meta: arch             = gptneox
0.00.053.251 I llm_load_print_meta: vocab type       = BPE
0.00.053.251 I llm_load_print_meta: n_vocab          = 50304
0.00.053.251 I llm_load_print_meta: n_merges         = 50009
0.00.053.252 I llm_load_print_meta: vocab_only       = 0
0.00.053.252 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.252 I llm_load_print_meta: n_embd           = 2048
0.00.053.252 I llm_load_print_meta: n_layer          = 24
0.00.053.255 I llm_load_print_meta: n_head           = 16
0.00.053.256 I llm_load_print_meta: n_head_kv        = 16
0.00.053.256 I llm_load_print_meta: n_rot            = 32
0.00.053.256 I llm_load_print_meta: n_swa            = 0
0.00.053.256 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.257 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.257 I llm_load_print_meta: n_gqa            = 1
0.00.053.258 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.259 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.259 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.260 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.260 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.261 I llm_load_print_meta: n_ff             = 8192
0.00.053.261 I llm_load_print_meta: n_expert         = 0
0.00.053.261 I llm_load_print_meta: n_expert_used    = 0
0.00.053.262 I llm_load_print_meta: causal attn      = 1
0.00.053.262 I llm_load_print_meta: pooling type     = 0
0.00.053.262 I llm_load_print_meta: rope type        = 2
0.00.053.262 I llm_load_print_meta: rope scaling     = linear
0.00.053.262 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.263 I llm_load_print_meta: freq_scale_train = 1
0.00.053.263 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.263 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.263 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.263 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.264 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.275 I llm_load_print_meta: model type       = 1.4B
0.00.053.275 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.276 I llm_load_print_meta: model params     = 1.41 B
0.00.053.276 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.276 I llm_load_print_meta: general.name     = 1.4B
0.00.053.276 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.277 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.279 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.279 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.279 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.279 I llm_load_print_meta: max token length = 1024
0.00.054.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.773 I llm_load_tensors: offloading output layer to GPU
0.00.054.774 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.783 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.784 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.604 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.604 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.605 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.605 I llama_new_context_with_model: n_batch       = 2048
0.00.055.605 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.605 I llama_new_context_with_model: flash_attn    = 0
0.00.055.606 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.606 I llama_new_context_with_model: freq_scale    = 1
0.00.055.607 I ggml_metal_init: allocating
0.00.055.613 I ggml_metal_init: found device: Apple M4
0.00.055.615 I ggml_metal_init: picking default device: Apple M4
0.00.056.158 I ggml_metal_init: using embedded metal library
0.00.058.092 I ggml_metal_init: GPU name:   Apple M4
0.00.058.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.095 I ggml_metal_init: simdgroup reduction   = true
0.00.058.095 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.095 I ggml_metal_init: has bfloat            = true
0.00.058.095 I ggml_metal_init: use bfloat            = true
0.00.058.096 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.403 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.410 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.427 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.518 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.519 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.520 I llama_new_context_with_model: graph nodes  = 967
0.00.086.520 I llama_new_context_with_model: graph splits = 2
0.00.086.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.947 I main: llama threadpool init, n_threads = 4
0.00.666.981 I 
0.00.666.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.666.998 I 
0.00.667.227 I sampler seed: 1234
0.00.667.231 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.251 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.251 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.251 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.391.624 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.01.391.624 I llama_perf_context_print:        load time =     657.60 ms
0.01.391.625 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.14 tokens per second)
0.01.391.626 I llama_perf_context_print:        eval time =     685.53 ms /    63 runs   (   10.88 ms per token,    91.90 tokens per second)
0.01.391.626 I llama_perf_context_print:       total time =     724.68 ms /    70 tokens
0.01.391.804 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.011 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.197 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.197 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.204 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.205 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.205 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.206 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.100 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.101 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.101 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.101 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.102 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.102 I llama_model_loader: - type  f32:  194 tensors
0.00.033.103 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.103 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.184 I llm_load_vocab: special tokens cache size = 25
0.00.061.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.130 I llm_load_print_meta: arch             = gptneox
0.00.061.130 I llm_load_print_meta: vocab type       = BPE
0.00.061.131 I llm_load_print_meta: n_vocab          = 50304
0.00.061.131 I llm_load_print_meta: n_merges         = 50009
0.00.061.131 I llm_load_print_meta: vocab_only       = 0
0.00.061.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.131 I llm_load_print_meta: n_embd           = 2048
0.00.061.131 I llm_load_print_meta: n_layer          = 24
0.00.061.134 I llm_load_print_meta: n_head           = 16
0.00.061.137 I llm_load_print_meta: n_head_kv        = 16
0.00.061.137 I llm_load_print_meta: n_rot            = 32
0.00.061.138 I llm_load_print_meta: n_swa            = 0
0.00.061.138 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.138 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.139 I llm_load_print_meta: n_gqa            = 1
0.00.061.139 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.140 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.141 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.141 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.141 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.141 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.141 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.142 I llm_load_print_meta: n_ff             = 8192
0.00.061.143 I llm_load_print_meta: n_expert         = 0
0.00.061.143 I llm_load_print_meta: n_expert_used    = 0
0.00.061.148 I llm_load_print_meta: causal attn      = 1
0.00.061.150 I llm_load_print_meta: pooling type     = 0
0.00.061.150 I llm_load_print_meta: rope type        = 2
0.00.061.151 I llm_load_print_meta: rope scaling     = linear
0.00.061.151 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.151 I llm_load_print_meta: freq_scale_train = 1
0.00.061.153 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.153 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.153 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.153 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.153 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.153 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.155 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.167 I llm_load_print_meta: model type       = 1.4B
0.00.061.168 I llm_load_print_meta: model ftype      = Q5_0
0.00.061.168 I llm_load_print_meta: model params     = 1.41 B
0.00.061.168 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.061.168 I llm_load_print_meta: general.name     = 1.4B
0.00.061.169 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.169 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.170 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.171 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.171 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.172 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.172 I llm_load_print_meta: max token length = 1024
0.00.063.209 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.209 I llm_load_tensors: offloading output layer to GPU
0.00.063.209 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.219 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.063.220 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.064.166 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.167 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.167 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.167 I llama_new_context_with_model: n_batch       = 2048
0.00.064.168 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.168 I llama_new_context_with_model: flash_attn    = 0
0.00.064.168 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.168 I llama_new_context_with_model: freq_scale    = 1
0.00.064.169 I ggml_metal_init: allocating
0.00.064.175 I ggml_metal_init: found device: Apple M4
0.00.064.177 I ggml_metal_init: picking default device: Apple M4
0.00.064.733 I ggml_metal_init: using embedded metal library
0.00.066.644 I ggml_metal_init: GPU name:   Apple M4
0.00.066.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.647 I ggml_metal_init: simdgroup reduction   = true
0.00.066.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.647 I ggml_metal_init: has bfloat            = true
0.00.066.647 I ggml_metal_init: use bfloat            = true
0.00.066.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.294 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.303 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.322 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.399 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.400 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.400 I llama_new_context_with_model: graph nodes  = 967
0.00.096.401 I llama_new_context_with_model: graph splits = 2
0.00.096.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.809 I main: llama threadpool init, n_threads = 4
0.00.841.842 I 
0.00.841.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.841.859 I 
0.00.842.079 I sampler seed: 1234
0.00.842.083 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.842.125 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.842.126 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.842.127 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.630.377 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.630.378 I llama_perf_context_print:        load time =     832.79 ms
0.01.630.378 I llama_perf_context_print: prompt eval time =      36.69 ms /     7 tokens (    5.24 ms per token,   190.80 tokens per second)
0.01.630.379 I llama_perf_context_print:        eval time =     748.54 ms /    63 runs   (   11.88 ms per token,    84.16 tokens per second)
0.01.630.379 I llama_perf_context_print:       total time =     788.57 ms /    70 tokens
0.01.630.558 I ggml_metal_free: deallocating

real	0m1.647s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.016.435 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.634 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.032.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.640 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.401 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.046.082 I llama_model_loader: - type  f32:  194 tensors
0.00.046.083 I llama_model_loader: - type q5_1:   97 tensors
0.00.046.083 I llama_model_loader: - type q6_K:    1 tensors
0.00.083.793 I llm_load_vocab: special tokens cache size = 25
0.00.092.607 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.611 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.611 I llm_load_print_meta: arch             = gptneox
0.00.092.612 I llm_load_print_meta: vocab type       = BPE
0.00.092.612 I llm_load_print_meta: n_vocab          = 50304
0.00.092.612 I llm_load_print_meta: n_merges         = 50009
0.00.092.612 I llm_load_print_meta: vocab_only       = 0
0.00.092.612 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.613 I llm_load_print_meta: n_embd           = 2048
0.00.092.613 I llm_load_print_meta: n_layer          = 24
0.00.092.616 I llm_load_print_meta: n_head           = 16
0.00.092.617 I llm_load_print_meta: n_head_kv        = 16
0.00.092.617 I llm_load_print_meta: n_rot            = 32
0.00.092.617 I llm_load_print_meta: n_swa            = 0
0.00.092.617 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.619 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.619 I llm_load_print_meta: n_gqa            = 1
0.00.092.620 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.623 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.623 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.624 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.624 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.624 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.625 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.625 I llm_load_print_meta: n_ff             = 8192
0.00.092.626 I llm_load_print_meta: n_expert         = 0
0.00.092.626 I llm_load_print_meta: n_expert_used    = 0
0.00.092.626 I llm_load_print_meta: causal attn      = 1
0.00.092.626 I llm_load_print_meta: pooling type     = 0
0.00.092.626 I llm_load_print_meta: rope type        = 2
0.00.092.627 I llm_load_print_meta: rope scaling     = linear
0.00.092.627 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.627 I llm_load_print_meta: freq_scale_train = 1
0.00.092.628 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.628 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.628 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.630 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.630 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.630 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.642 I llm_load_print_meta: model type       = 1.4B
0.00.092.642 I llm_load_print_meta: model ftype      = Q5_1
0.00.092.643 I llm_load_print_meta: model params     = 1.41 B
0.00.092.643 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.092.644 I llm_load_print_meta: general.name     = 1.4B
0.00.092.644 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.644 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.644 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.645 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.647 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.647 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.647 I llm_load_print_meta: max token length = 1024
0.00.095.243 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.243 I llm_load_tensors: offloading output layer to GPU
0.00.095.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.253 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.095.255 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.096.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.483 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.483 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.483 I llama_new_context_with_model: n_batch       = 2048
0.00.096.484 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.484 I llama_new_context_with_model: flash_attn    = 0
0.00.096.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.485 I llama_new_context_with_model: freq_scale    = 1
0.00.096.485 I ggml_metal_init: allocating
0.00.096.491 I ggml_metal_init: found device: Apple M4
0.00.096.493 I ggml_metal_init: picking default device: Apple M4
0.00.097.211 I ggml_metal_init: using embedded metal library
0.00.099.812 I ggml_metal_init: GPU name:   Apple M4
0.00.099.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.814 I ggml_metal_init: simdgroup reduction   = true
0.00.099.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.815 I ggml_metal_init: has bfloat            = true
0.00.099.815 I ggml_metal_init: use bfloat            = true
0.00.099.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.130.542 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.547 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.555 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.556 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.557 I llama_new_context_with_model: graph nodes  = 967
0.00.131.557 I llama_new_context_with_model: graph splits = 2
0.00.131.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.952.880 I main: llama threadpool init, n_threads = 4
0.00.952.952 I 
0.00.952.992 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.952.994 I 
0.00.953.505 I sampler seed: 1234
0.00.953.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.953.585 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.953.590 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.953.590 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.807.535 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55252.92 tokens per second)
0.01.807.538 I llama_perf_context_print:        load time =     936.44 ms
0.01.807.539 I llama_perf_context_print: prompt eval time =      46.77 ms /     7 tokens (    6.68 ms per token,   149.67 tokens per second)
0.01.807.540 I llama_perf_context_print:        eval time =     803.95 ms /    63 runs   (   12.76 ms per token,    78.36 tokens per second)
0.01.807.540 I llama_perf_context_print:       total time =     854.66 ms /    70 tokens
0.01.807.710 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.148s
sys	0m0.196s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.019.422 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.735 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.027.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.745 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.040.017 I llama_model_loader: - type  f32:  194 tensors
0.00.040.017 I llama_model_loader: - type q2_K:   49 tensors
0.00.040.018 I llama_model_loader: - type q3_K:   48 tensors
0.00.040.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.076.026 I llm_load_vocab: special tokens cache size = 25
0.00.086.204 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.207 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.207 I llm_load_print_meta: arch             = gptneox
0.00.086.208 I llm_load_print_meta: vocab type       = BPE
0.00.086.208 I llm_load_print_meta: n_vocab          = 50304
0.00.086.208 I llm_load_print_meta: n_merges         = 50009
0.00.086.208 I llm_load_print_meta: vocab_only       = 0
0.00.086.209 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.209 I llm_load_print_meta: n_embd           = 2048
0.00.086.209 I llm_load_print_meta: n_layer          = 24
0.00.086.212 I llm_load_print_meta: n_head           = 16
0.00.086.216 I llm_load_print_meta: n_head_kv        = 16
0.00.086.216 I llm_load_print_meta: n_rot            = 32
0.00.086.216 I llm_load_print_meta: n_swa            = 0
0.00.086.216 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.217 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.217 I llm_load_print_meta: n_gqa            = 1
0.00.086.218 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.225 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.227 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.227 I llm_load_print_meta: n_ff             = 8192
0.00.086.228 I llm_load_print_meta: n_expert         = 0
0.00.086.230 I llm_load_print_meta: n_expert_used    = 0
0.00.086.230 I llm_load_print_meta: causal attn      = 1
0.00.086.230 I llm_load_print_meta: pooling type     = 0
0.00.086.230 I llm_load_print_meta: rope type        = 2
0.00.086.231 I llm_load_print_meta: rope scaling     = linear
0.00.086.231 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.232 I llm_load_print_meta: freq_scale_train = 1
0.00.086.232 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.232 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.232 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.233 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.233 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.233 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.233 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.245 I llm_load_print_meta: model type       = 1.4B
0.00.086.245 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.086.246 I llm_load_print_meta: model params     = 1.41 B
0.00.086.247 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.086.247 I llm_load_print_meta: general.name     = 1.4B
0.00.086.247 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.250 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.251 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.251 I llm_load_print_meta: max token length = 1024
0.00.088.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.403 I llm_load_tensors: offloading output layer to GPU
0.00.088.404 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.414 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.088.415 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.089.594 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.595 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.595 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.596 I llama_new_context_with_model: n_batch       = 2048
0.00.089.596 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.596 I llama_new_context_with_model: flash_attn    = 0
0.00.089.597 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.597 I llama_new_context_with_model: freq_scale    = 1
0.00.089.597 I ggml_metal_init: allocating
0.00.089.603 I ggml_metal_init: found device: Apple M4
0.00.089.606 I ggml_metal_init: picking default device: Apple M4
0.00.090.317 I ggml_metal_init: using embedded metal library
0.00.093.052 I ggml_metal_init: GPU name:   Apple M4
0.00.093.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.055 I ggml_metal_init: simdgroup reduction   = true
0.00.093.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.055 I ggml_metal_init: has bfloat            = true
0.00.093.056 I ggml_metal_init: use bfloat            = true
0.00.093.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.322 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.342 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.364 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.358 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.360 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.360 I llama_new_context_with_model: graph nodes  = 967
0.00.124.360 I llama_new_context_with_model: graph splits = 2
0.00.124.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.967 I main: llama threadpool init, n_threads = 4
0.00.540.049 I 
0.00.540.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.540.106 I 
0.00.540.641 I sampler seed: 1234
0.00.540.655 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.675 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.678 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.678 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.222.177 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.222.178 I llama_perf_context_print:        load time =     520.53 ms
0.01.222.179 I llama_perf_context_print: prompt eval time =      36.29 ms /     7 tokens (    5.18 ms per token,   192.87 tokens per second)
0.01.222.179 I llama_perf_context_print:        eval time =     642.21 ms /    63 runs   (   10.19 ms per token,    98.10 tokens per second)
0.01.222.180 I llama_perf_context_print:       total time =     682.22 ms /    70 tokens
0.01.222.342 I ggml_metal_free: deallocating

real	0m1.263s
user	0m0.150s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.897 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.578 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.579 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.554 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.033.430 I llama_model_loader: - type  f32:  194 tensors
0.00.033.430 I llama_model_loader: - type q3_K:   25 tensors
0.00.033.430 I llama_model_loader: - type q4_K:   71 tensors
0.00.033.431 I llama_model_loader: - type q5_K:    1 tensors
0.00.033.431 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.538 I llm_load_vocab: special tokens cache size = 25
0.00.062.761 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.764 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.764 I llm_load_print_meta: arch             = gptneox
0.00.062.765 I llm_load_print_meta: vocab type       = BPE
0.00.062.765 I llm_load_print_meta: n_vocab          = 50304
0.00.062.765 I llm_load_print_meta: n_merges         = 50009
0.00.062.765 I llm_load_print_meta: vocab_only       = 0
0.00.062.766 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.766 I llm_load_print_meta: n_embd           = 2048
0.00.062.766 I llm_load_print_meta: n_layer          = 24
0.00.062.768 I llm_load_print_meta: n_head           = 16
0.00.062.769 I llm_load_print_meta: n_head_kv        = 16
0.00.062.769 I llm_load_print_meta: n_rot            = 32
0.00.062.769 I llm_load_print_meta: n_swa            = 0
0.00.062.769 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.770 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.770 I llm_load_print_meta: n_gqa            = 1
0.00.062.771 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.772 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.772 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.773 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.773 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.773 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.773 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.774 I llm_load_print_meta: n_ff             = 8192
0.00.062.775 I llm_load_print_meta: n_expert         = 0
0.00.062.777 I llm_load_print_meta: n_expert_used    = 0
0.00.062.777 I llm_load_print_meta: causal attn      = 1
0.00.062.777 I llm_load_print_meta: pooling type     = 0
0.00.062.778 I llm_load_print_meta: rope type        = 2
0.00.062.778 I llm_load_print_meta: rope scaling     = linear
0.00.062.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.779 I llm_load_print_meta: freq_scale_train = 1
0.00.062.779 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.779 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.779 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.779 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.780 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.785 I llm_load_print_meta: model type       = 1.4B
0.00.062.786 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.062.786 I llm_load_print_meta: model params     = 1.41 B
0.00.062.786 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.062.787 I llm_load_print_meta: general.name     = 1.4B
0.00.062.787 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.787 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.787 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.788 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.788 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.789 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.789 I llm_load_print_meta: max token length = 1024
0.00.065.201 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.202 I llm_load_tensors: offloading output layer to GPU
0.00.065.202 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.207 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.065.207 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.066.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.274 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.274 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.274 I llama_new_context_with_model: n_batch       = 2048
0.00.066.274 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.275 I llama_new_context_with_model: flash_attn    = 0
0.00.066.275 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.275 I llama_new_context_with_model: freq_scale    = 1
0.00.066.276 I ggml_metal_init: allocating
0.00.066.278 I ggml_metal_init: found device: Apple M4
0.00.066.280 I ggml_metal_init: picking default device: Apple M4
0.00.066.815 I ggml_metal_init: using embedded metal library
0.00.068.725 I ggml_metal_init: GPU name:   Apple M4
0.00.068.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.729 I ggml_metal_init: simdgroup reduction   = true
0.00.068.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.729 I ggml_metal_init: has bfloat            = true
0.00.068.729 I ggml_metal_init: use bfloat            = true
0.00.068.729 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.731 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.351 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.356 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.411 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.412 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.413 I llama_new_context_with_model: graph nodes  = 967
0.00.099.413 I llama_new_context_with_model: graph splits = 2
0.00.099.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.377 I main: llama threadpool init, n_threads = 4
0.00.682.412 I 
0.00.682.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.682.433 I 
0.00.682.685 I sampler seed: 1234
0.00.682.689 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.734 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.435.351 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.435.352 I llama_perf_context_print:        load time =     673.48 ms
0.01.435.353 I llama_perf_context_print: prompt eval time =      39.58 ms /     7 tokens (    5.65 ms per token,   176.88 tokens per second)
0.01.435.354 I llama_perf_context_print:        eval time =     709.95 ms /    63 runs   (   11.27 ms per token,    88.74 tokens per second)
0.01.435.354 I llama_perf_context_print:       total time =     752.98 ms /    70 tokens
0.01.435.521 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.112s
sys	0m0.133s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.015.542 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.029.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.527 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.042.528 I llama_model_loader: - type  f32:  194 tensors
0.00.042.529 I llama_model_loader: - type q4_K:   61 tensors
0.00.042.529 I llama_model_loader: - type q5_K:   24 tensors
0.00.042.529 I llama_model_loader: - type q6_K:   13 tensors
0.00.079.005 I llm_load_vocab: special tokens cache size = 25
0.00.087.998 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.002 I llm_load_print_meta: arch             = gptneox
0.00.088.002 I llm_load_print_meta: vocab type       = BPE
0.00.088.003 I llm_load_print_meta: n_vocab          = 50304
0.00.088.003 I llm_load_print_meta: n_merges         = 50009
0.00.088.003 I llm_load_print_meta: vocab_only       = 0
0.00.088.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.003 I llm_load_print_meta: n_embd           = 2048
0.00.088.004 I llm_load_print_meta: n_layer          = 24
0.00.088.006 I llm_load_print_meta: n_head           = 16
0.00.088.007 I llm_load_print_meta: n_head_kv        = 16
0.00.088.007 I llm_load_print_meta: n_rot            = 32
0.00.088.008 I llm_load_print_meta: n_swa            = 0
0.00.088.008 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.008 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.009 I llm_load_print_meta: n_gqa            = 1
0.00.088.010 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.011 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.011 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.012 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.012 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.012 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.012 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.013 I llm_load_print_meta: n_ff             = 8192
0.00.088.013 I llm_load_print_meta: n_expert         = 0
0.00.088.015 I llm_load_print_meta: n_expert_used    = 0
0.00.088.016 I llm_load_print_meta: causal attn      = 1
0.00.088.017 I llm_load_print_meta: pooling type     = 0
0.00.088.017 I llm_load_print_meta: rope type        = 2
0.00.088.017 I llm_load_print_meta: rope scaling     = linear
0.00.088.018 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.018 I llm_load_print_meta: freq_scale_train = 1
0.00.088.018 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.018 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.019 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.019 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.019 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.019 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.019 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.031 I llm_load_print_meta: model type       = 1.4B
0.00.088.032 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.088.032 I llm_load_print_meta: model params     = 1.41 B
0.00.088.033 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.088.033 I llm_load_print_meta: general.name     = 1.4B
0.00.088.033 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.035 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.036 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.036 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.036 I llm_load_print_meta: max token length = 1024
0.00.090.406 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.406 I llm_load_tensors: offloading output layer to GPU
0.00.090.407 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.417 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.090.418 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.091.600 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.601 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.601 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.602 I llama_new_context_with_model: n_batch       = 2048
0.00.091.602 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.602 I llama_new_context_with_model: flash_attn    = 0
0.00.091.603 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.603 I llama_new_context_with_model: freq_scale    = 1
0.00.091.603 I ggml_metal_init: allocating
0.00.091.611 I ggml_metal_init: found device: Apple M4
0.00.091.614 I ggml_metal_init: picking default device: Apple M4
0.00.092.310 I ggml_metal_init: using embedded metal library
0.00.094.831 I ggml_metal_init: GPU name:   Apple M4
0.00.094.833 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.833 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.834 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.835 I ggml_metal_init: simdgroup reduction   = true
0.00.094.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.836 I ggml_metal_init: has bfloat            = true
0.00.094.836 I ggml_metal_init: use bfloat            = true
0.00.094.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.124.072 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.124.079 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.098 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.028 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.029 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.030 I llama_new_context_with_model: graph nodes  = 967
0.00.125.030 I llama_new_context_with_model: graph splits = 2
0.00.125.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.509 I main: llama threadpool init, n_threads = 4
0.00.685.594 I 
0.00.685.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.685.649 I 
0.00.685.997 I sampler seed: 1234
0.00.686.007 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.099 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.114 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.126 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.443.309 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47715.05 tokens per second)
0.01.443.310 I llama_perf_context_print:        load time =     669.96 ms
0.01.443.311 I llama_perf_context_print: prompt eval time =      37.19 ms /     7 tokens (    5.31 ms per token,   188.21 tokens per second)
0.01.443.312 I llama_perf_context_print:        eval time =     716.68 ms /    63 runs   (   11.38 ms per token,    87.90 tokens per second)
0.01.443.312 I llama_perf_context_print:       total time =     757.81 ms /    70 tokens
0.01.443.483 I ggml_metal_free: deallocating

real	0m1.484s
user	0m0.158s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.669 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.676 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.677 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.683 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.577 I llama_model_loader: - type  f32:  194 tensors
0.00.024.577 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.577 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.745 I llm_load_vocab: special tokens cache size = 25
0.00.050.750 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.753 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.753 I llm_load_print_meta: arch             = gptneox
0.00.050.753 I llm_load_print_meta: vocab type       = BPE
0.00.050.753 I llm_load_print_meta: n_vocab          = 50304
0.00.050.754 I llm_load_print_meta: n_merges         = 50009
0.00.050.754 I llm_load_print_meta: vocab_only       = 0
0.00.050.754 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.754 I llm_load_print_meta: n_embd           = 2048
0.00.050.754 I llm_load_print_meta: n_layer          = 24
0.00.050.757 I llm_load_print_meta: n_head           = 16
0.00.050.758 I llm_load_print_meta: n_head_kv        = 16
0.00.050.758 I llm_load_print_meta: n_rot            = 32
0.00.050.758 I llm_load_print_meta: n_swa            = 0
0.00.050.758 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.758 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.759 I llm_load_print_meta: n_gqa            = 1
0.00.050.760 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.760 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.761 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.761 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.761 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.762 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.762 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.762 I llm_load_print_meta: n_ff             = 8192
0.00.050.763 I llm_load_print_meta: n_expert         = 0
0.00.050.763 I llm_load_print_meta: n_expert_used    = 0
0.00.050.763 I llm_load_print_meta: causal attn      = 1
0.00.050.763 I llm_load_print_meta: pooling type     = 0
0.00.050.763 I llm_load_print_meta: rope type        = 2
0.00.050.764 I llm_load_print_meta: rope scaling     = linear
0.00.050.766 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.766 I llm_load_print_meta: freq_scale_train = 1
0.00.050.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.767 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.767 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.768 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.779 I llm_load_print_meta: model type       = 1.4B
0.00.050.781 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.781 I llm_load_print_meta: model params     = 1.41 B
0.00.050.781 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.781 I llm_load_print_meta: general.name     = 1.4B
0.00.050.782 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.782 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: max token length = 1024
0.00.052.774 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.775 I llm_load_tensors: offloading output layer to GPU
0.00.052.775 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.785 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.786 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.734 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.734 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.734 I llama_new_context_with_model: n_batch       = 2048
0.00.053.734 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.735 I llama_new_context_with_model: flash_attn    = 0
0.00.053.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.735 I llama_new_context_with_model: freq_scale    = 1
0.00.053.736 I ggml_metal_init: allocating
0.00.053.739 I ggml_metal_init: found device: Apple M4
0.00.053.741 I ggml_metal_init: picking default device: Apple M4
0.00.054.299 I ggml_metal_init: using embedded metal library
0.00.056.205 I ggml_metal_init: GPU name:   Apple M4
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.207 I ggml_metal_init: simdgroup reduction   = true
0.00.056.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.208 I ggml_metal_init: has bfloat            = true
0.00.056.208 I ggml_metal_init: use bfloat            = true
0.00.056.208 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.095 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.100 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.118 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.998 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.999 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.999 I llama_new_context_with_model: graph nodes  = 967
0.00.083.999 I llama_new_context_with_model: graph splits = 2
0.00.084.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.833 I main: llama threadpool init, n_threads = 4
0.00.707.868 I 
0.00.707.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.707.885 I 
0.00.708.108 I sampler seed: 1234
0.00.708.113 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.124 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.124 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.124 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.549.184 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 63055.06 tokens per second)
0.01.549.184 I llama_perf_context_print:        load time =     699.16 ms
0.01.549.185 I llama_perf_context_print: prompt eval time =      38.61 ms /     7 tokens (    5.52 ms per token,   181.31 tokens per second)
0.01.549.186 I llama_perf_context_print:        eval time =     799.53 ms /    63 runs   (   12.69 ms per token,    78.80 tokens per second)
0.01.549.186 I llama_perf_context_print:       total time =     841.35 ms /    70 tokens
0.01.549.361 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.108s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.021 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.595 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.530 I llama_model_loader: - type  f32:  194 tensors
0.00.025.530 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.907 I llm_load_vocab: special tokens cache size = 25
0.00.052.887 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.889 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.890 I llm_load_print_meta: arch             = gptneox
0.00.052.890 I llm_load_print_meta: vocab type       = BPE
0.00.052.890 I llm_load_print_meta: n_vocab          = 50304
0.00.052.891 I llm_load_print_meta: n_merges         = 50009
0.00.052.891 I llm_load_print_meta: vocab_only       = 0
0.00.052.891 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.891 I llm_load_print_meta: n_embd           = 2048
0.00.052.891 I llm_load_print_meta: n_layer          = 24
0.00.052.894 I llm_load_print_meta: n_head           = 16
0.00.052.894 I llm_load_print_meta: n_head_kv        = 16
0.00.052.895 I llm_load_print_meta: n_rot            = 32
0.00.052.895 I llm_load_print_meta: n_swa            = 0
0.00.052.895 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.895 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.896 I llm_load_print_meta: n_gqa            = 1
0.00.052.897 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.898 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.898 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.899 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.899 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.899 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.899 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.900 I llm_load_print_meta: n_ff             = 8192
0.00.052.900 I llm_load_print_meta: n_expert         = 0
0.00.052.900 I llm_load_print_meta: n_expert_used    = 0
0.00.052.900 I llm_load_print_meta: causal attn      = 1
0.00.052.900 I llm_load_print_meta: pooling type     = 0
0.00.052.900 I llm_load_print_meta: rope type        = 2
0.00.052.901 I llm_load_print_meta: rope scaling     = linear
0.00.052.901 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.901 I llm_load_print_meta: freq_scale_train = 1
0.00.052.902 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.902 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.902 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.902 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.905 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.905 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.905 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.917 I llm_load_print_meta: model type       = 1.4B
0.00.052.917 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.917 I llm_load_print_meta: model params     = 1.41 B
0.00.052.918 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.918 I llm_load_print_meta: general.name     = 1.4B
0.00.052.918 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.918 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.918 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.918 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.919 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.919 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.919 I llm_load_print_meta: max token length = 1024
0.00.054.908 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.909 I llm_load_tensors: offloading output layer to GPU
0.00.054.909 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.919 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.920 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.856 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.857 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.857 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.857 I llama_new_context_with_model: n_batch       = 2048
0.00.055.857 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.858 I llama_new_context_with_model: flash_attn    = 0
0.00.055.858 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.858 I llama_new_context_with_model: freq_scale    = 1
0.00.055.859 I ggml_metal_init: allocating
0.00.055.861 I ggml_metal_init: found device: Apple M4
0.00.055.863 I ggml_metal_init: picking default device: Apple M4
0.00.056.418 I ggml_metal_init: using embedded metal library
0.00.058.339 I ggml_metal_init: GPU name:   Apple M4
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.342 I ggml_metal_init: simdgroup reduction   = true
0.00.058.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.342 I ggml_metal_init: has bfloat            = true
0.00.058.342 I ggml_metal_init: use bfloat            = true
0.00.058.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.082 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.100 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.126 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.127 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.128 I llama_new_context_with_model: graph nodes  = 967
0.00.087.128 I llama_new_context_with_model: graph splits = 2
0.00.087.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.852 I main: llama threadpool init, n_threads = 4
0.00.769.892 I 
0.00.769.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.769.908 I 
0.00.770.122 I sampler seed: 1234
0.00.770.127 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.183 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.185 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.185 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.640.536 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.640.536 I llama_perf_context_print:        load time =     759.83 ms
0.01.640.537 I llama_perf_context_print: prompt eval time =      38.48 ms /     7 tokens (    5.50 ms per token,   181.93 tokens per second)
0.01.640.538 I llama_perf_context_print:        eval time =     828.90 ms /    63 runs   (   13.16 ms per token,    76.00 tokens per second)
0.01.640.538 I llama_perf_context_print:       total time =     870.69 ms /    70 tokens
0.01.640.718 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.110s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.572 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.962 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.801 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.738 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.162 I llama_model_loader: - type  f32:  194 tensors
0.00.050.162 I llama_model_loader: - type  f16:   98 tensors
0.00.078.588 I llm_load_vocab: special tokens cache size = 25
0.00.085.090 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.093 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.093 I llm_load_print_meta: arch             = gptneox
0.00.085.094 I llm_load_print_meta: vocab type       = BPE
0.00.085.094 I llm_load_print_meta: n_vocab          = 50304
0.00.085.094 I llm_load_print_meta: n_merges         = 50009
0.00.085.094 I llm_load_print_meta: vocab_only       = 0
0.00.085.094 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.095 I llm_load_print_meta: n_embd           = 2048
0.00.085.095 I llm_load_print_meta: n_layer          = 24
0.00.085.097 I llm_load_print_meta: n_head           = 16
0.00.085.097 I llm_load_print_meta: n_head_kv        = 16
0.00.085.098 I llm_load_print_meta: n_rot            = 32
0.00.085.098 I llm_load_print_meta: n_swa            = 0
0.00.085.098 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.098 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.099 I llm_load_print_meta: n_gqa            = 1
0.00.085.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.100 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.100 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.101 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.101 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.101 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.101 I llm_load_print_meta: n_ff             = 8192
0.00.085.102 I llm_load_print_meta: n_expert         = 0
0.00.085.102 I llm_load_print_meta: n_expert_used    = 0
0.00.085.102 I llm_load_print_meta: causal attn      = 1
0.00.085.102 I llm_load_print_meta: pooling type     = 0
0.00.085.102 I llm_load_print_meta: rope type        = 2
0.00.085.103 I llm_load_print_meta: rope scaling     = linear
0.00.085.103 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.103 I llm_load_print_meta: freq_scale_train = 1
0.00.085.103 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.104 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.104 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.104 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.104 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.104 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.104 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.117 I llm_load_print_meta: model type       = 1.4B
0.00.085.117 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.117 I llm_load_print_meta: model params     = 1.41 B
0.00.085.118 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.118 I llm_load_print_meta: general.name     = 1.4B
0.00.085.120 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.120 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.120 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.120 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.120 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.121 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.122 I llm_load_print_meta: max token length = 1024
0.00.087.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.596 I llm_load_tensors: offloading output layer to GPU
0.00.087.597 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.607 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.608 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.503 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.504 I llama_new_context_with_model: n_ctx         = 128
0.00.088.504 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.504 I llama_new_context_with_model: n_batch       = 128
0.00.088.504 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.504 I llama_new_context_with_model: flash_attn    = 0
0.00.088.505 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.505 I llama_new_context_with_model: freq_scale    = 1
0.00.088.506 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.506 I ggml_metal_init: allocating
0.00.088.509 I ggml_metal_init: found device: Apple M4
0.00.088.511 I ggml_metal_init: picking default device: Apple M4
0.00.089.088 I ggml_metal_init: using embedded metal library
0.00.091.108 I ggml_metal_init: GPU name:   Apple M4
0.00.091.110 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.111 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.111 I ggml_metal_init: simdgroup reduction   = true
0.00.091.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.111 I ggml_metal_init: has bfloat            = true
0.00.091.111 I ggml_metal_init: use bfloat            = true
0.00.091.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.609 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.624 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.512 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.513 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.514 I llama_new_context_with_model: graph nodes  = 967
0.00.101.514 I llama_new_context_with_model: graph splits = 2
0.00.101.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.908.352 I 
0.00.908.430 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.908.446 I perplexity: tokenizing the input ..
0.00.922.253 I perplexity: tokenization took 13.809 ms
0.00.922.289 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.044.888 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.046.722 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.046.755 I llama_perf_context_print:        load time =     887.37 ms
0.01.046.757 I llama_perf_context_print: prompt eval time =     121.73 ms /   128 tokens (    0.95 ms per token,  1051.50 tokens per second)
0.01.046.758 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.046.759 I llama_perf_context_print:       total time =     138.41 ms /   129 tokens
0.01.047.524 I ggml_metal_free: deallocating

real	0m1.238s
user	0m0.123s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.235 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.042 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.044 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.045 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.046 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.047 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.399 I llama_model_loader: - type  f32:  194 tensors
0.00.031.399 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.877 I llm_load_vocab: special tokens cache size = 25
0.00.062.093 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.096 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.096 I llm_load_print_meta: arch             = gptneox
0.00.062.097 I llm_load_print_meta: vocab type       = BPE
0.00.062.097 I llm_load_print_meta: n_vocab          = 50304
0.00.062.097 I llm_load_print_meta: n_merges         = 50009
0.00.062.097 I llm_load_print_meta: vocab_only       = 0
0.00.062.097 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.098 I llm_load_print_meta: n_embd           = 2048
0.00.062.098 I llm_load_print_meta: n_layer          = 24
0.00.062.100 I llm_load_print_meta: n_head           = 16
0.00.062.101 I llm_load_print_meta: n_head_kv        = 16
0.00.062.101 I llm_load_print_meta: n_rot            = 32
0.00.062.102 I llm_load_print_meta: n_swa            = 0
0.00.062.102 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.103 I llm_load_print_meta: n_gqa            = 1
0.00.062.103 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.105 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.105 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.106 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.107 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.108 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.108 I llm_load_print_meta: n_ff             = 8192
0.00.062.108 I llm_load_print_meta: n_expert         = 0
0.00.062.109 I llm_load_print_meta: n_expert_used    = 0
0.00.062.109 I llm_load_print_meta: causal attn      = 1
0.00.062.109 I llm_load_print_meta: pooling type     = 0
0.00.062.109 I llm_load_print_meta: rope type        = 2
0.00.062.109 I llm_load_print_meta: rope scaling     = linear
0.00.062.110 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.110 I llm_load_print_meta: freq_scale_train = 1
0.00.062.110 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.111 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.111 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.111 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.111 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.111 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.111 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.118 I llm_load_print_meta: model type       = 1.4B
0.00.062.119 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.119 I llm_load_print_meta: model params     = 1.41 B
0.00.062.119 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.119 I llm_load_print_meta: general.name     = 1.4B
0.00.062.120 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.120 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.120 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.120 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.120 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.121 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.121 I llm_load_print_meta: max token length = 1024
0.00.063.930 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.930 I llm_load_tensors: offloading output layer to GPU
0.00.063.931 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.935 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.936 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.884 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.885 I llama_new_context_with_model: n_ctx         = 128
0.00.064.885 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.885 I llama_new_context_with_model: n_batch       = 128
0.00.064.886 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.886 I llama_new_context_with_model: flash_attn    = 0
0.00.064.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.887 I llama_new_context_with_model: freq_scale    = 1
0.00.064.887 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.887 I ggml_metal_init: allocating
0.00.064.890 I ggml_metal_init: found device: Apple M4
0.00.064.892 I ggml_metal_init: picking default device: Apple M4
0.00.065.424 I ggml_metal_init: using embedded metal library
0.00.067.469 I ggml_metal_init: GPU name:   Apple M4
0.00.067.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.472 I ggml_metal_init: simdgroup reduction   = true
0.00.067.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.472 I ggml_metal_init: has bfloat            = true
0.00.067.472 I ggml_metal_init: use bfloat            = true
0.00.067.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.372 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.388 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.342 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.343 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.344 I llama_new_context_with_model: graph nodes  = 967
0.00.078.344 I llama_new_context_with_model: graph splits = 2
0.00.078.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.867.813 I 
0.00.867.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.867.845 I perplexity: tokenizing the input ..
0.00.876.053 I perplexity: tokenization took 8.207 ms
0.00.876.065 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.997.765 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.998.916 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.998.930 I llama_perf_context_print:        load time =     856.58 ms
0.00.998.931 I llama_perf_context_print: prompt eval time =     121.48 ms /   128 tokens (    0.95 ms per token,  1053.71 tokens per second)
0.00.998.932 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.998.932 I llama_perf_context_print:       total time =     131.12 ms /   129 tokens
0.00.999.345 I ggml_metal_free: deallocating

real	0m1.016s
user	0m0.091s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.211 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.007 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.018 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.024 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.026 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.026 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.678 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.747 I llm_load_vocab: special tokens cache size = 25
0.00.050.890 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.893 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.893 I llm_load_print_meta: arch             = gptneox
0.00.050.893 I llm_load_print_meta: vocab type       = BPE
0.00.050.894 I llm_load_print_meta: n_vocab          = 50304
0.00.050.894 I llm_load_print_meta: n_merges         = 50009
0.00.050.894 I llm_load_print_meta: vocab_only       = 0
0.00.050.894 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.894 I llm_load_print_meta: n_embd           = 2048
0.00.050.895 I llm_load_print_meta: n_layer          = 24
0.00.050.898 I llm_load_print_meta: n_head           = 16
0.00.050.898 I llm_load_print_meta: n_head_kv        = 16
0.00.050.900 I llm_load_print_meta: n_rot            = 32
0.00.050.900 I llm_load_print_meta: n_swa            = 0
0.00.050.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.901 I llm_load_print_meta: n_gqa            = 1
0.00.050.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.904 I llm_load_print_meta: n_ff             = 8192
0.00.050.905 I llm_load_print_meta: n_expert         = 0
0.00.050.905 I llm_load_print_meta: n_expert_used    = 0
0.00.050.905 I llm_load_print_meta: causal attn      = 1
0.00.050.905 I llm_load_print_meta: pooling type     = 0
0.00.050.905 I llm_load_print_meta: rope type        = 2
0.00.050.906 I llm_load_print_meta: rope scaling     = linear
0.00.050.906 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.906 I llm_load_print_meta: freq_scale_train = 1
0.00.050.906 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.907 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.907 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.907 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.907 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.907 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.907 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.914 I llm_load_print_meta: model type       = 1.4B
0.00.050.914 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.915 I llm_load_print_meta: model params     = 1.41 B
0.00.050.917 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.917 I llm_load_print_meta: general.name     = 1.4B
0.00.050.917 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.917 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.917 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.917 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.917 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.918 I llm_load_print_meta: max token length = 1024
0.00.052.645 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.645 I llm_load_tensors: offloading output layer to GPU
0.00.052.645 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.650 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.650 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.527 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.528 I llama_new_context_with_model: n_ctx         = 128
0.00.053.528 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.528 I llama_new_context_with_model: n_batch       = 128
0.00.053.528 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.528 I llama_new_context_with_model: flash_attn    = 0
0.00.053.529 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.529 I llama_new_context_with_model: freq_scale    = 1
0.00.053.529 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.530 I ggml_metal_init: allocating
0.00.053.535 I ggml_metal_init: found device: Apple M4
0.00.053.537 I ggml_metal_init: picking default device: Apple M4
0.00.054.055 I ggml_metal_init: using embedded metal library
0.00.055.950 I ggml_metal_init: GPU name:   Apple M4
0.00.055.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.952 I ggml_metal_init: simdgroup reduction   = true
0.00.055.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.952 I ggml_metal_init: has bfloat            = true
0.00.055.952 I ggml_metal_init: use bfloat            = true
0.00.055.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.159 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.161 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.174 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.035 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.036 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.036 I llama_new_context_with_model: graph nodes  = 967
0.00.066.037 I llama_new_context_with_model: graph splits = 2
0.00.066.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.757 I 
0.00.633.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.633.831 I perplexity: tokenizing the input ..
0.00.641.759 I perplexity: tokenization took 7.926 ms
0.00.641.770 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.630 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.765.885 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.765.897 I llama_perf_context_print:        load time =     623.54 ms
0.00.765.898 I llama_perf_context_print: prompt eval time =     122.64 ms /   128 tokens (    0.96 ms per token,  1043.74 tokens per second)
0.00.765.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.899 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.766.212 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.077s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.716 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.757 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.558 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.558 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.559 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.559 I llama_model_loader: - type  f32:  194 tensors
0.00.023.559 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.560 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.423 I llm_load_vocab: special tokens cache size = 25
0.00.050.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.483 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.483 I llm_load_print_meta: arch             = gptneox
0.00.050.484 I llm_load_print_meta: vocab type       = BPE
0.00.050.484 I llm_load_print_meta: n_vocab          = 50304
0.00.050.484 I llm_load_print_meta: n_merges         = 50009
0.00.050.484 I llm_load_print_meta: vocab_only       = 0
0.00.050.485 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.485 I llm_load_print_meta: n_embd           = 2048
0.00.050.485 I llm_load_print_meta: n_layer          = 24
0.00.050.488 I llm_load_print_meta: n_head           = 16
0.00.050.488 I llm_load_print_meta: n_head_kv        = 16
0.00.050.488 I llm_load_print_meta: n_rot            = 32
0.00.050.489 I llm_load_print_meta: n_swa            = 0
0.00.050.489 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.489 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.490 I llm_load_print_meta: n_gqa            = 1
0.00.050.490 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.493 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.494 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.494 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.497 I llm_load_print_meta: n_ff             = 8192
0.00.050.497 I llm_load_print_meta: n_expert         = 0
0.00.050.497 I llm_load_print_meta: n_expert_used    = 0
0.00.050.497 I llm_load_print_meta: causal attn      = 1
0.00.050.498 I llm_load_print_meta: pooling type     = 0
0.00.050.498 I llm_load_print_meta: rope type        = 2
0.00.050.498 I llm_load_print_meta: rope scaling     = linear
0.00.050.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.499 I llm_load_print_meta: freq_scale_train = 1
0.00.050.499 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.499 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.513 I llm_load_print_meta: model type       = 1.4B
0.00.050.514 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.514 I llm_load_print_meta: model params     = 1.41 B
0.00.050.514 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.515 I llm_load_print_meta: general.name     = 1.4B
0.00.050.515 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.515 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.515 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.515 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.516 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.516 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.516 I llm_load_print_meta: max token length = 1024
0.00.052.530 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.530 I llm_load_tensors: offloading output layer to GPU
0.00.052.530 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.540 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.542 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.470 I llama_new_context_with_model: n_ctx         = 128
0.00.053.470 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.471 I llama_new_context_with_model: n_batch       = 128
0.00.053.471 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.471 I llama_new_context_with_model: flash_attn    = 0
0.00.053.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.471 I llama_new_context_with_model: freq_scale    = 1
0.00.053.472 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.472 I ggml_metal_init: allocating
0.00.053.478 I ggml_metal_init: found device: Apple M4
0.00.053.481 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.055.997 I ggml_metal_init: GPU name:   Apple M4
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.999 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.999 I ggml_metal_init: simdgroup reduction   = true
0.00.055.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.999 I ggml_metal_init: has bfloat            = true
0.00.056.000 I ggml_metal_init: use bfloat            = true
0.00.056.000 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.047 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.052 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.944 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.945 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.946 I llama_new_context_with_model: graph nodes  = 967
0.00.065.946 I llama_new_context_with_model: graph splits = 2
0.00.065.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.242 I 
0.00.637.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.637.265 I perplexity: tokenizing the input ..
0.00.645.396 I perplexity: tokenization took 8.129 ms
0.00.645.408 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.185 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.769.346 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.769.370 I llama_perf_context_print:        load time =     628.52 ms
0.00.769.371 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.55 tokens per second)
0.00.769.372 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.769.373 I llama_perf_context_print:       total time =     132.13 ms /   129 tokens
0.00.769.895 I ggml_metal_free: deallocating

real	0m0.784s
user	0m0.078s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.342 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.250 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.256 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.258 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.262 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.262 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.263 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.062 I llama_model_loader: - type  f32:  194 tensors
0.00.024.063 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.799 I llm_load_vocab: special tokens cache size = 25
0.00.050.874 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.877 I llm_load_print_meta: arch             = gptneox
0.00.050.877 I llm_load_print_meta: vocab type       = BPE
0.00.050.878 I llm_load_print_meta: n_vocab          = 50304
0.00.050.878 I llm_load_print_meta: n_merges         = 50009
0.00.050.878 I llm_load_print_meta: vocab_only       = 0
0.00.050.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.878 I llm_load_print_meta: n_embd           = 2048
0.00.050.878 I llm_load_print_meta: n_layer          = 24
0.00.050.881 I llm_load_print_meta: n_head           = 16
0.00.050.882 I llm_load_print_meta: n_head_kv        = 16
0.00.050.882 I llm_load_print_meta: n_rot            = 32
0.00.050.882 I llm_load_print_meta: n_swa            = 0
0.00.050.882 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.882 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.883 I llm_load_print_meta: n_gqa            = 1
0.00.050.884 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.885 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.885 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.885 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.886 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.886 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.886 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.887 I llm_load_print_meta: n_ff             = 8192
0.00.050.887 I llm_load_print_meta: n_expert         = 0
0.00.050.887 I llm_load_print_meta: n_expert_used    = 0
0.00.050.887 I llm_load_print_meta: causal attn      = 1
0.00.050.887 I llm_load_print_meta: pooling type     = 0
0.00.050.887 I llm_load_print_meta: rope type        = 2
0.00.050.888 I llm_load_print_meta: rope scaling     = linear
0.00.050.888 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.888 I llm_load_print_meta: freq_scale_train = 1
0.00.050.889 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.889 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.889 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.889 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.889 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.889 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.890 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.901 I llm_load_print_meta: model type       = 1.4B
0.00.050.901 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.902 I llm_load_print_meta: model params     = 1.41 B
0.00.050.902 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.903 I llm_load_print_meta: general.name     = 1.4B
0.00.050.903 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.904 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.905 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: max token length = 1024
0.00.052.442 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.443 I llm_load_tensors: offloading output layer to GPU
0.00.052.443 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.452 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.453 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.254 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.255 I llama_new_context_with_model: n_ctx         = 128
0.00.053.256 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.256 I llama_new_context_with_model: n_batch       = 128
0.00.053.256 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.256 I llama_new_context_with_model: flash_attn    = 0
0.00.053.256 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.257 I llama_new_context_with_model: freq_scale    = 1
0.00.053.257 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.257 I ggml_metal_init: allocating
0.00.053.260 I ggml_metal_init: found device: Apple M4
0.00.053.262 I ggml_metal_init: picking default device: Apple M4
0.00.053.809 I ggml_metal_init: using embedded metal library
0.00.055.739 I ggml_metal_init: GPU name:   Apple M4
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.742 I ggml_metal_init: simdgroup reduction   = true
0.00.055.742 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.742 I ggml_metal_init: has bfloat            = true
0.00.055.742 I ggml_metal_init: use bfloat            = true
0.00.055.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.979 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.982 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.996 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.862 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.864 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.864 I llama_new_context_with_model: graph nodes  = 967
0.00.065.864 I llama_new_context_with_model: graph splits = 2
0.00.065.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.173 I 
0.00.704.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.704.202 I perplexity: tokenizing the input ..
0.00.712.076 I perplexity: tokenization took 7.873 ms
0.00.712.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.980 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.848.131 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.848.140 I llama_perf_context_print:        load time =     694.83 ms
0.00.848.141 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.50 tokens per second)
0.00.848.142 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.142 I llama_perf_context_print:       total time =     143.97 ms /   129 tokens
0.00.848.561 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.078s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.949 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.857 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.866 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.868 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.543 I llama_model_loader: - type  f32:  194 tensors
0.00.023.543 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.313 I llm_load_vocab: special tokens cache size = 25
0.00.050.373 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.375 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.376 I llm_load_print_meta: arch             = gptneox
0.00.050.376 I llm_load_print_meta: vocab type       = BPE
0.00.050.376 I llm_load_print_meta: n_vocab          = 50304
0.00.050.377 I llm_load_print_meta: n_merges         = 50009
0.00.050.377 I llm_load_print_meta: vocab_only       = 0
0.00.050.377 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.377 I llm_load_print_meta: n_embd           = 2048
0.00.050.377 I llm_load_print_meta: n_layer          = 24
0.00.050.380 I llm_load_print_meta: n_head           = 16
0.00.050.380 I llm_load_print_meta: n_head_kv        = 16
0.00.050.381 I llm_load_print_meta: n_rot            = 32
0.00.050.381 I llm_load_print_meta: n_swa            = 0
0.00.050.381 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.384 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.384 I llm_load_print_meta: n_gqa            = 1
0.00.050.385 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.386 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.387 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.387 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.387 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.387 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.388 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.388 I llm_load_print_meta: n_ff             = 8192
0.00.050.388 I llm_load_print_meta: n_expert         = 0
0.00.050.389 I llm_load_print_meta: n_expert_used    = 0
0.00.050.389 I llm_load_print_meta: causal attn      = 1
0.00.050.389 I llm_load_print_meta: pooling type     = 0
0.00.050.389 I llm_load_print_meta: rope type        = 2
0.00.050.389 I llm_load_print_meta: rope scaling     = linear
0.00.050.391 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.392 I llm_load_print_meta: freq_scale_train = 1
0.00.050.392 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.392 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.392 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.392 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.392 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.392 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.393 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.404 I llm_load_print_meta: model type       = 1.4B
0.00.050.405 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.405 I llm_load_print_meta: model params     = 1.41 B
0.00.050.406 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.406 I llm_load_print_meta: general.name     = 1.4B
0.00.050.406 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.406 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.406 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.407 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.407 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.407 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.407 I llm_load_print_meta: max token length = 1024
0.00.052.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.046 I llm_load_tensors: offloading output layer to GPU
0.00.052.046 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.055 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.057 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.921 I llama_new_context_with_model: n_ctx         = 128
0.00.052.922 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.922 I llama_new_context_with_model: n_batch       = 128
0.00.052.922 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.922 I llama_new_context_with_model: flash_attn    = 0
0.00.052.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.923 I llama_new_context_with_model: freq_scale    = 1
0.00.052.923 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.923 I ggml_metal_init: allocating
0.00.052.929 I ggml_metal_init: found device: Apple M4
0.00.052.931 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.386 I ggml_metal_init: GPU name:   Apple M4
0.00.055.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.388 I ggml_metal_init: simdgroup reduction   = true
0.00.055.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.389 I ggml_metal_init: has bfloat            = true
0.00.055.389 I ggml_metal_init: use bfloat            = true
0.00.055.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.680 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.696 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.624 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.625 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.625 I llama_new_context_with_model: graph nodes  = 967
0.00.065.626 I llama_new_context_with_model: graph splits = 2
0.00.065.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.418 I 
0.00.796.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.796.445 I perplexity: tokenizing the input ..
0.00.803.940 I perplexity: tokenization took 7.494 ms
0.00.803.952 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.938.564 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.939.754 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.939.771 I llama_perf_context_print:        load time =     787.47 ms
0.00.939.772 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.71 tokens per second)
0.00.939.773 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.939.773 I llama_perf_context_print:       total time =     143.35 ms /   129 tokens
0.00.940.119 I ggml_metal_free: deallocating

real	0m0.953s
user	0m0.078s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.040 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.292 I llama_model_loader: - type  f32:  194 tensors
0.00.024.292 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.292 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.293 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.244 I llm_load_vocab: special tokens cache size = 25
0.00.050.109 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.112 I llm_load_print_meta: arch             = gptneox
0.00.050.112 I llm_load_print_meta: vocab type       = BPE
0.00.050.113 I llm_load_print_meta: n_vocab          = 50304
0.00.050.113 I llm_load_print_meta: n_merges         = 50009
0.00.050.113 I llm_load_print_meta: vocab_only       = 0
0.00.050.113 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.113 I llm_load_print_meta: n_embd           = 2048
0.00.050.113 I llm_load_print_meta: n_layer          = 24
0.00.050.116 I llm_load_print_meta: n_head           = 16
0.00.050.118 I llm_load_print_meta: n_head_kv        = 16
0.00.050.118 I llm_load_print_meta: n_rot            = 32
0.00.050.119 I llm_load_print_meta: n_swa            = 0
0.00.050.119 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.119 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.122 I llm_load_print_meta: n_gqa            = 1
0.00.050.122 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.123 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.124 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.124 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.124 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.125 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.125 I llm_load_print_meta: n_ff             = 8192
0.00.050.126 I llm_load_print_meta: n_expert         = 0
0.00.050.126 I llm_load_print_meta: n_expert_used    = 0
0.00.050.126 I llm_load_print_meta: causal attn      = 1
0.00.050.126 I llm_load_print_meta: pooling type     = 0
0.00.050.126 I llm_load_print_meta: rope type        = 2
0.00.050.126 I llm_load_print_meta: rope scaling     = linear
0.00.050.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.127 I llm_load_print_meta: freq_scale_train = 1
0.00.050.127 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.128 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.128 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.128 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.140 I llm_load_print_meta: model type       = 1.4B
0.00.050.140 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.140 I llm_load_print_meta: model params     = 1.41 B
0.00.050.141 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.141 I llm_load_print_meta: general.name     = 1.4B
0.00.050.141 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.141 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.141 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.142 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.142 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.142 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.142 I llm_load_print_meta: max token length = 1024
0.00.051.719 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.719 I llm_load_tensors: offloading output layer to GPU
0.00.051.719 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.729 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.730 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.583 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.584 I llama_new_context_with_model: n_ctx         = 128
0.00.052.585 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.585 I llama_new_context_with_model: n_batch       = 128
0.00.052.585 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.585 I llama_new_context_with_model: flash_attn    = 0
0.00.052.585 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.586 I llama_new_context_with_model: freq_scale    = 1
0.00.052.586 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.586 I ggml_metal_init: allocating
0.00.052.592 I ggml_metal_init: found device: Apple M4
0.00.052.594 I ggml_metal_init: picking default device: Apple M4
0.00.053.167 I ggml_metal_init: using embedded metal library
0.00.055.123 I ggml_metal_init: GPU name:   Apple M4
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.126 I ggml_metal_init: simdgroup reduction   = true
0.00.055.126 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.126 I ggml_metal_init: has bfloat            = true
0.00.055.126 I ggml_metal_init: use bfloat            = true
0.00.055.127 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.540 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.542 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.557 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.464 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.465 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.465 I llama_new_context_with_model: graph nodes  = 967
0.00.065.465 I llama_new_context_with_model: graph splits = 2
0.00.065.477 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.425.454 I 
0.00.425.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.425.478 I perplexity: tokenizing the input ..
0.00.433.289 I perplexity: tokenization took 7.811 ms
0.00.433.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.983 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.566.103 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.566.116 I llama_perf_context_print:        load time =     415.41 ms
0.00.566.117 I llama_perf_context_print: prompt eval time =     131.46 ms /   128 tokens (    1.03 ms per token,   973.70 tokens per second)
0.00.566.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.566.118 I llama_perf_context_print:       total time =     140.66 ms /   129 tokens
0.00.566.469 I ggml_metal_free: deallocating

real	0m0.582s
user	0m0.076s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.718 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.399 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.399 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.400 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.400 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.401 I llama_model_loader: - type  f32:  194 tensors
0.00.023.401 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.402 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.402 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.402 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.495 I llm_load_vocab: special tokens cache size = 25
0.00.049.516 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.518 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.519 I llm_load_print_meta: arch             = gptneox
0.00.049.519 I llm_load_print_meta: vocab type       = BPE
0.00.049.519 I llm_load_print_meta: n_vocab          = 50304
0.00.049.520 I llm_load_print_meta: n_merges         = 50009
0.00.049.520 I llm_load_print_meta: vocab_only       = 0
0.00.049.520 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.520 I llm_load_print_meta: n_embd           = 2048
0.00.049.520 I llm_load_print_meta: n_layer          = 24
0.00.049.523 I llm_load_print_meta: n_head           = 16
0.00.049.524 I llm_load_print_meta: n_head_kv        = 16
0.00.049.524 I llm_load_print_meta: n_rot            = 32
0.00.049.524 I llm_load_print_meta: n_swa            = 0
0.00.049.524 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.525 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.526 I llm_load_print_meta: n_gqa            = 1
0.00.049.527 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.528 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.528 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.528 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.529 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.529 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.529 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.530 I llm_load_print_meta: n_ff             = 8192
0.00.049.531 I llm_load_print_meta: n_expert         = 0
0.00.049.532 I llm_load_print_meta: n_expert_used    = 0
0.00.049.532 I llm_load_print_meta: causal attn      = 1
0.00.049.532 I llm_load_print_meta: pooling type     = 0
0.00.049.532 I llm_load_print_meta: rope type        = 2
0.00.049.532 I llm_load_print_meta: rope scaling     = linear
0.00.049.534 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.534 I llm_load_print_meta: freq_scale_train = 1
0.00.049.534 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.535 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.535 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.535 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.535 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.535 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.535 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.547 I llm_load_print_meta: model type       = 1.4B
0.00.049.547 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.548 I llm_load_print_meta: model params     = 1.41 B
0.00.049.548 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.549 I llm_load_print_meta: general.name     = 1.4B
0.00.049.549 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.550 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.550 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.551 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.551 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.551 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.551 I llm_load_print_meta: max token length = 1024
0.00.051.465 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.465 I llm_load_tensors: offloading output layer to GPU
0.00.051.465 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.475 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.476 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.415 I llama_new_context_with_model: n_ctx         = 128
0.00.052.415 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.415 I llama_new_context_with_model: n_batch       = 128
0.00.052.415 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.416 I llama_new_context_with_model: flash_attn    = 0
0.00.052.416 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.416 I llama_new_context_with_model: freq_scale    = 1
0.00.052.417 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.417 I ggml_metal_init: allocating
0.00.052.422 I ggml_metal_init: found device: Apple M4
0.00.052.424 I ggml_metal_init: picking default device: Apple M4
0.00.052.956 I ggml_metal_init: using embedded metal library
0.00.054.857 I ggml_metal_init: GPU name:   Apple M4
0.00.054.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.860 I ggml_metal_init: simdgroup reduction   = true
0.00.054.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.860 I ggml_metal_init: has bfloat            = true
0.00.054.860 I ggml_metal_init: use bfloat            = true
0.00.054.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.018 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.033 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.923 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.924 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.924 I llama_new_context_with_model: graph nodes  = 967
0.00.064.924 I llama_new_context_with_model: graph splits = 2
0.00.064.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.074 I 
0.00.490.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.490.098 I perplexity: tokenizing the input ..
0.00.498.004 I perplexity: tokenization took 7.904 ms
0.00.498.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.004 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.631.158 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.631.181 I llama_perf_context_print:        load time =     481.35 ms
0.00.631.183 I llama_perf_context_print: prompt eval time =     131.75 ms /   128 tokens (    1.03 ms per token,   971.50 tokens per second)
0.00.631.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.631.185 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.00.631.673 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.465 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.235 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.235 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.236 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.236 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.237 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.239 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.239 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.239 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.960 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.768 I llama_model_loader: - type  f32:  194 tensors
0.00.023.769 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.769 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.769 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.661 I llm_load_vocab: special tokens cache size = 25
0.00.049.784 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.786 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.787 I llm_load_print_meta: arch             = gptneox
0.00.049.787 I llm_load_print_meta: vocab type       = BPE
0.00.049.787 I llm_load_print_meta: n_vocab          = 50304
0.00.049.787 I llm_load_print_meta: n_merges         = 50009
0.00.049.788 I llm_load_print_meta: vocab_only       = 0
0.00.049.788 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.788 I llm_load_print_meta: n_embd           = 2048
0.00.049.788 I llm_load_print_meta: n_layer          = 24
0.00.049.790 I llm_load_print_meta: n_head           = 16
0.00.049.791 I llm_load_print_meta: n_head_kv        = 16
0.00.049.791 I llm_load_print_meta: n_rot            = 32
0.00.049.791 I llm_load_print_meta: n_swa            = 0
0.00.049.792 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.792 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.792 I llm_load_print_meta: n_gqa            = 1
0.00.049.793 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.795 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.795 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.796 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.796 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.796 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.798 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.799 I llm_load_print_meta: n_ff             = 8192
0.00.049.799 I llm_load_print_meta: n_expert         = 0
0.00.049.799 I llm_load_print_meta: n_expert_used    = 0
0.00.049.799 I llm_load_print_meta: causal attn      = 1
0.00.049.799 I llm_load_print_meta: pooling type     = 0
0.00.049.800 I llm_load_print_meta: rope type        = 2
0.00.049.800 I llm_load_print_meta: rope scaling     = linear
0.00.049.800 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.800 I llm_load_print_meta: freq_scale_train = 1
0.00.049.800 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.801 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.801 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.802 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.803 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.803 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.814 I llm_load_print_meta: model type       = 1.4B
0.00.049.814 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.815 I llm_load_print_meta: model params     = 1.41 B
0.00.049.815 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.815 I llm_load_print_meta: general.name     = 1.4B
0.00.049.816 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.816 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.817 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.817 I llm_load_print_meta: max token length = 1024
0.00.051.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.343 I llm_load_tensors: offloading output layer to GPU
0.00.051.343 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.352 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.353 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.210 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.211 I llama_new_context_with_model: n_ctx         = 128
0.00.052.211 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.211 I llama_new_context_with_model: n_batch       = 128
0.00.052.212 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.212 I llama_new_context_with_model: flash_attn    = 0
0.00.052.212 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.212 I llama_new_context_with_model: freq_scale    = 1
0.00.052.213 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.213 I ggml_metal_init: allocating
0.00.052.216 I ggml_metal_init: found device: Apple M4
0.00.052.218 I ggml_metal_init: picking default device: Apple M4
0.00.052.752 I ggml_metal_init: using embedded metal library
0.00.054.664 I ggml_metal_init: GPU name:   Apple M4
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.666 I ggml_metal_init: simdgroup reduction   = true
0.00.054.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.667 I ggml_metal_init: has bfloat            = true
0.00.054.667 I ggml_metal_init: use bfloat            = true
0.00.054.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.740 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.742 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.758 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.679 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.681 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.681 I llama_new_context_with_model: graph nodes  = 967
0.00.064.681 I llama_new_context_with_model: graph splits = 2
0.00.064.693 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.847 I 
0.00.576.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.576.869 I perplexity: tokenizing the input ..
0.00.584.892 I perplexity: tokenization took 8.022 ms
0.00.584.904 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.421 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.719.551 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.719.573 I llama_perf_context_print:        load time =     567.38 ms
0.00.719.574 I llama_perf_context_print: prompt eval time =     133.29 ms /   128 tokens (    1.04 ms per token,   960.29 tokens per second)
0.00.719.575 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.575 I llama_perf_context_print:       total time =     142.72 ms /   129 tokens
0.00.720.072 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.077s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.595 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.231 I llama_model_loader: - type  f32:  194 tensors
0.00.023.231 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.232 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.225 I llm_load_vocab: special tokens cache size = 25
0.00.049.350 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.353 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.353 I llm_load_print_meta: arch             = gptneox
0.00.049.353 I llm_load_print_meta: vocab type       = BPE
0.00.049.354 I llm_load_print_meta: n_vocab          = 50304
0.00.049.354 I llm_load_print_meta: n_merges         = 50009
0.00.049.354 I llm_load_print_meta: vocab_only       = 0
0.00.049.354 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.354 I llm_load_print_meta: n_embd           = 2048
0.00.049.355 I llm_load_print_meta: n_layer          = 24
0.00.049.357 I llm_load_print_meta: n_head           = 16
0.00.049.358 I llm_load_print_meta: n_head_kv        = 16
0.00.049.358 I llm_load_print_meta: n_rot            = 32
0.00.049.359 I llm_load_print_meta: n_swa            = 0
0.00.049.359 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.359 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.361 I llm_load_print_meta: n_gqa            = 1
0.00.049.362 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.362 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.363 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.363 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.363 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.364 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.364 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.364 I llm_load_print_meta: n_ff             = 8192
0.00.049.365 I llm_load_print_meta: n_expert         = 0
0.00.049.367 I llm_load_print_meta: n_expert_used    = 0
0.00.049.367 I llm_load_print_meta: causal attn      = 1
0.00.049.368 I llm_load_print_meta: pooling type     = 0
0.00.049.368 I llm_load_print_meta: rope type        = 2
0.00.049.368 I llm_load_print_meta: rope scaling     = linear
0.00.049.368 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.369 I llm_load_print_meta: freq_scale_train = 1
0.00.049.369 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.369 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.369 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.369 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.369 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.369 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.370 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.381 I llm_load_print_meta: model type       = 1.4B
0.00.049.381 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.382 I llm_load_print_meta: model params     = 1.41 B
0.00.049.382 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.382 I llm_load_print_meta: general.name     = 1.4B
0.00.049.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.383 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.384 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.384 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.384 I llm_load_print_meta: max token length = 1024
0.00.050.920 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.920 I llm_load_tensors: offloading output layer to GPU
0.00.050.920 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.930 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.931 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.738 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.739 I llama_new_context_with_model: n_ctx         = 128
0.00.051.740 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.740 I llama_new_context_with_model: n_batch       = 128
0.00.051.740 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.740 I llama_new_context_with_model: flash_attn    = 0
0.00.051.740 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.741 I llama_new_context_with_model: freq_scale    = 1
0.00.051.741 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.741 I ggml_metal_init: allocating
0.00.051.744 I ggml_metal_init: found device: Apple M4
0.00.051.746 I ggml_metal_init: picking default device: Apple M4
0.00.052.307 I ggml_metal_init: using embedded metal library
0.00.054.322 I ggml_metal_init: GPU name:   Apple M4
0.00.054.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.324 I ggml_metal_init: simdgroup reduction   = true
0.00.054.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.325 I ggml_metal_init: has bfloat            = true
0.00.054.325 I ggml_metal_init: use bfloat            = true
0.00.054.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.423 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.427 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.442 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.327 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.328 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.329 I llama_new_context_with_model: graph nodes  = 967
0.00.064.329 I llama_new_context_with_model: graph splits = 2
0.00.064.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.103 I 
0.00.648.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.648.137 I perplexity: tokenizing the input ..
0.00.656.072 I perplexity: tokenization took 7.935 ms
0.00.656.085 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.744 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.798.019 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.798.034 I llama_perf_context_print:        load time =     639.50 ms
0.00.798.035 I llama_perf_context_print: prompt eval time =     140.42 ms /   128 tokens (    1.10 ms per token,   911.54 tokens per second)
0.00.798.036 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.036 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.798.477 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.076s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.936 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.649 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.652 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.655 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.655 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.655 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.322 I llama_model_loader: - type  f32:  194 tensors
0.00.024.323 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.381 I llm_load_vocab: special tokens cache size = 25
0.00.050.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.515 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.516 I llm_load_print_meta: arch             = gptneox
0.00.050.516 I llm_load_print_meta: vocab type       = BPE
0.00.050.516 I llm_load_print_meta: n_vocab          = 50304
0.00.050.516 I llm_load_print_meta: n_merges         = 50009
0.00.050.517 I llm_load_print_meta: vocab_only       = 0
0.00.050.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.517 I llm_load_print_meta: n_embd           = 2048
0.00.050.517 I llm_load_print_meta: n_layer          = 24
0.00.050.520 I llm_load_print_meta: n_head           = 16
0.00.050.521 I llm_load_print_meta: n_head_kv        = 16
0.00.050.521 I llm_load_print_meta: n_rot            = 32
0.00.050.521 I llm_load_print_meta: n_swa            = 0
0.00.050.521 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.521 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.522 I llm_load_print_meta: n_gqa            = 1
0.00.050.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.523 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.524 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.524 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.525 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.525 I llm_load_print_meta: n_ff             = 8192
0.00.050.526 I llm_load_print_meta: n_expert         = 0
0.00.050.526 I llm_load_print_meta: n_expert_used    = 0
0.00.050.526 I llm_load_print_meta: causal attn      = 1
0.00.050.526 I llm_load_print_meta: pooling type     = 0
0.00.050.526 I llm_load_print_meta: rope type        = 2
0.00.050.527 I llm_load_print_meta: rope scaling     = linear
0.00.050.527 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.529 I llm_load_print_meta: freq_scale_train = 1
0.00.050.529 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.529 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.531 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.532 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.532 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.532 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.532 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.544 I llm_load_print_meta: model type       = 1.4B
0.00.050.544 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.545 I llm_load_print_meta: model params     = 1.41 B
0.00.050.545 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.545 I llm_load_print_meta: general.name     = 1.4B
0.00.050.546 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.546 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.547 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.547 I llm_load_print_meta: max token length = 1024
0.00.052.595 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.596 I llm_load_tensors: offloading output layer to GPU
0.00.052.596 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.606 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.607 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.644 I llama_new_context_with_model: n_ctx         = 128
0.00.053.644 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.644 I llama_new_context_with_model: n_batch       = 128
0.00.053.644 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.645 I llama_new_context_with_model: flash_attn    = 0
0.00.053.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.645 I llama_new_context_with_model: freq_scale    = 1
0.00.053.646 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.646 I ggml_metal_init: allocating
0.00.053.649 I ggml_metal_init: found device: Apple M4
0.00.053.651 I ggml_metal_init: picking default device: Apple M4
0.00.054.203 I ggml_metal_init: using embedded metal library
0.00.056.101 I ggml_metal_init: GPU name:   Apple M4
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.103 I ggml_metal_init: simdgroup reduction   = true
0.00.056.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.103 I ggml_metal_init: has bfloat            = true
0.00.056.103 I ggml_metal_init: use bfloat            = true
0.00.056.105 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.328 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.263 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.264 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.264 I llama_new_context_with_model: graph nodes  = 967
0.00.066.264 I llama_new_context_with_model: graph splits = 2
0.00.066.277 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.215.655 I 
0.00.215.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.215.676 I perplexity: tokenizing the input ..
0.00.223.252 I perplexity: tokenization took 7.574 ms
0.00.223.266 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.363.173 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.364.318 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.364.334 I llama_perf_context_print:        load time =     205.72 ms
0.00.364.335 I llama_perf_context_print: prompt eval time =     139.61 ms /   128 tokens (    1.09 ms per token,   916.86 tokens per second)
0.00.364.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.337 I llama_perf_context_print:       total time =     148.68 ms /   129 tokens
0.00.364.752 I ggml_metal_free: deallocating

real	0m0.380s
user	0m0.076s
sys	0m0.050s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.238 I build: 4177 (fb35a994) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.500 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.512 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.518 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.568 I llama_model_loader: - type  f32:  194 tensors
0.00.050.568 I llama_model_loader: - type  f16:   98 tensors
0.00.078.571 I llm_load_vocab: special tokens cache size = 25
0.00.085.048 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.051 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.051 I llm_load_print_meta: arch             = gptneox
0.00.085.052 I llm_load_print_meta: vocab type       = BPE
0.00.085.052 I llm_load_print_meta: n_vocab          = 50304
0.00.085.052 I llm_load_print_meta: n_merges         = 50009
0.00.085.052 I llm_load_print_meta: vocab_only       = 0
0.00.085.052 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.052 I llm_load_print_meta: n_embd           = 2048
0.00.085.053 I llm_load_print_meta: n_layer          = 24
0.00.085.055 I llm_load_print_meta: n_head           = 16
0.00.085.057 I llm_load_print_meta: n_head_kv        = 16
0.00.085.058 I llm_load_print_meta: n_rot            = 32
0.00.085.058 I llm_load_print_meta: n_swa            = 0
0.00.085.058 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.058 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.059 I llm_load_print_meta: n_gqa            = 1
0.00.085.063 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.063 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.064 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.064 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.064 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.065 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.065 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.065 I llm_load_print_meta: n_ff             = 8192
0.00.085.065 I llm_load_print_meta: n_expert         = 0
0.00.085.066 I llm_load_print_meta: n_expert_used    = 0
0.00.085.066 I llm_load_print_meta: causal attn      = 1
0.00.085.066 I llm_load_print_meta: pooling type     = 0
0.00.085.066 I llm_load_print_meta: rope type        = 2
0.00.085.066 I llm_load_print_meta: rope scaling     = linear
0.00.085.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.067 I llm_load_print_meta: freq_scale_train = 1
0.00.085.067 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.067 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.067 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.067 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.068 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.068 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.080 I llm_load_print_meta: model type       = 1.4B
0.00.085.081 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.081 I llm_load_print_meta: model params     = 1.41 B
0.00.085.082 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.082 I llm_load_print_meta: general.name     = 1.4B
0.00.085.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.082 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.082 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.083 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.083 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.083 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.084 I llm_load_print_meta: max token length = 1024
0.00.087.508 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.508 I llm_load_tensors: offloading output layer to GPU
0.00.087.508 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.518 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.519 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.455 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.456 I llama_new_context_with_model: n_ctx         = 128
0.00.088.456 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.456 I llama_new_context_with_model: n_batch       = 128
0.00.088.456 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.456 I llama_new_context_with_model: flash_attn    = 0
0.00.088.457 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.457 I llama_new_context_with_model: freq_scale    = 1
0.00.088.458 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.458 I ggml_metal_init: allocating
0.00.088.461 I ggml_metal_init: found device: Apple M4
0.00.088.463 I ggml_metal_init: picking default device: Apple M4
0.00.089.013 I ggml_metal_init: using embedded metal library
0.00.090.998 I ggml_metal_init: GPU name:   Apple M4
0.00.090.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.000 I ggml_metal_init: simdgroup reduction   = true
0.00.091.000 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.001 I ggml_metal_init: has bfloat            = true
0.00.091.001 I ggml_metal_init: use bfloat            = true
0.00.091.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.475 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.481 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.495 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.347 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.348 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.348 I llama_new_context_with_model: graph nodes  = 967
0.00.100.348 I llama_new_context_with_model: graph splits = 2
0.00.100.361 I 
0.00.100.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.100.393 I compute_imatrix: tokenizing the input ..
0.00.107.476 I compute_imatrix: tokenization took 7.082 ms
0.00.107.477 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.633.897 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.636.667 I llama_perf_context_print:        load time =    1612.15 ms
0.01.636.668 I llama_perf_context_print: prompt eval time =    1525.74 ms /   128 tokens (   11.92 ms per token,    83.89 tokens per second)
0.01.636.672 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.636.672 I llama_perf_context_print:       total time =    1614.91 ms /   129 tokens
0.01.637.123 I ggml_metal_free: deallocating

real	0m1.835s
user	0m0.166s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4177 (fb35a994)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12550a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12550a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12550adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12550b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12550b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12550bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12550c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12550ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12550cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12550d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12550d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12550dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12550ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12550f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12550f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1255100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125510800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125510f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125511640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125511e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125512530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125512c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125513370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125513c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125514330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1255145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125514c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125515870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125515db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125516070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125516510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1255167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125517060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1255175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125517860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125517d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1255181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125518640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125518ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125518f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125519420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1255198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125519d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12551a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12551a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12551aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12551b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12551ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12551c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12551c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12551cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12551d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12551d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12551de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12551e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12551eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12551ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12551f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12551f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125520050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125520310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1255207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125520c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1255210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125521590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125521a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125521ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125522370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125522810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125522cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125523150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1255235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125523a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125523f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1255243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125524870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125524d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1255251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125525650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125525af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125525f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125526430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1255268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125526d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125527210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1255276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125527b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125527ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125528490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125528930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125528dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125529270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125529710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125529bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12552a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12552a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12552a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12551b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12552afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12552b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12552b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12552bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12552c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12552c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12552cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12552d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12552d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12552d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12552de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12552e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12552e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12552ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12552f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12552f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12552f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12552fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125530320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1255307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125530c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125531100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1255315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125531a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125531ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125532380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125532820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125532cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125533160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125533600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125533aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125533f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1255343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125534880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125534d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1255351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125535660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125535b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125535fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125536440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1255368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125536d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125537220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1255376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125537b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125538000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1255384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125538940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125538de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125539280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125539720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125539bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12553a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12553a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12553a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12553aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12553b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12553b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12553bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12553c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12553c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12553cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12553d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12553d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12553dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12553e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12553ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12553f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12553f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12553fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1255402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125540810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125540d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1255412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125541800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125541d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1255422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1255427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125542d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125543290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1255437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125543d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125544280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1255447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125544d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125545270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1255457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125545d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125546260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1255467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125546d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125547250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1255477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125547cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125548240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125548790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125548ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125549230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125549780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125549cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12554a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12554a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12554acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12554b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12554b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12554bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12554c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12554c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12554cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12554d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12554d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12554dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12554e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12554e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12554ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12554f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12554f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12554fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1255501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125550710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125550c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1255511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125551700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125551c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1255521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1255526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125552b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125553030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1255534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125553970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125553e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1255542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125554750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125554bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125555090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125555530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1255559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125555e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125556310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125556860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125556f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1255576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125557dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1255584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1255587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125558db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1255593c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.139.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125404bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125405040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1254054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125405920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125405d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125406200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125406670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125406ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125406f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1254073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125407830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125407f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125408a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1254091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125409a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12540a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12540a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12540af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12540b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12540bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12540c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12540cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12540d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12540da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12540e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12540e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12540e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12540eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12540efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12540f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12540f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12540fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125410230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1254104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125410960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125410dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125411240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1254116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125411b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125411f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125412400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125412870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125412ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125413150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1254135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125413a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125413ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125414310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125414780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125414bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125415060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1254154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125415940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125415db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125416220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125416690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125416c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125417100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125417570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1254179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125417e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1254182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125418730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125418ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125419010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125419480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1254198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125419d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12541a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12541a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12541aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12541af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12541b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12541b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12541bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12541c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12541c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12541c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12541ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12541d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12541d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12541db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12541dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12541e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12541e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12541ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12541f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12541f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12541fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12541ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125420370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1254207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125420c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1254210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125421530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1254219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125421e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125422280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1254226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125422b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125422fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125423440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1254238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125423d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125424190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125424600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125424a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125424ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125425350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1254257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125425c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1254260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125426510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125426980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125426df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125427260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1254276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125427b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125427fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125428420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125428890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125428d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125429170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1254295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125429a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125429ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12542a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12542a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12542ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12542b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12542b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12542b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12542bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12542c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12542c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12542cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12542cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12542d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12542d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12542dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12542e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12542e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12542ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12542eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12542f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12542f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12542fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125430060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1254304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125430940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125430db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125431220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125431690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125431b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125431f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1254323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125432850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125432cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125433130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1254335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125433a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125433e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1254342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125434760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125434bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125435040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1254354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125436040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125436300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1254365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125436a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125436ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125437310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125437780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125437bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125438060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1254384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125438940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125438db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125439220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125439690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125439b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125439f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12543a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12543a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12543acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12543b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12543b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12543ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12543be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12543c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12543c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12543cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12543d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12543d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12543d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12543dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12543e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12543e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12543eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12543ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12543f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12543f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12543fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125440110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125440580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1254409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125440e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1254412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125441740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125441bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125442020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125442490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125442900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125442d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1254431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125443650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125443ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125443f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1254443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125444810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125444c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1254450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125445560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1254459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125445e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1254462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125446720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125446b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125447000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125447470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1254478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125447d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1254481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125448630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125448aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125448f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125449380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125449ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12544a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12544ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12544b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12544b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12544b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12544be10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1173046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117304b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117304fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117305430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1173058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117305d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117306180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1173065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117306a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117306ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117307340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1173079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117308500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117308cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1173094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117309be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11730a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11730aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11730b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11730b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11730c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11730c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11730ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11730d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11730dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11730df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11730e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11730e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11730eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11730ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11730f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11730f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11730fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117310050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1173104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117310930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117310da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117311210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117311680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117311af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117311f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1173123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117312840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117312cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117313120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117313590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117313a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117313e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1173142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117314750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117314bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117315030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1173154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117315910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117315d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1173161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117316760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117316c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1173170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117317540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1173179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117317e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117318290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117318700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117318b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117318fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117319450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1173198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117319d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11731a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11731a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11731aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11731aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11731b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11731b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11731bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11731c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11731c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11731c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11731ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11731d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11731d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11731db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11731dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11731e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11731e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11731ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11731f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11731f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11731fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11731fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117320340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1173207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117320c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117321090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117321500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117321970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117321de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117322250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1173226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117322b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117322fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117323410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117323880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117323cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117324160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1173245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117324a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117324eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117325320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117325790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117325c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117326070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1173264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117326950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117326dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117327230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1173276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117327b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117327f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1173283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117328860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117328cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117329140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1173295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117329a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117329e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11732a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11732a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11732abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11732b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11732b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11732b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11732bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11732c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11732c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11732caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11732cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11732d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11732d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11732dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11732e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11732e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11732ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11732ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11732f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11732f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11732fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117330030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1173304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117330910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117330d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1173311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117331660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117331ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117331f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1173323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117332820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117332c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117333100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117333570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1173339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117333e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1173342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117334730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117334ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117335010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117335ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117335e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117336120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117336590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117336a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117336e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1173372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117337750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117337bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117338030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1173384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117338910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117338d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1173391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117339660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117339ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117339f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11733a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11733a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11733ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11733b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11733b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11733b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11733be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11733c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11733c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11733cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11733d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11733d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11733d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11733dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11733e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11733e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11733eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11733ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11733f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11733f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11733fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1173400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117340550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1173409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117340e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1173412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117341710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117341b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117341ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117342460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1173428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117342d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1173431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117343620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117343a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117343f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117344370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1173447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117344c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1173450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117345530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1173459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117345e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117346280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1173466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117346b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117346fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117347440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1173478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117347d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117348190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117348600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117348a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117348ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117349a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11734a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11734a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11734af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11734b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11734b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11734b970 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.811s
user	0m0.289s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4177 (fb35a994)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146007170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1460078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146007e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146008400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1460089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146008f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146009510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146009ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14600a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14600a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14600aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14600af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14600ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14600c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14600ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14600d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14600d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14600dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14600e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14600eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14600f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14600fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146010400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146010ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1460113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146011680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146011c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146012900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146012e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146013100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1460135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146013860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1460140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146014630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1460148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146015230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1460156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146015b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1460164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146016950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146016df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146017290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146017550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146017b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146018170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146018a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1460190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1460196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146019cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14601a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14601a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14601aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14601b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14601bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14601c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14601c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14601c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14601d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14601d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14601d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14601dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14601e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14601e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14601eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14601ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14601f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14601f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14601fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1460201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146020680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146020b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146020fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146021460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146021900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146021da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146022240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1460226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146022b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146023020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1460234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146023960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146023e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1460242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146024740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146024be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146025520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1460259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146025e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146026300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1460267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146026c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1460270e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146027580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146018780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146028070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146028510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1460289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146028e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1460292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146029790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146029c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14602a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14602a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14602aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14602aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14602b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14602b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14602bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14602c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14602c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14602ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14602cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14602d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14602d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14602dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14602e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14602e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14602ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14602ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14602f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14602f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14602fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1460301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146030690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146030b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146030fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146031470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146031910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146031db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146032250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1460326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146032b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146033030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1460334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146033970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146033e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1460342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146034750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146034bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146035090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1460359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146035e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146036310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1460367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146036c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1460370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146037590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146037a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146037f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1460384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146038a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146038f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146039230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146039840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146039e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14603a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14603aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14603b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14603b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14603bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14603c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14603c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14603ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14603d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14603d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14603ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14603e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14603e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14603ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14603f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14603f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14603fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146040320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146040870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146041310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146041860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146041db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146042300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146042850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146042da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1460432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146043840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146043d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1460442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146044830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146044d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1460452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146045820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146045d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1460462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146046810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146046d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1460472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146047800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146047d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1460482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1460487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146048d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146049290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1460497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146049d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14604a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14604a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14604ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14604b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14604b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14604bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14604c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14604c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14604cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14604d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14604d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14604dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14604e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14604e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14604ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14604f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14604f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14604fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1460500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146050560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146050a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146050ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146051340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1460517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146051c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146052120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1460525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146052a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146052f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1460533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1460538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146054010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146054730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146054e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146055570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146055830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146055e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146056450 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1446088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144608d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1446091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144609610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144609a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144609ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14460a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14460a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14460ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14460b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14460b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14460bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14460c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14460cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14460d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14460de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14460e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14460ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14460f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14460fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144610200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144610920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144611040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144611e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144612140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144612400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144612870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144612ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144613150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144613650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144613b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144613fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144614290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144614b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1446150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1446155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144615ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1446164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1446169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144616ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1446173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1446178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1446181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144618620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144618a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144618f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144619370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1446197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144619c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14461a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14461a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14461ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14461b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14461b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14461ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14461c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14461c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14461cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14461d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14461d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14461d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14461de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14461e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14461e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14461ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14461f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14461f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14461f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14461fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144620320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1446207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144620c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144621100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1446215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144621a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144621ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144622380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144622820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144622cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144623160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144623600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144623f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1446243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144624880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1446251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144625660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144625b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144625fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144626440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1446268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144626d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144627220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1446276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144628000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1446284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144628940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144628de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144629280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144629720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144629bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14462a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14462a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14462a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14462ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14462b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14462b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14462bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14462c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14462c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14462ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14462cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14462d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14462d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14462dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14462e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14462e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14462ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14462ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14462f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14462f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14462fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144630180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144630620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144630ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144630f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1446318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144631d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1446321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144632b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144632fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144633460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144633900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144633da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144634240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1446346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144634b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144635020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1446354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144635960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144635e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1446362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144636740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144636be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144637080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1446375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144638070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1446385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144638880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1446394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144639ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14463a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14463a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14463aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14463b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14463b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14463bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14463c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14463c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14463cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14463d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14463d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14463dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14463e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14463e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14463eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14463f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14463f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14463fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144640410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144640960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144640eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144641400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144641950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144641ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1446423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144642940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144642e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1446433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144643e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1446443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144644920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144644e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1446453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144645910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1446463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144646900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144646e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1446473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1446478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144647e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144648390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1446488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14470b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1447073d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144707840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144707cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14470bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14470bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14470c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14470cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14470d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14470d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14470daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14470dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14470e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14470ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14470efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14470f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14470fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14470ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144710470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144711250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1447116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144711b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144712030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1447124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144712970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144712e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1447132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144713750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144713bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144714140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144714860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144714f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1447156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144715dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144716690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144716ca0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14470bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14470c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14470c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14470c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14470ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14470d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14470d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14470dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14470e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14470e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14470e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14470eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14470f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14470ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144710730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144710e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144711510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144711c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1447122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144712c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144713360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144713a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144714140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144714830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144714f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144715390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144715800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144715c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1447160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144716550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1447169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144716e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144707580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144707840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144707cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144708590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1447172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144717780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144717c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1447180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144718560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144718a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144719160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1447195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144719a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144719eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14471a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14471a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14471ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14471b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14471b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14471b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14471bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14471c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14471c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14471cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14471d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14471d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14471de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14471e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14471e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14471ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14471f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14471f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14471fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14471fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144720360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144720800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144720ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144721140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1447215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1447223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144722860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144722d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1447231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144723ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144723f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144724420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1447248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144724d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144725200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1447256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144725fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144726480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144726920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144726dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144727260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144727700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144727ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144728040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1447284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144728980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144728e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1447292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144729760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144729c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14472a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14472a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14472a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14472ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14472b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14472b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14472bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14472c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14472c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14472ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14472cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14472d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14472d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14472dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14472e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14472e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14472eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14472ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14472f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14472f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14472fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1447301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144730660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144730b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144731440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1447318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144731d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1447326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144732b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1447334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144733940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144733de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144734280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144734720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144734bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144735060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144735500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1447359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144735e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1447362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144736780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144736c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1447370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144737560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144737a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144737ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144738340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1447387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144739280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1447397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14473a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14473ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14473b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14473b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14473be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14473c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14473cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14473cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14473d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14473dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14473e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14473e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14473eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14473f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14473f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14473fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1447400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144740630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144740b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1447410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144741620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144741b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1447420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144742610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144742b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1447430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144743600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144743b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1447440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1447445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144744b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144745090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1447455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144745b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144746080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1447465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144746b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1447475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144747b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144748060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1447485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144748b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144749050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1447495a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144749af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14474a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14474a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14474aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14474b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14474b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14474bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14474c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14474c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14474cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14474d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14474d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14474dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14474e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14474e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14474eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14474eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14474f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14474fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14474ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144750530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1447509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144750e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144751310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1447517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144751c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1447520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144752590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144752a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144752ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144753810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144753cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144754150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1447546a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144754dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1447554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144755c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1447565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144756bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144757200 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.924s
user	0m0.239s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
