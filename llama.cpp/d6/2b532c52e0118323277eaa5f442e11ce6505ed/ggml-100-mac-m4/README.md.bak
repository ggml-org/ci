### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.69 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.63 sec*proc (28 tests)

Total Test time (real) = 220.64 sec

real	3m40.673s
user	7m33.433s
sys	0m6.774s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.34 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.21 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.11 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.24 sec*proc (28 tests)

Total Test time (real) =  51.25 sec

real	0m51.261s
user	1m11.121s
sys	0m5.591s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.119 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.976 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.995 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.003 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.006 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.008 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.009 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.010 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.011 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.012 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.013 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.014 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.014 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.018 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.019 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.020 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.021 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.021 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.022 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.023 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[CLS] [PAD] [SEP]", "[unused0]", "[...
0.00.034.615 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.617 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.617 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.618 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.619 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.619 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.619 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.620 I llama_model_loader: - type  f32:  124 tensors
0.00.034.621 I llama_model_loader: - type  f16:   73 tensors
0.00.039.559 I llm_load_vocab: special tokens cache size = 5
0.00.041.984 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.988 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.989 I llm_load_print_meta: arch             = bert
0.00.041.989 I llm_load_print_meta: vocab type       = WPM
0.00.041.990 I llm_load_print_meta: n_vocab          = 30522
0.00.041.990 I llm_load_print_meta: n_merges         = 0
0.00.041.990 I llm_load_print_meta: vocab_only       = 0
0.00.041.991 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.991 I llm_load_print_meta: n_embd           = 384
0.00.041.991 I llm_load_print_meta: n_layer          = 12
0.00.042.009 I llm_load_print_meta: n_head           = 12
0.00.042.010 I llm_load_print_meta: n_head_kv        = 12
0.00.042.010 I llm_load_print_meta: n_rot            = 32
0.00.042.010 I llm_load_print_meta: n_swa            = 0
0.00.042.011 I llm_load_print_meta: n_embd_head_k    = 32
0.00.042.011 I llm_load_print_meta: n_embd_head_v    = 32
0.00.042.012 I llm_load_print_meta: n_gqa            = 1
0.00.042.013 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.042.014 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.042.014 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.042.017 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.042.017 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.042.017 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.042.017 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.042.018 I llm_load_print_meta: n_ff             = 1536
0.00.042.021 I llm_load_print_meta: n_expert         = 0
0.00.042.021 I llm_load_print_meta: n_expert_used    = 0
0.00.042.022 I llm_load_print_meta: causal attn      = 0
0.00.042.022 I llm_load_print_meta: pooling type     = 2
0.00.042.022 I llm_load_print_meta: rope type        = 2
0.00.042.023 I llm_load_print_meta: rope scaling     = linear
0.00.042.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.042.024 I llm_load_print_meta: freq_scale_train = 1
0.00.042.024 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.042.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.042.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.042.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.042.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.042.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.042.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.042.028 I llm_load_print_meta: model type       = 33M
0.00.042.029 I llm_load_print_meta: model ftype      = F16
0.00.042.029 I llm_load_print_meta: model params     = 33.21 M
0.00.042.030 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.042.031 I llm_load_print_meta: general.name     = Bge Small
0.00.042.031 I llm_load_print_meta: UNK token        = 100 '[CLS] [UNK] [SEP]'
0.00.042.031 I llm_load_print_meta: SEP token        = 102 '[CLS] [SEP] [SEP]'
0.00.042.032 I llm_load_print_meta: PAD token        = 0 '[CLS] [PAD] [SEP]'
0.00.042.033 I llm_load_print_meta: CLS token        = 101 '[CLS] [CLS] [SEP]'
0.00.042.033 I llm_load_print_meta: MASK token       = 103 '[CLS] [MASK] [SEP]'
0.00.042.034 I llm_load_print_meta: LF token         = 0 '[CLS] [PAD] [SEP]'
0.00.042.034 I llm_load_print_meta: max token length = 21
0.00.044.248 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.044.248 I llm_load_tensors: offloading output layer to GPU
0.00.044.251 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.044.278 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.280 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.872 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.873 I llama_new_context_with_model: n_ctx         = 512
0.00.044.873 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.874 I llama_new_context_with_model: n_batch       = 2048
0.00.044.874 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.874 I llama_new_context_with_model: flash_attn    = 0
0.00.044.875 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.875 I llama_new_context_with_model: freq_scale    = 1
0.00.044.876 I ggml_metal_init: allocating
0.00.044.880 I ggml_metal_init: found device: Apple M4
0.00.044.883 I ggml_metal_init: picking default device: Apple M4
0.00.045.770 I ggml_metal_init: using embedded metal library
0.00.050.175 I ggml_metal_init: GPU name:   Apple M4
0.00.050.178 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.179 I ggml_metal_init: simdgroup reduction   = true
0.00.050.179 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.180 I ggml_metal_init: has bfloat            = true
0.00.050.180 I ggml_metal_init: use bfloat            = true
0.00.050.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.796 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.063.799 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.063.800 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.064.581 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.064.582 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.064.583 I llama_new_context_with_model: graph nodes  = 429
0.00.064.583 I llama_new_context_with_model: graph splits = 2
0.00.064.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.064.605 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.197 I 
0.00.071.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.902 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.620 I llama_perf_context_print:        load time =      48.21 ms
0.00.076.621 I llama_perf_context_print: prompt eval time =       4.56 ms /     9 tokens (    0.51 ms per token,  1971.95 tokens per second)
0.00.076.621 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.622 I llama_perf_context_print:       total time =       5.42 ms /    10 tokens
0.00.076.761 I ggml_metal_free: deallocating

real	0m0.270s
user	0m0.053s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.439 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.633 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.638 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.640 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.640 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.642 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.642 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.643 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.645 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.645 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.645 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.647 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.648 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.648 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.648 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.649 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.649 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.649 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[CLS] [PAD] [SEP]", "[unused0]", "[...
0.00.014.810 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.811 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.811 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.812 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.812 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.812 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.812 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.813 I llama_model_loader: - type  f32:  124 tensors
0.00.014.813 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.371 I llm_load_vocab: special tokens cache size = 5
0.00.018.738 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.740 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.741 I llm_load_print_meta: arch             = bert
0.00.018.741 I llm_load_print_meta: vocab type       = WPM
0.00.018.741 I llm_load_print_meta: n_vocab          = 30522
0.00.018.742 I llm_load_print_meta: n_merges         = 0
0.00.018.742 I llm_load_print_meta: vocab_only       = 0
0.00.018.742 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.742 I llm_load_print_meta: n_embd           = 384
0.00.018.742 I llm_load_print_meta: n_layer          = 12
0.00.018.752 I llm_load_print_meta: n_head           = 12
0.00.018.752 I llm_load_print_meta: n_head_kv        = 12
0.00.018.752 I llm_load_print_meta: n_rot            = 32
0.00.018.752 I llm_load_print_meta: n_swa            = 0
0.00.018.753 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.754 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.754 I llm_load_print_meta: n_gqa            = 1
0.00.018.755 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.755 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.756 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.756 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.756 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.757 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.757 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.757 I llm_load_print_meta: n_ff             = 1536
0.00.018.758 I llm_load_print_meta: n_expert         = 0
0.00.018.758 I llm_load_print_meta: n_expert_used    = 0
0.00.018.758 I llm_load_print_meta: causal attn      = 0
0.00.018.758 I llm_load_print_meta: pooling type     = 2
0.00.018.758 I llm_load_print_meta: rope type        = 2
0.00.018.760 I llm_load_print_meta: rope scaling     = linear
0.00.018.760 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.761 I llm_load_print_meta: freq_scale_train = 1
0.00.018.761 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.761 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.761 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.761 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.761 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.761 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.763 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.763 I llm_load_print_meta: model type       = 33M
0.00.018.763 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.763 I llm_load_print_meta: model params     = 33.21 M
0.00.018.764 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.764 I llm_load_print_meta: general.name     = Bge Small
0.00.018.765 I llm_load_print_meta: UNK token        = 100 '[CLS] [UNK] [SEP]'
0.00.018.765 I llm_load_print_meta: SEP token        = 102 '[CLS] [SEP] [SEP]'
0.00.018.765 I llm_load_print_meta: PAD token        = 0 '[CLS] [PAD] [SEP]'
0.00.018.765 I llm_load_print_meta: CLS token        = 101 '[CLS] [CLS] [SEP]'
0.00.018.767 I llm_load_print_meta: MASK token       = 103 '[CLS] [MASK] [SEP]'
0.00.018.767 I llm_load_print_meta: LF token         = 0 '[CLS] [PAD] [SEP]'
0.00.018.767 I llm_load_print_meta: max token length = 21
0.00.020.051 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.051 I llm_load_tensors: offloading output layer to GPU
0.00.020.051 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.061 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.062 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.413 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.414 I llama_new_context_with_model: n_ctx         = 512
0.00.020.414 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.415 I llama_new_context_with_model: n_batch       = 2048
0.00.020.415 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.415 I llama_new_context_with_model: flash_attn    = 0
0.00.020.415 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.415 I llama_new_context_with_model: freq_scale    = 1
0.00.020.416 I ggml_metal_init: allocating
0.00.020.421 I ggml_metal_init: found device: Apple M4
0.00.020.423 I ggml_metal_init: picking default device: Apple M4
0.00.021.047 I ggml_metal_init: using embedded metal library
0.00.023.548 I ggml_metal_init: GPU name:   Apple M4
0.00.023.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.551 I ggml_metal_init: simdgroup reduction   = true
0.00.023.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.551 I ggml_metal_init: has bfloat            = true
0.00.023.551 I ggml_metal_init: use bfloat            = true
0.00.023.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.280 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.282 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.283 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.898 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.899 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.900 I llama_new_context_with_model: graph nodes  = 429
0.00.034.900 I llama_new_context_with_model: graph splits = 2
0.00.034.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.914 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.321 I 
0.00.041.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.858 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.342 I llama_perf_context_print:        load time =      31.88 ms
0.00.045.343 I llama_perf_context_print: prompt eval time =       3.36 ms /     9 tokens (    0.37 ms per token,  2680.17 tokens per second)
0.00.045.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.345 I llama_perf_context_print:       total time =       4.02 ms /    10 tokens
0.00.045.504 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.142 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.988 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.645 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.652 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.656 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.657 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.657 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.659 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.659 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.660 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.661 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.661 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.664 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.665 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.666 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.419 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s> <s> </s>", "<s> <pad> </s>", "<...
0.00.044.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.163 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.163 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.163 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.164 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.164 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.164 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.165 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.165 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.166 I llama_model_loader: - type  f32:   41 tensors
0.00.049.171 I llama_model_loader: - type  f16:   29 tensors
0.00.067.330 W llm_load_vocab: empty token at index 5
0.00.071.873 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.073.143 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.170 I llm_load_vocab: special tokens cache size = 5
0.00.330.511 I llm_load_vocab: token to piece cache size = 1.5061 MB
0.00.330.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.520 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.520 I llm_load_print_meta: vocab type       = BPE
0.00.330.520 I llm_load_print_meta: n_vocab          = 61056
0.00.330.520 I llm_load_print_meta: n_merges         = 39382
0.00.330.520 I llm_load_print_meta: vocab_only       = 0
0.00.330.521 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.521 I llm_load_print_meta: n_embd           = 384
0.00.330.521 I llm_load_print_meta: n_layer          = 4
0.00.330.544 I llm_load_print_meta: n_head           = 12
0.00.330.545 I llm_load_print_meta: n_head_kv        = 12
0.00.330.546 I llm_load_print_meta: n_rot            = 32
0.00.330.546 I llm_load_print_meta: n_swa            = 0
0.00.330.546 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.546 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.546 I llm_load_print_meta: n_gqa            = 1
0.00.330.547 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.547 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.548 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.548 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.548 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.548 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.548 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.549 I llm_load_print_meta: n_ff             = 1536
0.00.330.549 I llm_load_print_meta: n_expert         = 0
0.00.330.549 I llm_load_print_meta: n_expert_used    = 0
0.00.330.549 I llm_load_print_meta: causal attn      = 0
0.00.330.551 I llm_load_print_meta: pooling type     = -1
0.00.330.551 I llm_load_print_meta: rope type        = -1
0.00.330.551 I llm_load_print_meta: rope scaling     = linear
0.00.330.551 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.552 I llm_load_print_meta: freq_scale_train = 1
0.00.330.552 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.552 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.552 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.552 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.552 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.552 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.552 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.553 I llm_load_print_meta: model type       = 33M
0.00.330.553 I llm_load_print_meta: model ftype      = F16
0.00.330.553 I llm_load_print_meta: model params     = 32.90 M
0.00.330.555 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.555 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.555 I llm_load_print_meta: BOS token        = 0 '<s> <s> </s>'
0.00.330.555 I llm_load_print_meta: EOS token        = 2 '<s> </s> </s>'
0.00.330.555 I llm_load_print_meta: UNK token        = 3 '<s> <unk> </s>'
0.00.330.556 I llm_load_print_meta: SEP token        = 2 '<s> </s> </s>'
0.00.330.556 I llm_load_print_meta: PAD token        = 1 '<s> <pad> </s>'
0.00.330.556 I llm_load_print_meta: CLS token        = 0 '<s> <s> </s>'
0.00.330.556 I llm_load_print_meta: MASK token       = 4 '<s> <mask> </s>'
0.00.330.556 I llm_load_print_meta: EOG token        = 2 '<s> </s> </s>'
0.00.330.556 I llm_load_print_meta: max token length = 45
0.00.331.348 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.348 I llm_load_tensors: offloading output layer to GPU
0.00.331.348 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.368 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.369 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.332.010 I llama_new_context_with_model: n_seq_max     = 1
0.00.332.010 I llama_new_context_with_model: n_ctx         = 8192
0.00.332.011 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.332.011 I llama_new_context_with_model: n_batch       = 2048
0.00.332.011 I llama_new_context_with_model: n_ubatch      = 2048
0.00.332.011 I llama_new_context_with_model: flash_attn    = 0
0.00.332.011 I llama_new_context_with_model: freq_base     = 10000.0
0.00.332.012 I llama_new_context_with_model: freq_scale    = 1
0.00.332.012 I ggml_metal_init: allocating
0.00.332.016 I ggml_metal_init: found device: Apple M4
0.00.332.018 I ggml_metal_init: picking default device: Apple M4
0.00.332.644 I ggml_metal_init: using embedded metal library
0.00.335.158 I ggml_metal_init: GPU name:   Apple M4
0.00.335.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.161 I ggml_metal_init: simdgroup reduction   = true
0.00.335.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.161 I ggml_metal_init: has bfloat            = true
0.00.335.161 I ggml_metal_init: use bfloat            = true
0.00.335.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.638 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.642 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.644 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.283 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.284 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.285 I llama_new_context_with_model: graph nodes  = 154
0.00.348.285 I llama_new_context_with_model: graph splits = 2
0.00.348.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.559 I 
0.00.361.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.879 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.880 I main: number of tokens in prompt = 12
     0 -> '<s> <s> </s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
 36946 -> '</'
    87 -> 's'
 53192 -> '></'
    87 -> 's'
    60 -> '>'
 23233 -> 'hi'
     2 -> '<s> </s> </s>'


0.00.361.894 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.894 I main: number of tokens in prompt = 16
     0 -> '<s> <s> </s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
 36946 -> '</'
    87 -> 's'
 53192 -> '></'
    87 -> 's'
    60 -> '>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '<s> </s> </s>'


0.00.361.900 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.902 I main: number of tokens in prompt = 43
     0 -> '<s> <s> </s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
 36946 -> '</'
    87 -> 's'
 53192 -> '></'
    87 -> 's'
    60 -> '>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '<s> </s> </s>'


0.00.362.427 I batch_decode: n_tokens = 71, n_seq = 3

rerank score 0:    0.035
rerank score 1:    0.022
rerank score 2:    0.191

0.00.366.445 I llama_perf_context_print:        load time =     337.56 ms
0.00.366.446 I llama_perf_context_print: prompt eval time =       4.01 ms /    71 tokens (    0.06 ms per token, 17718.99 tokens per second)
0.00.366.449 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.449 I llama_perf_context_print:       total time =       4.89 ms /    72 tokens
0.00.366.659 I ggml_metal_free: deallocating

real	0m1.056s
user	0m0.337s
sys	0m0.048s
  - rerank score 0 @ 0.035 OK
  - rerank score 1 @ 0.022 OK
  - rerank score 2 @ 0.191 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.107 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.217 I main: llama backend init
0.00.000.223 I main: load the model and apply lora adapter, if any
0.00.068.412 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.079.271 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.283 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.088.412 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.095.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.441 I llama_model_loader: - type  f32:  194 tensors
0.00.095.441 I llama_model_loader: - type  f16:   98 tensors
0.00.134.885 I llm_load_vocab: special tokens cache size = 25
0.00.142.777 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.142.781 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.142.781 I llm_load_print_meta: arch             = gptneox
0.00.142.782 I llm_load_print_meta: vocab type       = BPE
0.00.142.782 I llm_load_print_meta: n_vocab          = 50304
0.00.142.782 I llm_load_print_meta: n_merges         = 50009
0.00.142.782 I llm_load_print_meta: vocab_only       = 0
0.00.142.782 I llm_load_print_meta: n_ctx_train      = 2048
0.00.142.782 I llm_load_print_meta: n_embd           = 2048
0.00.142.783 I llm_load_print_meta: n_layer          = 24
0.00.142.798 I llm_load_print_meta: n_head           = 16
0.00.142.799 I llm_load_print_meta: n_head_kv        = 16
0.00.142.799 I llm_load_print_meta: n_rot            = 32
0.00.142.799 I llm_load_print_meta: n_swa            = 0
0.00.142.799 I llm_load_print_meta: n_embd_head_k    = 128
0.00.142.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.142.800 I llm_load_print_meta: n_gqa            = 1
0.00.142.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.142.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.142.802 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.142.802 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.142.807 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.142.807 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.142.808 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.142.808 I llm_load_print_meta: n_ff             = 8192
0.00.142.809 I llm_load_print_meta: n_expert         = 0
0.00.142.809 I llm_load_print_meta: n_expert_used    = 0
0.00.142.809 I llm_load_print_meta: causal attn      = 1
0.00.142.809 I llm_load_print_meta: pooling type     = 0
0.00.142.809 I llm_load_print_meta: rope type        = 2
0.00.142.811 I llm_load_print_meta: rope scaling     = linear
0.00.142.811 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.142.811 I llm_load_print_meta: freq_scale_train = 1
0.00.142.812 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.142.812 I llm_load_print_meta: rope_finetuned   = unknown
0.00.142.812 I llm_load_print_meta: ssm_d_conv       = 0
0.00.142.812 I llm_load_print_meta: ssm_d_inner      = 0
0.00.142.812 I llm_load_print_meta: ssm_d_state      = 0
0.00.142.812 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.142.813 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.142.813 I llm_load_print_meta: model type       = 1.4B
0.00.142.813 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.142.814 I llm_load_print_meta: model params     = 1.41 B
0.00.142.814 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.142.815 I llm_load_print_meta: general.name     = 1.4B
0.00.142.815 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.142.815 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.142.815 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.142.817 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.142.817 I llm_load_print_meta: LF token         = 128 ''
0.00.142.817 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.142.818 I llm_load_print_meta: max token length = 1024
0.00.145.568 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.145.568 I llm_load_tensors: offloading output layer to GPU
0.00.145.568 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.145.588 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.145.589 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.146.631 I llama_new_context_with_model: n_seq_max     = 1
0.00.146.632 I llama_new_context_with_model: n_ctx         = 2048
0.00.146.632 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.146.632 I llama_new_context_with_model: n_batch       = 2048
0.00.146.632 I llama_new_context_with_model: n_ubatch      = 512
0.00.146.633 I llama_new_context_with_model: flash_attn    = 0
0.00.146.633 I llama_new_context_with_model: freq_base     = 10000.0
0.00.146.633 I llama_new_context_with_model: freq_scale    = 1
0.00.146.634 I ggml_metal_init: allocating
0.00.146.637 I ggml_metal_init: found device: Apple M4
0.00.146.639 I ggml_metal_init: picking default device: Apple M4
0.00.147.330 I ggml_metal_init: using embedded metal library
0.00.157.397 I ggml_metal_init: GPU name:   Apple M4
0.00.157.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.157.400 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.157.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.157.401 I ggml_metal_init: simdgroup reduction   = true
0.00.157.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.157.401 I ggml_metal_init: has bfloat            = true
0.00.157.401 I ggml_metal_init: use bfloat            = true
0.00.157.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.157.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.205.819 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.205.825 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.205.843 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.206.858 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.206.859 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.206.860 I llama_new_context_with_model: graph nodes  = 967
0.00.206.860 I llama_new_context_with_model: graph splits = 2
0.00.206.900 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.207.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.207.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.290.139 I main: llama threadpool init, n_threads = 4
0.00.290.171 I 
0.00.290.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.290.220 I 
0.00.290.296 I sampler seed: 1234
0.00.290.300 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.290.334 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.290.336 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.290.336 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.138.883 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.02.138.884 I llama_perf_context_print:        load time =     221.71 ms
0.02.138.885 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.138.885 I llama_perf_context_print:        eval time =    1801.96 ms /    63 runs   (   28.60 ms per token,    34.96 tokens per second)
0.02.138.886 I llama_perf_context_print:       total time =    1848.75 ms /    70 tokens
0.02.139.093 I ggml_metal_free: deallocating

real	0m2.447s
user	0m0.152s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.002.058 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.031.665 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.873 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.900 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.904 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.913 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.914 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.894 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.896 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.898 I llama_model_loader: - type  f32:  194 tensors
0.00.059.899 I llama_model_loader: - type  f16:   98 tensors
0.00.100.240 I llm_load_vocab: special tokens cache size = 25
0.00.108.157 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.108.161 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.108.161 I llm_load_print_meta: arch             = gptneox
0.00.108.162 I llm_load_print_meta: vocab type       = BPE
0.00.108.162 I llm_load_print_meta: n_vocab          = 50304
0.00.108.162 I llm_load_print_meta: n_merges         = 50009
0.00.108.162 I llm_load_print_meta: vocab_only       = 0
0.00.108.163 I llm_load_print_meta: n_ctx_train      = 2048
0.00.108.163 I llm_load_print_meta: n_embd           = 2048
0.00.108.163 I llm_load_print_meta: n_layer          = 24
0.00.108.179 I llm_load_print_meta: n_head           = 16
0.00.108.180 I llm_load_print_meta: n_head_kv        = 16
0.00.108.180 I llm_load_print_meta: n_rot            = 32
0.00.108.180 I llm_load_print_meta: n_swa            = 0
0.00.108.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.108.181 I llm_load_print_meta: n_embd_head_v    = 128
0.00.108.181 I llm_load_print_meta: n_gqa            = 1
0.00.108.182 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.108.186 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.108.186 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.108.187 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.108.187 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.108.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.108.190 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.108.190 I llm_load_print_meta: n_ff             = 8192
0.00.108.190 I llm_load_print_meta: n_expert         = 0
0.00.108.191 I llm_load_print_meta: n_expert_used    = 0
0.00.108.192 I llm_load_print_meta: causal attn      = 1
0.00.108.193 I llm_load_print_meta: pooling type     = 0
0.00.108.193 I llm_load_print_meta: rope type        = 2
0.00.108.193 I llm_load_print_meta: rope scaling     = linear
0.00.108.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.108.194 I llm_load_print_meta: freq_scale_train = 1
0.00.108.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.108.194 I llm_load_print_meta: rope_finetuned   = unknown
0.00.108.194 I llm_load_print_meta: ssm_d_conv       = 0
0.00.108.195 I llm_load_print_meta: ssm_d_inner      = 0
0.00.108.195 I llm_load_print_meta: ssm_d_state      = 0
0.00.108.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.108.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.108.195 I llm_load_print_meta: model type       = 1.4B
0.00.108.196 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.108.196 I llm_load_print_meta: model params     = 1.41 B
0.00.108.197 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.108.197 I llm_load_print_meta: general.name     = 1.4B
0.00.108.198 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.108.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.108.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.108.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.108.199 I llm_load_print_meta: LF token         = 128 ''
0.00.108.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.108.199 I llm_load_print_meta: max token length = 1024
0.00.111.015 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.111.015 I llm_load_tensors: offloading output layer to GPU
0.00.111.015 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.111.026 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.111.028 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.112.130 I llama_new_context_with_model: n_seq_max     = 1
0.00.112.131 I llama_new_context_with_model: n_ctx         = 128
0.00.112.131 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.112.131 I llama_new_context_with_model: n_batch       = 128
0.00.112.132 I llama_new_context_with_model: n_ubatch      = 128
0.00.112.132 I llama_new_context_with_model: flash_attn    = 0
0.00.112.132 I llama_new_context_with_model: freq_base     = 10000.0
0.00.112.133 I llama_new_context_with_model: freq_scale    = 1
0.00.112.133 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.112.134 I ggml_metal_init: allocating
0.00.112.138 I ggml_metal_init: found device: Apple M4
0.00.112.140 I ggml_metal_init: picking default device: Apple M4
0.00.112.831 I ggml_metal_init: using embedded metal library
0.00.115.819 I ggml_metal_init: GPU name:   Apple M4
0.00.115.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.115.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.115.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.115.822 I ggml_metal_init: simdgroup reduction   = true
0.00.115.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.115.822 I ggml_metal_init: has bfloat            = true
0.00.115.823 I ggml_metal_init: use bfloat            = true
0.00.115.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.115.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.126.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.126.965 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.126.978 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.925 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.127.926 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.127.926 I llama_new_context_with_model: graph nodes  = 967
0.00.127.927 I llama_new_context_with_model: graph splits = 2
0.00.127.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.127.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.112.936 I 
0.01.113.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.113.065 I perplexity: tokenizing the input ..
0.01.126.308 I perplexity: tokenization took 13.239 ms
0.01.126.314 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.248.692 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.250.406 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.250.437 I llama_perf_context_print:        load time =    1081.25 ms
0.01.250.439 I llama_perf_context_print: prompt eval time =     121.44 ms /   128 tokens (    0.95 ms per token,  1053.98 tokens per second)
0.01.250.447 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.250.448 I llama_perf_context_print:       total time =     137.51 ms /   129 tokens
0.01.251.139 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.133s
sys	0m0.218s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.810 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.970 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.976 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.257 I llama_model_loader: - type  f32:  194 tensors
0.00.039.257 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.979 I llm_load_vocab: special tokens cache size = 25
0.00.069.314 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.317 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.318 I llm_load_print_meta: arch             = gptneox
0.00.069.318 I llm_load_print_meta: vocab type       = BPE
0.00.069.318 I llm_load_print_meta: n_vocab          = 50304
0.00.069.318 I llm_load_print_meta: n_merges         = 50009
0.00.069.318 I llm_load_print_meta: vocab_only       = 0
0.00.069.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.319 I llm_load_print_meta: n_embd           = 2048
0.00.069.319 I llm_load_print_meta: n_layer          = 24
0.00.069.338 I llm_load_print_meta: n_head           = 16
0.00.069.338 I llm_load_print_meta: n_head_kv        = 16
0.00.069.338 I llm_load_print_meta: n_rot            = 32
0.00.069.338 I llm_load_print_meta: n_swa            = 0
0.00.069.339 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.339 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.339 I llm_load_print_meta: n_gqa            = 1
0.00.069.340 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.341 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.341 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.341 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.341 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.342 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.342 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.342 I llm_load_print_meta: n_ff             = 8192
0.00.069.343 I llm_load_print_meta: n_expert         = 0
0.00.069.343 I llm_load_print_meta: n_expert_used    = 0
0.00.069.343 I llm_load_print_meta: causal attn      = 1
0.00.069.343 I llm_load_print_meta: pooling type     = 0
0.00.069.347 I llm_load_print_meta: rope type        = 2
0.00.069.349 I llm_load_print_meta: rope scaling     = linear
0.00.069.349 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.349 I llm_load_print_meta: freq_scale_train = 1
0.00.069.350 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.350 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.350 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.350 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.350 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.350 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.350 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.351 I llm_load_print_meta: model type       = 1.4B
0.00.069.351 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.351 I llm_load_print_meta: model params     = 1.41 B
0.00.069.352 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.352 I llm_load_print_meta: general.name     = 1.4B
0.00.069.352 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.352 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.353 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.353 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.353 I llm_load_print_meta: LF token         = 128 ''
0.00.069.353 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.353 I llm_load_print_meta: max token length = 1024
0.00.071.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.673 I llm_load_tensors: offloading output layer to GPU
0.00.071.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.684 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.685 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.633 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.633 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.634 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.634 I llama_new_context_with_model: n_batch       = 2048
0.00.072.634 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.634 I llama_new_context_with_model: flash_attn    = 0
0.00.072.635 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.635 I llama_new_context_with_model: freq_scale    = 1
0.00.072.635 I ggml_metal_init: allocating
0.00.072.638 I ggml_metal_init: found device: Apple M4
0.00.072.640 I ggml_metal_init: picking default device: Apple M4
0.00.073.294 I ggml_metal_init: using embedded metal library
0.00.075.980 I ggml_metal_init: GPU name:   Apple M4
0.00.075.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.982 I ggml_metal_init: simdgroup reduction   = true
0.00.075.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.983 I ggml_metal_init: has bfloat            = true
0.00.075.983 I ggml_metal_init: use bfloat            = true
0.00.075.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.085 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.094 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.113 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.118 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.120 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.120 I llama_new_context_with_model: graph nodes  = 967
0.00.111.121 I llama_new_context_with_model: graph splits = 2
0.00.111.148 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.290 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.291 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.389.088 I main: llama threadpool init, n_threads = 4
0.01.389.125 I 
0.01.389.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.389.157 I 
0.01.389.372 I sampler seed: 1234
0.01.389.377 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.389.430 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.389.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.389.431 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.490.970 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48005.41 tokens per second)
0.02.490.970 I llama_perf_context_print:        load time =    1379.27 ms
0.02.490.972 I llama_perf_context_print: prompt eval time =      42.97 ms /     7 tokens (    6.14 ms per token,   162.92 tokens per second)
0.02.490.972 I llama_perf_context_print:        eval time =    1055.81 ms /    63 runs   (   16.76 ms per token,    59.67 tokens per second)
0.02.490.973 I llama_perf_context_print:       total time =    1101.89 ms /    70 tokens
0.02.491.197 I ggml_metal_free: deallocating

real	0m2.510s
user	0m0.119s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.142 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.040 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.649 I llama_model_loader: - type  f32:  194 tensors
0.00.033.649 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.657 I llm_load_vocab: special tokens cache size = 25
0.00.063.434 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.438 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.438 I llm_load_print_meta: arch             = gptneox
0.00.063.438 I llm_load_print_meta: vocab type       = BPE
0.00.063.439 I llm_load_print_meta: n_vocab          = 50304
0.00.063.439 I llm_load_print_meta: n_merges         = 50009
0.00.063.439 I llm_load_print_meta: vocab_only       = 0
0.00.063.439 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.439 I llm_load_print_meta: n_embd           = 2048
0.00.063.440 I llm_load_print_meta: n_layer          = 24
0.00.063.456 I llm_load_print_meta: n_head           = 16
0.00.063.457 I llm_load_print_meta: n_head_kv        = 16
0.00.063.457 I llm_load_print_meta: n_rot            = 32
0.00.063.457 I llm_load_print_meta: n_swa            = 0
0.00.063.459 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.460 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.460 I llm_load_print_meta: n_gqa            = 1
0.00.063.461 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.461 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.462 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.462 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.463 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.463 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.463 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.464 I llm_load_print_meta: n_ff             = 8192
0.00.063.464 I llm_load_print_meta: n_expert         = 0
0.00.063.464 I llm_load_print_meta: n_expert_used    = 0
0.00.063.464 I llm_load_print_meta: causal attn      = 1
0.00.063.464 I llm_load_print_meta: pooling type     = 0
0.00.063.464 I llm_load_print_meta: rope type        = 2
0.00.063.464 I llm_load_print_meta: rope scaling     = linear
0.00.063.465 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.465 I llm_load_print_meta: freq_scale_train = 1
0.00.063.465 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.466 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.467 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.467 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.467 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.467 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.467 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.467 I llm_load_print_meta: model type       = 1.4B
0.00.063.467 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.469 I llm_load_print_meta: model params     = 1.41 B
0.00.063.469 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.471 I llm_load_print_meta: general.name     = 1.4B
0.00.063.471 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.471 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.471 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.471 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.471 I llm_load_print_meta: LF token         = 128 ''
0.00.063.472 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.473 I llm_load_print_meta: max token length = 1024
0.00.065.710 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.710 I llm_load_tensors: offloading output layer to GPU
0.00.065.711 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.721 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.723 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.630 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.631 I llama_new_context_with_model: n_ctx         = 128
0.00.066.631 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.631 I llama_new_context_with_model: n_batch       = 128
0.00.066.631 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.631 I llama_new_context_with_model: flash_attn    = 0
0.00.066.632 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.632 I llama_new_context_with_model: freq_scale    = 1
0.00.066.632 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.633 I ggml_metal_init: allocating
0.00.066.639 I ggml_metal_init: found device: Apple M4
0.00.066.643 I ggml_metal_init: picking default device: Apple M4
0.00.067.248 I ggml_metal_init: using embedded metal library
0.00.069.703 I ggml_metal_init: GPU name:   Apple M4
0.00.069.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.706 I ggml_metal_init: simdgroup reduction   = true
0.00.069.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.706 I ggml_metal_init: has bfloat            = true
0.00.069.706 I ggml_metal_init: use bfloat            = true
0.00.069.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.211 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.216 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.138 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.139 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.139 I llama_new_context_with_model: graph nodes  = 967
0.00.081.140 I llama_new_context_with_model: graph splits = 2
0.00.081.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.189.853 I 
0.01.189.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.189.898 I perplexity: tokenizing the input ..
0.01.197.606 I perplexity: tokenization took 7.706 ms
0.01.197.608 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.322.042 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.323.214 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.323.234 I llama_perf_context_print:        load time =    1178.81 ms
0.01.323.235 I llama_perf_context_print: prompt eval time =     124.20 ms /   128 tokens (    0.97 ms per token,  1030.56 tokens per second)
0.01.323.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.323.236 I llama_perf_context_print:       total time =     133.38 ms /   129 tokens
0.01.323.753 I ggml_metal_free: deallocating

real	0m1.342s
user	0m0.088s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.016.574 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.942 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.947 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.949 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.949 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.952 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.769 I llama_model_loader: - type  f32:  194 tensors
0.00.040.770 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.069 I llm_load_vocab: special tokens cache size = 25
0.00.075.131 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.135 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.135 I llm_load_print_meta: arch             = gptneox
0.00.075.136 I llm_load_print_meta: vocab type       = BPE
0.00.075.136 I llm_load_print_meta: n_vocab          = 50304
0.00.075.136 I llm_load_print_meta: n_merges         = 50009
0.00.075.136 I llm_load_print_meta: vocab_only       = 0
0.00.075.137 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.137 I llm_load_print_meta: n_embd           = 2048
0.00.075.137 I llm_load_print_meta: n_layer          = 24
0.00.075.153 I llm_load_print_meta: n_head           = 16
0.00.075.155 I llm_load_print_meta: n_head_kv        = 16
0.00.075.155 I llm_load_print_meta: n_rot            = 32
0.00.075.155 I llm_load_print_meta: n_swa            = 0
0.00.075.155 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.155 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.156 I llm_load_print_meta: n_gqa            = 1
0.00.075.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.160 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.161 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.162 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.162 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.164 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.164 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.165 I llm_load_print_meta: n_ff             = 8192
0.00.075.165 I llm_load_print_meta: n_expert         = 0
0.00.075.165 I llm_load_print_meta: n_expert_used    = 0
0.00.075.167 I llm_load_print_meta: causal attn      = 1
0.00.075.169 I llm_load_print_meta: pooling type     = 0
0.00.075.169 I llm_load_print_meta: rope type        = 2
0.00.075.169 I llm_load_print_meta: rope scaling     = linear
0.00.075.169 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.170 I llm_load_print_meta: freq_scale_train = 1
0.00.075.170 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.170 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.170 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.170 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.171 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.171 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.171 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.171 I llm_load_print_meta: model type       = 1.4B
0.00.075.171 I llm_load_print_meta: model ftype      = Q4_0
0.00.075.172 I llm_load_print_meta: model params     = 1.41 B
0.00.075.172 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.075.173 I llm_load_print_meta: general.name     = 1.4B
0.00.075.173 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.173 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.174 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.174 I llm_load_print_meta: LF token         = 128 ''
0.00.075.174 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.176 I llm_load_print_meta: max token length = 1024
0.00.077.904 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.904 I llm_load_tensors: offloading output layer to GPU
0.00.077.904 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.917 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.077.918 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.079.410 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.412 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.412 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.412 I llama_new_context_with_model: n_batch       = 2048
0.00.079.412 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.413 I llama_new_context_with_model: flash_attn    = 0
0.00.079.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.414 I llama_new_context_with_model: freq_scale    = 1
0.00.079.414 I ggml_metal_init: allocating
0.00.079.419 I ggml_metal_init: found device: Apple M4
0.00.079.422 I ggml_metal_init: picking default device: Apple M4
0.00.080.441 I ggml_metal_init: using embedded metal library
0.00.084.246 I ggml_metal_init: GPU name:   Apple M4
0.00.084.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.250 I ggml_metal_init: simdgroup reduction   = true
0.00.084.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.250 I ggml_metal_init: has bfloat            = true
0.00.084.252 I ggml_metal_init: use bfloat            = true
0.00.084.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.125.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.936 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.968 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.127.122 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.127.124 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.127.125 I llama_new_context_with_model: graph nodes  = 967
0.00.127.125 I llama_new_context_with_model: graph splits = 2
0.00.127.154 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.127.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.127.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.123 I main: llama threadpool init, n_threads = 4
0.00.730.228 I 
0.00.730.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.299 I 
0.00.730.595 I sampler seed: 1234
0.00.730.603 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.661 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.661 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.418.627 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.418.628 I llama_perf_context_print:        load time =     713.54 ms
0.01.418.628 I llama_perf_context_print: prompt eval time =      46.00 ms /     7 tokens (    6.57 ms per token,   152.16 tokens per second)
0.01.418.633 I llama_perf_context_print:        eval time =     638.88 ms /    63 runs   (   10.14 ms per token,    98.61 tokens per second)
0.01.418.635 I llama_perf_context_print:       total time =     688.51 ms /    70 tokens
0.01.418.805 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.129s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.289 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.320 I llama_model_loader: - type  f32:  194 tensors
0.00.026.320 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.320 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.665 I llm_load_vocab: special tokens cache size = 25
0.00.052.653 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.656 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.656 I llm_load_print_meta: arch             = gptneox
0.00.052.656 I llm_load_print_meta: vocab type       = BPE
0.00.052.656 I llm_load_print_meta: n_vocab          = 50304
0.00.052.657 I llm_load_print_meta: n_merges         = 50009
0.00.052.657 I llm_load_print_meta: vocab_only       = 0
0.00.052.657 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.657 I llm_load_print_meta: n_embd           = 2048
0.00.052.657 I llm_load_print_meta: n_layer          = 24
0.00.052.672 I llm_load_print_meta: n_head           = 16
0.00.052.672 I llm_load_print_meta: n_head_kv        = 16
0.00.052.673 I llm_load_print_meta: n_rot            = 32
0.00.052.673 I llm_load_print_meta: n_swa            = 0
0.00.052.673 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.673 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.674 I llm_load_print_meta: n_gqa            = 1
0.00.052.675 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.675 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.676 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.676 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.677 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.677 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.677 I llm_load_print_meta: n_ff             = 8192
0.00.052.678 I llm_load_print_meta: n_expert         = 0
0.00.052.678 I llm_load_print_meta: n_expert_used    = 0
0.00.052.678 I llm_load_print_meta: causal attn      = 1
0.00.052.678 I llm_load_print_meta: pooling type     = 0
0.00.052.678 I llm_load_print_meta: rope type        = 2
0.00.052.678 I llm_load_print_meta: rope scaling     = linear
0.00.052.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.679 I llm_load_print_meta: freq_scale_train = 1
0.00.052.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.680 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.680 I llm_load_print_meta: model type       = 1.4B
0.00.052.680 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.680 I llm_load_print_meta: model params     = 1.41 B
0.00.052.681 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.681 I llm_load_print_meta: general.name     = 1.4B
0.00.052.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.682 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.682 I llm_load_print_meta: LF token         = 128 ''
0.00.052.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.684 I llm_load_print_meta: max token length = 1024
0.00.054.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.594 I llm_load_tensors: offloading output layer to GPU
0.00.054.594 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.605 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.607 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.517 I llama_new_context_with_model: n_ctx         = 128
0.00.055.517 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.518 I llama_new_context_with_model: n_batch       = 128
0.00.055.518 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.518 I llama_new_context_with_model: flash_attn    = 0
0.00.055.518 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.518 I llama_new_context_with_model: freq_scale    = 1
0.00.055.519 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.519 I ggml_metal_init: allocating
0.00.055.522 I ggml_metal_init: found device: Apple M4
0.00.055.524 I ggml_metal_init: picking default device: Apple M4
0.00.056.091 I ggml_metal_init: using embedded metal library
0.00.058.445 I ggml_metal_init: GPU name:   Apple M4
0.00.058.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.447 I ggml_metal_init: simdgroup reduction   = true
0.00.058.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.447 I ggml_metal_init: has bfloat            = true
0.00.058.448 I ggml_metal_init: use bfloat            = true
0.00.058.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.448 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.599 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.601 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.615 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.556 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.557 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.557 I llama_new_context_with_model: graph nodes  = 967
0.00.072.558 I llama_new_context_with_model: graph splits = 2
0.00.072.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.072.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.367 I 
0.00.688.408 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.421 I perplexity: tokenizing the input ..
0.00.696.716 I perplexity: tokenization took 8.293 ms
0.00.696.721 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.475 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.820.631 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.820.646 I llama_perf_context_print:        load time =     679.07 ms
0.00.820.648 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.67 tokens per second)
0.00.820.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.649 I llama_perf_context_print:       total time =     132.28 ms /   129 tokens
0.00.821.091 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.078s
sys	0m0.103s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.748 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.788 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.790 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.797 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.000 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.000 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.001 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.001 I llama_model_loader: - type  f32:  194 tensors
0.00.037.002 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.002 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.293 I llm_load_vocab: special tokens cache size = 25
0.00.067.559 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.562 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.563 I llm_load_print_meta: arch             = gptneox
0.00.067.563 I llm_load_print_meta: vocab type       = BPE
0.00.067.563 I llm_load_print_meta: n_vocab          = 50304
0.00.067.563 I llm_load_print_meta: n_merges         = 50009
0.00.067.564 I llm_load_print_meta: vocab_only       = 0
0.00.067.564 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.564 I llm_load_print_meta: n_embd           = 2048
0.00.067.564 I llm_load_print_meta: n_layer          = 24
0.00.067.577 I llm_load_print_meta: n_head           = 16
0.00.067.577 I llm_load_print_meta: n_head_kv        = 16
0.00.067.577 I llm_load_print_meta: n_rot            = 32
0.00.067.578 I llm_load_print_meta: n_swa            = 0
0.00.067.578 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.578 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.579 I llm_load_print_meta: n_gqa            = 1
0.00.067.579 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.580 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.580 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.581 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.581 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.581 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.581 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.582 I llm_load_print_meta: n_ff             = 8192
0.00.067.582 I llm_load_print_meta: n_expert         = 0
0.00.067.582 I llm_load_print_meta: n_expert_used    = 0
0.00.067.583 I llm_load_print_meta: causal attn      = 1
0.00.067.584 I llm_load_print_meta: pooling type     = 0
0.00.067.584 I llm_load_print_meta: rope type        = 2
0.00.067.584 I llm_load_print_meta: rope scaling     = linear
0.00.067.584 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.585 I llm_load_print_meta: freq_scale_train = 1
0.00.067.585 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.585 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.585 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.585 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.585 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.585 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.586 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.586 I llm_load_print_meta: model type       = 1.4B
0.00.067.586 I llm_load_print_meta: model ftype      = Q4_1
0.00.067.586 I llm_load_print_meta: model params     = 1.41 B
0.00.067.587 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.067.587 I llm_load_print_meta: general.name     = 1.4B
0.00.067.587 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.587 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.588 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.588 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.588 I llm_load_print_meta: LF token         = 128 ''
0.00.067.588 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.588 I llm_load_print_meta: max token length = 1024
0.00.069.291 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.292 I llm_load_tensors: offloading output layer to GPU
0.00.069.292 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.301 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.069.303 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.070.197 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.198 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.199 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.199 I llama_new_context_with_model: n_batch       = 2048
0.00.070.199 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.199 I llama_new_context_with_model: flash_attn    = 0
0.00.070.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.200 I llama_new_context_with_model: freq_scale    = 1
0.00.070.200 I ggml_metal_init: allocating
0.00.070.203 I ggml_metal_init: found device: Apple M4
0.00.070.205 I ggml_metal_init: picking default device: Apple M4
0.00.070.821 I ggml_metal_init: using embedded metal library
0.00.073.469 I ggml_metal_init: GPU name:   Apple M4
0.00.073.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.472 I ggml_metal_init: simdgroup reduction   = true
0.00.073.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.472 I ggml_metal_init: has bfloat            = true
0.00.073.473 I ggml_metal_init: use bfloat            = true
0.00.073.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.231 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.238 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.258 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.298 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.299 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.299 I llama_new_context_with_model: graph nodes  = 967
0.00.106.299 I llama_new_context_with_model: graph splits = 2
0.00.106.324 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.477 I main: llama threadpool init, n_threads = 4
0.00.825.513 I 
0.00.825.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.543 I 
0.00.825.773 I sampler seed: 1234
0.00.825.777 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.823 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.823 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.824 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.557.118 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.557.119 I llama_perf_context_print:        load time =     816.73 ms
0.01.557.123 I llama_perf_context_print: prompt eval time =      44.09 ms /     7 tokens (    6.30 ms per token,   158.77 tokens per second)
0.01.557.124 I llama_perf_context_print:        eval time =     684.19 ms /    63 runs   (   10.86 ms per token,    92.08 tokens per second)
0.01.557.124 I llama_perf_context_print:       total time =     731.64 ms /    70 tokens
0.01.557.312 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.115s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.999 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.331 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.332 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.335 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.337 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.338 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.147 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.148 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.148 I llama_model_loader: - type  f32:  194 tensors
0.00.032.149 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.966 I llm_load_vocab: special tokens cache size = 25
0.00.060.846 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.849 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.849 I llm_load_print_meta: arch             = gptneox
0.00.060.850 I llm_load_print_meta: vocab type       = BPE
0.00.060.850 I llm_load_print_meta: n_vocab          = 50304
0.00.060.850 I llm_load_print_meta: n_merges         = 50009
0.00.060.850 I llm_load_print_meta: vocab_only       = 0
0.00.060.850 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.851 I llm_load_print_meta: n_embd           = 2048
0.00.060.851 I llm_load_print_meta: n_layer          = 24
0.00.060.866 I llm_load_print_meta: n_head           = 16
0.00.060.867 I llm_load_print_meta: n_head_kv        = 16
0.00.060.867 I llm_load_print_meta: n_rot            = 32
0.00.060.867 I llm_load_print_meta: n_swa            = 0
0.00.060.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.868 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.868 I llm_load_print_meta: n_gqa            = 1
0.00.060.869 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.870 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.870 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.871 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.871 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.874 I llm_load_print_meta: n_ff             = 8192
0.00.060.874 I llm_load_print_meta: n_expert         = 0
0.00.060.874 I llm_load_print_meta: n_expert_used    = 0
0.00.060.875 I llm_load_print_meta: causal attn      = 1
0.00.060.875 I llm_load_print_meta: pooling type     = 0
0.00.060.875 I llm_load_print_meta: rope type        = 2
0.00.060.876 I llm_load_print_meta: rope scaling     = linear
0.00.060.876 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.876 I llm_load_print_meta: freq_scale_train = 1
0.00.060.876 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.877 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.877 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.877 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.877 I llm_load_print_meta: model type       = 1.4B
0.00.060.878 I llm_load_print_meta: model ftype      = Q4_1
0.00.060.878 I llm_load_print_meta: model params     = 1.41 B
0.00.060.879 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.060.879 I llm_load_print_meta: general.name     = 1.4B
0.00.060.879 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.880 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.881 I llm_load_print_meta: LF token         = 128 ''
0.00.060.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.881 I llm_load_print_meta: max token length = 1024
0.00.062.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.908 I llm_load_tensors: offloading output layer to GPU
0.00.062.908 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.918 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.920 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.063.820 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.820 I llama_new_context_with_model: n_ctx         = 128
0.00.063.821 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.821 I llama_new_context_with_model: n_batch       = 128
0.00.063.821 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.821 I llama_new_context_with_model: flash_attn    = 0
0.00.063.822 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.822 I llama_new_context_with_model: freq_scale    = 1
0.00.063.822 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.823 I ggml_metal_init: allocating
0.00.063.826 I ggml_metal_init: found device: Apple M4
0.00.063.828 I ggml_metal_init: picking default device: Apple M4
0.00.064.397 I ggml_metal_init: using embedded metal library
0.00.066.840 I ggml_metal_init: GPU name:   Apple M4
0.00.066.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.843 I ggml_metal_init: simdgroup reduction   = true
0.00.066.843 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.843 I ggml_metal_init: has bfloat            = true
0.00.066.843 I ggml_metal_init: use bfloat            = true
0.00.066.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.873 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.875 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.889 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.796 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.797 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.797 I llama_new_context_with_model: graph nodes  = 967
0.00.078.797 I llama_new_context_with_model: graph splits = 2
0.00.078.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.078.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.014 I 
0.00.738.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.072 I perplexity: tokenizing the input ..
0.00.746.146 I perplexity: tokenization took 8.072 ms
0.00.746.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.058 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.870.460 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.870.489 I llama_perf_context_print:        load time =     729.01 ms
0.00.870.490 I llama_perf_context_print: prompt eval time =     122.68 ms /   128 tokens (    0.96 ms per token,  1043.37 tokens per second)
0.00.870.491 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.492 I llama_perf_context_print:       total time =     132.48 ms /   129 tokens
0.00.870.901 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.080s
sys	0m0.101s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.407 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.725 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.727 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.728 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.629 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.392 I llama_model_loader: - type  f32:  194 tensors
0.00.024.392 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.392 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.429 I llm_load_vocab: special tokens cache size = 25
0.00.051.445 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.448 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.448 I llm_load_print_meta: arch             = gptneox
0.00.051.449 I llm_load_print_meta: vocab type       = BPE
0.00.051.449 I llm_load_print_meta: n_vocab          = 50304
0.00.051.449 I llm_load_print_meta: n_merges         = 50009
0.00.051.449 I llm_load_print_meta: vocab_only       = 0
0.00.051.449 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.450 I llm_load_print_meta: n_embd           = 2048
0.00.051.450 I llm_load_print_meta: n_layer          = 24
0.00.051.465 I llm_load_print_meta: n_head           = 16
0.00.051.466 I llm_load_print_meta: n_head_kv        = 16
0.00.051.466 I llm_load_print_meta: n_rot            = 32
0.00.051.466 I llm_load_print_meta: n_swa            = 0
0.00.051.466 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.467 I llm_load_print_meta: n_gqa            = 1
0.00.051.468 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.469 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.469 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.469 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.470 I llm_load_print_meta: n_ff             = 8192
0.00.051.471 I llm_load_print_meta: n_expert         = 0
0.00.051.471 I llm_load_print_meta: n_expert_used    = 0
0.00.051.471 I llm_load_print_meta: causal attn      = 1
0.00.051.471 I llm_load_print_meta: pooling type     = 0
0.00.051.471 I llm_load_print_meta: rope type        = 2
0.00.051.471 I llm_load_print_meta: rope scaling     = linear
0.00.051.472 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.472 I llm_load_print_meta: freq_scale_train = 1
0.00.051.472 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.472 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.473 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.473 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.474 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.474 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.474 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.474 I llm_load_print_meta: model type       = 1.4B
0.00.051.474 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.475 I llm_load_print_meta: model params     = 1.41 B
0.00.051.475 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.476 I llm_load_print_meta: general.name     = 1.4B
0.00.051.476 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.476 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.476 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.476 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.476 I llm_load_print_meta: LF token         = 128 ''
0.00.051.477 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.477 I llm_load_print_meta: max token length = 1024
0.00.053.495 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.495 I llm_load_tensors: offloading output layer to GPU
0.00.053.495 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.506 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.507 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.403 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.404 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.404 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.404 I llama_new_context_with_model: n_batch       = 2048
0.00.054.404 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.405 I llama_new_context_with_model: flash_attn    = 0
0.00.054.405 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.405 I llama_new_context_with_model: freq_scale    = 1
0.00.054.406 I ggml_metal_init: allocating
0.00.054.409 I ggml_metal_init: found device: Apple M4
0.00.054.411 I ggml_metal_init: picking default device: Apple M4
0.00.055.039 I ggml_metal_init: using embedded metal library
0.00.057.378 I ggml_metal_init: GPU name:   Apple M4
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.380 I ggml_metal_init: simdgroup reduction   = true
0.00.057.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.380 I ggml_metal_init: has bfloat            = true
0.00.057.380 I ggml_metal_init: use bfloat            = true
0.00.057.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.174 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.180 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.205 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.331 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.333 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.333 I llama_new_context_with_model: graph nodes  = 967
0.00.088.333 I llama_new_context_with_model: graph splits = 2
0.00.088.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.520 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.421 I main: llama threadpool init, n_threads = 4
0.00.758.460 I 
0.00.758.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.489 I 
0.00.758.720 I sampler seed: 1234
0.00.758.724 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.748 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.748 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.748 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.548.645 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.548.647 I llama_perf_context_print:        load time =     749.01 ms
0.01.548.648 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.47 tokens per second)
0.01.548.648 I llama_perf_context_print:        eval time =     739.68 ms /    63 runs   (   11.74 ms per token,    85.17 tokens per second)
0.01.548.649 I llama_perf_context_print:       total time =     790.23 ms /    70 tokens
0.01.548.847 I ggml_metal_free: deallocating

real	0m1.568s
user	0m0.112s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.577 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.101 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.111 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.902 I llama_model_loader: - type  f32:  194 tensors
0.00.033.902 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.903 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.587 I llm_load_vocab: special tokens cache size = 25
0.00.061.474 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.477 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.477 I llm_load_print_meta: arch             = gptneox
0.00.061.477 I llm_load_print_meta: vocab type       = BPE
0.00.061.478 I llm_load_print_meta: n_vocab          = 50304
0.00.061.478 I llm_load_print_meta: n_merges         = 50009
0.00.061.478 I llm_load_print_meta: vocab_only       = 0
0.00.061.478 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.478 I llm_load_print_meta: n_embd           = 2048
0.00.061.479 I llm_load_print_meta: n_layer          = 24
0.00.061.493 I llm_load_print_meta: n_head           = 16
0.00.061.495 I llm_load_print_meta: n_head_kv        = 16
0.00.061.495 I llm_load_print_meta: n_rot            = 32
0.00.061.495 I llm_load_print_meta: n_swa            = 0
0.00.061.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.496 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.496 I llm_load_print_meta: n_gqa            = 1
0.00.061.497 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.498 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.499 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.499 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.499 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.501 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.502 I llm_load_print_meta: n_ff             = 8192
0.00.061.502 I llm_load_print_meta: n_expert         = 0
0.00.061.502 I llm_load_print_meta: n_expert_used    = 0
0.00.061.502 I llm_load_print_meta: causal attn      = 1
0.00.061.502 I llm_load_print_meta: pooling type     = 0
0.00.061.502 I llm_load_print_meta: rope type        = 2
0.00.061.502 I llm_load_print_meta: rope scaling     = linear
0.00.061.503 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.503 I llm_load_print_meta: freq_scale_train = 1
0.00.061.503 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.504 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.504 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.504 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.504 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.504 I llm_load_print_meta: model type       = 1.4B
0.00.061.505 I llm_load_print_meta: model ftype      = Q5_0
0.00.061.505 I llm_load_print_meta: model params     = 1.41 B
0.00.061.506 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.061.507 I llm_load_print_meta: general.name     = 1.4B
0.00.061.507 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.507 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.507 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.507 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.508 I llm_load_print_meta: LF token         = 128 ''
0.00.061.508 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.508 I llm_load_print_meta: max token length = 1024
0.00.063.520 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.520 I llm_load_tensors: offloading output layer to GPU
0.00.063.520 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.531 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.063.532 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.064.509 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.510 I llama_new_context_with_model: n_ctx         = 128
0.00.064.510 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.510 I llama_new_context_with_model: n_batch       = 128
0.00.064.510 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.510 I llama_new_context_with_model: flash_attn    = 0
0.00.064.511 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.511 I llama_new_context_with_model: freq_scale    = 1
0.00.064.511 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.512 I ggml_metal_init: allocating
0.00.064.515 I ggml_metal_init: found device: Apple M4
0.00.064.517 I ggml_metal_init: picking default device: Apple M4
0.00.065.113 I ggml_metal_init: using embedded metal library
0.00.067.462 I ggml_metal_init: GPU name:   Apple M4
0.00.067.464 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.465 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.465 I ggml_metal_init: simdgroup reduction   = true
0.00.067.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.465 I ggml_metal_init: has bfloat            = true
0.00.067.465 I ggml_metal_init: use bfloat            = true
0.00.067.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.421 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.435 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.372 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.373 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.374 I llama_new_context_with_model: graph nodes  = 967
0.00.079.374 I llama_new_context_with_model: graph splits = 2
0.00.079.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.388 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.226 I 
0.00.800.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.324 I perplexity: tokenizing the input ..
0.00.808.663 I perplexity: tokenization took 8.338 ms
0.00.808.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.943.507 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.944.733 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.944.749 I llama_perf_context_print:        load time =     784.64 ms
0.00.944.751 I llama_perf_context_print: prompt eval time =     134.62 ms /   128 tokens (    1.05 ms per token,   950.86 tokens per second)
0.00.944.752 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.944.753 I llama_perf_context_print:       total time =     144.53 ms /   129 tokens
0.00.945.205 I ggml_metal_free: deallocating

real	0m0.960s
user	0m0.080s
sys	0m0.115s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.463 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.822 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.836 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.539 I llama_model_loader: - type  f32:  194 tensors
0.00.024.539 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.540 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.835 I llm_load_vocab: special tokens cache size = 25
0.00.051.868 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.871 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.872 I llm_load_print_meta: arch             = gptneox
0.00.051.872 I llm_load_print_meta: vocab type       = BPE
0.00.051.872 I llm_load_print_meta: n_vocab          = 50304
0.00.051.873 I llm_load_print_meta: n_merges         = 50009
0.00.051.873 I llm_load_print_meta: vocab_only       = 0
0.00.051.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.873 I llm_load_print_meta: n_embd           = 2048
0.00.051.873 I llm_load_print_meta: n_layer          = 24
0.00.051.888 I llm_load_print_meta: n_head           = 16
0.00.051.890 I llm_load_print_meta: n_head_kv        = 16
0.00.051.890 I llm_load_print_meta: n_rot            = 32
0.00.051.891 I llm_load_print_meta: n_swa            = 0
0.00.051.891 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.891 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.892 I llm_load_print_meta: n_gqa            = 1
0.00.051.894 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.895 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.897 I llm_load_print_meta: n_ff             = 8192
0.00.051.897 I llm_load_print_meta: n_expert         = 0
0.00.051.897 I llm_load_print_meta: n_expert_used    = 0
0.00.051.898 I llm_load_print_meta: causal attn      = 1
0.00.051.900 I llm_load_print_meta: pooling type     = 0
0.00.051.900 I llm_load_print_meta: rope type        = 2
0.00.051.900 I llm_load_print_meta: rope scaling     = linear
0.00.051.900 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.900 I llm_load_print_meta: freq_scale_train = 1
0.00.051.901 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.901 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.902 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.902 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.902 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.902 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.902 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.902 I llm_load_print_meta: model type       = 1.4B
0.00.051.903 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.903 I llm_load_print_meta: model params     = 1.41 B
0.00.051.904 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.904 I llm_load_print_meta: general.name     = 1.4B
0.00.051.904 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.904 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: LF token         = 128 ''
0.00.051.905 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: max token length = 1024
0.00.054.011 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.012 I llm_load_tensors: offloading output layer to GPU
0.00.054.012 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.022 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.023 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.984 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.985 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.986 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.986 I llama_new_context_with_model: n_batch       = 2048
0.00.054.986 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.986 I llama_new_context_with_model: flash_attn    = 0
0.00.054.986 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.987 I llama_new_context_with_model: freq_scale    = 1
0.00.054.987 I ggml_metal_init: allocating
0.00.054.990 I ggml_metal_init: found device: Apple M4
0.00.054.992 I ggml_metal_init: picking default device: Apple M4
0.00.055.593 I ggml_metal_init: using embedded metal library
0.00.057.991 I ggml_metal_init: GPU name:   Apple M4
0.00.057.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.993 I ggml_metal_init: simdgroup reduction   = true
0.00.057.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.995 I ggml_metal_init: has bfloat            = true
0.00.057.995 I ggml_metal_init: use bfloat            = true
0.00.057.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.049 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.054 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.070 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.129 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.131 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.131 I llama_new_context_with_model: graph nodes  = 967
0.00.089.131 I llama_new_context_with_model: graph splits = 2
0.00.089.156 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.302 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.483 I main: llama threadpool init, n_threads = 4
0.00.711.528 I 
0.00.711.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.578 I 
0.00.711.811 I sampler seed: 1234
0.00.711.818 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.863 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.867 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.867 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.550.629 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.01.550.630 I llama_perf_context_print:        load time =     702.01 ms
0.01.550.631 I llama_perf_context_print: prompt eval time =      42.26 ms /     7 tokens (    6.04 ms per token,   165.66 tokens per second)
0.01.550.631 I llama_perf_context_print:        eval time =     793.57 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.550.632 I llama_perf_context_print:       total time =     839.15 ms /    70 tokens
0.01.550.832 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.124 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.605 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.609 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.611 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.612 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.612 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.160 I llama_model_loader: - type  f32:  194 tensors
0.00.023.160 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.161 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.283 I llm_load_vocab: special tokens cache size = 25
0.00.049.314 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.317 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.317 I llm_load_print_meta: arch             = gptneox
0.00.049.318 I llm_load_print_meta: vocab type       = BPE
0.00.049.318 I llm_load_print_meta: n_vocab          = 50304
0.00.049.318 I llm_load_print_meta: n_merges         = 50009
0.00.049.318 I llm_load_print_meta: vocab_only       = 0
0.00.049.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.319 I llm_load_print_meta: n_embd           = 2048
0.00.049.319 I llm_load_print_meta: n_layer          = 24
0.00.049.333 I llm_load_print_meta: n_head           = 16
0.00.049.335 I llm_load_print_meta: n_head_kv        = 16
0.00.049.335 I llm_load_print_meta: n_rot            = 32
0.00.049.335 I llm_load_print_meta: n_swa            = 0
0.00.049.336 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.336 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.336 I llm_load_print_meta: n_gqa            = 1
0.00.049.337 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.338 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.339 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.339 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.339 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.339 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.340 I llm_load_print_meta: n_ff             = 8192
0.00.049.340 I llm_load_print_meta: n_expert         = 0
0.00.049.340 I llm_load_print_meta: n_expert_used    = 0
0.00.049.340 I llm_load_print_meta: causal attn      = 1
0.00.049.341 I llm_load_print_meta: pooling type     = 0
0.00.049.341 I llm_load_print_meta: rope type        = 2
0.00.049.341 I llm_load_print_meta: rope scaling     = linear
0.00.049.341 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.341 I llm_load_print_meta: freq_scale_train = 1
0.00.049.342 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.342 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.342 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.342 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.342 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.343 I llm_load_print_meta: model type       = 1.4B
0.00.049.343 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.343 I llm_load_print_meta: model params     = 1.41 B
0.00.049.343 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.344 I llm_load_print_meta: general.name     = 1.4B
0.00.049.344 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.344 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.344 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: LF token         = 128 ''
0.00.049.346 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.347 I llm_load_print_meta: max token length = 1024
0.00.051.402 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.402 I llm_load_tensors: offloading output layer to GPU
0.00.051.402 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.412 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.414 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.350 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.351 I llama_new_context_with_model: n_ctx         = 128
0.00.052.351 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.351 I llama_new_context_with_model: n_batch       = 128
0.00.052.351 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.351 I llama_new_context_with_model: flash_attn    = 0
0.00.052.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.352 I llama_new_context_with_model: freq_scale    = 1
0.00.052.352 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.353 I ggml_metal_init: allocating
0.00.052.359 I ggml_metal_init: found device: Apple M4
0.00.052.361 I ggml_metal_init: picking default device: Apple M4
0.00.052.949 I ggml_metal_init: using embedded metal library
0.00.055.278 I ggml_metal_init: GPU name:   Apple M4
0.00.055.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.280 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.280 I ggml_metal_init: simdgroup reduction   = true
0.00.055.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.280 I ggml_metal_init: has bfloat            = true
0.00.055.281 I ggml_metal_init: use bfloat            = true
0.00.055.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.282 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.123 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.128 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.143 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.026 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.027 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.028 I llama_new_context_with_model: graph nodes  = 967
0.00.067.028 I llama_new_context_with_model: graph splits = 2
0.00.067.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.984 I 
0.00.726.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.027 I perplexity: tokenizing the input ..
0.00.734.730 I perplexity: tokenization took 8.701 ms
0.00.734.734 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.869.054 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.870.457 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.870.479 I llama_perf_context_print:        load time =     716.85 ms
0.00.870.481 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.63 tokens per second)
0.00.870.482 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.482 I llama_perf_context_print:       total time =     144.50 ms /   129 tokens
0.00.870.855 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.078s
sys	0m0.132s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.836 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.065 I llama_model_loader: - type  f32:  194 tensors
0.00.025.065 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.065 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.494 I llm_load_vocab: special tokens cache size = 25
0.00.051.379 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.382 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.382 I llm_load_print_meta: arch             = gptneox
0.00.051.382 I llm_load_print_meta: vocab type       = BPE
0.00.051.383 I llm_load_print_meta: n_vocab          = 50304
0.00.051.383 I llm_load_print_meta: n_merges         = 50009
0.00.051.383 I llm_load_print_meta: vocab_only       = 0
0.00.051.383 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.383 I llm_load_print_meta: n_embd           = 2048
0.00.051.384 I llm_load_print_meta: n_layer          = 24
0.00.051.398 I llm_load_print_meta: n_head           = 16
0.00.051.400 I llm_load_print_meta: n_head_kv        = 16
0.00.051.400 I llm_load_print_meta: n_rot            = 32
0.00.051.401 I llm_load_print_meta: n_swa            = 0
0.00.051.401 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.401 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.402 I llm_load_print_meta: n_gqa            = 1
0.00.051.402 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.403 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.403 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.404 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.404 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.404 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.404 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.405 I llm_load_print_meta: n_ff             = 8192
0.00.051.405 I llm_load_print_meta: n_expert         = 0
0.00.051.405 I llm_load_print_meta: n_expert_used    = 0
0.00.051.405 I llm_load_print_meta: causal attn      = 1
0.00.051.406 I llm_load_print_meta: pooling type     = 0
0.00.051.406 I llm_load_print_meta: rope type        = 2
0.00.051.407 I llm_load_print_meta: rope scaling     = linear
0.00.051.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.408 I llm_load_print_meta: freq_scale_train = 1
0.00.051.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.409 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.409 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.409 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.411 I llm_load_print_meta: model type       = 1.4B
0.00.051.411 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.412 I llm_load_print_meta: model params     = 1.41 B
0.00.051.412 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.412 I llm_load_print_meta: general.name     = 1.4B
0.00.051.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: LF token         = 128 ''
0.00.051.415 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.415 I llm_load_print_meta: max token length = 1024
0.00.053.313 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.313 I llm_load_tensors: offloading output layer to GPU
0.00.053.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.324 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.325 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.218 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.219 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.219 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.219 I llama_new_context_with_model: n_batch       = 2048
0.00.054.220 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.220 I llama_new_context_with_model: flash_attn    = 0
0.00.054.220 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.221 I llama_new_context_with_model: freq_scale    = 1
0.00.054.221 I ggml_metal_init: allocating
0.00.054.224 I ggml_metal_init: found device: Apple M4
0.00.054.226 I ggml_metal_init: picking default device: Apple M4
0.00.054.790 I ggml_metal_init: using embedded metal library
0.00.057.092 I ggml_metal_init: GPU name:   Apple M4
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.094 I ggml_metal_init: simdgroup reduction   = true
0.00.057.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.094 I ggml_metal_init: has bfloat            = true
0.00.057.094 I ggml_metal_init: use bfloat            = true
0.00.057.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.315 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.321 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.420 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.422 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.422 I llama_new_context_with_model: graph nodes  = 967
0.00.087.422 I llama_new_context_with_model: graph splits = 2
0.00.087.446 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.588 I main: llama threadpool init, n_threads = 4
0.00.443.633 I 
0.00.443.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.443.659 I 
0.00.443.902 I sampler seed: 1234
0.00.443.908 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.944 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.945 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.116.606 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.116.607 I llama_perf_context_print:        load time =     433.75 ms
0.01.116.608 I llama_perf_context_print: prompt eval time =      36.02 ms /     7 tokens (    5.15 ms per token,   194.34 tokens per second)
0.01.116.608 I llama_perf_context_print:        eval time =     634.02 ms /    63 runs   (   10.06 ms per token,    99.37 tokens per second)
0.01.116.609 I llama_perf_context_print:       total time =     673.02 ms /    70 tokens
0.01.116.819 I ggml_metal_free: deallocating

real	0m1.136s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.660 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.682 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.687 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.687 I llama_model_loader: - type  f32:  194 tensors
0.00.026.687 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.687 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.688 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.669 I llm_load_vocab: special tokens cache size = 25
0.00.053.689 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.693 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.694 I llm_load_print_meta: arch             = gptneox
0.00.053.694 I llm_load_print_meta: vocab type       = BPE
0.00.053.694 I llm_load_print_meta: n_vocab          = 50304
0.00.053.694 I llm_load_print_meta: n_merges         = 50009
0.00.053.695 I llm_load_print_meta: vocab_only       = 0
0.00.053.695 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.695 I llm_load_print_meta: n_embd           = 2048
0.00.053.695 I llm_load_print_meta: n_layer          = 24
0.00.053.713 I llm_load_print_meta: n_head           = 16
0.00.053.713 I llm_load_print_meta: n_head_kv        = 16
0.00.053.713 I llm_load_print_meta: n_rot            = 32
0.00.053.714 I llm_load_print_meta: n_swa            = 0
0.00.053.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.714 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.718 I llm_load_print_meta: n_gqa            = 1
0.00.053.718 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.719 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.719 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.724 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.725 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.725 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.725 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.727 I llm_load_print_meta: n_ff             = 8192
0.00.053.727 I llm_load_print_meta: n_expert         = 0
0.00.053.727 I llm_load_print_meta: n_expert_used    = 0
0.00.053.727 I llm_load_print_meta: causal attn      = 1
0.00.053.727 I llm_load_print_meta: pooling type     = 0
0.00.053.727 I llm_load_print_meta: rope type        = 2
0.00.053.727 I llm_load_print_meta: rope scaling     = linear
0.00.053.728 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.728 I llm_load_print_meta: freq_scale_train = 1
0.00.053.728 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.729 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.729 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.729 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.729 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.730 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.735 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.735 I llm_load_print_meta: model type       = 1.4B
0.00.053.736 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.053.736 I llm_load_print_meta: model params     = 1.41 B
0.00.053.738 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.738 I llm_load_print_meta: general.name     = 1.4B
0.00.053.738 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.738 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.742 I llm_load_print_meta: LF token         = 128 ''
0.00.053.742 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.742 I llm_load_print_meta: max token length = 1024
0.00.055.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.680 I llm_load_tensors: offloading output layer to GPU
0.00.055.680 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.691 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.692 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.599 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.600 I llama_new_context_with_model: n_ctx         = 128
0.00.056.600 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.600 I llama_new_context_with_model: n_batch       = 128
0.00.056.600 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.600 I llama_new_context_with_model: flash_attn    = 0
0.00.056.601 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.601 I llama_new_context_with_model: freq_scale    = 1
0.00.056.601 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.602 I ggml_metal_init: allocating
0.00.056.605 I ggml_metal_init: found device: Apple M4
0.00.056.607 I ggml_metal_init: picking default device: Apple M4
0.00.057.213 I ggml_metal_init: using embedded metal library
0.00.059.651 I ggml_metal_init: GPU name:   Apple M4
0.00.059.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.653 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.654 I ggml_metal_init: simdgroup reduction   = true
0.00.059.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.654 I ggml_metal_init: has bfloat            = true
0.00.059.654 I ggml_metal_init: use bfloat            = true
0.00.059.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.959 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.962 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.977 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.941 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.942 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.942 I llama_new_context_with_model: graph nodes  = 967
0.00.071.943 I llama_new_context_with_model: graph splits = 2
0.00.071.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.389.996 I 
0.00.390.082 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.390.108 I perplexity: tokenizing the input ..
0.00.397.837 I perplexity: tokenization took 7.727 ms
0.00.397.840 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.530.558 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.531.918 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.531.940 I llama_perf_context_print:        load time =     378.32 ms
0.00.531.941 I llama_perf_context_print: prompt eval time =     132.48 ms /   128 tokens (    1.04 ms per token,   966.17 tokens per second)
0.00.531.942 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.531.943 I llama_perf_context_print:       total time =     141.95 ms /   129 tokens
0.00.532.480 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.079s
sys	0m0.071s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.217 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.647 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.656 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.656 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.656 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.338 I llama_model_loader: - type  f32:  194 tensors
0.00.023.339 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.339 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.339 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.339 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.846 I llm_load_vocab: special tokens cache size = 25
0.00.049.707 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.710 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.710 I llm_load_print_meta: arch             = gptneox
0.00.049.710 I llm_load_print_meta: vocab type       = BPE
0.00.049.711 I llm_load_print_meta: n_vocab          = 50304
0.00.049.711 I llm_load_print_meta: n_merges         = 50009
0.00.049.711 I llm_load_print_meta: vocab_only       = 0
0.00.049.711 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.711 I llm_load_print_meta: n_embd           = 2048
0.00.049.711 I llm_load_print_meta: n_layer          = 24
0.00.049.726 I llm_load_print_meta: n_head           = 16
0.00.049.726 I llm_load_print_meta: n_head_kv        = 16
0.00.049.727 I llm_load_print_meta: n_rot            = 32
0.00.049.727 I llm_load_print_meta: n_swa            = 0
0.00.049.727 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.728 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.729 I llm_load_print_meta: n_gqa            = 1
0.00.049.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.730 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.731 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.733 I llm_load_print_meta: n_ff             = 8192
0.00.049.733 I llm_load_print_meta: n_expert         = 0
0.00.049.733 I llm_load_print_meta: n_expert_used    = 0
0.00.049.733 I llm_load_print_meta: causal attn      = 1
0.00.049.734 I llm_load_print_meta: pooling type     = 0
0.00.049.734 I llm_load_print_meta: rope type        = 2
0.00.049.734 I llm_load_print_meta: rope scaling     = linear
0.00.049.734 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.734 I llm_load_print_meta: freq_scale_train = 1
0.00.049.735 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.735 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.735 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.735 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.735 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.736 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.737 I llm_load_print_meta: model type       = 1.4B
0.00.049.737 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.737 I llm_load_print_meta: model params     = 1.41 B
0.00.049.738 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.738 I llm_load_print_meta: general.name     = 1.4B
0.00.049.738 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.738 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: LF token         = 128 ''
0.00.049.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: max token length = 1024
0.00.051.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.679 I llm_load_tensors: offloading output layer to GPU
0.00.051.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.690 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.691 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.637 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.638 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.638 I llama_new_context_with_model: n_batch       = 2048
0.00.052.638 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.638 I llama_new_context_with_model: flash_attn    = 0
0.00.052.639 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.639 I llama_new_context_with_model: freq_scale    = 1
0.00.052.639 I ggml_metal_init: allocating
0.00.052.645 I ggml_metal_init: found device: Apple M4
0.00.052.647 I ggml_metal_init: picking default device: Apple M4
0.00.053.224 I ggml_metal_init: using embedded metal library
0.00.055.546 I ggml_metal_init: GPU name:   Apple M4
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.548 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.548 I ggml_metal_init: simdgroup reduction   = true
0.00.055.548 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.548 I ggml_metal_init: has bfloat            = true
0.00.055.548 I ggml_metal_init: use bfloat            = true
0.00.055.549 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.549 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.584 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.589 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.540 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.542 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.542 I llama_new_context_with_model: graph nodes  = 967
0.00.084.542 I llama_new_context_with_model: graph splits = 2
0.00.084.567 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.737 I main: llama threadpool init, n_threads = 4
0.00.539.782 I 
0.00.539.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.843 I 
0.00.540.074 I sampler seed: 1234
0.00.540.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.114 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.115 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.115 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.286.146 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.286.147 I llama_perf_context_print:        load time =     530.51 ms
0.01.286.148 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.87 tokens per second)
0.01.286.148 I llama_perf_context_print:        eval time =     702.38 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.286.149 I llama_perf_context_print:       total time =     746.42 ms /    70 tokens
0.01.286.348 I ggml_metal_free: deallocating

real	0m1.303s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.100 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.638 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.647 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.649 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.651 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.478 I llama_model_loader: - type  f32:  194 tensors
0.00.023.478 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.478 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.479 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.486 I llm_load_vocab: special tokens cache size = 25
0.00.050.241 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.244 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.244 I llm_load_print_meta: arch             = gptneox
0.00.050.245 I llm_load_print_meta: vocab type       = BPE
0.00.050.245 I llm_load_print_meta: n_vocab          = 50304
0.00.050.245 I llm_load_print_meta: n_merges         = 50009
0.00.050.245 I llm_load_print_meta: vocab_only       = 0
0.00.050.246 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.246 I llm_load_print_meta: n_embd           = 2048
0.00.050.246 I llm_load_print_meta: n_layer          = 24
0.00.050.260 I llm_load_print_meta: n_head           = 16
0.00.050.261 I llm_load_print_meta: n_head_kv        = 16
0.00.050.261 I llm_load_print_meta: n_rot            = 32
0.00.050.261 I llm_load_print_meta: n_swa            = 0
0.00.050.261 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.262 I llm_load_print_meta: n_gqa            = 1
0.00.050.263 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.264 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.264 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.265 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.265 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.265 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.265 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.266 I llm_load_print_meta: n_ff             = 8192
0.00.050.266 I llm_load_print_meta: n_expert         = 0
0.00.050.266 I llm_load_print_meta: n_expert_used    = 0
0.00.050.267 I llm_load_print_meta: causal attn      = 1
0.00.050.267 I llm_load_print_meta: pooling type     = 0
0.00.050.267 I llm_load_print_meta: rope type        = 2
0.00.050.267 I llm_load_print_meta: rope scaling     = linear
0.00.050.269 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.269 I llm_load_print_meta: freq_scale_train = 1
0.00.050.270 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.270 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.270 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.270 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.270 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.270 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.271 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.271 I llm_load_print_meta: model type       = 1.4B
0.00.050.271 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.271 I llm_load_print_meta: model params     = 1.41 B
0.00.050.272 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.272 I llm_load_print_meta: general.name     = 1.4B
0.00.050.272 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.272 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.272 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.273 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.273 I llm_load_print_meta: LF token         = 128 ''
0.00.050.273 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.273 I llm_load_print_meta: max token length = 1024
0.00.052.254 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.254 I llm_load_tensors: offloading output layer to GPU
0.00.052.255 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.266 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.267 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.169 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.170 I llama_new_context_with_model: n_ctx         = 128
0.00.053.170 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.170 I llama_new_context_with_model: n_batch       = 128
0.00.053.170 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.170 I llama_new_context_with_model: flash_attn    = 0
0.00.053.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.171 I llama_new_context_with_model: freq_scale    = 1
0.00.053.171 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.172 I ggml_metal_init: allocating
0.00.053.177 I ggml_metal_init: found device: Apple M4
0.00.053.179 I ggml_metal_init: picking default device: Apple M4
0.00.053.751 I ggml_metal_init: using embedded metal library
0.00.056.085 I ggml_metal_init: GPU name:   Apple M4
0.00.056.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.087 I ggml_metal_init: simdgroup reduction   = true
0.00.056.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.087 I ggml_metal_init: has bfloat            = true
0.00.056.087 I ggml_metal_init: use bfloat            = true
0.00.056.088 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.645 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.648 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.663 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.614 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.615 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.615 I llama_new_context_with_model: graph nodes  = 967
0.00.067.615 I llama_new_context_with_model: graph splits = 2
0.00.067.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.629 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.225 I 
0.00.486.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.309 I perplexity: tokenizing the input ..
0.00.494.033 I perplexity: tokenization took 7.723 ms
0.00.494.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.228 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.627.434 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.627.447 I llama_perf_context_print:        load time =     477.12 ms
0.00.627.448 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   969.96 tokens per second)
0.00.627.449 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.627.449 I llama_perf_context_print:       total time =     141.23 ms /   129 tokens
0.00.627.967 I ggml_metal_free: deallocating

real	0m0.641s
user	0m0.078s
sys	0m0.091s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.343 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.533 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.533 I llama_model_loader: - type  f32:  194 tensors
0.00.024.533 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.534 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.534 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.995 I llm_load_vocab: special tokens cache size = 25
0.00.050.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.872 I llm_load_print_meta: arch             = gptneox
0.00.050.873 I llm_load_print_meta: vocab type       = BPE
0.00.050.873 I llm_load_print_meta: n_vocab          = 50304
0.00.050.873 I llm_load_print_meta: n_merges         = 50009
0.00.050.873 I llm_load_print_meta: vocab_only       = 0
0.00.050.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.874 I llm_load_print_meta: n_embd           = 2048
0.00.050.874 I llm_load_print_meta: n_layer          = 24
0.00.050.888 I llm_load_print_meta: n_head           = 16
0.00.050.890 I llm_load_print_meta: n_head_kv        = 16
0.00.050.890 I llm_load_print_meta: n_rot            = 32
0.00.050.890 I llm_load_print_meta: n_swa            = 0
0.00.050.890 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.890 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.891 I llm_load_print_meta: n_gqa            = 1
0.00.050.892 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.893 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.893 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.894 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.894 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.894 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.894 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.896 I llm_load_print_meta: n_ff             = 8192
0.00.050.896 I llm_load_print_meta: n_expert         = 0
0.00.050.896 I llm_load_print_meta: n_expert_used    = 0
0.00.050.897 I llm_load_print_meta: causal attn      = 1
0.00.050.897 I llm_load_print_meta: pooling type     = 0
0.00.050.898 I llm_load_print_meta: rope type        = 2
0.00.050.898 I llm_load_print_meta: rope scaling     = linear
0.00.050.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.899 I llm_load_print_meta: freq_scale_train = 1
0.00.050.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.899 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.900 I llm_load_print_meta: model type       = 1.4B
0.00.050.900 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.901 I llm_load_print_meta: model params     = 1.41 B
0.00.050.901 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.901 I llm_load_print_meta: general.name     = 1.4B
0.00.050.902 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: LF token         = 128 ''
0.00.050.903 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.903 I llm_load_print_meta: max token length = 1024
0.00.052.825 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.825 I llm_load_tensors: offloading output layer to GPU
0.00.052.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.836 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.837 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.777 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.778 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.778 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.779 I llama_new_context_with_model: n_batch       = 2048
0.00.053.779 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.779 I llama_new_context_with_model: flash_attn    = 0
0.00.053.779 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.780 I llama_new_context_with_model: freq_scale    = 1
0.00.053.780 I ggml_metal_init: allocating
0.00.053.783 I ggml_metal_init: found device: Apple M4
0.00.053.785 I ggml_metal_init: picking default device: Apple M4
0.00.054.360 I ggml_metal_init: using embedded metal library
0.00.056.673 I ggml_metal_init: GPU name:   Apple M4
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.676 I ggml_metal_init: simdgroup reduction   = true
0.00.056.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.676 I ggml_metal_init: has bfloat            = true
0.00.056.676 I ggml_metal_init: use bfloat            = true
0.00.056.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.163 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.171 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.190 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.153 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.153 I llama_new_context_with_model: graph nodes  = 967
0.00.086.153 I llama_new_context_with_model: graph splits = 2
0.00.086.178 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.322 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.676 I main: llama threadpool init, n_threads = 4
0.00.642.716 I 
0.00.642.750 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.751 I 
0.00.642.967 I sampler seed: 1234
0.00.642.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.643.026 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.643.030 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.643.031 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.399.579 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.399.580 I llama_perf_context_print:        load time =     633.33 ms
0.01.399.581 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.39 tokens per second)
0.01.399.582 I llama_perf_context_print:        eval time =     706.21 ms /    63 runs   (   11.21 ms per token,    89.21 tokens per second)
0.01.399.582 I llama_perf_context_print:       total time =     756.91 ms /    70 tokens
0.01.399.766 I ggml_metal_free: deallocating

real	0m1.417s
user	0m0.108s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.287 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.289 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.290 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.270 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.199 I llama_model_loader: - type  f32:  194 tensors
0.00.025.199 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.200 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.200 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.461 I llm_load_vocab: special tokens cache size = 25
0.00.051.395 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.398 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.398 I llm_load_print_meta: arch             = gptneox
0.00.051.398 I llm_load_print_meta: vocab type       = BPE
0.00.051.398 I llm_load_print_meta: n_vocab          = 50304
0.00.051.399 I llm_load_print_meta: n_merges         = 50009
0.00.051.399 I llm_load_print_meta: vocab_only       = 0
0.00.051.399 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.399 I llm_load_print_meta: n_embd           = 2048
0.00.051.399 I llm_load_print_meta: n_layer          = 24
0.00.051.413 I llm_load_print_meta: n_head           = 16
0.00.051.414 I llm_load_print_meta: n_head_kv        = 16
0.00.051.414 I llm_load_print_meta: n_rot            = 32
0.00.051.414 I llm_load_print_meta: n_swa            = 0
0.00.051.415 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.415 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.418 I llm_load_print_meta: n_gqa            = 1
0.00.051.419 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.420 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.423 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.424 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.424 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.424 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.424 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.425 I llm_load_print_meta: n_ff             = 8192
0.00.051.425 I llm_load_print_meta: n_expert         = 0
0.00.051.425 I llm_load_print_meta: n_expert_used    = 0
0.00.051.425 I llm_load_print_meta: causal attn      = 1
0.00.051.425 I llm_load_print_meta: pooling type     = 0
0.00.051.425 I llm_load_print_meta: rope type        = 2
0.00.051.426 I llm_load_print_meta: rope scaling     = linear
0.00.051.426 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.426 I llm_load_print_meta: freq_scale_train = 1
0.00.051.426 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.426 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.427 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.427 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.427 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.427 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.427 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.427 I llm_load_print_meta: model type       = 1.4B
0.00.051.428 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.428 I llm_load_print_meta: model params     = 1.41 B
0.00.051.428 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.429 I llm_load_print_meta: general.name     = 1.4B
0.00.051.429 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.431 I llm_load_print_meta: LF token         = 128 ''
0.00.051.431 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.431 I llm_load_print_meta: max token length = 1024
0.00.053.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.440 I llm_load_tensors: offloading output layer to GPU
0.00.053.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.451 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.452 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.397 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.398 I llama_new_context_with_model: n_ctx         = 128
0.00.054.398 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.399 I llama_new_context_with_model: n_batch       = 128
0.00.054.399 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.399 I llama_new_context_with_model: flash_attn    = 0
0.00.054.399 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.400 I llama_new_context_with_model: freq_scale    = 1
0.00.054.400 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.401 I ggml_metal_init: allocating
0.00.054.407 I ggml_metal_init: found device: Apple M4
0.00.054.410 I ggml_metal_init: picking default device: Apple M4
0.00.055.003 I ggml_metal_init: using embedded metal library
0.00.057.377 I ggml_metal_init: GPU name:   Apple M4
0.00.057.378 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.379 I ggml_metal_init: simdgroup reduction   = true
0.00.057.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.379 I ggml_metal_init: has bfloat            = true
0.00.057.380 I ggml_metal_init: use bfloat            = true
0.00.057.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.060 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.073 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.046 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.047 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.047 I llama_new_context_with_model: graph nodes  = 967
0.00.069.047 I llama_new_context_with_model: graph splits = 2
0.00.069.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.827 I 
0.00.587.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.887 I perplexity: tokenizing the input ..
0.00.596.124 I perplexity: tokenization took 8.235 ms
0.00.596.127 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.627 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.803 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.821 I llama_perf_context_print:        load time =     577.07 ms
0.00.731.821 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.33 tokens per second)
0.00.731.822 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.823 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.732.379 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.078s
sys	0m0.125s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.712 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.948 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.952 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.960 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.862 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.766 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.767 I llama_model_loader: - type  f32:  194 tensors
0.00.024.768 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.768 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.437 I llm_load_vocab: special tokens cache size = 25
0.00.051.423 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.426 I llm_load_print_meta: arch             = gptneox
0.00.051.427 I llm_load_print_meta: vocab type       = BPE
0.00.051.427 I llm_load_print_meta: n_vocab          = 50304
0.00.051.427 I llm_load_print_meta: n_merges         = 50009
0.00.051.427 I llm_load_print_meta: vocab_only       = 0
0.00.051.427 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.428 I llm_load_print_meta: n_embd           = 2048
0.00.051.428 I llm_load_print_meta: n_layer          = 24
0.00.051.443 I llm_load_print_meta: n_head           = 16
0.00.051.444 I llm_load_print_meta: n_head_kv        = 16
0.00.051.444 I llm_load_print_meta: n_rot            = 32
0.00.051.444 I llm_load_print_meta: n_swa            = 0
0.00.051.444 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.445 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.445 I llm_load_print_meta: n_gqa            = 1
0.00.051.446 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.447 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.448 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.448 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.449 I llm_load_print_meta: n_ff             = 8192
0.00.051.449 I llm_load_print_meta: n_expert         = 0
0.00.051.449 I llm_load_print_meta: n_expert_used    = 0
0.00.051.451 I llm_load_print_meta: causal attn      = 1
0.00.051.452 I llm_load_print_meta: pooling type     = 0
0.00.051.452 I llm_load_print_meta: rope type        = 2
0.00.051.452 I llm_load_print_meta: rope scaling     = linear
0.00.051.453 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.453 I llm_load_print_meta: freq_scale_train = 1
0.00.051.453 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.453 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.453 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.454 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.454 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.454 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.454 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.454 I llm_load_print_meta: model type       = 1.4B
0.00.051.454 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.456 I llm_load_print_meta: model params     = 1.41 B
0.00.051.456 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.457 I llm_load_print_meta: general.name     = 1.4B
0.00.051.457 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.457 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.457 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.457 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.457 I llm_load_print_meta: LF token         = 128 ''
0.00.051.458 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.458 I llm_load_print_meta: max token length = 1024
0.00.053.525 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.525 I llm_load_tensors: offloading output layer to GPU
0.00.053.526 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.536 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.537 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.510 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.510 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.511 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.511 I llama_new_context_with_model: n_batch       = 2048
0.00.054.511 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.511 I llama_new_context_with_model: flash_attn    = 0
0.00.054.511 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.512 I llama_new_context_with_model: freq_scale    = 1
0.00.054.512 I ggml_metal_init: allocating
0.00.054.515 I ggml_metal_init: found device: Apple M4
0.00.054.517 I ggml_metal_init: picking default device: Apple M4
0.00.055.103 I ggml_metal_init: using embedded metal library
0.00.057.401 I ggml_metal_init: GPU name:   Apple M4
0.00.057.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.404 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.404 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.405 I ggml_metal_init: simdgroup reduction   = true
0.00.057.405 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.405 I ggml_metal_init: has bfloat            = true
0.00.057.405 I ggml_metal_init: use bfloat            = true
0.00.057.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.151 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.156 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.173 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.186 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.187 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.188 I llama_new_context_with_model: graph nodes  = 967
0.00.087.188 I llama_new_context_with_model: graph splits = 2
0.00.087.213 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.359 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.890 I main: llama threadpool init, n_threads = 4
0.00.689.924 I 
0.00.689.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.969 I 
0.00.690.193 I sampler seed: 1234
0.00.690.200 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.215 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.217 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.217 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.540.115 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.540.115 I llama_perf_context_print:        load time =     681.17 ms
0.01.540.116 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.76 tokens per second)
0.01.540.117 I llama_perf_context_print:        eval time =     795.35 ms /    63 runs   (   12.62 ms per token,    79.21 tokens per second)
0.01.540.118 I llama_perf_context_print:       total time =     850.23 ms /    70 tokens
0.01.540.316 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.639 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.646 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.444 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.444 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.445 I llama_model_loader: - type  f32:  194 tensors
0.00.023.445 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.446 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.622 I llm_load_vocab: special tokens cache size = 25
0.00.050.606 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.609 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.609 I llm_load_print_meta: arch             = gptneox
0.00.050.609 I llm_load_print_meta: vocab type       = BPE
0.00.050.609 I llm_load_print_meta: n_vocab          = 50304
0.00.050.610 I llm_load_print_meta: n_merges         = 50009
0.00.050.610 I llm_load_print_meta: vocab_only       = 0
0.00.050.610 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.610 I llm_load_print_meta: n_embd           = 2048
0.00.050.610 I llm_load_print_meta: n_layer          = 24
0.00.050.626 I llm_load_print_meta: n_head           = 16
0.00.050.626 I llm_load_print_meta: n_head_kv        = 16
0.00.050.626 I llm_load_print_meta: n_rot            = 32
0.00.050.627 I llm_load_print_meta: n_swa            = 0
0.00.050.627 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.627 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.630 I llm_load_print_meta: n_gqa            = 1
0.00.050.631 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.632 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.632 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.633 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.633 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.633 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.633 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.634 I llm_load_print_meta: n_ff             = 8192
0.00.050.634 I llm_load_print_meta: n_expert         = 0
0.00.050.634 I llm_load_print_meta: n_expert_used    = 0
0.00.050.634 I llm_load_print_meta: causal attn      = 1
0.00.050.635 I llm_load_print_meta: pooling type     = 0
0.00.050.635 I llm_load_print_meta: rope type        = 2
0.00.050.635 I llm_load_print_meta: rope scaling     = linear
0.00.050.636 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.637 I llm_load_print_meta: freq_scale_train = 1
0.00.050.637 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.637 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.637 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.637 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.637 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.637 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.638 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.638 I llm_load_print_meta: model type       = 1.4B
0.00.050.639 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.639 I llm_load_print_meta: model params     = 1.41 B
0.00.050.640 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.640 I llm_load_print_meta: general.name     = 1.4B
0.00.050.640 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.641 I llm_load_print_meta: LF token         = 128 ''
0.00.050.641 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.641 I llm_load_print_meta: max token length = 1024
0.00.052.669 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.669 I llm_load_tensors: offloading output layer to GPU
0.00.052.670 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.680 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.681 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.631 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.631 I llama_new_context_with_model: n_ctx         = 128
0.00.053.632 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.632 I llama_new_context_with_model: n_batch       = 128
0.00.053.632 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.632 I llama_new_context_with_model: flash_attn    = 0
0.00.053.632 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.633 I llama_new_context_with_model: freq_scale    = 1
0.00.053.633 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.634 I ggml_metal_init: allocating
0.00.053.637 I ggml_metal_init: found device: Apple M4
0.00.053.639 I ggml_metal_init: picking default device: Apple M4
0.00.054.214 I ggml_metal_init: using embedded metal library
0.00.056.565 I ggml_metal_init: GPU name:   Apple M4
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.567 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.567 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.567 I ggml_metal_init: simdgroup reduction   = true
0.00.056.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.568 I ggml_metal_init: has bfloat            = true
0.00.056.568 I ggml_metal_init: use bfloat            = true
0.00.056.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.799 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.803 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.816 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.738 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.739 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.739 I llama_new_context_with_model: graph nodes  = 967
0.00.068.740 I llama_new_context_with_model: graph splits = 2
0.00.068.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.999 I 
0.00.647.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.043 I perplexity: tokenizing the input ..
0.00.654.863 I perplexity: tokenization took 7.819 ms
0.00.654.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.871 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.083 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.097 I llama_perf_context_print:        load time =     638.18 ms
0.00.797.098 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.28 tokens per second)
0.00.797.099 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.100 I llama_perf_context_print:       total time =     150.10 ms /   129 tokens
0.00.797.520 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.079s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.683 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.415 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.223 I llama_model_loader: - type  f32:  194 tensors
0.00.025.223 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.747 I llm_load_vocab: special tokens cache size = 25
0.00.051.766 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.769 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.769 I llm_load_print_meta: arch             = gptneox
0.00.051.770 I llm_load_print_meta: vocab type       = BPE
0.00.051.770 I llm_load_print_meta: n_vocab          = 50304
0.00.051.770 I llm_load_print_meta: n_merges         = 50009
0.00.051.770 I llm_load_print_meta: vocab_only       = 0
0.00.051.771 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.771 I llm_load_print_meta: n_embd           = 2048
0.00.051.771 I llm_load_print_meta: n_layer          = 24
0.00.051.785 I llm_load_print_meta: n_head           = 16
0.00.051.786 I llm_load_print_meta: n_head_kv        = 16
0.00.051.786 I llm_load_print_meta: n_rot            = 32
0.00.051.786 I llm_load_print_meta: n_swa            = 0
0.00.051.787 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.787 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.787 I llm_load_print_meta: n_gqa            = 1
0.00.051.788 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.789 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.789 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.790 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.790 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.790 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.791 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.792 I llm_load_print_meta: n_ff             = 8192
0.00.051.792 I llm_load_print_meta: n_expert         = 0
0.00.051.792 I llm_load_print_meta: n_expert_used    = 0
0.00.051.792 I llm_load_print_meta: causal attn      = 1
0.00.051.792 I llm_load_print_meta: pooling type     = 0
0.00.051.792 I llm_load_print_meta: rope type        = 2
0.00.051.794 I llm_load_print_meta: rope scaling     = linear
0.00.051.794 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.794 I llm_load_print_meta: freq_scale_train = 1
0.00.051.795 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.795 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.795 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.795 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.795 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.795 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.795 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.795 I llm_load_print_meta: model type       = 1.4B
0.00.051.796 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.797 I llm_load_print_meta: model params     = 1.41 B
0.00.051.797 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.797 I llm_load_print_meta: general.name     = 1.4B
0.00.051.798 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.798 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.798 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.798 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.798 I llm_load_print_meta: LF token         = 128 ''
0.00.051.799 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.799 I llm_load_print_meta: max token length = 1024
0.00.053.899 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.899 I llm_load_tensors: offloading output layer to GPU
0.00.053.899 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.909 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.910 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.888 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.889 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.890 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.890 I llama_new_context_with_model: n_batch       = 2048
0.00.054.890 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.890 I llama_new_context_with_model: flash_attn    = 0
0.00.054.891 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.891 I llama_new_context_with_model: freq_scale    = 1
0.00.054.891 I ggml_metal_init: allocating
0.00.054.899 I ggml_metal_init: found device: Apple M4
0.00.054.902 I ggml_metal_init: picking default device: Apple M4
0.00.055.529 I ggml_metal_init: using embedded metal library
0.00.057.922 I ggml_metal_init: GPU name:   Apple M4
0.00.057.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.924 I ggml_metal_init: simdgroup reduction   = true
0.00.057.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.925 I ggml_metal_init: has bfloat            = true
0.00.057.925 I ggml_metal_init: use bfloat            = true
0.00.057.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.246 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.346 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.347 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.347 I llama_new_context_with_model: graph nodes  = 967
0.00.089.348 I llama_new_context_with_model: graph splits = 2
0.00.089.373 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.382 I main: llama threadpool init, n_threads = 4
0.00.751.429 I 
0.00.751.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.461 I 
0.00.751.682 I sampler seed: 1234
0.00.751.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.741 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.743 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.743 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.635.801 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48365.12 tokens per second)
0.01.635.802 I llama_perf_context_print:        load time =     741.69 ms
0.01.635.802 I llama_perf_context_print: prompt eval time =      54.19 ms /     7 tokens (    7.74 ms per token,   129.18 tokens per second)
0.01.635.804 I llama_perf_context_print:        eval time =     827.29 ms /    63 runs   (   13.13 ms per token,    76.15 tokens per second)
0.01.635.804 I llama_perf_context_print:       total time =     884.42 ms /    70 tokens
0.01.636.032 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4350 (d62b532c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.981 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.612 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.615 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.236 I llama_model_loader: - type  f32:  194 tensors
0.00.024.236 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.407 I llm_load_vocab: special tokens cache size = 25
0.00.050.483 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.485 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.486 I llm_load_print_meta: arch             = gptneox
0.00.050.486 I llm_load_print_meta: vocab type       = BPE
0.00.050.486 I llm_load_print_meta: n_vocab          = 50304
0.00.050.487 I llm_load_print_meta: n_merges         = 50009
0.00.050.487 I llm_load_print_meta: vocab_only       = 0
0.00.050.487 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.487 I llm_load_print_meta: n_embd           = 2048
0.00.050.487 I llm_load_print_meta: n_layer          = 24
0.00.050.502 I llm_load_print_meta: n_head           = 16
0.00.050.503 I llm_load_print_meta: n_head_kv        = 16
0.00.050.503 I llm_load_print_meta: n_rot            = 32
0.00.050.503 I llm_load_print_meta: n_swa            = 0
0.00.050.503 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.504 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.507 I llm_load_print_meta: n_gqa            = 1
0.00.050.508 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.508 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.509 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.509 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.509 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.510 I llm_load_print_meta: n_ff             = 8192
0.00.050.510 I llm_load_print_meta: n_expert         = 0
0.00.050.510 I llm_load_print_meta: n_expert_used    = 0
0.00.050.511 I llm_load_print_meta: causal attn      = 1
0.00.050.511 I llm_load_print_meta: pooling type     = 0
0.00.050.511 I llm_load_print_meta: rope type        = 2
0.00.050.511 I llm_load_print_meta: rope scaling     = linear
0.00.050.511 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.511 I llm_load_print_meta: freq_scale_train = 1
0.00.050.512 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.512 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.512 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.512 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.515 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.516 I llm_load_print_meta: model type       = 1.4B
0.00.050.516 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.517 I llm_load_print_meta: model params     = 1.41 B
0.00.050.517 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.517 I llm_load_print_meta: general.name     = 1.4B
0.00.050.517 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.517 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: LF token         = 128 ''
0.00.050.519 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: max token length = 1024
0.00.052.549 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.550 I llm_load_tensors: offloading output layer to GPU
0.00.052.550 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.560 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.561 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.439 I llama_new_context_with_model: n_ctx         = 128
0.00.053.439 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.439 I llama_new_context_with_model: n_batch       = 128
0.00.053.440 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.440 I llama_new_context_with_model: flash_attn    = 0
0.00.053.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.441 I llama_new_context_with_model: freq_scale    = 1
0.00.053.441 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.441 I ggml_metal_init: allocating
0.00.053.444 I ggml_metal_init: found device: Apple M4
0.00.053.446 I ggml_metal_init: picking default device: Apple M4
0.00.053.993 I ggml_metal_init: using embedded metal library
0.00.056.310 I ggml_metal_init: GPU name:   Apple M4
0.00.056.311 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.312 I ggml_metal_init: simdgroup reduction   = true
0.00.056.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.313 I ggml_metal_init: has bfloat            = true
0.00.056.313 I ggml_metal_init: use bfloat            = true
0.00.056.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.107 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.109 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.066 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.067 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.067 I llama_new_context_with_model: graph nodes  = 967
0.00.068.067 I llama_new_context_with_model: graph splits = 2
0.00.068.080 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.353.234 I 
0.00.353.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.353.286 I perplexity: tokenizing the input ..
0.00.361.544 I perplexity: tokenization took 8.256 ms
0.00.361.552 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.501.068 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.502.333 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.502.350 I llama_perf_context_print:        load time =     343.25 ms
0.00.502.351 I llama_perf_context_print: prompt eval time =     139.29 ms /   128 tokens (    1.09 ms per token,   918.94 tokens per second)
0.00.502.352 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.502.353 I llama_perf_context_print:       total time =     149.12 ms /   129 tokens
0.00.502.754 I ggml_metal_free: deallocating

real	0m0.517s
user	0m0.077s
sys	0m0.072s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4350 (d62b532c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148e0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148e0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148e0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148e0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148e0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148e0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148e0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148e0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148e0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148e0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148e0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148e0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148e0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148e0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148e0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148e101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148e10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148e11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148e11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148e11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148e12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148e12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148e13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148e13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148e14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148e14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148e14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148e15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148e16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148e16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148e168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148e17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148e176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148e17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148e17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148e182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148e18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148e18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148e19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148e19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148e199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148e19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148e1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148e1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148e1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148e1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148e1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148e1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148e1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148e1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148e1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148e1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148e1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148e1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148e20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148e20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148e208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148e20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148e21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148e216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148e21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148e21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148e22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148e22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148e23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148e23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148e23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148e240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148e24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148e250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148e25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148e25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148e260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148e26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148e26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148e270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148e27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148e27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148e280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148e28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148e28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148e290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148e295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148e29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148e2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148e2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148e2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148e2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148e2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148e1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148e2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148e2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148e2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148e2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148e2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148e2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148e2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148e2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148e2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148e2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148e2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148e301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148e310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148e31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148e31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148e31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148e32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148e32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148e33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148e335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148e33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148e33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148e343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148e34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148e351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148e35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148e35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148e35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148e36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148e37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148e376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148e37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148e37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148e38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148e38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148e38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148e39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148e3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148e3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148e3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148e3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148e3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148e3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148e3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148e3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148e3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148e3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.834 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x139f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x139f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x139f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x139f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x139f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x139f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x139f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x139f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x139f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x139f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x139f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x139f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x139f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x139f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x139f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x139f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x139f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x139f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108705560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108705820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108706100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1087063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108706830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108706ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108707110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108707580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1087079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108707e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108704230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1087046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108704b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1087082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13dd04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13dd046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13dd04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13dd04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13dd053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13dd05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13dd05cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13dd06140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13dd065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13dd06a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13dd06e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13dd07300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108708740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108708bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108709020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108709490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13dd075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13dd07aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13dd07f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13dd08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13dd087f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13dd08c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13dd090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13dd09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13dd099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13dd09e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13dd0a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13dd0a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13dd0ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13dd0afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13dd0b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13dd0b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13dd0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13dd0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13dd0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13dd0ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13dd0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13dd0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13dd0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13dd0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13dd0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13dd0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13dd0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13dd0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13dd0f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13dd0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13dd0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108709900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108709e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10870a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10870a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10870ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10870aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10870b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10870b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10870bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10870c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10870c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10870ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10870cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10870d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10870d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10870dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10870e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10870e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10870e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10870ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10870f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10870f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10870fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10870ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1087108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108710d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108711600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108711a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1087127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1087130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1087140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108714390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108714650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108714ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108714f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1087153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108715c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1087160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1087169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1087172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108717720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108718000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108718470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1087188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108718d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1087191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108719630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108719aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108719f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10871a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10871a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10871ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10871b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10871b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10871b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10871be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10871c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10871c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10871cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10871cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10871d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10871d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10871dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10871e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10871e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10871ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10871eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10871f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10871f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10871fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1087200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108720520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108720990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108720e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108721270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1087216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108721b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108721fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108722430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1087228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108722d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108723180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1087235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108723a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108723ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108724340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1087247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108724c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108725090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108725500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108725970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108725de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108726250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1087266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108726b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108726fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108727880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108727cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108728760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1087295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108729cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108729f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10872a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10872a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10872b000 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1087063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108706820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108706c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108707100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108707570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1087079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108707e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1087082c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108708730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108708ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108709010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1087095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108709ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10870a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10870ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10870b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10870bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10870c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10870ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10870d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10870da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10870e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10870e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10870ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10870f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10870faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10870ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1087107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1087110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1087119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108711c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1087120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108712550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1087129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108712e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1087132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108713710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108713b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108713ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108714460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1087148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108714d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1087151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108715620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108715a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108715f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108716370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1087167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108716c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1087170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1087179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108718280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1087186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108718b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108718fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108719440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1087198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108719d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10871a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10871a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10871aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10871aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10871b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10871b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10871bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10871c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10871c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10871c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10871cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10871d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10871d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10871db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10871dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10871e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10871e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10871ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10871f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10871f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10871fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10871fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108720330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1087207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108720c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108721080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1087214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108721960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108721dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108722240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1087226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108722b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108722f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108723870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108723ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108724150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1087245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108724a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108724ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108725310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108725780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108725bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108726060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1087264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108726940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108726db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108727220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108727690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108727b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108727f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1087283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108728850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108728cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1087295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108729a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108729e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10872a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10872a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10872abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10872b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108705710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108705b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108704230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1087046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108704b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10872b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10872b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10872bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10872c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10872c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10872ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10872d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10872d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10872dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10872e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10872e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10872ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10872f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10872f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10872fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1087300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1087305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108730ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108730fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1087316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108731b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108731e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108732340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108732d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108733270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108733780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108733c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1087341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1087346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108734bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1087350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1087355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108736000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108736510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108736a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108736f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108737440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108737950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108737e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108738370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108738880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108738d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1087392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1087397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108739cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10873a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10873a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10873abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10873b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10873b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10873bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10873c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10873c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10873ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10873cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10873d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10873d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10873de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10873e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10873e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10873edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10873f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10873fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108740000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108740550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108740810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108741a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108742230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1087426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108742990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108742fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1087435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108743da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108744240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1087446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108744b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108745330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108745880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108745dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108746320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108746870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108746dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108747310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108747860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108748300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1087492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108749840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108749d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10874a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10874a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10874ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10874b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10874b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10874bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10874c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10874c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10874cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10874d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10874d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10874dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10874e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10874e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10874ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10874f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10874f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10874fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108750280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1087507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108750d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108751270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1087517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108751d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108752260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1087527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108752d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108753250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1087537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108753cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108754790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108754ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108755230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108755780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108755cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108756770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108756cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108757210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108757760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108757cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108758150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1087585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108758a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108758f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1087593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108759870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108759d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10875a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10875a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10875aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10875af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10875b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10875b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10875bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10875c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10875c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10875ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10875d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10875dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10875e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10875e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10875ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10875f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10875f760 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.799s
user	0m0.296s
sys	0m0.302s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4350 (d62b532c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d70ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d70f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d70f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d70ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d710520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d710ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d711630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d711be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d7120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d7125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d712ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d713db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d7145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d715400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d716a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d717130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d717850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d7191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d719800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d71a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d71a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d71ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d71b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d71b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d71bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d71c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d71c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d71c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d71cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d71d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d71d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d71db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d71e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d71e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d71e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d71ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d71f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d71f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d71fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d720600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d720c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d721220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d721e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d722a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d723250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d7236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d723b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d723e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d724460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d724f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d7253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d725850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d725cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d726190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d726630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d726ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d726f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d7278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d727d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d7281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d728690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d728be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d729680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d729bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d72a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d72a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d72abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d72b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d72b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d72bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d72c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d72c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d72cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d72d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d72d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d72db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d72e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d72e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d72eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d72f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d72f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d72fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d7300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d730610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d7202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d730a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d731230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d731780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d731cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d732220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d732770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d733210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d733760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d733cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d734200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d734750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d734ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d7351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d735740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d735be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d736080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d7369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d736e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d7377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d737c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d7380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d738a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d738ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d739360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d739800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d739ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d73a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d73a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d73aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d73af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d73b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d73b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d73bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d73c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d73c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d73cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d73cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d73d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d73d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d73dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d73e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d73e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d73eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d73efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d73f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d73f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d73fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d740260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d740700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d741040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d7414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d741980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d741e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d7422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d742760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d742c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d7430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d743540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d7439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d743e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d744320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d7447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d744c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d745100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d7455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d745a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d745ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d746380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d746820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d746cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d747160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d747600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d747aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d747f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d7483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d748880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d748d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d7491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d749660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d749b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d749fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d74a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d74a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d74ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d74b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d74b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d74bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d74c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d74c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d74c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d74ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d74d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d74d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d74de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d74e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d74e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d74ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d74f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d74fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d750000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d7502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d7508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d750ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d7516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d751b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d752010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d7524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d752c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d7531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d753700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d753c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d7541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d7546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d754c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d755190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d7556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d755c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d756180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d7566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d756c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d7576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d757c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d758160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d7586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d758c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d759150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d7596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d759bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d75a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d75a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d75abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d75b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d75b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d75bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d75c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d75c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d75cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d75d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d75d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d75dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d75e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d75e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d75eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d75f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d75f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d75fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d7600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d760630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d760b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d7610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d761620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d7620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d762b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d7630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d763600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d763b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d7640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d7645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d764b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d7655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d765a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d765f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d7663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d766860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d766d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d7671a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d767640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d767ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d767f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d768420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d7688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d768d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d769200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d7696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d769b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d76a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d76a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d76aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d76b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d76bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d76bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d76c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d76ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d76d090 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e805320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e805790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e805c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e806070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e8064e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e806950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e806dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e807230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e8076a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e807c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e808070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e8086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e809210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e80a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e80a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e80b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e80b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e80be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e80c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e80cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e80d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e80db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e80e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e80e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e80ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e80ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e80f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e80f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e80fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e810100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e810630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e810aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e810d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e8111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e811640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e811ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e811f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e812390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e812800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e812c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e8130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e813550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e8139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e813e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e8142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e814710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e814b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e814ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e815460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e8158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e815d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e8161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e816620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e817470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e817970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e817de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e818250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e8186c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e818b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e818fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e819410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e819880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e819cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e81a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e81a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e81aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e81aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e81b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e81b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e81bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e81c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e81c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e81c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e81cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e81d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e81d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e81db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e81df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e81e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e81e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e81ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e81f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e81f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e81fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e81fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e820300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e820770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e820be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e821050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e8214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e821930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e821da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e822210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e822680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e822af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e822f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e8233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e823840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e823cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e824120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e824590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e824a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e824e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e8252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e825750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e825bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e826030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e8264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e826910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e826d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e8271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e827660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e827ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e827f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e8283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e828820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e828c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e829100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e829570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e8299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e829e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e82a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e82a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e82aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e82b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e82b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e82b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e82bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e82c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e82c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e82cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e82cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e82d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e82d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e82dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e82e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e82e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e82e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e82ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e82f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e82f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e82fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e82fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e830460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e8308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e830d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e8311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e831620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e831a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e831f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e832370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e8327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e832c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e8330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e833530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e8339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e833e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e834280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e8346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e834b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e834fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e835440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e8358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e835d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d605d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d6061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d606650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d606ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d606f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d6073a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d607810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d607c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d6080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d608560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d6089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d6092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d609720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d609b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d60a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d60a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d60ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d60b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d60b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d60baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d60bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d60c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d60c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d60cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d60d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d60d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d60dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d60df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d60ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d60ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d60f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d60f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d60f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d60fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d6101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d610640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d611390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d611800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d611c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d6120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d612550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d6129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d6132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d613710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d613b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d613ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d614460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d6148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d614d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d6151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d615620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d615a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d615f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d616370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d6167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d616c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d6170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d617530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d6179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d618280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d6186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d618b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d618fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d6198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d619d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d61a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d61a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d61aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d61aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d61b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d61b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d61bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d61c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d61c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d61c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d61cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d61d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d61d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d61db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d61dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d61e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d61e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d61ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d61f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d61f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d61fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d61fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d6207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d621080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d6214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d621960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d621dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d622240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d6226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d623120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d623840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d623f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d624db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d6253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d6259c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d6061d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d606640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d606ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d606f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d607390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d607800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d607c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d6080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d608550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d6089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d608e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d609410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d609d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d60a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d60ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d60b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d60ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d60c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d60c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d60d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d60d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d60df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d60e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d60ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d60f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d60f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d60fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d6101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d610610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d611360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d6117d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d611a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d611f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d612370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d6127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d612c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d6130c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d613530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d6139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d613e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d614280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d6146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d614b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d614fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d615440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d6158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d615d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d616190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d616a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d617350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d6177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d617c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d6180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d618510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d618980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d618df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d619260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d6196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d619b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d619fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d61a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d61a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d61ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d61b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d61b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d61ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d61bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d61c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d61c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d61cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d61d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d61d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d61d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d61ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d61e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d61e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d61eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d61ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d61f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d61f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d61fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d620150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d6205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d620a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d620ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d621310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d621780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d621bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d622060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d6224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d622db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d623220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d623690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d623b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d623f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d6243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d624850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d624cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d625130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d6255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d625a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d625e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d6262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d626760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d626bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d627470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d6279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d627f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d6283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d628850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d628cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d629190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d629630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d629ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d629f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d62a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d62a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d62ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d62b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d62b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d62bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d62bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d62c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d62c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d62cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d62d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d62d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d62db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d62e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d62e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d62e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d62ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d62f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d62f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d62fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d630090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d630530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d6309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d630e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d631310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d6317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d631c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d6320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d632590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d632a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d632ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d633810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d634150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d6345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d6353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d635870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d635d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d6361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d636650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d6378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d637d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d6386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d639490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d63a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d63a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d63abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d63b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d63b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d63b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d63be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d63c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d63c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d63cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d63d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d63d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d63d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d63de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d63e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d63e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d63ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d63f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d63f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d63fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d640100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d640650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d640910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d641530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d641b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d642330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d6427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d642a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d6430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d6436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d643ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d644340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d6447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d645430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d646420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d646970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d646ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d647410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d647960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d648950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d6493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d64a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d64a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d64ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d64b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d64b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d64be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d64c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d64c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d64ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d64d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d64d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d64de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d64e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d64e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d64ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d64f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d64f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d64fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d650380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d6508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d650e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d651370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d6518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d651e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d652360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d6528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d652e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d6538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d653df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d654340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d654890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d654de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d655330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d655880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d655dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d656870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d656dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d657310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d657860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d657db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d658250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d6586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d658b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d659030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d6594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d659970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d65a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d65a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d65abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d65b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d65b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d65b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d65be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d65c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d65c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d65cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d65d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d65ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d65e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d65e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d65ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d65f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d65f860 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.937s
user	0m0.244s
sys	0m0.151s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.16 sec*proc (2 tests)

Total Test time (real) =   1.17 sec
        1.19 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.14 user         0.04 sys
```
