### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.35 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.11 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.22 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.30 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.91 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.18 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  191.13 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.33 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 235.33 sec*proc (28 tests)

Total Test time (real) = 235.35 sec

real	3m55.509s
user	8m21.519s
sys	0m7.166s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.38 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.44 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.41 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.40 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.09 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.60 sec*proc (28 tests)

Total Test time (real) =  52.61 sec

real	0m52.621s
user	1m14.705s
sys	0m6.218s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.069 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.763 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.326 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.337 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.339 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.340 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.341 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.343 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.343 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.344 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.345 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.346 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.350 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.351 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.352 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.352 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.353 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.354 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.354 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.802 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.804 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.805 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.806 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.806 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.806 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.032.807 I llama_model_loader: - type  f32:  124 tensors
0.00.032.808 I llama_model_loader: - type  f16:   73 tensors
0.00.032.809 I print_info: file format = GGUF V3 (latest)
0.00.032.809 I print_info: file type   = F16
0.00.032.810 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.356 I load: special tokens cache size = 5
0.00.039.562 I load: token to piece cache size = 0.2032 MB
0.00.039.566 I print_info: arch             = bert
0.00.039.566 I print_info: vocab_only       = 0
0.00.039.567 I print_info: n_ctx_train      = 512
0.00.039.567 I print_info: n_embd           = 384
0.00.039.567 I print_info: n_layer          = 12
0.00.039.571 I print_info: n_head           = 12
0.00.039.572 I print_info: n_head_kv        = 12
0.00.039.572 I print_info: n_rot            = 32
0.00.039.574 I print_info: n_swa            = 0
0.00.039.574 I print_info: n_embd_head_k    = 32
0.00.039.574 I print_info: n_embd_head_v    = 32
0.00.039.580 I print_info: n_gqa            = 1
0.00.039.581 I print_info: n_embd_k_gqa     = 384
0.00.039.582 I print_info: n_embd_v_gqa     = 384
0.00.039.583 I print_info: f_norm_eps       = 1.0e-12
0.00.039.584 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.584 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.585 I print_info: f_logit_scale    = 0.0e+00
0.00.039.588 I print_info: n_ff             = 1536
0.00.039.588 I print_info: n_expert         = 0
0.00.039.588 I print_info: n_expert_used    = 0
0.00.039.588 I print_info: causal attn      = 0
0.00.039.588 I print_info: pooling type     = 2
0.00.039.589 I print_info: rope type        = 2
0.00.039.589 I print_info: rope scaling     = linear
0.00.039.590 I print_info: freq_base_train  = 10000.0
0.00.039.590 I print_info: freq_scale_train = 1
0.00.039.591 I print_info: n_ctx_orig_yarn  = 512
0.00.039.591 I print_info: rope_finetuned   = unknown
0.00.039.591 I print_info: ssm_d_conv       = 0
0.00.039.592 I print_info: ssm_d_inner      = 0
0.00.039.592 I print_info: ssm_d_state      = 0
0.00.039.592 I print_info: ssm_dt_rank      = 0
0.00.039.592 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.599 I print_info: model type       = 33M
0.00.039.599 I print_info: model params     = 33.21 M
0.00.039.600 I print_info: general.name     = Bge Small
0.00.039.600 I print_info: vocab type       = WPM
0.00.039.601 I print_info: n_vocab          = 30522
0.00.039.601 I print_info: n_merges         = 0
0.00.039.603 I print_info: BOS token        = 101 '[CLS]'
0.00.039.603 I print_info: UNK token        = 100 '[UNK]'
0.00.039.604 I print_info: SEP token        = 102 '[SEP]'
0.00.039.604 I print_info: PAD token        = 0 '[PAD]'
0.00.039.604 I print_info: MASK token       = 103 '[MASK]'
0.00.039.605 I print_info: LF token         = 0 '[PAD]'
0.00.039.605 I print_info: max token length = 21
0.00.042.988 I load_tensors: offloading 12 repeating layers to GPU
0.00.042.989 I load_tensors: offloading output layer to GPU
0.00.042.990 I load_tensors: offloaded 13/13 layers to GPU
0.00.043.016 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.017 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.043.283 I llama_init_from_model: n_seq_max     = 1
0.00.043.284 I llama_init_from_model: n_ctx         = 512
0.00.043.285 I llama_init_from_model: n_ctx_per_seq = 512
0.00.043.285 I llama_init_from_model: n_batch       = 2048
0.00.043.285 I llama_init_from_model: n_ubatch      = 2048
0.00.043.286 I llama_init_from_model: flash_attn    = 0
0.00.043.286 I llama_init_from_model: freq_base     = 10000.0
0.00.043.287 I llama_init_from_model: freq_scale    = 1
0.00.043.287 I ggml_metal_init: allocating
0.00.043.292 I ggml_metal_init: found device: Apple M4
0.00.043.297 I ggml_metal_init: picking default device: Apple M4
0.00.044.030 I ggml_metal_init: using embedded metal library
0.00.048.256 I ggml_metal_init: GPU name:   Apple M4
0.00.048.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.261 I ggml_metal_init: simdgroup reduction   = true
0.00.048.261 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.261 I ggml_metal_init: has residency sets    = true
0.00.048.261 I ggml_metal_init: has bfloat            = true
0.00.048.261 I ggml_metal_init: use bfloat            = true
0.00.048.262 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.081 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.061.771 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.061.773 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.061.775 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.063.014 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.063.016 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.063.016 I llama_init_from_model: graph nodes  = 429
0.00.063.016 I llama_init_from_model: graph splits = 2
0.00.063.017 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.018 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.678 I 
0.00.068.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.069.377 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.074.513 I llama_perf_context_print:        load time =      48.91 ms
0.00.074.514 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1807.96 tokens per second)
0.00.074.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.074.515 I llama_perf_context_print:       total time =       5.84 ms /    10 tokens
0.00.074.672 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.051s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.756 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.762 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.763 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.763 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.763 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.764 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.765 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.765 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.766 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.766 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.768 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.768 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.769 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.769 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.769 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.770 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.368 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.054 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.056 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.056 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.056 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.057 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.057 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.057 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.058 I llama_model_loader: - type  f32:  124 tensors
0.00.016.058 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.058 I print_info: file format = GGUF V3 (latest)
0.00.016.059 I print_info: file type   = Q8_0
0.00.016.060 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.672 I load: special tokens cache size = 5
0.00.019.972 I load: token to piece cache size = 0.2032 MB
0.00.019.975 I print_info: arch             = bert
0.00.019.975 I print_info: vocab_only       = 0
0.00.019.976 I print_info: n_ctx_train      = 512
0.00.019.976 I print_info: n_embd           = 384
0.00.019.976 I print_info: n_layer          = 12
0.00.019.978 I print_info: n_head           = 12
0.00.019.978 I print_info: n_head_kv        = 12
0.00.019.979 I print_info: n_rot            = 32
0.00.019.979 I print_info: n_swa            = 0
0.00.019.979 I print_info: n_embd_head_k    = 32
0.00.019.979 I print_info: n_embd_head_v    = 32
0.00.019.980 I print_info: n_gqa            = 1
0.00.019.980 I print_info: n_embd_k_gqa     = 384
0.00.019.981 I print_info: n_embd_v_gqa     = 384
0.00.019.981 I print_info: f_norm_eps       = 1.0e-12
0.00.019.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.982 I print_info: f_logit_scale    = 0.0e+00
0.00.019.983 I print_info: n_ff             = 1536
0.00.019.983 I print_info: n_expert         = 0
0.00.019.983 I print_info: n_expert_used    = 0
0.00.019.983 I print_info: causal attn      = 0
0.00.019.983 I print_info: pooling type     = 2
0.00.019.983 I print_info: rope type        = 2
0.00.019.983 I print_info: rope scaling     = linear
0.00.019.984 I print_info: freq_base_train  = 10000.0
0.00.019.984 I print_info: freq_scale_train = 1
0.00.019.984 I print_info: n_ctx_orig_yarn  = 512
0.00.019.984 I print_info: rope_finetuned   = unknown
0.00.019.984 I print_info: ssm_d_conv       = 0
0.00.019.985 I print_info: ssm_d_inner      = 0
0.00.019.985 I print_info: ssm_d_state      = 0
0.00.019.985 I print_info: ssm_dt_rank      = 0
0.00.019.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.985 I print_info: model type       = 33M
0.00.019.985 I print_info: model params     = 33.21 M
0.00.019.986 I print_info: general.name     = Bge Small
0.00.019.986 I print_info: vocab type       = WPM
0.00.019.986 I print_info: n_vocab          = 30522
0.00.019.986 I print_info: n_merges         = 0
0.00.019.987 I print_info: BOS token        = 101 '[CLS]'
0.00.019.987 I print_info: UNK token        = 100 '[UNK]'
0.00.019.987 I print_info: SEP token        = 102 '[SEP]'
0.00.019.987 I print_info: PAD token        = 0 '[PAD]'
0.00.019.987 I print_info: MASK token       = 103 '[MASK]'
0.00.019.987 I print_info: LF token         = 0 '[PAD]'
0.00.019.988 I print_info: max token length = 21
0.00.021.692 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.693 I load_tensors: offloading output layer to GPU
0.00.021.693 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.699 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.700 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.848 I llama_init_from_model: n_seq_max     = 1
0.00.021.849 I llama_init_from_model: n_ctx         = 512
0.00.021.849 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.849 I llama_init_from_model: n_batch       = 2048
0.00.021.849 I llama_init_from_model: n_ubatch      = 2048
0.00.021.849 I llama_init_from_model: flash_attn    = 0
0.00.021.850 I llama_init_from_model: freq_base     = 10000.0
0.00.021.850 I llama_init_from_model: freq_scale    = 1
0.00.021.850 I ggml_metal_init: allocating
0.00.021.853 I ggml_metal_init: found device: Apple M4
0.00.021.857 I ggml_metal_init: picking default device: Apple M4
0.00.022.360 I ggml_metal_init: using embedded metal library
0.00.024.895 I ggml_metal_init: GPU name:   Apple M4
0.00.024.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.898 I ggml_metal_init: simdgroup reduction   = true
0.00.024.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.898 I ggml_metal_init: has residency sets    = true
0.00.024.898 I ggml_metal_init: has bfloat            = true
0.00.024.898 I ggml_metal_init: use bfloat            = true
0.00.024.899 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.199 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.805 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.807 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.809 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.685 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.687 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.687 I llama_init_from_model: graph nodes  = 429
0.00.036.687 I llama_init_from_model: graph splits = 2
0.00.036.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.645 I 
0.00.040.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.174 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.582 I llama_perf_context_print:        load time =      30.86 ms
0.00.045.583 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2102.80 tokens per second)
0.00.045.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.584 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.045.783 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.170 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.135 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.139 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.142 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.143 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.144 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.145 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.146 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.147 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.147 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.148 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.152 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.156 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.156 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.157 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.490 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.686 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.689 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.689 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.690 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.690 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.690 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.691 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.691 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.692 I llama_model_loader: - type  f32:   40 tensors
0.00.047.692 I llama_model_loader: - type  f16:   30 tensors
0.00.047.693 I print_info: file format = GGUF V3 (latest)
0.00.047.693 I print_info: file type   = F16
0.00.047.695 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.051.994 W load: empty token at index 5
0.00.056.992 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.398 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.433 I load: special tokens cache size = 5
0.00.323.087 I load: token to piece cache size = 1.5060 MB
0.00.323.093 I print_info: arch             = jina-bert-v2
0.00.323.093 I print_info: vocab_only       = 0
0.00.323.094 I print_info: n_ctx_train      = 8192
0.00.323.096 I print_info: n_embd           = 384
0.00.323.096 I print_info: n_layer          = 4
0.00.323.108 I print_info: n_head           = 12
0.00.323.110 I print_info: n_head_kv        = 12
0.00.323.111 I print_info: n_rot            = 32
0.00.323.111 I print_info: n_swa            = 0
0.00.323.111 I print_info: n_embd_head_k    = 32
0.00.323.111 I print_info: n_embd_head_v    = 32
0.00.323.112 I print_info: n_gqa            = 1
0.00.323.112 I print_info: n_embd_k_gqa     = 384
0.00.323.113 I print_info: n_embd_v_gqa     = 384
0.00.323.114 I print_info: f_norm_eps       = 1.0e-12
0.00.323.114 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.323.114 I print_info: f_clamp_kqv      = 0.0e+00
0.00.323.115 I print_info: f_max_alibi_bias = 8.0e+00
0.00.323.115 I print_info: f_logit_scale    = 0.0e+00
0.00.323.116 I print_info: n_ff             = 1536
0.00.323.116 I print_info: n_expert         = 0
0.00.323.116 I print_info: n_expert_used    = 0
0.00.323.116 I print_info: causal attn      = 0
0.00.323.117 I print_info: pooling type     = -1
0.00.323.117 I print_info: rope type        = -1
0.00.323.117 I print_info: rope scaling     = linear
0.00.323.118 I print_info: freq_base_train  = 10000.0
0.00.323.118 I print_info: freq_scale_train = 1
0.00.323.118 I print_info: n_ctx_orig_yarn  = 8192
0.00.323.118 I print_info: rope_finetuned   = unknown
0.00.323.119 I print_info: ssm_d_conv       = 0
0.00.323.119 I print_info: ssm_d_inner      = 0
0.00.323.119 I print_info: ssm_d_state      = 0
0.00.323.119 I print_info: ssm_dt_rank      = 0
0.00.323.119 I print_info: ssm_dt_b_c_rms   = 0
0.00.323.119 I print_info: model type       = 33M
0.00.323.120 I print_info: model params     = 32.90 M
0.00.323.120 I print_info: general.name     = Jina Bert Implementation
0.00.323.121 I print_info: vocab type       = BPE
0.00.323.121 I print_info: n_vocab          = 61056
0.00.323.122 I print_info: n_merges         = 39382
0.00.323.122 I print_info: BOS token        = 0 '<s>'
0.00.323.122 I print_info: EOS token        = 2 '</s>'
0.00.323.122 I print_info: UNK token        = 3 '<unk>'
0.00.323.122 I print_info: SEP token        = 2 '</s>'
0.00.323.123 I print_info: PAD token        = 1 '<pad>'
0.00.323.123 I print_info: MASK token       = 4 '<mask>'
0.00.323.123 I print_info: EOG token        = 2 '</s>'
0.00.323.123 I print_info: max token length = 45
0.00.325.207 I load_tensors: offloading 4 repeating layers to GPU
0.00.325.208 I load_tensors: offloading output layer to GPU
0.00.325.208 I load_tensors: offloaded 5/5 layers to GPU
0.00.325.232 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.325.233 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.325.531 I llama_init_from_model: n_seq_max     = 1
0.00.325.532 I llama_init_from_model: n_ctx         = 8192
0.00.325.533 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.325.533 I llama_init_from_model: n_batch       = 2048
0.00.325.533 I llama_init_from_model: n_ubatch      = 2048
0.00.325.533 I llama_init_from_model: flash_attn    = 0
0.00.325.533 I llama_init_from_model: freq_base     = 10000.0
0.00.325.534 I llama_init_from_model: freq_scale    = 1
0.00.325.534 I ggml_metal_init: allocating
0.00.325.538 I ggml_metal_init: found device: Apple M4
0.00.325.541 I ggml_metal_init: picking default device: Apple M4
0.00.326.397 I ggml_metal_init: using embedded metal library
0.00.329.210 I ggml_metal_init: GPU name:   Apple M4
0.00.329.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.329.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.329.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.329.213 I ggml_metal_init: simdgroup reduction   = true
0.00.329.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.329.214 I ggml_metal_init: has residency sets    = true
0.00.329.214 I ggml_metal_init: has bfloat            = true
0.00.329.214 I ggml_metal_init: use bfloat            = true
0.00.329.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.329.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.713 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.791 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.793 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.794 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.348.526 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.348.527 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.348.528 I llama_init_from_model: graph nodes  = 154
0.00.348.528 I llama_init_from_model: graph splits = 2
0.00.348.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.348.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.355.911 I 
0.00.355.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.087 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.088 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.090 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.090 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.093 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.094 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.356.634 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.368 I llama_perf_context_print:        load time =     333.95 ms
0.00.360.369 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16693.59 tokens per second)
0.00.360.370 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.370 I llama_perf_context_print:       total time =       4.46 ms /    63 tokens
0.00.360.583 I ggml_metal_free: deallocating

real	0m1.093s
user	0m0.331s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.315 I main: llama backend init
0.00.000.322 I main: load the model and apply lora adapter, if any
0.00.033.238 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.046.853 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.877 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.885 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.770 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.063.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.759 I llama_model_loader: - type  f32:  194 tensors
0.00.063.760 I llama_model_loader: - type  f16:   98 tensors
0.00.063.761 I print_info: file format = GGUF V3 (latest)
0.00.063.762 I print_info: file type   = all F32 (guessed)
0.00.063.763 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.076.625 I load: special tokens cache size = 25
0.00.084.626 I load: token to piece cache size = 0.2984 MB
0.00.084.629 I print_info: arch             = gptneox
0.00.084.629 I print_info: vocab_only       = 0
0.00.084.629 I print_info: n_ctx_train      = 2048
0.00.084.629 I print_info: n_embd           = 2048
0.00.084.630 I print_info: n_layer          = 24
0.00.084.633 I print_info: n_head           = 16
0.00.084.634 I print_info: n_head_kv        = 16
0.00.084.634 I print_info: n_rot            = 32
0.00.084.635 I print_info: n_swa            = 0
0.00.084.635 I print_info: n_embd_head_k    = 128
0.00.084.637 I print_info: n_embd_head_v    = 128
0.00.084.638 I print_info: n_gqa            = 1
0.00.084.639 I print_info: n_embd_k_gqa     = 2048
0.00.084.640 I print_info: n_embd_v_gqa     = 2048
0.00.084.640 I print_info: f_norm_eps       = 1.0e-05
0.00.084.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.084.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.084.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.084.641 I print_info: f_logit_scale    = 0.0e+00
0.00.084.642 I print_info: n_ff             = 8192
0.00.084.642 I print_info: n_expert         = 0
0.00.084.642 I print_info: n_expert_used    = 0
0.00.084.642 I print_info: causal attn      = 1
0.00.084.643 I print_info: pooling type     = 0
0.00.084.643 I print_info: rope type        = 2
0.00.084.643 I print_info: rope scaling     = linear
0.00.084.645 I print_info: freq_base_train  = 10000.0
0.00.084.645 I print_info: freq_scale_train = 1
0.00.084.645 I print_info: n_ctx_orig_yarn  = 2048
0.00.084.645 I print_info: rope_finetuned   = unknown
0.00.084.645 I print_info: ssm_d_conv       = 0
0.00.084.646 I print_info: ssm_d_inner      = 0
0.00.084.646 I print_info: ssm_d_state      = 0
0.00.084.646 I print_info: ssm_dt_rank      = 0
0.00.084.646 I print_info: ssm_dt_b_c_rms   = 0
0.00.084.646 I print_info: model type       = 1.4B
0.00.084.647 I print_info: model params     = 1.41 B
0.00.084.647 I print_info: general.name     = 1.4B
0.00.084.651 I print_info: vocab type       = BPE
0.00.084.651 I print_info: n_vocab          = 50304
0.00.084.651 I print_info: n_merges         = 50009
0.00.084.651 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.084.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.084.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.084.652 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.084.652 I print_info: LF token         = 128 'Ä'
0.00.084.653 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.084.653 I print_info: max token length = 1024
0.00.120.482 I load_tensors: offloading 24 repeating layers to GPU
0.00.120.486 I load_tensors: offloading output layer to GPU
0.00.120.487 I load_tensors: offloaded 25/25 layers to GPU
0.00.120.510 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.120.512 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.120.766 I llama_init_from_model: n_seq_max     = 1
0.00.120.767 I llama_init_from_model: n_ctx         = 2048
0.00.120.767 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.120.767 I llama_init_from_model: n_batch       = 2048
0.00.120.767 I llama_init_from_model: n_ubatch      = 512
0.00.120.767 I llama_init_from_model: flash_attn    = 0
0.00.120.768 I llama_init_from_model: freq_base     = 10000.0
0.00.120.768 I llama_init_from_model: freq_scale    = 1
0.00.120.769 I ggml_metal_init: allocating
0.00.120.787 I ggml_metal_init: found device: Apple M4
0.00.120.792 I ggml_metal_init: picking default device: Apple M4
0.00.121.350 I ggml_metal_init: using embedded metal library
0.00.129.999 I ggml_metal_init: GPU name:   Apple M4
0.00.130.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.130.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.130.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.130.002 I ggml_metal_init: simdgroup reduction   = true
0.00.130.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.130.002 I ggml_metal_init: has residency sets    = true
0.00.130.002 I ggml_metal_init: has bfloat            = true
0.00.130.003 I ggml_metal_init: use bfloat            = true
0.00.130.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.130.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.153.714 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.182.112 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.182.119 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.182.142 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.186.014 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.186.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.186.016 I llama_init_from_model: graph nodes  = 967
0.00.186.016 I llama_init_from_model: graph splits = 2
0.00.186.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.186.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.186.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.517 I main: llama threadpool init, n_threads = 4
0.00.243.557 I 
0.00.243.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.593 I 
0.00.243.635 I sampler seed: 1234
0.00.243.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.665 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.666 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.666 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.088.349 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.088.350 I llama_perf_context_print:        load time =     209.25 ms
0.02.088.350 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.11 tokens per second)
0.02.088.351 I llama_perf_context_print:        eval time =    1797.95 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.088.351 I llama_perf_context_print:       total time =    1845.85 ms /    70 tokens
0.02.088.590 I ggml_metal_free: deallocating

real	0m2.393s
user	0m0.127s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.532 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.527 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.280 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.329 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.331 I llama_model_loader: - type  f32:  194 tensors
0.00.044.331 I llama_model_loader: - type  f16:   98 tensors
0.00.044.332 I print_info: file format = GGUF V3 (latest)
0.00.044.333 I print_info: file type   = all F32 (guessed)
0.00.044.340 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.057.744 I load: special tokens cache size = 25
0.00.066.058 I load: token to piece cache size = 0.2984 MB
0.00.066.067 I print_info: arch             = gptneox
0.00.066.068 I print_info: vocab_only       = 0
0.00.066.068 I print_info: n_ctx_train      = 2048
0.00.066.068 I print_info: n_embd           = 2048
0.00.066.068 I print_info: n_layer          = 24
0.00.066.072 I print_info: n_head           = 16
0.00.066.073 I print_info: n_head_kv        = 16
0.00.066.073 I print_info: n_rot            = 32
0.00.066.074 I print_info: n_swa            = 0
0.00.066.074 I print_info: n_embd_head_k    = 128
0.00.066.074 I print_info: n_embd_head_v    = 128
0.00.066.075 I print_info: n_gqa            = 1
0.00.066.076 I print_info: n_embd_k_gqa     = 2048
0.00.066.076 I print_info: n_embd_v_gqa     = 2048
0.00.066.077 I print_info: f_norm_eps       = 1.0e-05
0.00.066.077 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.077 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.077 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.078 I print_info: f_logit_scale    = 0.0e+00
0.00.066.078 I print_info: n_ff             = 8192
0.00.066.078 I print_info: n_expert         = 0
0.00.066.079 I print_info: n_expert_used    = 0
0.00.066.079 I print_info: causal attn      = 1
0.00.066.079 I print_info: pooling type     = 0
0.00.066.079 I print_info: rope type        = 2
0.00.066.079 I print_info: rope scaling     = linear
0.00.066.080 I print_info: freq_base_train  = 10000.0
0.00.066.080 I print_info: freq_scale_train = 1
0.00.066.080 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.080 I print_info: rope_finetuned   = unknown
0.00.066.083 I print_info: ssm_d_conv       = 0
0.00.066.083 I print_info: ssm_d_inner      = 0
0.00.066.083 I print_info: ssm_d_state      = 0
0.00.066.083 I print_info: ssm_dt_rank      = 0
0.00.066.084 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.084 I print_info: model type       = 1.4B
0.00.066.084 I print_info: model params     = 1.41 B
0.00.066.084 I print_info: general.name     = 1.4B
0.00.066.085 I print_info: vocab type       = BPE
0.00.066.085 I print_info: n_vocab          = 50304
0.00.066.087 I print_info: n_merges         = 50009
0.00.066.087 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.088 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.088 I print_info: LF token         = 128 'Ä'
0.00.066.089 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.089 I print_info: max token length = 1024
0.01.186.186 I load_tensors: offloading 24 repeating layers to GPU
0.01.186.197 I load_tensors: offloading output layer to GPU
0.01.186.198 I load_tensors: offloaded 25/25 layers to GPU
0.01.186.227 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.186.230 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.186.800 I llama_init_from_model: n_seq_max     = 1
0.01.186.803 I llama_init_from_model: n_ctx         = 128
0.01.186.803 I llama_init_from_model: n_ctx_per_seq = 128
0.01.186.803 I llama_init_from_model: n_batch       = 128
0.01.186.803 I llama_init_from_model: n_ubatch      = 128
0.01.186.804 I llama_init_from_model: flash_attn    = 0
0.01.186.804 I llama_init_from_model: freq_base     = 10000.0
0.01.186.805 I llama_init_from_model: freq_scale    = 1
0.01.186.805 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.186.806 I ggml_metal_init: allocating
0.01.186.838 I ggml_metal_init: found device: Apple M4
0.01.186.844 I ggml_metal_init: picking default device: Apple M4
0.01.187.819 I ggml_metal_init: using embedded metal library
0.01.191.670 I ggml_metal_init: GPU name:   Apple M4
0.01.191.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.191.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.191.674 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.191.674 I ggml_metal_init: simdgroup reduction   = true
0.01.191.674 I ggml_metal_init: simdgroup matrix mul. = true
0.01.191.674 I ggml_metal_init: has residency sets    = true
0.01.191.674 I ggml_metal_init: has bfloat            = true
0.01.191.675 I ggml_metal_init: use bfloat            = true
0.01.191.675 I ggml_metal_init: hasUnifiedMemory      = true
0.01.191.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.202.907 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.204.660 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.204.663 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.204.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.206.414 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.206.415 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.206.415 I llama_init_from_model: graph nodes  = 967
0.01.206.416 I llama_init_from_model: graph splits = 2
0.01.206.417 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.206.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.243.095 I 
0.01.243.130 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.243.150 I perplexity: tokenizing the input ..
0.01.248.540 I perplexity: tokenization took 5.388 ms
0.01.248.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.379.097 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.380.507 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.380.520 I llama_perf_context_print:        load time =    1225.56 ms
0.01.380.521 I llama_perf_context_print: prompt eval time =     130.21 ms /   128 tokens (    1.02 ms per token,   983.02 tokens per second)
0.01.380.522 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.380.522 I llama_perf_context_print:       total time =     137.43 ms /   129 tokens
0.01.380.948 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.093s
sys	0m0.221s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.019.401 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.956 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.962 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.873 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.874 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.874 I llama_model_loader: - type  f32:  194 tensors
0.00.038.875 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.875 I print_info: file format = GGUF V3 (latest)
0.00.038.876 I print_info: file type   = Q8_0
0.00.038.877 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.521 I load: special tokens cache size = 25
0.00.054.954 I load: token to piece cache size = 0.2984 MB
0.00.054.958 I print_info: arch             = gptneox
0.00.054.958 I print_info: vocab_only       = 0
0.00.054.958 I print_info: n_ctx_train      = 2048
0.00.054.959 I print_info: n_embd           = 2048
0.00.054.959 I print_info: n_layer          = 24
0.00.054.964 I print_info: n_head           = 16
0.00.054.965 I print_info: n_head_kv        = 16
0.00.054.965 I print_info: n_rot            = 32
0.00.054.965 I print_info: n_swa            = 0
0.00.054.965 I print_info: n_embd_head_k    = 128
0.00.054.966 I print_info: n_embd_head_v    = 128
0.00.054.966 I print_info: n_gqa            = 1
0.00.054.967 I print_info: n_embd_k_gqa     = 2048
0.00.054.968 I print_info: n_embd_v_gqa     = 2048
0.00.054.969 I print_info: f_norm_eps       = 1.0e-05
0.00.054.969 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.969 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.970 I print_info: f_logit_scale    = 0.0e+00
0.00.054.971 I print_info: n_ff             = 8192
0.00.054.971 I print_info: n_expert         = 0
0.00.054.971 I print_info: n_expert_used    = 0
0.00.054.971 I print_info: causal attn      = 1
0.00.054.971 I print_info: pooling type     = 0
0.00.054.971 I print_info: rope type        = 2
0.00.054.972 I print_info: rope scaling     = linear
0.00.054.972 I print_info: freq_base_train  = 10000.0
0.00.054.972 I print_info: freq_scale_train = 1
0.00.054.973 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.973 I print_info: rope_finetuned   = unknown
0.00.054.973 I print_info: ssm_d_conv       = 0
0.00.054.973 I print_info: ssm_d_inner      = 0
0.00.054.973 I print_info: ssm_d_state      = 0
0.00.054.974 I print_info: ssm_dt_rank      = 0
0.00.054.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.974 I print_info: model type       = 1.4B
0.00.054.974 I print_info: model params     = 1.41 B
0.00.054.974 I print_info: general.name     = 1.4B
0.00.054.975 I print_info: vocab type       = BPE
0.00.054.975 I print_info: n_vocab          = 50304
0.00.054.975 I print_info: n_merges         = 50009
0.00.054.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.976 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.976 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.976 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.976 I print_info: LF token         = 128 'Ä'
0.00.054.977 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.977 I print_info: max token length = 1024
0.01.105.742 I load_tensors: offloading 24 repeating layers to GPU
0.01.105.748 I load_tensors: offloading output layer to GPU
0.01.105.749 I load_tensors: offloaded 25/25 layers to GPU
0.01.105.777 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.105.780 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.106.491 I llama_init_from_model: n_seq_max     = 1
0.01.106.494 I llama_init_from_model: n_ctx         = 2048
0.01.106.494 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.106.494 I llama_init_from_model: n_batch       = 2048
0.01.106.495 I llama_init_from_model: n_ubatch      = 512
0.01.106.495 I llama_init_from_model: flash_attn    = 0
0.01.106.496 I llama_init_from_model: freq_base     = 10000.0
0.01.106.497 I llama_init_from_model: freq_scale    = 1
0.01.106.498 I ggml_metal_init: allocating
0.01.106.549 I ggml_metal_init: found device: Apple M4
0.01.106.559 I ggml_metal_init: picking default device: Apple M4
0.01.107.867 I ggml_metal_init: using embedded metal library
0.01.113.398 I ggml_metal_init: GPU name:   Apple M4
0.01.113.401 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.113.402 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.113.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.113.404 I ggml_metal_init: simdgroup reduction   = true
0.01.113.404 I ggml_metal_init: simdgroup matrix mul. = true
0.01.113.404 I ggml_metal_init: has residency sets    = true
0.01.113.404 I ggml_metal_init: has bfloat            = true
0.01.113.405 I ggml_metal_init: use bfloat            = true
0.01.113.405 I ggml_metal_init: hasUnifiedMemory      = true
0.01.113.407 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.129.552 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.189.783 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.189.790 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.189.813 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.195.244 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.195.247 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.195.247 I llama_init_from_model: graph nodes  = 967
0.01.195.248 I llama_init_from_model: graph splits = 2
0.01.195.253 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.195.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.195.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.249.810 I main: llama threadpool init, n_threads = 4
0.01.249.856 I 
0.01.249.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.249.887 I 
0.01.250.041 I sampler seed: 1234
0.01.250.046 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.250.057 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.250.057 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.250.058 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.336.186 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.02.336.187 I llama_perf_context_print:        load time =    1229.54 ms
0.02.336.187 I llama_perf_context_print: prompt eval time =      39.62 ms /     7 tokens (    5.66 ms per token,   176.67 tokens per second)
0.02.336.188 I llama_perf_context_print:        eval time =    1043.52 ms /    63 runs   (   16.56 ms per token,    60.37 tokens per second)
0.02.336.188 I llama_perf_context_print:       total time =    1087.24 ms /    70 tokens
0.02.336.408 I ggml_metal_free: deallocating

real	0m2.356s
user	0m0.110s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.253 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.000 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.611 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.117 I llama_model_loader: - type  f32:  194 tensors
0.00.036.117 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.118 I print_info: file format = GGUF V3 (latest)
0.00.036.119 I print_info: file type   = Q8_0
0.00.036.120 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.918 I load: special tokens cache size = 25
0.00.053.729 I load: token to piece cache size = 0.2984 MB
0.00.053.733 I print_info: arch             = gptneox
0.00.053.733 I print_info: vocab_only       = 0
0.00.053.733 I print_info: n_ctx_train      = 2048
0.00.053.734 I print_info: n_embd           = 2048
0.00.053.734 I print_info: n_layer          = 24
0.00.053.737 I print_info: n_head           = 16
0.00.053.738 I print_info: n_head_kv        = 16
0.00.053.738 I print_info: n_rot            = 32
0.00.053.738 I print_info: n_swa            = 0
0.00.053.739 I print_info: n_embd_head_k    = 128
0.00.053.739 I print_info: n_embd_head_v    = 128
0.00.053.740 I print_info: n_gqa            = 1
0.00.053.740 I print_info: n_embd_k_gqa     = 2048
0.00.053.741 I print_info: n_embd_v_gqa     = 2048
0.00.053.742 I print_info: f_norm_eps       = 1.0e-05
0.00.053.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.742 I print_info: f_logit_scale    = 0.0e+00
0.00.053.743 I print_info: n_ff             = 8192
0.00.053.743 I print_info: n_expert         = 0
0.00.053.743 I print_info: n_expert_used    = 0
0.00.053.744 I print_info: causal attn      = 1
0.00.053.744 I print_info: pooling type     = 0
0.00.053.744 I print_info: rope type        = 2
0.00.053.744 I print_info: rope scaling     = linear
0.00.053.745 I print_info: freq_base_train  = 10000.0
0.00.053.745 I print_info: freq_scale_train = 1
0.00.053.745 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.745 I print_info: rope_finetuned   = unknown
0.00.053.746 I print_info: ssm_d_conv       = 0
0.00.053.746 I print_info: ssm_d_inner      = 0
0.00.053.746 I print_info: ssm_d_state      = 0
0.00.053.746 I print_info: ssm_dt_rank      = 0
0.00.053.746 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.746 I print_info: model type       = 1.4B
0.00.053.747 I print_info: model params     = 1.41 B
0.00.053.747 I print_info: general.name     = 1.4B
0.00.053.747 I print_info: vocab type       = BPE
0.00.053.748 I print_info: n_vocab          = 50304
0.00.053.748 I print_info: n_merges         = 50009
0.00.053.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.748 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.749 I print_info: LF token         = 128 'Ä'
0.00.053.749 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.750 I print_info: max token length = 1024
0.00.927.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.927.927 I load_tensors: offloading output layer to GPU
0.00.927.928 I load_tensors: offloaded 25/25 layers to GPU
0.00.927.948 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.927.949 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.929.225 I llama_init_from_model: n_seq_max     = 1
0.00.929.226 I llama_init_from_model: n_ctx         = 128
0.00.929.227 I llama_init_from_model: n_ctx_per_seq = 128
0.00.929.227 I llama_init_from_model: n_batch       = 128
0.00.929.227 I llama_init_from_model: n_ubatch      = 128
0.00.929.228 I llama_init_from_model: flash_attn    = 0
0.00.929.228 I llama_init_from_model: freq_base     = 10000.0
0.00.929.229 I llama_init_from_model: freq_scale    = 1
0.00.929.230 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.929.231 I ggml_metal_init: allocating
0.00.929.254 I ggml_metal_init: found device: Apple M4
0.00.929.265 I ggml_metal_init: picking default device: Apple M4
0.00.930.500 I ggml_metal_init: using embedded metal library
0.00.935.946 I ggml_metal_init: GPU name:   Apple M4
0.00.935.949 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.935.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.935.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.935.951 I ggml_metal_init: simdgroup reduction   = true
0.00.935.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.935.952 I ggml_metal_init: has residency sets    = true
0.00.935.952 I ggml_metal_init: has bfloat            = true
0.00.935.952 I ggml_metal_init: use bfloat            = true
0.00.935.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.935.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.951.359 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.954.676 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.954.684 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.954.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.957.728 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.957.729 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.957.730 I llama_init_from_model: graph nodes  = 967
0.00.957.730 I llama_init_from_model: graph splits = 2
0.00.957.732 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.957.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.986.836 I 
0.00.986.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.986.933 I perplexity: tokenizing the input ..
0.00.993.787 I perplexity: tokenization took 6.852 ms
0.00.993.798 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.128.807 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.130.217 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.130.237 I llama_perf_context_print:        load time =     974.83 ms
0.01.130.238 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.70 tokens per second)
0.01.130.239 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.130.239 I llama_perf_context_print:       total time =     143.41 ms /   129 tokens
0.01.130.662 I ggml_metal_free: deallocating

real	0m1.147s
user	0m0.082s
sys	0m0.171s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.016.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.642 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.656 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.662 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.297 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.297 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.298 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.298 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.299 I llama_model_loader: - type  f32:  194 tensors
0.00.042.299 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.299 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.300 I print_info: file format = GGUF V3 (latest)
0.00.042.301 I print_info: file type   = Q4_0
0.00.042.302 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.052.164 I load: special tokens cache size = 25
0.00.059.137 I load: token to piece cache size = 0.2984 MB
0.00.059.141 I print_info: arch             = gptneox
0.00.059.141 I print_info: vocab_only       = 0
0.00.059.141 I print_info: n_ctx_train      = 2048
0.00.059.141 I print_info: n_embd           = 2048
0.00.059.142 I print_info: n_layer          = 24
0.00.059.145 I print_info: n_head           = 16
0.00.059.146 I print_info: n_head_kv        = 16
0.00.059.146 I print_info: n_rot            = 32
0.00.059.146 I print_info: n_swa            = 0
0.00.059.147 I print_info: n_embd_head_k    = 128
0.00.059.147 I print_info: n_embd_head_v    = 128
0.00.059.148 I print_info: n_gqa            = 1
0.00.059.148 I print_info: n_embd_k_gqa     = 2048
0.00.059.149 I print_info: n_embd_v_gqa     = 2048
0.00.059.150 I print_info: f_norm_eps       = 1.0e-05
0.00.059.150 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.150 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.150 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.150 I print_info: f_logit_scale    = 0.0e+00
0.00.059.151 I print_info: n_ff             = 8192
0.00.059.151 I print_info: n_expert         = 0
0.00.059.151 I print_info: n_expert_used    = 0
0.00.059.152 I print_info: causal attn      = 1
0.00.059.152 I print_info: pooling type     = 0
0.00.059.152 I print_info: rope type        = 2
0.00.059.152 I print_info: rope scaling     = linear
0.00.059.155 I print_info: freq_base_train  = 10000.0
0.00.059.155 I print_info: freq_scale_train = 1
0.00.059.155 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.155 I print_info: rope_finetuned   = unknown
0.00.059.155 I print_info: ssm_d_conv       = 0
0.00.059.156 I print_info: ssm_d_inner      = 0
0.00.059.156 I print_info: ssm_d_state      = 0
0.00.059.156 I print_info: ssm_dt_rank      = 0
0.00.059.156 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.156 I print_info: model type       = 1.4B
0.00.059.157 I print_info: model params     = 1.41 B
0.00.059.157 I print_info: general.name     = 1.4B
0.00.059.158 I print_info: vocab type       = BPE
0.00.059.158 I print_info: n_vocab          = 50304
0.00.059.158 I print_info: n_merges         = 50009
0.00.059.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.158 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.159 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.161 I print_info: LF token         = 128 'Ä'
0.00.059.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.161 I print_info: max token length = 1024
0.00.564.769 I load_tensors: offloading 24 repeating layers to GPU
0.00.564.784 I load_tensors: offloading output layer to GPU
0.00.564.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.564.817 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.564.818 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.566.141 I llama_init_from_model: n_seq_max     = 1
0.00.566.147 I llama_init_from_model: n_ctx         = 2048
0.00.566.148 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.566.148 I llama_init_from_model: n_batch       = 2048
0.00.566.149 I llama_init_from_model: n_ubatch      = 512
0.00.566.149 I llama_init_from_model: flash_attn    = 0
0.00.566.151 I llama_init_from_model: freq_base     = 10000.0
0.00.566.151 I llama_init_from_model: freq_scale    = 1
0.00.566.154 I ggml_metal_init: allocating
0.00.566.195 I ggml_metal_init: found device: Apple M4
0.00.566.208 I ggml_metal_init: picking default device: Apple M4
0.00.567.954 I ggml_metal_init: using embedded metal library
0.00.573.570 I ggml_metal_init: GPU name:   Apple M4
0.00.573.574 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.573.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.573.576 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.573.577 I ggml_metal_init: simdgroup reduction   = true
0.00.573.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.573.578 I ggml_metal_init: has residency sets    = true
0.00.573.578 I ggml_metal_init: has bfloat            = true
0.00.573.578 I ggml_metal_init: use bfloat            = true
0.00.573.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.573.581 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.593.359 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.652.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.652.123 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.652.148 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.355 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.656.357 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.656.357 I llama_init_from_model: graph nodes  = 967
0.00.656.357 I llama_init_from_model: graph splits = 2
0.00.656.363 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.656.496 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.656.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.853 I main: llama threadpool init, n_threads = 4
0.00.710.895 I 
0.00.710.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.921 I 
0.00.711.075 I sampler seed: 1234
0.00.711.080 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.091 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.091 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.093 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.400.203 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.400.203 I llama_perf_context_print:        load time =     693.10 ms
0.01.400.208 I llama_perf_context_print: prompt eval time =      50.34 ms /     7 tokens (    7.19 ms per token,   139.07 tokens per second)
0.01.400.209 I llama_perf_context_print:        eval time =     635.83 ms /    63 runs   (   10.09 ms per token,    99.08 tokens per second)
0.01.400.209 I llama_perf_context_print:       total time =     690.25 ms /    70 tokens
0.01.400.449 I ggml_metal_free: deallocating

real	0m1.418s
user	0m0.116s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.399 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.537 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.747 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.749 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.750 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.750 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.751 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.752 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.752 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.753 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.487 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.491 I llama_model_loader: - type  f32:  194 tensors
0.00.033.491 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.491 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.492 I print_info: file format = GGUF V3 (latest)
0.00.033.492 I print_info: file type   = Q4_0
0.00.033.493 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.799 I load: special tokens cache size = 25
0.00.050.734 I load: token to piece cache size = 0.2984 MB
0.00.050.737 I print_info: arch             = gptneox
0.00.050.737 I print_info: vocab_only       = 0
0.00.050.737 I print_info: n_ctx_train      = 2048
0.00.050.737 I print_info: n_embd           = 2048
0.00.050.738 I print_info: n_layer          = 24
0.00.050.741 I print_info: n_head           = 16
0.00.050.742 I print_info: n_head_kv        = 16
0.00.050.742 I print_info: n_rot            = 32
0.00.050.742 I print_info: n_swa            = 0
0.00.050.742 I print_info: n_embd_head_k    = 128
0.00.050.743 I print_info: n_embd_head_v    = 128
0.00.050.744 I print_info: n_gqa            = 1
0.00.050.744 I print_info: n_embd_k_gqa     = 2048
0.00.050.745 I print_info: n_embd_v_gqa     = 2048
0.00.050.748 I print_info: f_norm_eps       = 1.0e-05
0.00.050.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.749 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.749 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.749 I print_info: f_logit_scale    = 0.0e+00
0.00.050.750 I print_info: n_ff             = 8192
0.00.050.750 I print_info: n_expert         = 0
0.00.050.750 I print_info: n_expert_used    = 0
0.00.050.750 I print_info: causal attn      = 1
0.00.050.751 I print_info: pooling type     = 0
0.00.050.751 I print_info: rope type        = 2
0.00.050.751 I print_info: rope scaling     = linear
0.00.050.753 I print_info: freq_base_train  = 10000.0
0.00.050.753 I print_info: freq_scale_train = 1
0.00.050.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.754 I print_info: rope_finetuned   = unknown
0.00.050.754 I print_info: ssm_d_conv       = 0
0.00.050.754 I print_info: ssm_d_inner      = 0
0.00.050.754 I print_info: ssm_d_state      = 0
0.00.050.754 I print_info: ssm_dt_rank      = 0
0.00.050.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.755 I print_info: model type       = 1.4B
0.00.050.755 I print_info: model params     = 1.41 B
0.00.050.755 I print_info: general.name     = 1.4B
0.00.050.756 I print_info: vocab type       = BPE
0.00.050.756 I print_info: n_vocab          = 50304
0.00.050.756 I print_info: n_merges         = 50009
0.00.050.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.757 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.757 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.758 I print_info: LF token         = 128 'Ä'
0.00.050.762 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.762 I print_info: max token length = 1024
0.00.555.968 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.976 I load_tensors: offloading output layer to GPU
0.00.555.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.556.005 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.556.006 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.557.112 I llama_init_from_model: n_seq_max     = 1
0.00.557.118 I llama_init_from_model: n_ctx         = 128
0.00.557.118 I llama_init_from_model: n_ctx_per_seq = 128
0.00.557.119 I llama_init_from_model: n_batch       = 128
0.00.557.120 I llama_init_from_model: n_ubatch      = 128
0.00.557.120 I llama_init_from_model: flash_attn    = 0
0.00.557.123 I llama_init_from_model: freq_base     = 10000.0
0.00.557.123 I llama_init_from_model: freq_scale    = 1
0.00.557.124 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.557.127 I ggml_metal_init: allocating
0.00.557.199 I ggml_metal_init: found device: Apple M4
0.00.557.212 I ggml_metal_init: picking default device: Apple M4
0.00.559.091 I ggml_metal_init: using embedded metal library
0.00.565.596 I ggml_metal_init: GPU name:   Apple M4
0.00.565.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.565.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.565.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.565.603 I ggml_metal_init: simdgroup reduction   = true
0.00.565.604 I ggml_metal_init: simdgroup matrix mul. = true
0.00.565.604 I ggml_metal_init: has residency sets    = true
0.00.565.604 I ggml_metal_init: has bfloat            = true
0.00.565.605 I ggml_metal_init: use bfloat            = true
0.00.565.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.565.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.584.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.587.926 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.587.932 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.587.963 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.591.192 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.591.194 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.591.194 I llama_init_from_model: graph nodes  = 967
0.00.591.195 I llama_init_from_model: graph splits = 2
0.00.591.198 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.591.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.533 I 
0.00.618.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.638 I perplexity: tokenizing the input ..
0.00.626.048 I perplexity: tokenization took 7.406 ms
0.00.626.066 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.702 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.763.100 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.763.120 I llama_perf_context_print:        load time =     603.99 ms
0.00.763.121 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.44 tokens per second)
0.00.763.125 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.125 I llama_perf_context_print:       total time =     144.59 ms /   129 tokens
0.00.763.562 I ggml_metal_free: deallocating

real	0m0.789s
user	0m0.085s
sys	0m0.125s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.888 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.637 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.638 I llama_model_loader: - type  f32:  194 tensors
0.00.027.638 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.638 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.639 I print_info: file format = GGUF V3 (latest)
0.00.027.639 I print_info: file type   = Q4_1
0.00.027.640 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.870 I load: special tokens cache size = 25
0.00.041.757 I load: token to piece cache size = 0.2984 MB
0.00.041.760 I print_info: arch             = gptneox
0.00.041.760 I print_info: vocab_only       = 0
0.00.041.760 I print_info: n_ctx_train      = 2048
0.00.041.760 I print_info: n_embd           = 2048
0.00.041.761 I print_info: n_layer          = 24
0.00.041.763 I print_info: n_head           = 16
0.00.041.764 I print_info: n_head_kv        = 16
0.00.041.764 I print_info: n_rot            = 32
0.00.041.764 I print_info: n_swa            = 0
0.00.041.767 I print_info: n_embd_head_k    = 128
0.00.041.767 I print_info: n_embd_head_v    = 128
0.00.041.768 I print_info: n_gqa            = 1
0.00.041.768 I print_info: n_embd_k_gqa     = 2048
0.00.041.769 I print_info: n_embd_v_gqa     = 2048
0.00.041.770 I print_info: f_norm_eps       = 1.0e-05
0.00.041.770 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.770 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.770 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.770 I print_info: f_logit_scale    = 0.0e+00
0.00.041.771 I print_info: n_ff             = 8192
0.00.041.771 I print_info: n_expert         = 0
0.00.041.772 I print_info: n_expert_used    = 0
0.00.041.772 I print_info: causal attn      = 1
0.00.041.772 I print_info: pooling type     = 0
0.00.041.772 I print_info: rope type        = 2
0.00.041.772 I print_info: rope scaling     = linear
0.00.041.773 I print_info: freq_base_train  = 10000.0
0.00.041.773 I print_info: freq_scale_train = 1
0.00.041.773 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.773 I print_info: rope_finetuned   = unknown
0.00.041.774 I print_info: ssm_d_conv       = 0
0.00.041.774 I print_info: ssm_d_inner      = 0
0.00.041.774 I print_info: ssm_d_state      = 0
0.00.041.774 I print_info: ssm_dt_rank      = 0
0.00.041.774 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.774 I print_info: model type       = 1.4B
0.00.041.775 I print_info: model params     = 1.41 B
0.00.041.775 I print_info: general.name     = 1.4B
0.00.041.776 I print_info: vocab type       = BPE
0.00.041.776 I print_info: n_vocab          = 50304
0.00.041.776 I print_info: n_merges         = 50009
0.00.041.776 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.776 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.777 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.777 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.777 I print_info: LF token         = 128 'Ä'
0.00.041.778 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.778 I print_info: max token length = 1024
0.00.666.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.193 I load_tensors: offloading output layer to GPU
0.00.666.194 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.228 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.666.229 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.667.632 I llama_init_from_model: n_seq_max     = 1
0.00.667.637 I llama_init_from_model: n_ctx         = 2048
0.00.667.638 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.667.638 I llama_init_from_model: n_batch       = 2048
0.00.667.639 I llama_init_from_model: n_ubatch      = 512
0.00.667.639 I llama_init_from_model: flash_attn    = 0
0.00.667.641 I llama_init_from_model: freq_base     = 10000.0
0.00.667.641 I llama_init_from_model: freq_scale    = 1
0.00.667.644 I ggml_metal_init: allocating
0.00.667.723 I ggml_metal_init: found device: Apple M4
0.00.667.737 I ggml_metal_init: picking default device: Apple M4
0.00.669.529 I ggml_metal_init: using embedded metal library
0.00.675.779 I ggml_metal_init: GPU name:   Apple M4
0.00.675.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.785 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.790 I ggml_metal_init: simdgroup reduction   = true
0.00.675.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.790 I ggml_metal_init: has residency sets    = true
0.00.675.791 I ggml_metal_init: has bfloat            = true
0.00.675.791 I ggml_metal_init: use bfloat            = true
0.00.675.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.798 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.694.364 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.087 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.094 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.116 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.366 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.368 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.368 I llama_init_from_model: graph nodes  = 967
0.00.756.368 I llama_init_from_model: graph splits = 2
0.00.756.374 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.504 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.707 I main: llama threadpool init, n_threads = 4
0.00.807.747 I 
0.00.807.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.772 I 
0.00.807.887 I sampler seed: 1234
0.00.807.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.902 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.905 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.549.638 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.549.639 I llama_perf_context_print:        load time =     798.05 ms
0.01.549.640 I llama_perf_context_print: prompt eval time =      49.93 ms /     7 tokens (    7.13 ms per token,   140.19 tokens per second)
0.01.549.640 I llama_perf_context_print:        eval time =     689.44 ms /    63 runs   (   10.94 ms per token,    91.38 tokens per second)
0.01.549.640 I llama_perf_context_print:       total time =     742.81 ms /    70 tokens
0.01.549.894 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.111s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.309 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.028.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.899 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.902 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.903 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.904 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.905 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.909 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.909 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.909 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.668 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.702 I llama_model_loader: - type  f32:  194 tensors
0.00.037.702 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.703 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.703 I print_info: file format = GGUF V3 (latest)
0.00.037.704 I print_info: file type   = Q4_1
0.00.037.705 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.083 I load: special tokens cache size = 25
0.00.054.217 I load: token to piece cache size = 0.2984 MB
0.00.054.220 I print_info: arch             = gptneox
0.00.054.220 I print_info: vocab_only       = 0
0.00.054.221 I print_info: n_ctx_train      = 2048
0.00.054.221 I print_info: n_embd           = 2048
0.00.054.221 I print_info: n_layer          = 24
0.00.054.225 I print_info: n_head           = 16
0.00.054.227 I print_info: n_head_kv        = 16
0.00.054.227 I print_info: n_rot            = 32
0.00.054.227 I print_info: n_swa            = 0
0.00.054.227 I print_info: n_embd_head_k    = 128
0.00.054.227 I print_info: n_embd_head_v    = 128
0.00.054.228 I print_info: n_gqa            = 1
0.00.054.229 I print_info: n_embd_k_gqa     = 2048
0.00.054.230 I print_info: n_embd_v_gqa     = 2048
0.00.054.230 I print_info: f_norm_eps       = 1.0e-05
0.00.054.231 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.231 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.231 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.231 I print_info: f_logit_scale    = 0.0e+00
0.00.054.232 I print_info: n_ff             = 8192
0.00.054.234 I print_info: n_expert         = 0
0.00.054.234 I print_info: n_expert_used    = 0
0.00.054.234 I print_info: causal attn      = 1
0.00.054.235 I print_info: pooling type     = 0
0.00.054.235 I print_info: rope type        = 2
0.00.054.235 I print_info: rope scaling     = linear
0.00.054.235 I print_info: freq_base_train  = 10000.0
0.00.054.236 I print_info: freq_scale_train = 1
0.00.054.236 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.236 I print_info: rope_finetuned   = unknown
0.00.054.236 I print_info: ssm_d_conv       = 0
0.00.054.236 I print_info: ssm_d_inner      = 0
0.00.054.236 I print_info: ssm_d_state      = 0
0.00.054.237 I print_info: ssm_dt_rank      = 0
0.00.054.237 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.237 I print_info: model type       = 1.4B
0.00.054.238 I print_info: model params     = 1.41 B
0.00.054.238 I print_info: general.name     = 1.4B
0.00.054.238 I print_info: vocab type       = BPE
0.00.054.238 I print_info: n_vocab          = 50304
0.00.054.242 I print_info: n_merges         = 50009
0.00.054.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.243 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.243 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.243 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.244 I print_info: LF token         = 128 'Ä'
0.00.054.244 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.244 I print_info: max token length = 1024
0.00.641.201 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.217 I load_tensors: offloading output layer to GPU
0.00.641.217 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.252 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.641.254 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.642.806 I llama_init_from_model: n_seq_max     = 1
0.00.642.812 I llama_init_from_model: n_ctx         = 128
0.00.642.812 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.813 I llama_init_from_model: n_batch       = 128
0.00.642.813 I llama_init_from_model: n_ubatch      = 128
0.00.642.814 I llama_init_from_model: flash_attn    = 0
0.00.642.816 I llama_init_from_model: freq_base     = 10000.0
0.00.642.816 I llama_init_from_model: freq_scale    = 1
0.00.642.817 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.819 I ggml_metal_init: allocating
0.00.642.897 I ggml_metal_init: found device: Apple M4
0.00.642.910 I ggml_metal_init: picking default device: Apple M4
0.00.644.508 I ggml_metal_init: using embedded metal library
0.00.650.906 I ggml_metal_init: GPU name:   Apple M4
0.00.650.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.912 I ggml_metal_init: simdgroup reduction   = true
0.00.650.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.913 I ggml_metal_init: has residency sets    = true
0.00.650.913 I ggml_metal_init: has bfloat            = true
0.00.650.913 I ggml_metal_init: use bfloat            = true
0.00.650.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.106 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.672.611 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.637 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.005 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.007 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.007 I llama_init_from_model: graph nodes  = 967
0.00.676.008 I llama_init_from_model: graph splits = 2
0.00.676.011 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.126 I 
0.00.705.214 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.248 I perplexity: tokenizing the input ..
0.00.712.157 I perplexity: tokenization took 6.905 ms
0.00.712.177 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.881 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.843.223 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.843.239 I llama_perf_context_print:        load time =     691.81 ms
0.00.843.240 I llama_perf_context_print: prompt eval time =     128.85 ms /   128 tokens (    1.01 ms per token,   993.41 tokens per second)
0.00.843.241 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.241 I llama_perf_context_print:       total time =     138.12 ms /   129 tokens
0.00.843.618 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.083s
sys	0m0.122s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.013.480 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.021 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.023.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.033 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.034 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.848 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.783 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.786 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.786 I llama_model_loader: - type  f32:  194 tensors
0.00.031.787 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.787 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.788 I print_info: file format = GGUF V3 (latest)
0.00.031.788 I print_info: file type   = Q5_0
0.00.031.789 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.039.945 I load: special tokens cache size = 25
0.00.045.790 I load: token to piece cache size = 0.2984 MB
0.00.045.795 I print_info: arch             = gptneox
0.00.045.795 I print_info: vocab_only       = 0
0.00.045.795 I print_info: n_ctx_train      = 2048
0.00.045.795 I print_info: n_embd           = 2048
0.00.045.796 I print_info: n_layer          = 24
0.00.045.800 I print_info: n_head           = 16
0.00.045.800 I print_info: n_head_kv        = 16
0.00.045.801 I print_info: n_rot            = 32
0.00.045.801 I print_info: n_swa            = 0
0.00.045.801 I print_info: n_embd_head_k    = 128
0.00.045.802 I print_info: n_embd_head_v    = 128
0.00.045.803 I print_info: n_gqa            = 1
0.00.045.804 I print_info: n_embd_k_gqa     = 2048
0.00.045.804 I print_info: n_embd_v_gqa     = 2048
0.00.045.805 I print_info: f_norm_eps       = 1.0e-05
0.00.045.805 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.805 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.806 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.806 I print_info: f_logit_scale    = 0.0e+00
0.00.045.806 I print_info: n_ff             = 8192
0.00.045.807 I print_info: n_expert         = 0
0.00.045.807 I print_info: n_expert_used    = 0
0.00.045.808 I print_info: causal attn      = 1
0.00.045.808 I print_info: pooling type     = 0
0.00.045.808 I print_info: rope type        = 2
0.00.045.808 I print_info: rope scaling     = linear
0.00.045.808 I print_info: freq_base_train  = 10000.0
0.00.045.810 I print_info: freq_scale_train = 1
0.00.045.810 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.811 I print_info: rope_finetuned   = unknown
0.00.045.811 I print_info: ssm_d_conv       = 0
0.00.045.812 I print_info: ssm_d_inner      = 0
0.00.045.812 I print_info: ssm_d_state      = 0
0.00.045.812 I print_info: ssm_dt_rank      = 0
0.00.045.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.813 I print_info: model type       = 1.4B
0.00.045.813 I print_info: model params     = 1.41 B
0.00.045.814 I print_info: general.name     = 1.4B
0.00.045.815 I print_info: vocab type       = BPE
0.00.045.815 I print_info: n_vocab          = 50304
0.00.045.815 I print_info: n_merges         = 50009
0.00.045.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.816 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.816 I print_info: LF token         = 128 'Ä'
0.00.045.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.816 I print_info: max token length = 1024
0.01.305.332 I load_tensors: offloading 24 repeating layers to GPU
0.01.305.340 I load_tensors: offloading output layer to GPU
0.01.305.341 I load_tensors: offloaded 25/25 layers to GPU
0.01.305.360 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.01.305.360 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.01.306.036 I llama_init_from_model: n_seq_max     = 1
0.01.306.040 I llama_init_from_model: n_ctx         = 2048
0.01.306.040 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.306.040 I llama_init_from_model: n_batch       = 2048
0.01.306.041 I llama_init_from_model: n_ubatch      = 512
0.01.306.041 I llama_init_from_model: flash_attn    = 0
0.01.306.042 I llama_init_from_model: freq_base     = 10000.0
0.01.306.043 I llama_init_from_model: freq_scale    = 1
0.01.306.044 I ggml_metal_init: allocating
0.01.306.095 I ggml_metal_init: found device: Apple M4
0.01.306.108 I ggml_metal_init: picking default device: Apple M4
0.01.307.175 I ggml_metal_init: using embedded metal library
0.01.311.842 I ggml_metal_init: GPU name:   Apple M4
0.01.311.849 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.311.850 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.311.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.311.851 I ggml_metal_init: simdgroup reduction   = true
0.01.311.851 I ggml_metal_init: simdgroup matrix mul. = true
0.01.311.851 I ggml_metal_init: has residency sets    = true
0.01.311.852 I ggml_metal_init: has bfloat            = true
0.01.311.852 I ggml_metal_init: use bfloat            = true
0.01.311.853 I ggml_metal_init: hasUnifiedMemory      = true
0.01.311.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.329.065 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.360.278 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.360.284 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.360.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.364.596 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.364.598 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.364.598 I llama_init_from_model: graph nodes  = 967
0.01.364.598 I llama_init_from_model: graph splits = 2
0.01.364.604 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.364.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.364.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.423.009 I main: llama threadpool init, n_threads = 4
0.01.423.059 I 
0.01.423.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.423.081 I 
0.01.423.256 I sampler seed: 1234
0.01.423.260 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.423.280 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.423.280 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.423.280 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.225.241 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.02.225.241 I llama_perf_context_print:        load time =    1408.53 ms
0.02.225.242 I llama_perf_context_print: prompt eval time =      53.32 ms /     7 tokens (    7.62 ms per token,   131.28 tokens per second)
0.02.225.243 I llama_perf_context_print:        eval time =     745.59 ms /    63 runs   (   11.83 ms per token,    84.50 tokens per second)
0.02.225.243 I llama_perf_context_print:       total time =     803.23 ms /    70 tokens
0.02.225.458 I ggml_metal_free: deallocating

real	0m2.244s
user	0m0.107s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.655 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.847 I llama_model_loader: - type  f32:  194 tensors
0.00.043.847 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.848 I print_info: file format = GGUF V3 (latest)
0.00.043.848 I print_info: file type   = Q5_0
0.00.043.849 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.055.001 I load: special tokens cache size = 25
0.00.063.395 I load: token to piece cache size = 0.2984 MB
0.00.063.398 I print_info: arch             = gptneox
0.00.063.398 I print_info: vocab_only       = 0
0.00.063.399 I print_info: n_ctx_train      = 2048
0.00.063.399 I print_info: n_embd           = 2048
0.00.063.399 I print_info: n_layer          = 24
0.00.063.402 I print_info: n_head           = 16
0.00.063.403 I print_info: n_head_kv        = 16
0.00.063.403 I print_info: n_rot            = 32
0.00.063.403 I print_info: n_swa            = 0
0.00.063.403 I print_info: n_embd_head_k    = 128
0.00.063.404 I print_info: n_embd_head_v    = 128
0.00.063.405 I print_info: n_gqa            = 1
0.00.063.405 I print_info: n_embd_k_gqa     = 2048
0.00.063.406 I print_info: n_embd_v_gqa     = 2048
0.00.063.407 I print_info: f_norm_eps       = 1.0e-05
0.00.063.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.408 I print_info: f_logit_scale    = 0.0e+00
0.00.063.409 I print_info: n_ff             = 8192
0.00.063.409 I print_info: n_expert         = 0
0.00.063.409 I print_info: n_expert_used    = 0
0.00.063.409 I print_info: causal attn      = 1
0.00.063.410 I print_info: pooling type     = 0
0.00.063.410 I print_info: rope type        = 2
0.00.063.410 I print_info: rope scaling     = linear
0.00.063.411 I print_info: freq_base_train  = 10000.0
0.00.063.411 I print_info: freq_scale_train = 1
0.00.063.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.411 I print_info: rope_finetuned   = unknown
0.00.063.412 I print_info: ssm_d_conv       = 0
0.00.063.415 I print_info: ssm_d_inner      = 0
0.00.063.415 I print_info: ssm_d_state      = 0
0.00.063.415 I print_info: ssm_dt_rank      = 0
0.00.063.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.416 I print_info: model type       = 1.4B
0.00.063.416 I print_info: model params     = 1.41 B
0.00.063.416 I print_info: general.name     = 1.4B
0.00.063.417 I print_info: vocab type       = BPE
0.00.063.417 I print_info: n_vocab          = 50304
0.00.063.417 I print_info: n_merges         = 50009
0.00.063.418 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.418 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.418 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.419 I print_info: LF token         = 128 'Ä'
0.00.063.424 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.424 I print_info: max token length = 1024
0.00.806.687 I load_tensors: offloading 24 repeating layers to GPU
0.00.806.701 I load_tensors: offloading output layer to GPU
0.00.806.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.806.734 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.806.741 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.808.081 I llama_init_from_model: n_seq_max     = 1
0.00.808.083 I llama_init_from_model: n_ctx         = 128
0.00.808.084 I llama_init_from_model: n_ctx_per_seq = 128
0.00.808.084 I llama_init_from_model: n_batch       = 128
0.00.808.085 I llama_init_from_model: n_ubatch      = 128
0.00.808.085 I llama_init_from_model: flash_attn    = 0
0.00.808.086 I llama_init_from_model: freq_base     = 10000.0
0.00.808.087 I llama_init_from_model: freq_scale    = 1
0.00.808.088 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.808.089 I ggml_metal_init: allocating
0.00.808.137 I ggml_metal_init: found device: Apple M4
0.00.808.148 I ggml_metal_init: picking default device: Apple M4
0.00.809.494 I ggml_metal_init: using embedded metal library
0.00.815.586 I ggml_metal_init: GPU name:   Apple M4
0.00.815.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.815.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.815.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.815.592 I ggml_metal_init: simdgroup reduction   = true
0.00.815.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.815.593 I ggml_metal_init: has residency sets    = true
0.00.815.593 I ggml_metal_init: has bfloat            = true
0.00.815.593 I ggml_metal_init: use bfloat            = true
0.00.815.594 I ggml_metal_init: hasUnifiedMemory      = true
0.00.815.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.832.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.836.047 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.836.056 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.836.111 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.839.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.839.254 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.839.254 I llama_init_from_model: graph nodes  = 967
0.00.839.255 I llama_init_from_model: graph splits = 2
0.00.839.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.839.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.868.901 I 
0.00.868.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.869.002 I perplexity: tokenizing the input ..
0.00.876.243 I perplexity: tokenization took 7.237 ms
0.00.876.261 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.025.235 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.01.026.648 I Final estimate: PPL = 10.0972 +/- 3.20136

0.01.026.670 I llama_perf_context_print:        load time =     852.24 ms
0.01.026.670 I llama_perf_context_print: prompt eval time =     148.08 ms /   128 tokens (    1.16 ms per token,   864.42 tokens per second)
0.01.026.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.026.672 I llama_perf_context_print:       total time =     157.77 ms /   129 tokens
0.01.027.039 I ggml_metal_free: deallocating

real	0m1.056s
user	0m0.088s
sys	0m0.137s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.799 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.727 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.731 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.522 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.307 I llama_model_loader: - type  f32:  194 tensors
0.00.025.307 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.307 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.308 I print_info: file format = GGUF V3 (latest)
0.00.025.308 I print_info: file type   = Q5_1
0.00.025.309 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.158 I load: special tokens cache size = 25
0.00.038.887 I load: token to piece cache size = 0.2984 MB
0.00.038.890 I print_info: arch             = gptneox
0.00.038.890 I print_info: vocab_only       = 0
0.00.038.890 I print_info: n_ctx_train      = 2048
0.00.038.890 I print_info: n_embd           = 2048
0.00.038.891 I print_info: n_layer          = 24
0.00.038.893 I print_info: n_head           = 16
0.00.038.894 I print_info: n_head_kv        = 16
0.00.038.894 I print_info: n_rot            = 32
0.00.038.894 I print_info: n_swa            = 0
0.00.038.895 I print_info: n_embd_head_k    = 128
0.00.038.895 I print_info: n_embd_head_v    = 128
0.00.038.895 I print_info: n_gqa            = 1
0.00.038.896 I print_info: n_embd_k_gqa     = 2048
0.00.038.897 I print_info: n_embd_v_gqa     = 2048
0.00.038.897 I print_info: f_norm_eps       = 1.0e-05
0.00.038.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.900 I print_info: f_logit_scale    = 0.0e+00
0.00.038.900 I print_info: n_ff             = 8192
0.00.038.901 I print_info: n_expert         = 0
0.00.038.901 I print_info: n_expert_used    = 0
0.00.038.901 I print_info: causal attn      = 1
0.00.038.901 I print_info: pooling type     = 0
0.00.038.903 I print_info: rope type        = 2
0.00.038.904 I print_info: rope scaling     = linear
0.00.038.905 I print_info: freq_base_train  = 10000.0
0.00.038.905 I print_info: freq_scale_train = 1
0.00.038.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.906 I print_info: rope_finetuned   = unknown
0.00.038.906 I print_info: ssm_d_conv       = 0
0.00.038.906 I print_info: ssm_d_inner      = 0
0.00.038.906 I print_info: ssm_d_state      = 0
0.00.038.906 I print_info: ssm_dt_rank      = 0
0.00.038.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.907 I print_info: model type       = 1.4B
0.00.038.907 I print_info: model params     = 1.41 B
0.00.038.907 I print_info: general.name     = 1.4B
0.00.038.907 I print_info: vocab type       = BPE
0.00.038.908 I print_info: n_vocab          = 50304
0.00.038.908 I print_info: n_merges         = 50009
0.00.038.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: LF token         = 128 'Ä'
0.00.038.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: max token length = 1024
0.00.672.556 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.570 I load_tensors: offloading output layer to GPU
0.00.672.571 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.608 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.672.610 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.674.168 I llama_init_from_model: n_seq_max     = 1
0.00.674.172 I llama_init_from_model: n_ctx         = 2048
0.00.674.173 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.674.173 I llama_init_from_model: n_batch       = 2048
0.00.674.174 I llama_init_from_model: n_ubatch      = 512
0.00.674.174 I llama_init_from_model: flash_attn    = 0
0.00.674.176 I llama_init_from_model: freq_base     = 10000.0
0.00.674.177 I llama_init_from_model: freq_scale    = 1
0.00.674.183 I ggml_metal_init: allocating
0.00.674.278 I ggml_metal_init: found device: Apple M4
0.00.674.291 I ggml_metal_init: picking default device: Apple M4
0.00.676.074 I ggml_metal_init: using embedded metal library
0.00.682.611 I ggml_metal_init: GPU name:   Apple M4
0.00.682.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.616 I ggml_metal_init: simdgroup reduction   = true
0.00.682.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.617 I ggml_metal_init: has residency sets    = true
0.00.682.617 I ggml_metal_init: has bfloat            = true
0.00.682.617 I ggml_metal_init: use bfloat            = true
0.00.682.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.619 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.524 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.547 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.552 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.702 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.704 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.704 I llama_init_from_model: graph nodes  = 967
0.00.756.704 I llama_init_from_model: graph splits = 2
0.00.756.711 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.846 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.058 I main: llama threadpool init, n_threads = 4
0.00.814.111 I 
0.00.814.134 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.135 I 
0.00.814.306 I sampler seed: 1234
0.00.814.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.330 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.330 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.330 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.664.224 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.664.225 I llama_perf_context_print:        load time =     804.37 ms
0.01.664.226 I llama_perf_context_print: prompt eval time =      51.54 ms /     7 tokens (    7.36 ms per token,   135.81 tokens per second)
0.01.664.226 I llama_perf_context_print:        eval time =     795.49 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.664.227 I llama_perf_context_print:       total time =     851.05 ms /    70 tokens
0.01.664.506 I ggml_metal_free: deallocating

real	0m1.681s
user	0m0.108s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.923 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.253 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.905 I llama_model_loader: - type  f32:  194 tensors
0.00.026.906 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.906 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.907 I print_info: file format = GGUF V3 (latest)
0.00.026.907 I print_info: file type   = Q5_1
0.00.026.911 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.774 I load: special tokens cache size = 25
0.00.040.502 I load: token to piece cache size = 0.2984 MB
0.00.040.505 I print_info: arch             = gptneox
0.00.040.505 I print_info: vocab_only       = 0
0.00.040.505 I print_info: n_ctx_train      = 2048
0.00.040.506 I print_info: n_embd           = 2048
0.00.040.506 I print_info: n_layer          = 24
0.00.040.508 I print_info: n_head           = 16
0.00.040.509 I print_info: n_head_kv        = 16
0.00.040.509 I print_info: n_rot            = 32
0.00.040.510 I print_info: n_swa            = 0
0.00.040.511 I print_info: n_embd_head_k    = 128
0.00.040.511 I print_info: n_embd_head_v    = 128
0.00.040.512 I print_info: n_gqa            = 1
0.00.040.514 I print_info: n_embd_k_gqa     = 2048
0.00.040.515 I print_info: n_embd_v_gqa     = 2048
0.00.040.516 I print_info: f_norm_eps       = 1.0e-05
0.00.040.516 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.517 I print_info: f_logit_scale    = 0.0e+00
0.00.040.517 I print_info: n_ff             = 8192
0.00.040.518 I print_info: n_expert         = 0
0.00.040.518 I print_info: n_expert_used    = 0
0.00.040.518 I print_info: causal attn      = 1
0.00.040.518 I print_info: pooling type     = 0
0.00.040.518 I print_info: rope type        = 2
0.00.040.518 I print_info: rope scaling     = linear
0.00.040.519 I print_info: freq_base_train  = 10000.0
0.00.040.519 I print_info: freq_scale_train = 1
0.00.040.519 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.519 I print_info: rope_finetuned   = unknown
0.00.040.520 I print_info: ssm_d_conv       = 0
0.00.040.520 I print_info: ssm_d_inner      = 0
0.00.040.521 I print_info: ssm_d_state      = 0
0.00.040.521 I print_info: ssm_dt_rank      = 0
0.00.040.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.522 I print_info: model type       = 1.4B
0.00.040.522 I print_info: model params     = 1.41 B
0.00.040.522 I print_info: general.name     = 1.4B
0.00.040.523 I print_info: vocab type       = BPE
0.00.040.523 I print_info: n_vocab          = 50304
0.00.040.523 I print_info: n_merges         = 50009
0.00.040.523 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.523 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.524 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.524 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.524 I print_info: LF token         = 128 'Ä'
0.00.040.525 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.525 I print_info: max token length = 1024
0.00.737.528 I load_tensors: offloading 24 repeating layers to GPU
0.00.737.545 I load_tensors: offloading output layer to GPU
0.00.737.546 I load_tensors: offloaded 25/25 layers to GPU
0.00.737.579 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.737.580 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.738.919 I llama_init_from_model: n_seq_max     = 1
0.00.738.923 I llama_init_from_model: n_ctx         = 128
0.00.738.924 I llama_init_from_model: n_ctx_per_seq = 128
0.00.738.925 I llama_init_from_model: n_batch       = 128
0.00.738.925 I llama_init_from_model: n_ubatch      = 128
0.00.738.925 I llama_init_from_model: flash_attn    = 0
0.00.738.928 I llama_init_from_model: freq_base     = 10000.0
0.00.738.928 I llama_init_from_model: freq_scale    = 1
0.00.738.929 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.738.931 I ggml_metal_init: allocating
0.00.739.028 I ggml_metal_init: found device: Apple M4
0.00.739.054 I ggml_metal_init: picking default device: Apple M4
0.00.740.339 I ggml_metal_init: using embedded metal library
0.00.746.603 I ggml_metal_init: GPU name:   Apple M4
0.00.746.606 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.607 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.608 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.608 I ggml_metal_init: simdgroup reduction   = true
0.00.746.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.609 I ggml_metal_init: has residency sets    = true
0.00.746.609 I ggml_metal_init: has bfloat            = true
0.00.746.610 I ggml_metal_init: use bfloat            = true
0.00.746.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.763.469 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.766.866 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.766.870 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.766.921 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.770.031 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.770.033 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.770.034 I llama_init_from_model: graph nodes  = 967
0.00.770.034 I llama_init_from_model: graph splits = 2
0.00.770.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.770.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.558 I 
0.00.798.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.663 I perplexity: tokenizing the input ..
0.00.806.187 I perplexity: tokenization took 7.52 ms
0.00.806.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.942.367 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.943.715 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.943.730 I llama_perf_context_print:        load time =     789.63 ms
0.00.943.731 I llama_perf_context_print: prompt eval time =     135.27 ms /   128 tokens (    1.06 ms per token,   946.27 tokens per second)
0.00.943.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.943.735 I llama_perf_context_print:       total time =     145.18 ms /   129 tokens
0.00.944.165 I ggml_metal_free: deallocating

real	0m0.957s
user	0m0.078s
sys	0m0.155s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.484 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.223 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.225 I llama_model_loader: - type  f32:  194 tensors
0.00.026.226 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.226 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.226 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.227 I print_info: file format = GGUF V3 (latest)
0.00.026.227 I print_info: file type   = Q2_K - Medium
0.00.026.228 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.505 I load: special tokens cache size = 25
0.00.040.446 I load: token to piece cache size = 0.2984 MB
0.00.040.449 I print_info: arch             = gptneox
0.00.040.449 I print_info: vocab_only       = 0
0.00.040.449 I print_info: n_ctx_train      = 2048
0.00.040.449 I print_info: n_embd           = 2048
0.00.040.450 I print_info: n_layer          = 24
0.00.040.453 I print_info: n_head           = 16
0.00.040.454 I print_info: n_head_kv        = 16
0.00.040.454 I print_info: n_rot            = 32
0.00.040.454 I print_info: n_swa            = 0
0.00.040.454 I print_info: n_embd_head_k    = 128
0.00.040.454 I print_info: n_embd_head_v    = 128
0.00.040.455 I print_info: n_gqa            = 1
0.00.040.456 I print_info: n_embd_k_gqa     = 2048
0.00.040.457 I print_info: n_embd_v_gqa     = 2048
0.00.040.457 I print_info: f_norm_eps       = 1.0e-05
0.00.040.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.458 I print_info: f_logit_scale    = 0.0e+00
0.00.040.459 I print_info: n_ff             = 8192
0.00.040.459 I print_info: n_expert         = 0
0.00.040.459 I print_info: n_expert_used    = 0
0.00.040.460 I print_info: causal attn      = 1
0.00.040.460 I print_info: pooling type     = 0
0.00.040.460 I print_info: rope type        = 2
0.00.040.460 I print_info: rope scaling     = linear
0.00.040.461 I print_info: freq_base_train  = 10000.0
0.00.040.461 I print_info: freq_scale_train = 1
0.00.040.464 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.464 I print_info: rope_finetuned   = unknown
0.00.040.464 I print_info: ssm_d_conv       = 0
0.00.040.464 I print_info: ssm_d_inner      = 0
0.00.040.464 I print_info: ssm_d_state      = 0
0.00.040.464 I print_info: ssm_dt_rank      = 0
0.00.040.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.465 I print_info: model type       = 1.4B
0.00.040.465 I print_info: model params     = 1.41 B
0.00.040.465 I print_info: general.name     = 1.4B
0.00.040.466 I print_info: vocab type       = BPE
0.00.040.466 I print_info: n_vocab          = 50304
0.00.040.466 I print_info: n_merges         = 50009
0.00.040.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.467 I print_info: LF token         = 128 'Ä'
0.00.040.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.468 I print_info: max token length = 1024
0.00.363.408 I load_tensors: offloading 24 repeating layers to GPU
0.00.363.421 I load_tensors: offloading output layer to GPU
0.00.363.422 I load_tensors: offloaded 25/25 layers to GPU
0.00.363.457 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.363.458 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.934 I llama_init_from_model: n_seq_max     = 1
0.00.364.938 I llama_init_from_model: n_ctx         = 2048
0.00.364.939 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.364.939 I llama_init_from_model: n_batch       = 2048
0.00.364.940 I llama_init_from_model: n_ubatch      = 512
0.00.364.940 I llama_init_from_model: flash_attn    = 0
0.00.364.942 I llama_init_from_model: freq_base     = 10000.0
0.00.364.943 I llama_init_from_model: freq_scale    = 1
0.00.364.950 I ggml_metal_init: allocating
0.00.365.054 I ggml_metal_init: found device: Apple M4
0.00.365.070 I ggml_metal_init: picking default device: Apple M4
0.00.366.907 I ggml_metal_init: using embedded metal library
0.00.372.486 I ggml_metal_init: GPU name:   Apple M4
0.00.372.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.506 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.507 I ggml_metal_init: simdgroup reduction   = true
0.00.372.507 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.508 I ggml_metal_init: has residency sets    = true
0.00.372.508 I ggml_metal_init: has bfloat            = true
0.00.372.508 I ggml_metal_init: use bfloat            = true
0.00.372.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.882 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.454.769 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.454.775 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.454.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.459.506 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.459.509 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.459.509 I llama_init_from_model: graph nodes  = 967
0.00.459.509 I llama_init_from_model: graph splits = 2
0.00.459.516 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.459.637 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.459.637 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.517.147 I main: llama threadpool init, n_threads = 4
0.00.517.200 I 
0.00.517.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.229 I 
0.00.517.409 I sampler seed: 1234
0.00.517.414 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.517.436 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.517.436 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.517.436 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.190.670 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.190.671 I llama_perf_context_print:        load time =     505.54 ms
0.01.190.672 I llama_perf_context_print: prompt eval time =      35.48 ms /     7 tokens (    5.07 ms per token,   197.29 tokens per second)
0.01.190.672 I llama_perf_context_print:        eval time =     634.81 ms /    63 runs   (   10.08 ms per token,    99.24 tokens per second)
0.01.190.673 I llama_perf_context_print:       total time =     674.37 ms /    70 tokens
0.01.190.870 I ggml_metal_free: deallocating

real	0m1.209s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.147 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.031.261 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.266 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.266 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.267 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.040.807 I llama_model_loader: - type  f32:  194 tensors
0.00.040.808 I llama_model_loader: - type q2_K:   49 tensors
0.00.040.808 I llama_model_loader: - type q3_K:   48 tensors
0.00.040.808 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.809 I print_info: file format = GGUF V3 (latest)
0.00.040.809 I print_info: file type   = Q2_K - Medium
0.00.040.812 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.050.403 I load: special tokens cache size = 25
0.00.057.964 I load: token to piece cache size = 0.2984 MB
0.00.057.967 I print_info: arch             = gptneox
0.00.057.967 I print_info: vocab_only       = 0
0.00.057.967 I print_info: n_ctx_train      = 2048
0.00.057.967 I print_info: n_embd           = 2048
0.00.057.968 I print_info: n_layer          = 24
0.00.057.971 I print_info: n_head           = 16
0.00.057.972 I print_info: n_head_kv        = 16
0.00.057.972 I print_info: n_rot            = 32
0.00.057.972 I print_info: n_swa            = 0
0.00.057.972 I print_info: n_embd_head_k    = 128
0.00.057.973 I print_info: n_embd_head_v    = 128
0.00.057.973 I print_info: n_gqa            = 1
0.00.057.974 I print_info: n_embd_k_gqa     = 2048
0.00.057.975 I print_info: n_embd_v_gqa     = 2048
0.00.057.975 I print_info: f_norm_eps       = 1.0e-05
0.00.057.976 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.976 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.976 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.976 I print_info: f_logit_scale    = 0.0e+00
0.00.057.977 I print_info: n_ff             = 8192
0.00.057.977 I print_info: n_expert         = 0
0.00.057.978 I print_info: n_expert_used    = 0
0.00.057.978 I print_info: causal attn      = 1
0.00.057.978 I print_info: pooling type     = 0
0.00.057.978 I print_info: rope type        = 2
0.00.057.978 I print_info: rope scaling     = linear
0.00.057.979 I print_info: freq_base_train  = 10000.0
0.00.057.979 I print_info: freq_scale_train = 1
0.00.057.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.979 I print_info: rope_finetuned   = unknown
0.00.057.979 I print_info: ssm_d_conv       = 0
0.00.057.980 I print_info: ssm_d_inner      = 0
0.00.057.980 I print_info: ssm_d_state      = 0
0.00.057.980 I print_info: ssm_dt_rank      = 0
0.00.057.982 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.982 I print_info: model type       = 1.4B
0.00.057.983 I print_info: model params     = 1.41 B
0.00.057.983 I print_info: general.name     = 1.4B
0.00.057.983 I print_info: vocab type       = BPE
0.00.057.983 I print_info: n_vocab          = 50304
0.00.057.984 I print_info: n_merges         = 50009
0.00.057.984 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.984 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.984 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.984 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.985 I print_info: LF token         = 128 'Ä'
0.00.057.985 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.989 I print_info: max token length = 1024
0.00.488.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.488.775 I load_tensors: offloading output layer to GPU
0.00.488.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.488.811 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.488.812 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.490.193 I llama_init_from_model: n_seq_max     = 1
0.00.490.199 I llama_init_from_model: n_ctx         = 128
0.00.490.199 I llama_init_from_model: n_ctx_per_seq = 128
0.00.490.200 I llama_init_from_model: n_batch       = 128
0.00.490.200 I llama_init_from_model: n_ubatch      = 128
0.00.490.200 I llama_init_from_model: flash_attn    = 0
0.00.490.202 I llama_init_from_model: freq_base     = 10000.0
0.00.490.203 I llama_init_from_model: freq_scale    = 1
0.00.490.204 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.490.206 I ggml_metal_init: allocating
0.00.490.278 I ggml_metal_init: found device: Apple M4
0.00.490.292 I ggml_metal_init: picking default device: Apple M4
0.00.491.932 I ggml_metal_init: using embedded metal library
0.00.497.355 I ggml_metal_init: GPU name:   Apple M4
0.00.497.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.497.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.497.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.497.372 I ggml_metal_init: simdgroup reduction   = true
0.00.497.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.497.372 I ggml_metal_init: has residency sets    = true
0.00.497.373 I ggml_metal_init: has bfloat            = true
0.00.497.373 I ggml_metal_init: use bfloat            = true
0.00.497.377 I ggml_metal_init: hasUnifiedMemory      = true
0.00.497.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.519.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.710 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.522.726 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.522.760 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.526.356 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.526.358 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.526.359 I llama_init_from_model: graph nodes  = 967
0.00.526.359 I llama_init_from_model: graph splits = 2
0.00.526.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.526.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.191 I 
0.00.559.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.289 I perplexity: tokenizing the input ..
0.00.566.281 I perplexity: tokenization took 6.989 ms
0.00.566.301 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.948 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.713.287 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.713.314 I llama_perf_context_print:        load time =     538.03 ms
0.00.713.315 I llama_perf_context_print: prompt eval time =     144.87 ms /   128 tokens (    1.13 ms per token,   883.56 tokens per second)
0.00.713.316 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.316 I llama_perf_context_print:       total time =     154.13 ms /   129 tokens
0.00.713.696 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.085s
sys	0m0.090s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.464 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.007 I llama_model_loader: - type  f32:  194 tensors
0.00.025.007 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.007 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.008 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.008 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.008 I print_info: file format = GGUF V3 (latest)
0.00.025.009 I print_info: file type   = Q3_K - Medium
0.00.025.009 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.957 I load: special tokens cache size = 25
0.00.038.871 I load: token to piece cache size = 0.2984 MB
0.00.038.874 I print_info: arch             = gptneox
0.00.038.875 I print_info: vocab_only       = 0
0.00.038.875 I print_info: n_ctx_train      = 2048
0.00.038.875 I print_info: n_embd           = 2048
0.00.038.875 I print_info: n_layer          = 24
0.00.038.878 I print_info: n_head           = 16
0.00.038.879 I print_info: n_head_kv        = 16
0.00.038.879 I print_info: n_rot            = 32
0.00.038.881 I print_info: n_swa            = 0
0.00.038.881 I print_info: n_embd_head_k    = 128
0.00.038.882 I print_info: n_embd_head_v    = 128
0.00.038.882 I print_info: n_gqa            = 1
0.00.038.883 I print_info: n_embd_k_gqa     = 2048
0.00.038.884 I print_info: n_embd_v_gqa     = 2048
0.00.038.884 I print_info: f_norm_eps       = 1.0e-05
0.00.038.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.885 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.885 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.885 I print_info: f_logit_scale    = 0.0e+00
0.00.038.886 I print_info: n_ff             = 8192
0.00.038.886 I print_info: n_expert         = 0
0.00.038.886 I print_info: n_expert_used    = 0
0.00.038.888 I print_info: causal attn      = 1
0.00.038.889 I print_info: pooling type     = 0
0.00.038.889 I print_info: rope type        = 2
0.00.038.889 I print_info: rope scaling     = linear
0.00.038.890 I print_info: freq_base_train  = 10000.0
0.00.038.894 I print_info: freq_scale_train = 1
0.00.038.894 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.894 I print_info: rope_finetuned   = unknown
0.00.038.894 I print_info: ssm_d_conv       = 0
0.00.038.894 I print_info: ssm_d_inner      = 0
0.00.038.895 I print_info: ssm_d_state      = 0
0.00.038.895 I print_info: ssm_dt_rank      = 0
0.00.038.895 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.895 I print_info: model type       = 1.4B
0.00.038.896 I print_info: model params     = 1.41 B
0.00.038.896 I print_info: general.name     = 1.4B
0.00.038.896 I print_info: vocab type       = BPE
0.00.038.896 I print_info: n_vocab          = 50304
0.00.038.897 I print_info: n_merges         = 50009
0.00.038.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.905 I print_info: LF token         = 128 'Ä'
0.00.038.907 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.907 I print_info: max token length = 1024
0.00.438.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.438.268 I load_tensors: offloading output layer to GPU
0.00.438.269 I load_tensors: offloaded 25/25 layers to GPU
0.00.438.303 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.438.304 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.439.648 I llama_init_from_model: n_seq_max     = 1
0.00.439.654 I llama_init_from_model: n_ctx         = 2048
0.00.439.654 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.439.655 I llama_init_from_model: n_batch       = 2048
0.00.439.655 I llama_init_from_model: n_ubatch      = 512
0.00.439.656 I llama_init_from_model: flash_attn    = 0
0.00.439.662 I llama_init_from_model: freq_base     = 10000.0
0.00.439.667 I llama_init_from_model: freq_scale    = 1
0.00.439.669 I ggml_metal_init: allocating
0.00.439.744 I ggml_metal_init: found device: Apple M4
0.00.439.758 I ggml_metal_init: picking default device: Apple M4
0.00.441.566 I ggml_metal_init: using embedded metal library
0.00.447.602 I ggml_metal_init: GPU name:   Apple M4
0.00.447.607 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.609 I ggml_metal_init: simdgroup reduction   = true
0.00.447.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.610 I ggml_metal_init: has residency sets    = true
0.00.447.610 I ggml_metal_init: has bfloat            = true
0.00.447.611 I ggml_metal_init: use bfloat            = true
0.00.447.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.614 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.521.978 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.521.985 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.007 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.526.589 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.526.592 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.526.592 I llama_init_from_model: graph nodes  = 967
0.00.526.592 I llama_init_from_model: graph splits = 2
0.00.526.599 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.526.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.526.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.574 I main: llama threadpool init, n_threads = 4
0.00.584.620 I 
0.00.584.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.646 I 
0.00.584.795 I sampler seed: 1234
0.00.584.799 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.810 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.811 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.330.270 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.330.271 I llama_perf_context_print:        load time =     574.74 ms
0.01.330.272 I llama_perf_context_print: prompt eval time =      49.79 ms /     7 tokens (    7.11 ms per token,   140.59 tokens per second)
0.01.330.272 I llama_perf_context_print:        eval time =     692.70 ms /    63 runs   (   11.00 ms per token,    90.95 tokens per second)
0.01.330.273 I llama_perf_context_print:       total time =     746.61 ms /    70 tokens
0.01.330.514 I ggml_metal_free: deallocating

real	0m1.349s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.676 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.487 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.296 I llama_model_loader: - type  f32:  194 tensors
0.00.024.296 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.297 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.297 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.298 I print_info: file format = GGUF V3 (latest)
0.00.024.298 I print_info: file type   = Q3_K - Medium
0.00.024.299 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.111 I load: special tokens cache size = 25
0.00.038.143 I load: token to piece cache size = 0.2984 MB
0.00.038.146 I print_info: arch             = gptneox
0.00.038.146 I print_info: vocab_only       = 0
0.00.038.146 I print_info: n_ctx_train      = 2048
0.00.038.146 I print_info: n_embd           = 2048
0.00.038.147 I print_info: n_layer          = 24
0.00.038.149 I print_info: n_head           = 16
0.00.038.150 I print_info: n_head_kv        = 16
0.00.038.150 I print_info: n_rot            = 32
0.00.038.150 I print_info: n_swa            = 0
0.00.038.150 I print_info: n_embd_head_k    = 128
0.00.038.150 I print_info: n_embd_head_v    = 128
0.00.038.151 I print_info: n_gqa            = 1
0.00.038.152 I print_info: n_embd_k_gqa     = 2048
0.00.038.153 I print_info: n_embd_v_gqa     = 2048
0.00.038.153 I print_info: f_norm_eps       = 1.0e-05
0.00.038.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.154 I print_info: f_logit_scale    = 0.0e+00
0.00.038.155 I print_info: n_ff             = 8192
0.00.038.155 I print_info: n_expert         = 0
0.00.038.155 I print_info: n_expert_used    = 0
0.00.038.156 I print_info: causal attn      = 1
0.00.038.156 I print_info: pooling type     = 0
0.00.038.156 I print_info: rope type        = 2
0.00.038.156 I print_info: rope scaling     = linear
0.00.038.157 I print_info: freq_base_train  = 10000.0
0.00.038.157 I print_info: freq_scale_train = 1
0.00.038.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.157 I print_info: rope_finetuned   = unknown
0.00.038.157 I print_info: ssm_d_conv       = 0
0.00.038.158 I print_info: ssm_d_inner      = 0
0.00.038.158 I print_info: ssm_d_state      = 0
0.00.038.160 I print_info: ssm_dt_rank      = 0
0.00.038.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.161 I print_info: model type       = 1.4B
0.00.038.161 I print_info: model params     = 1.41 B
0.00.038.161 I print_info: general.name     = 1.4B
0.00.038.162 I print_info: vocab type       = BPE
0.00.038.162 I print_info: n_vocab          = 50304
0.00.038.162 I print_info: n_merges         = 50009
0.00.038.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.163 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.167 I print_info: LF token         = 128 'Ä'
0.00.038.167 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.168 I print_info: max token length = 1024
0.00.437.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.330 I load_tensors: offloading output layer to GPU
0.00.437.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.363 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.364 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.438.874 I llama_init_from_model: n_seq_max     = 1
0.00.438.878 I llama_init_from_model: n_ctx         = 128
0.00.438.879 I llama_init_from_model: n_ctx_per_seq = 128
0.00.438.879 I llama_init_from_model: n_batch       = 128
0.00.438.880 I llama_init_from_model: n_ubatch      = 128
0.00.438.880 I llama_init_from_model: flash_attn    = 0
0.00.438.881 I llama_init_from_model: freq_base     = 10000.0
0.00.438.882 I llama_init_from_model: freq_scale    = 1
0.00.438.882 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.438.884 I ggml_metal_init: allocating
0.00.438.936 I ggml_metal_init: found device: Apple M4
0.00.438.950 I ggml_metal_init: picking default device: Apple M4
0.00.440.817 I ggml_metal_init: using embedded metal library
0.00.447.376 I ggml_metal_init: GPU name:   Apple M4
0.00.447.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.391 I ggml_metal_init: simdgroup reduction   = true
0.00.447.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.391 I ggml_metal_init: has residency sets    = true
0.00.447.391 I ggml_metal_init: has bfloat            = true
0.00.447.392 I ggml_metal_init: use bfloat            = true
0.00.447.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.719 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.471.329 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.471.336 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.471.368 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.651 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.474.653 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.474.654 I llama_init_from_model: graph nodes  = 967
0.00.474.654 I llama_init_from_model: graph splits = 2
0.00.474.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.474.658 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.656 I 
0.00.502.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.750 I perplexity: tokenizing the input ..
0.00.509.782 I perplexity: tokenization took 7.031 ms
0.00.509.802 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.200 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.644.696 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.704 I llama_perf_context_print:        load time =     493.98 ms
0.00.644.707 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.54 tokens per second)
0.00.644.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.708 I llama_perf_context_print:       total time =     142.05 ms /   129 tokens
0.00.645.107 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.079s
sys	0m0.108s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.030 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.438 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.451 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.112 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.112 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.113 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.113 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.113 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.114 I llama_model_loader: - type  f32:  194 tensors
0.00.027.114 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.114 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.114 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.115 I print_info: file format = GGUF V3 (latest)
0.00.027.115 I print_info: file type   = Q4_K - Medium
0.00.027.116 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.025 I load: special tokens cache size = 25
0.00.040.828 I load: token to piece cache size = 0.2984 MB
0.00.040.831 I print_info: arch             = gptneox
0.00.040.831 I print_info: vocab_only       = 0
0.00.040.831 I print_info: n_ctx_train      = 2048
0.00.040.831 I print_info: n_embd           = 2048
0.00.040.831 I print_info: n_layer          = 24
0.00.040.834 I print_info: n_head           = 16
0.00.040.835 I print_info: n_head_kv        = 16
0.00.040.835 I print_info: n_rot            = 32
0.00.040.835 I print_info: n_swa            = 0
0.00.040.835 I print_info: n_embd_head_k    = 128
0.00.040.837 I print_info: n_embd_head_v    = 128
0.00.040.838 I print_info: n_gqa            = 1
0.00.040.839 I print_info: n_embd_k_gqa     = 2048
0.00.040.840 I print_info: n_embd_v_gqa     = 2048
0.00.040.840 I print_info: f_norm_eps       = 1.0e-05
0.00.040.840 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.841 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.841 I print_info: f_logit_scale    = 0.0e+00
0.00.040.842 I print_info: n_ff             = 8192
0.00.040.842 I print_info: n_expert         = 0
0.00.040.842 I print_info: n_expert_used    = 0
0.00.040.842 I print_info: causal attn      = 1
0.00.040.844 I print_info: pooling type     = 0
0.00.040.846 I print_info: rope type        = 2
0.00.040.846 I print_info: rope scaling     = linear
0.00.040.846 I print_info: freq_base_train  = 10000.0
0.00.040.847 I print_info: freq_scale_train = 1
0.00.040.847 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.847 I print_info: rope_finetuned   = unknown
0.00.040.847 I print_info: ssm_d_conv       = 0
0.00.040.847 I print_info: ssm_d_inner      = 0
0.00.040.848 I print_info: ssm_d_state      = 0
0.00.040.848 I print_info: ssm_dt_rank      = 0
0.00.040.848 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.851 I print_info: model type       = 1.4B
0.00.040.852 I print_info: model params     = 1.41 B
0.00.040.852 I print_info: general.name     = 1.4B
0.00.040.853 I print_info: vocab type       = BPE
0.00.040.853 I print_info: n_vocab          = 50304
0.00.040.853 I print_info: n_merges         = 50009
0.00.040.853 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.853 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.854 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.854 I print_info: LF token         = 128 'Ä'
0.00.040.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.854 I print_info: max token length = 1024
0.00.534.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.302 I load_tensors: offloading output layer to GPU
0.00.534.303 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.337 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.338 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.535.772 I llama_init_from_model: n_seq_max     = 1
0.00.535.777 I llama_init_from_model: n_ctx         = 2048
0.00.535.777 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.535.778 I llama_init_from_model: n_batch       = 2048
0.00.535.778 I llama_init_from_model: n_ubatch      = 512
0.00.535.779 I llama_init_from_model: flash_attn    = 0
0.00.535.781 I llama_init_from_model: freq_base     = 10000.0
0.00.535.786 I llama_init_from_model: freq_scale    = 1
0.00.535.791 I ggml_metal_init: allocating
0.00.535.874 I ggml_metal_init: found device: Apple M4
0.00.535.888 I ggml_metal_init: picking default device: Apple M4
0.00.537.718 I ggml_metal_init: using embedded metal library
0.00.544.283 I ggml_metal_init: GPU name:   Apple M4
0.00.544.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.544.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.544.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.544.290 I ggml_metal_init: simdgroup reduction   = true
0.00.544.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.544.290 I ggml_metal_init: has residency sets    = true
0.00.544.290 I ggml_metal_init: has bfloat            = true
0.00.544.291 I ggml_metal_init: use bfloat            = true
0.00.544.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.544.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.305 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.882 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.618.888 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.618.912 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.634 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.623.636 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.623.637 I llama_init_from_model: graph nodes  = 967
0.00.623.637 I llama_init_from_model: graph splits = 2
0.00.623.643 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.623.756 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.623.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.763 I main: llama threadpool init, n_threads = 4
0.00.683.806 I 
0.00.683.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.830 I 
0.00.684.006 I sampler seed: 1234
0.00.684.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.022 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.023 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.446.697 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.446.698 I llama_perf_context_print:        load time =     671.85 ms
0.01.446.699 I llama_perf_context_print: prompt eval time =      57.45 ms /     7 tokens (    8.21 ms per token,   121.85 tokens per second)
0.01.446.699 I llama_perf_context_print:        eval time =     702.22 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.446.700 I llama_perf_context_print:       total time =     763.82 ms /    70 tokens
0.01.446.914 I ggml_metal_free: deallocating

real	0m1.463s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.825 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.840 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.662 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.662 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.663 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.663 I llama_model_loader: - type  f32:  194 tensors
0.00.024.664 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.664 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.664 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.665 I print_info: file format = GGUF V3 (latest)
0.00.024.665 I print_info: file type   = Q4_K - Medium
0.00.024.666 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.912 I load: special tokens cache size = 25
0.00.038.860 I load: token to piece cache size = 0.2984 MB
0.00.038.863 I print_info: arch             = gptneox
0.00.038.863 I print_info: vocab_only       = 0
0.00.038.864 I print_info: n_ctx_train      = 2048
0.00.038.864 I print_info: n_embd           = 2048
0.00.038.864 I print_info: n_layer          = 24
0.00.038.867 I print_info: n_head           = 16
0.00.038.868 I print_info: n_head_kv        = 16
0.00.038.868 I print_info: n_rot            = 32
0.00.038.868 I print_info: n_swa            = 0
0.00.038.868 I print_info: n_embd_head_k    = 128
0.00.038.869 I print_info: n_embd_head_v    = 128
0.00.038.871 I print_info: n_gqa            = 1
0.00.038.872 I print_info: n_embd_k_gqa     = 2048
0.00.038.873 I print_info: n_embd_v_gqa     = 2048
0.00.038.874 I print_info: f_norm_eps       = 1.0e-05
0.00.038.874 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.874 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.875 I print_info: f_logit_scale    = 0.0e+00
0.00.038.875 I print_info: n_ff             = 8192
0.00.038.876 I print_info: n_expert         = 0
0.00.038.876 I print_info: n_expert_used    = 0
0.00.038.876 I print_info: causal attn      = 1
0.00.038.876 I print_info: pooling type     = 0
0.00.038.876 I print_info: rope type        = 2
0.00.038.877 I print_info: rope scaling     = linear
0.00.038.877 I print_info: freq_base_train  = 10000.0
0.00.038.877 I print_info: freq_scale_train = 1
0.00.038.877 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.878 I print_info: rope_finetuned   = unknown
0.00.038.880 I print_info: ssm_d_conv       = 0
0.00.038.880 I print_info: ssm_d_inner      = 0
0.00.038.880 I print_info: ssm_d_state      = 0
0.00.038.880 I print_info: ssm_dt_rank      = 0
0.00.038.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.880 I print_info: model type       = 1.4B
0.00.038.881 I print_info: model params     = 1.41 B
0.00.038.881 I print_info: general.name     = 1.4B
0.00.038.882 I print_info: vocab type       = BPE
0.00.038.882 I print_info: n_vocab          = 50304
0.00.038.882 I print_info: n_merges         = 50009
0.00.038.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.882 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.883 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.883 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.883 I print_info: LF token         = 128 'Ä'
0.00.038.883 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.883 I print_info: max token length = 1024
0.00.519.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.757 I load_tensors: offloading output layer to GPU
0.00.519.757 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.791 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.793 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.521.352 I llama_init_from_model: n_seq_max     = 1
0.00.521.358 I llama_init_from_model: n_ctx         = 128
0.00.521.358 I llama_init_from_model: n_ctx_per_seq = 128
0.00.521.359 I llama_init_from_model: n_batch       = 128
0.00.521.359 I llama_init_from_model: n_ubatch      = 128
0.00.521.360 I llama_init_from_model: flash_attn    = 0
0.00.521.362 I llama_init_from_model: freq_base     = 10000.0
0.00.521.363 I llama_init_from_model: freq_scale    = 1
0.00.521.363 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.521.366 I ggml_metal_init: allocating
0.00.521.437 I ggml_metal_init: found device: Apple M4
0.00.521.451 I ggml_metal_init: picking default device: Apple M4
0.00.523.233 I ggml_metal_init: using embedded metal library
0.00.529.753 I ggml_metal_init: GPU name:   Apple M4
0.00.529.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.759 I ggml_metal_init: simdgroup reduction   = true
0.00.529.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.760 I ggml_metal_init: has residency sets    = true
0.00.529.760 I ggml_metal_init: has bfloat            = true
0.00.529.760 I ggml_metal_init: use bfloat            = true
0.00.529.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.806 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.197 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.550.200 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.245 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.553.368 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.553.370 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.553.370 I llama_init_from_model: graph nodes  = 967
0.00.553.371 I llama_init_from_model: graph splits = 2
0.00.553.374 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.553.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.352 I 
0.00.584.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.455 I perplexity: tokenizing the input ..
0.00.592.174 I perplexity: tokenization took 7.714 ms
0.00.592.202 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.006 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.741.352 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.741.365 I llama_perf_context_print:        load time =     575.47 ms
0.00.741.366 I llama_perf_context_print: prompt eval time =     146.91 ms /   128 tokens (    1.15 ms per token,   871.29 tokens per second)
0.00.741.367 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.367 I llama_perf_context_print:       total time =     157.02 ms /   129 tokens
0.00.741.736 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.080s
sys	0m0.126s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.785 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.525 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.526 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.526 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.527 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.528 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.528 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.529 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.529 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.530 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.530 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.165 I llama_model_loader: - type  f32:  194 tensors
0.00.026.165 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.165 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.166 I print_info: file format = GGUF V3 (latest)
0.00.026.166 I print_info: file type   = Q5_K - Medium
0.00.026.167 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.009 I load: special tokens cache size = 25
0.00.039.908 I load: token to piece cache size = 0.2984 MB
0.00.039.911 I print_info: arch             = gptneox
0.00.039.911 I print_info: vocab_only       = 0
0.00.039.912 I print_info: n_ctx_train      = 2048
0.00.039.912 I print_info: n_embd           = 2048
0.00.039.912 I print_info: n_layer          = 24
0.00.039.914 I print_info: n_head           = 16
0.00.039.915 I print_info: n_head_kv        = 16
0.00.039.915 I print_info: n_rot            = 32
0.00.039.916 I print_info: n_swa            = 0
0.00.039.916 I print_info: n_embd_head_k    = 128
0.00.039.916 I print_info: n_embd_head_v    = 128
0.00.039.917 I print_info: n_gqa            = 1
0.00.039.918 I print_info: n_embd_k_gqa     = 2048
0.00.039.918 I print_info: n_embd_v_gqa     = 2048
0.00.039.919 I print_info: f_norm_eps       = 1.0e-05
0.00.039.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.922 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.922 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.922 I print_info: f_logit_scale    = 0.0e+00
0.00.039.923 I print_info: n_ff             = 8192
0.00.039.923 I print_info: n_expert         = 0
0.00.039.924 I print_info: n_expert_used    = 0
0.00.039.925 I print_info: causal attn      = 1
0.00.039.925 I print_info: pooling type     = 0
0.00.039.926 I print_info: rope type        = 2
0.00.039.927 I print_info: rope scaling     = linear
0.00.039.927 I print_info: freq_base_train  = 10000.0
0.00.039.927 I print_info: freq_scale_train = 1
0.00.039.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.928 I print_info: rope_finetuned   = unknown
0.00.039.928 I print_info: ssm_d_conv       = 0
0.00.039.928 I print_info: ssm_d_inner      = 0
0.00.039.928 I print_info: ssm_d_state      = 0
0.00.039.928 I print_info: ssm_dt_rank      = 0
0.00.039.929 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.929 I print_info: model type       = 1.4B
0.00.039.929 I print_info: model params     = 1.41 B
0.00.039.933 I print_info: general.name     = 1.4B
0.00.039.934 I print_info: vocab type       = BPE
0.00.039.934 I print_info: n_vocab          = 50304
0.00.039.934 I print_info: n_merges         = 50009
0.00.039.934 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.934 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.934 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.935 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.935 I print_info: LF token         = 128 'Ä'
0.00.039.935 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.936 I print_info: max token length = 1024
0.00.611.463 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.476 I load_tensors: offloading output layer to GPU
0.00.611.477 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.513 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.611.518 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.612.995 I llama_init_from_model: n_seq_max     = 1
0.00.613.000 I llama_init_from_model: n_ctx         = 2048
0.00.613.000 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.613.001 I llama_init_from_model: n_batch       = 2048
0.00.613.001 I llama_init_from_model: n_ubatch      = 512
0.00.613.001 I llama_init_from_model: flash_attn    = 0
0.00.613.005 I llama_init_from_model: freq_base     = 10000.0
0.00.613.006 I llama_init_from_model: freq_scale    = 1
0.00.613.013 I ggml_metal_init: allocating
0.00.613.095 I ggml_metal_init: found device: Apple M4
0.00.613.108 I ggml_metal_init: picking default device: Apple M4
0.00.614.917 I ggml_metal_init: using embedded metal library
0.00.621.385 I ggml_metal_init: GPU name:   Apple M4
0.00.621.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.390 I ggml_metal_init: simdgroup reduction   = true
0.00.621.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.391 I ggml_metal_init: has residency sets    = true
0.00.621.391 I ggml_metal_init: has bfloat            = true
0.00.621.391 I ggml_metal_init: use bfloat            = true
0.00.621.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.354 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.360 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.384 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.889 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.701.891 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.701.891 I llama_init_from_model: graph nodes  = 967
0.00.701.892 I llama_init_from_model: graph splits = 2
0.00.701.898 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.909 I main: llama threadpool init, n_threads = 4
0.00.764.951 I 
0.00.764.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.975 I 
0.00.765.126 I sampler seed: 1234
0.00.765.130 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.141 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.142 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.606.460 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.01.606.461 I llama_perf_context_print:        load time =     755.24 ms
0.01.606.462 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.63 tokens per second)
0.01.606.462 I llama_perf_context_print:        eval time =     787.25 ms /    63 runs   (   12.50 ms per token,    80.03 tokens per second)
0.01.606.463 I llama_perf_context_print:       total time =     842.44 ms /    70 tokens
0.01.606.735 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.085 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.277 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.280 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.281 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.828 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.829 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.829 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.830 I print_info: file type   = Q5_K - Medium
0.00.025.831 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.011 I load: special tokens cache size = 25
0.00.040.036 I load: token to piece cache size = 0.2984 MB
0.00.040.038 I print_info: arch             = gptneox
0.00.040.039 I print_info: vocab_only       = 0
0.00.040.039 I print_info: n_ctx_train      = 2048
0.00.040.039 I print_info: n_embd           = 2048
0.00.040.039 I print_info: n_layer          = 24
0.00.040.042 I print_info: n_head           = 16
0.00.040.043 I print_info: n_head_kv        = 16
0.00.040.043 I print_info: n_rot            = 32
0.00.040.043 I print_info: n_swa            = 0
0.00.040.043 I print_info: n_embd_head_k    = 128
0.00.040.045 I print_info: n_embd_head_v    = 128
0.00.040.046 I print_info: n_gqa            = 1
0.00.040.047 I print_info: n_embd_k_gqa     = 2048
0.00.040.048 I print_info: n_embd_v_gqa     = 2048
0.00.040.048 I print_info: f_norm_eps       = 1.0e-05
0.00.040.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.049 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.049 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.049 I print_info: f_logit_scale    = 0.0e+00
0.00.040.050 I print_info: n_ff             = 8192
0.00.040.050 I print_info: n_expert         = 0
0.00.040.050 I print_info: n_expert_used    = 0
0.00.040.050 I print_info: causal attn      = 1
0.00.040.050 I print_info: pooling type     = 0
0.00.040.050 I print_info: rope type        = 2
0.00.040.050 I print_info: rope scaling     = linear
0.00.040.055 I print_info: freq_base_train  = 10000.0
0.00.040.055 I print_info: freq_scale_train = 1
0.00.040.055 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.055 I print_info: rope_finetuned   = unknown
0.00.040.055 I print_info: ssm_d_conv       = 0
0.00.040.056 I print_info: ssm_d_inner      = 0
0.00.040.056 I print_info: ssm_d_state      = 0
0.00.040.056 I print_info: ssm_dt_rank      = 0
0.00.040.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.056 I print_info: model type       = 1.4B
0.00.040.057 I print_info: model params     = 1.41 B
0.00.040.057 I print_info: general.name     = 1.4B
0.00.040.057 I print_info: vocab type       = BPE
0.00.040.058 I print_info: n_vocab          = 50304
0.00.040.058 I print_info: n_merges         = 50009
0.00.040.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: LF token         = 128 'Ä'
0.00.040.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.062 I print_info: max token length = 1024
0.00.586.115 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.131 I load_tensors: offloading output layer to GPU
0.00.586.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.165 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.166 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.587.731 I llama_init_from_model: n_seq_max     = 1
0.00.587.734 I llama_init_from_model: n_ctx         = 128
0.00.587.734 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.735 I llama_init_from_model: n_batch       = 128
0.00.587.735 I llama_init_from_model: n_ubatch      = 128
0.00.587.735 I llama_init_from_model: flash_attn    = 0
0.00.587.736 I llama_init_from_model: freq_base     = 10000.0
0.00.587.737 I llama_init_from_model: freq_scale    = 1
0.00.587.738 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.739 I ggml_metal_init: allocating
0.00.587.754 I ggml_metal_init: found device: Apple M4
0.00.587.763 I ggml_metal_init: picking default device: Apple M4
0.00.589.054 I ggml_metal_init: using embedded metal library
0.00.595.361 I ggml_metal_init: GPU name:   Apple M4
0.00.595.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.365 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.370 I ggml_metal_init: simdgroup reduction   = true
0.00.595.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.371 I ggml_metal_init: has residency sets    = true
0.00.595.371 I ggml_metal_init: has bfloat            = true
0.00.595.371 I ggml_metal_init: use bfloat            = true
0.00.595.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.374 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.941 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.499 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.502 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.529 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.719 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.618.721 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.618.721 I llama_init_from_model: graph nodes  = 967
0.00.618.722 I llama_init_from_model: graph splits = 2
0.00.618.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.952 I 
0.00.652.035 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.054 I perplexity: tokenizing the input ..
0.00.659.128 I perplexity: tokenization took 7.071 ms
0.00.659.149 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.575 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.898 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.913 I llama_perf_context_print:        load time =     641.86 ms
0.00.801.915 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.76 tokens per second)
0.00.801.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.916 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.802.277 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.078s
sys	0m0.132s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.176 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.189 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.190 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.196 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.197 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.197 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.871 I llama_model_loader: - type  f32:  194 tensors
0.00.026.871 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.872 I print_info: file format = GGUF V3 (latest)
0.00.026.873 I print_info: file type   = Q6_K
0.00.026.873 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.798 I load: special tokens cache size = 25
0.00.040.708 I load: token to piece cache size = 0.2984 MB
0.00.040.711 I print_info: arch             = gptneox
0.00.040.711 I print_info: vocab_only       = 0
0.00.040.712 I print_info: n_ctx_train      = 2048
0.00.040.712 I print_info: n_embd           = 2048
0.00.040.712 I print_info: n_layer          = 24
0.00.040.715 I print_info: n_head           = 16
0.00.040.716 I print_info: n_head_kv        = 16
0.00.040.716 I print_info: n_rot            = 32
0.00.040.716 I print_info: n_swa            = 0
0.00.040.716 I print_info: n_embd_head_k    = 128
0.00.040.717 I print_info: n_embd_head_v    = 128
0.00.040.717 I print_info: n_gqa            = 1
0.00.040.718 I print_info: n_embd_k_gqa     = 2048
0.00.040.719 I print_info: n_embd_v_gqa     = 2048
0.00.040.719 I print_info: f_norm_eps       = 1.0e-05
0.00.040.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.720 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.720 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.720 I print_info: f_logit_scale    = 0.0e+00
0.00.040.721 I print_info: n_ff             = 8192
0.00.040.721 I print_info: n_expert         = 0
0.00.040.721 I print_info: n_expert_used    = 0
0.00.040.721 I print_info: causal attn      = 1
0.00.040.721 I print_info: pooling type     = 0
0.00.040.721 I print_info: rope type        = 2
0.00.040.725 I print_info: rope scaling     = linear
0.00.040.726 I print_info: freq_base_train  = 10000.0
0.00.040.726 I print_info: freq_scale_train = 1
0.00.040.726 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.726 I print_info: rope_finetuned   = unknown
0.00.040.727 I print_info: ssm_d_conv       = 0
0.00.040.727 I print_info: ssm_d_inner      = 0
0.00.040.727 I print_info: ssm_d_state      = 0
0.00.040.727 I print_info: ssm_dt_rank      = 0
0.00.040.727 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.728 I print_info: model type       = 1.4B
0.00.040.728 I print_info: model params     = 1.41 B
0.00.040.728 I print_info: general.name     = 1.4B
0.00.040.728 I print_info: vocab type       = BPE
0.00.040.729 I print_info: n_vocab          = 50304
0.00.040.729 I print_info: n_merges         = 50009
0.00.040.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.731 I print_info: LF token         = 128 'Ä'
0.00.040.732 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.732 I print_info: max token length = 1024
0.00.653.294 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.298 I load_tensors: offloading output layer to GPU
0.00.653.300 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.322 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.653.323 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.654.490 I llama_init_from_model: n_seq_max     = 1
0.00.654.492 I llama_init_from_model: n_ctx         = 2048
0.00.654.493 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.493 I llama_init_from_model: n_batch       = 2048
0.00.654.494 I llama_init_from_model: n_ubatch      = 512
0.00.654.494 I llama_init_from_model: flash_attn    = 0
0.00.654.495 I llama_init_from_model: freq_base     = 10000.0
0.00.654.495 I llama_init_from_model: freq_scale    = 1
0.00.654.496 I ggml_metal_init: allocating
0.00.654.510 I ggml_metal_init: found device: Apple M4
0.00.654.520 I ggml_metal_init: picking default device: Apple M4
0.00.656.016 I ggml_metal_init: using embedded metal library
0.00.661.912 I ggml_metal_init: GPU name:   Apple M4
0.00.661.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.917 I ggml_metal_init: simdgroup reduction   = true
0.00.661.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.918 I ggml_metal_init: has residency sets    = true
0.00.661.918 I ggml_metal_init: has bfloat            = true
0.00.661.918 I ggml_metal_init: use bfloat            = true
0.00.661.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.162 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.363 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.369 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.392 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.733.914 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.733.916 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.733.916 I llama_init_from_model: graph nodes  = 967
0.00.733.917 I llama_init_from_model: graph splits = 2
0.00.733.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.532 I main: llama threadpool init, n_threads = 4
0.00.796.584 I 
0.00.796.609 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.611 I 
0.00.796.780 I sampler seed: 1234
0.00.796.784 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.832 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.671.677 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.671.677 I llama_perf_context_print:        load time =     785.49 ms
0.01.671.678 I llama_perf_context_print: prompt eval time =      54.35 ms /     7 tokens (    7.76 ms per token,   128.79 tokens per second)
0.01.671.679 I llama_perf_context_print:        eval time =     817.66 ms /    63 runs   (   12.98 ms per token,    77.05 tokens per second)
0.01.671.679 I llama_perf_context_print:       total time =     876.01 ms /    70 tokens
0.01.671.938 I ggml_metal_free: deallocating

real	0m1.689s
user	0m0.107s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4567 (d6d24cd9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.203 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.494 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.496 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.497 I llama_model_loader: - type  f32:  194 tensors
0.00.024.497 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.498 I print_info: file format = GGUF V3 (latest)
0.00.024.498 I print_info: file type   = Q6_K
0.00.024.499 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.333 I load: special tokens cache size = 25
0.00.038.317 I load: token to piece cache size = 0.2984 MB
0.00.038.320 I print_info: arch             = gptneox
0.00.038.320 I print_info: vocab_only       = 0
0.00.038.320 I print_info: n_ctx_train      = 2048
0.00.038.320 I print_info: n_embd           = 2048
0.00.038.321 I print_info: n_layer          = 24
0.00.038.323 I print_info: n_head           = 16
0.00.038.324 I print_info: n_head_kv        = 16
0.00.038.324 I print_info: n_rot            = 32
0.00.038.324 I print_info: n_swa            = 0
0.00.038.325 I print_info: n_embd_head_k    = 128
0.00.038.325 I print_info: n_embd_head_v    = 128
0.00.038.326 I print_info: n_gqa            = 1
0.00.038.329 I print_info: n_embd_k_gqa     = 2048
0.00.038.329 I print_info: n_embd_v_gqa     = 2048
0.00.038.330 I print_info: f_norm_eps       = 1.0e-05
0.00.038.330 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.332 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.333 I print_info: f_logit_scale    = 0.0e+00
0.00.038.333 I print_info: n_ff             = 8192
0.00.038.334 I print_info: n_expert         = 0
0.00.038.334 I print_info: n_expert_used    = 0
0.00.038.334 I print_info: causal attn      = 1
0.00.038.334 I print_info: pooling type     = 0
0.00.038.334 I print_info: rope type        = 2
0.00.038.334 I print_info: rope scaling     = linear
0.00.038.335 I print_info: freq_base_train  = 10000.0
0.00.038.335 I print_info: freq_scale_train = 1
0.00.038.335 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.335 I print_info: rope_finetuned   = unknown
0.00.038.335 I print_info: ssm_d_conv       = 0
0.00.038.336 I print_info: ssm_d_inner      = 0
0.00.038.336 I print_info: ssm_d_state      = 0
0.00.038.336 I print_info: ssm_dt_rank      = 0
0.00.038.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.336 I print_info: model type       = 1.4B
0.00.038.336 I print_info: model params     = 1.41 B
0.00.038.340 I print_info: general.name     = 1.4B
0.00.038.341 I print_info: vocab type       = BPE
0.00.038.341 I print_info: n_vocab          = 50304
0.00.038.341 I print_info: n_merges         = 50009
0.00.038.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.342 I print_info: LF token         = 128 'Ä'
0.00.038.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: max token length = 1024
0.00.423.155 I load_tensors: offloading 24 repeating layers to GPU
0.00.423.159 I load_tensors: offloading output layer to GPU
0.00.423.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.423.183 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.423.184 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.424.454 I llama_init_from_model: n_seq_max     = 1
0.00.424.457 I llama_init_from_model: n_ctx         = 128
0.00.424.457 I llama_init_from_model: n_ctx_per_seq = 128
0.00.424.458 I llama_init_from_model: n_batch       = 128
0.00.424.458 I llama_init_from_model: n_ubatch      = 128
0.00.424.458 I llama_init_from_model: flash_attn    = 0
0.00.424.459 I llama_init_from_model: freq_base     = 10000.0
0.00.424.460 I llama_init_from_model: freq_scale    = 1
0.00.424.461 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.424.462 I ggml_metal_init: allocating
0.00.424.490 I ggml_metal_init: found device: Apple M4
0.00.424.503 I ggml_metal_init: picking default device: Apple M4
0.00.425.891 I ggml_metal_init: using embedded metal library
0.00.432.063 I ggml_metal_init: GPU name:   Apple M4
0.00.432.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.432.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.432.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.432.069 I ggml_metal_init: simdgroup reduction   = true
0.00.432.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.432.070 I ggml_metal_init: has residency sets    = true
0.00.432.070 I ggml_metal_init: has bfloat            = true
0.00.432.070 I ggml_metal_init: use bfloat            = true
0.00.432.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.432.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.448.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.452.088 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.452.091 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.452.112 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.455.167 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.455.169 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.455.169 I llama_init_from_model: graph nodes  = 967
0.00.455.169 I llama_init_from_model: graph splits = 2
0.00.455.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.455.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.792 I 
0.00.488.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.488.895 I perplexity: tokenizing the input ..
0.00.495.780 I perplexity: tokenization took 6.881 ms
0.00.495.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.710 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.637.042 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.637.057 I llama_perf_context_print:        load time =     479.58 ms
0.00.637.058 I llama_perf_context_print: prompt eval time =     138.98 ms /   128 tokens (    1.09 ms per token,   920.98 tokens per second)
0.00.637.059 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.060 I llama_perf_context_print:       total time =     148.27 ms /   129 tokens
0.00.637.487 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.077s
sys	0m0.119s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4567 (d6d24cd9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ae04a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ae05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ae05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ae05cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ae06270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ae06820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ae06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ae07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ae07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ae07e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ae08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ae08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ae09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ae09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ae0a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ae0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ae0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ae0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ae0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ae0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ae0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ae0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ae0dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ae0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ae0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ae0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ae0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ae101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ae10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ae109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ae10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ae11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ae119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ae11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ae121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ae12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ae12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ae12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ae13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ae138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ae13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ae14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ae146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ae14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ae14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ae15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ae15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ae16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ae16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ae16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ae17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ae17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ae181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ae187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ae18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ae19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ae198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ae19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ae1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ae1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ae1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ae1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ae1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ae1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ae1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ae1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ae1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ae1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ae1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ae1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ae1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ae1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ae1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ae1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ae1ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ae1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ae1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ae1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ae203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ae20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ae20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ae213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ae21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ae21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ae223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ae228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ae22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ae23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ae238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ae23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ae24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ae248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ae24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ae25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ae258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ae25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ae26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ae16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ae267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ae26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ae274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ae27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ae27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ae284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ae28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ae28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ae294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ae29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ae29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ae2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ae2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ae2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ae2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ae2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ae2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ae2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ae2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ae2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ae2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ae2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ae2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ae2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ae2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ae2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ae2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ae2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ae2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ae2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ae2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ae30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ae307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ae30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ae31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ae315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ae31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ae31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ae32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ae32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ae32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ae33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ae33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ae33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ae33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ae343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ae34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ae34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ae351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ae35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ae35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ae35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ae36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ae368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ae36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ae37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ae376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ae37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ae38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ae384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ae38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ae38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ae39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ae39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ae39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ae3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ae3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ae3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ae3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ae3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ae3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ae3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ae3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ae3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ae3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ae3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ae3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ae3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ae3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ae3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ae3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ae3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ae3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ae3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ae3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ae3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ae40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ae40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ae40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ae40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ae41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ae418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ae41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ae421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ae42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ae42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ae43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ae43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ae43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ae43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ae444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ae44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ae450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ae458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ae45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ae46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ae46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ae46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ae47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ae478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ae47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ae48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ae489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ae48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ae49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ae499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ae49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ae4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ae4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ae4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ae4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ae4b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ae4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ae4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ae4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ae4cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ae4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ae4d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ae4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ae4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ae4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ae4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ae4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ae4f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ae4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ae503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ae50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ae50e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ae513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ae51920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ae51e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ae523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ae52910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ae52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ae533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ae53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ae53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ae543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ae548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ae54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ae55390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ae558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ae55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ae56380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ae568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ae56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ae57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ae578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ae57e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ae58360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ae588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ae58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ae59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ae598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ae59df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ae5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ae5a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ae5ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ae5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ae5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ae5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ae5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ae5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ae5ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ae5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ae5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ae5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ae5dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ae5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ae5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ae5eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ae5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ae5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ae5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ae5fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ae60500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ae60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ae61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ae61a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ae61d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ae62510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ae627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ae62de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.677.247 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ae05220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ae05690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ae05b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ae05f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ae063e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ae06850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ae06cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ae07130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ae075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ae07a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ae07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ae08540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ae09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ae09810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ae0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ae0a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ae0ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ae0b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ae0bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ae0c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ae0cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ae0d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ae0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ae0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ae0e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ae0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ae0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ae0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ae0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ae0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ae0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ae10480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ae108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ae10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ae11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ae11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ae11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ae11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ae121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ae12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ae12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ae12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ae133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ae13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ae13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ae140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ae14560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ae149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ae14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ae152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ae15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ae15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ae16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ae16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ae168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ae16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ae172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ae177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ae17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ae180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ae18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ae18980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ae18df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ae19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ae196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ae19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ae19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ae1a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ae1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ae1ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ae1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ae1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ae1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ae1bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ae1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ae1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ae1cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ae1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ae1d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ae1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ae1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ae1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ae1e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ae1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ae1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ae1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ae1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ae1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ae20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ae205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ae20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ae20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ae21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ae21780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ae21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ae22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ae224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ae22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ae22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ae23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ae23690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ae23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ae23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ae243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ae24850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ae24cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ae25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ae255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ae25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ae25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ae262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ae26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ae26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ae27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ae274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ae27920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ae27d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ae28200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ae28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ae28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ae28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ae293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ae29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ae29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ae2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ae2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ae2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ae2ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ae2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ae2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ae2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ae2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ae2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ae2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ae2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ae2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ae2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ae2dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ae2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ae2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ae2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ae2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ae2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ae2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ae2f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ae2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ae302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ae30720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ae30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ae31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ae31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ae318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ae31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ae321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ae32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ae32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ae32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ae33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ae337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ae33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ae340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ae34540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ae349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ae34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ae35290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ae35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ae36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ae365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ae368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ae36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ae37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ae37600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ae37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ae37ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ae38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ae387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ae38c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ae390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ae39510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ae39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ae39df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ae3a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ae3a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ae3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ae3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ae3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ae3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ae3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ae3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ae3c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ae3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ae3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ae3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ae3d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ae3dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ae3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ae3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ae3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ae3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ae3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ae3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ae3fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ae40080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ae40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ae40a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ae40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ae412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ae41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ae41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ae42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ae42cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ae42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ae43570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ae43b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ae440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ae446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ae44c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ae45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ae457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ae45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ae46370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ae46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ae46ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ae474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ae47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ae48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ae485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ae48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ae49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ae49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ae49cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ae4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ae4a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ae4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ae4b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ae4b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ae4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ae4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ae4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ae4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ae4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ae4dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ae4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ae4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ae4ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ae4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ae4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ae4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ae50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ae50a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ae50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ae515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ae51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ae52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ae526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ae52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ae53270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ae53830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ae53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ae543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ae54970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ae54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ae554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ae55ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ae56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ae56630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ae56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ae571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ae576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ae57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ae580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ae585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ae58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ae58fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ae594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ae599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ae59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ae5a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ae5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ae5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ae5b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ae5b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ae5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ae5c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ae5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ae5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ae5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ae5dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ae5e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ae5e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ae5efa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ae07330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ae077a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ae07c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ae08080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ae084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ae08960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ae08dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ae09240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ae096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ae09b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ae09f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ae0a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ae0b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ae0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ae0c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ae0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ae0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ae0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ae0ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ae0e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ae0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ae0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ae0fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ae10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ae10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ae10c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ae10ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ae11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ae117a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ae11c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ae12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ae125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ae12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ae12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ae13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ae135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ae13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ae13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ae14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ae14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ae14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ae15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ae154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ae15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ae15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ae16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ae16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ae16b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ae16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ae173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ae17850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ae17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ae18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ae185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ae18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ae18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ae193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ae198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ae19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ae1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ae1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ae1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ae1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ae1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ae1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ae1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ae1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ae1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ae1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ae1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ae1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ae1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ae1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ae1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ae1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ae1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ae1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ae1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ae1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ae1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ae1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ae20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ae207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ae20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ae210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ae21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ae219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ae21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ae22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ae226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ae22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ae22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ae23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ae238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ae23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ae24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ae24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ae24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ae24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ae25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ae257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ae25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ae260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ae26930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ae26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ae27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ae274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ae27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ae27db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ae28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ae28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ae28b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ae28f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ae293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ae29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ae29cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ae2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ae2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ae2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ae2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ae2b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ae2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ae2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ae2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ae2c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ae2c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ae2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ae2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ae2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ae2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ae2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ae2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ae2e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ae2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ae2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ae2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ae2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ae2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ae302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ae30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ae30bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ae31020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ae31490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ae31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ae31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ae321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ae32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ae32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ae32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ae333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ae33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ae33c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ae340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ae34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ae349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ae34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ae352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ae35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ae35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ae36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ae36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ae368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ae36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ae371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ae37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ae37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ae37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ae38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ae387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ae38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ae390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ae39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ae399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ae39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ae3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ae3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ae3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ae3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ae3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ae3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ae3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ae3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ae3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ae3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ae3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ae3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ae3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ae3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ae3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ae3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ae3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ae3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ae3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ae3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ae3ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ae40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ae408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ae40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ae41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ae415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ae41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ae41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ae42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ae427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ae42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ae43090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ae43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ae43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ae43de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ae44960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ae44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ae44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ae45350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ae457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ae45c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ae460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ae46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ae46980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ae46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ae47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ae476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ae47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ae47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ae48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ae48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ae48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ae49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ae495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ae49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ae49ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ae4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ae4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ae4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ae4b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ae4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ae4b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ae4bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ae4c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ae4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ae4cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ae4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ae4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ae4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ae4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ae4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ae4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ae4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ae4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ae4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ae4f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ae4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ae50060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ae504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ae50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ae50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ae51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ae51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ae51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ae51f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ae523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ae52850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ae52cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ae53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ae535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ae53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ae53e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ae542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ae54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ae54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ae55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ae554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ae55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ae55d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ae56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ae56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ae56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ae56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ae573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ae57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ae57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ae58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ae58580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ae58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ae59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ae59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ae5a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ae5a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ae5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ae5b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ae5b890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.753s
user	0m0.288s
sys	0m0.339s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4567 (d6d24cd9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125709f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12570a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12570abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12570b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12570b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12570bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12570c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12570c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12570cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12570d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12570d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12570dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12570e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12570efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12570f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12570fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125710420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125710b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125711260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125711a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125712150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125712870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126806540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126806c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1268070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126807510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1268096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1268099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126809e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12680a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12680a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12680acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12680b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12680b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12680bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12680c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12680c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12680cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12680d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12680d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12680da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12680def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12680e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12680e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12680ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12680f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12680f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12680fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1268101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126810650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126810ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126810f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1268113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126811a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126811f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1268123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126812660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126812c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126813460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126813720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126813bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126814060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126814500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1268149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1268152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126815780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126815c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1268160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126816560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126816a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126816ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1268173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126817940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126817e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1268183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126818930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126818e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1268193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126819920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126819e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12681a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12681a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12681ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12681b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12681b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12681be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12681c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12681c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12681ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12681d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12681d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12681de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12681e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12681e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12681ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12680f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12681f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12681fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12681ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1268204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126820a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126820f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1268214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126821a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126821f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1268224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126822a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126822f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1268234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126823a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126823f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1268243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126824890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126824d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1268251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126825670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126825b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126825fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126826450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1268268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1268274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126827760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126827c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126828160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126828660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126828b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126829560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126829a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126829f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12682a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12682a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12682ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12682b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12682b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12682bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12682c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12682c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12682cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12682d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12682d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12682db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12682e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12682e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12682ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12682ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12682f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12682f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12682fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126830360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126830860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126830d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126831260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126831760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126831c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126832160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126832660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126832b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126833060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126833a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126833f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126834460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126834960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126834e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126835360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126835860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126835d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126836260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126836760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126836c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126837160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126837660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126837b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126838060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126838560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126838f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126839960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126839e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12683a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12683a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12683ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12683b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12683b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12683bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12683c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12683c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12683cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12683d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12683d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12683dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12683e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12683e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12683ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12683f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12683f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126840140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1268405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1268408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126840eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1268414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126841cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126842150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1268425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126842a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126843790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126843ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126844230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126844780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126844cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126845220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126845770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126845cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126846760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126846cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126847200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126847750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126847ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1268481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126848740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126848c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1268491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126849730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126849c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12684a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12684a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12684ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12684b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12684b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12684bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12684c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12684c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12684cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12684d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12684d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12684dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12684e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12684e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12684ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12684f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12684f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12684fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126850170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1268506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126850c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126851160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1268516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126851c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126852150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1268526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126852bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126853140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126853690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126853be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126854130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126854680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126854bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126855120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126855670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126855bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126856060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126856500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1268569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126856e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1268572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126857780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126857c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1268580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126858560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126858a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126858ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126859340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1268597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126859c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12685a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12685a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12685ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12685b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12685bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12685c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12685c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12685cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12685d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12685d670 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.904 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.908 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12570caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12570bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12570c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12570b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12570a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12570ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12570dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125709530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125713250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1257139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125713c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125714660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125715620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125715d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125716460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125716b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1257172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125717c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125718370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125718a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1257191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1257198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125719ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12571a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12571a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12571aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12571b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12571bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12571c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12571c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12571ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12571d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12571d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12571d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12571de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12571e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12571e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12571ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12571f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12571f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12571f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12571fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125720d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125721350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125721960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125721f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125722b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1257231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1257237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125723fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1257248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125724ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1257251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1257259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125725e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1257262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125726780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125726c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1257270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125727a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125727ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125728340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1257287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125728c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1257295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125729b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12572a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12572a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12572ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12572b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12572b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12572baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12572c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12572c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12572cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12572d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12572d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12572dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12572e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12572e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12572eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12572f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12572f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12572fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125730000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125730550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125730aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125730ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125731540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125731a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125731fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125732530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125732a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125732fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125733520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125733a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125733fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125734510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125734a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125734fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125735500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125735a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125735fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1257364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125736a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125736ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125737380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125737820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125737cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125738160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125738600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125738aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125738f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1257393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125739880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12573a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12573a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12573ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12573afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12573b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12573b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12573bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12573c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12573c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12573cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12573d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12573d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12573d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12573dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12573e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12573e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12573ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12573f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12573f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12573f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12573fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1257402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125740780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125740c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1257410c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125741560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125741a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125741ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125742340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1257427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125742c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125743120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1257435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125743a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125743f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1257443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125744840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125744ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125745620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125745f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125746400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1257468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125746d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1257471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125747680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125747b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125747fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125748900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125749240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1257496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12574a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12574a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12574a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12574ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12574b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12574b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12574bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12574c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12574c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12574c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12574ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12574d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12574d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12574dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12574e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12574e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12574ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12574f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12574f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12574fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125750060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125750670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125750e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125751300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1257515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125751bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1257521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1257529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125752e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125753310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1257537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125753f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1257544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125754a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125754f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1257554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1257559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125755f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125756490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1257569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125756f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125757480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1257579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125757f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125758470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1257589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125758f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125759460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1257599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125759f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12575a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12575a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12575aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12575b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12575b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12575bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12575c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12575c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12575ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12575d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12575d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12575dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12575e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12575e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12575eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12575f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12575f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12575fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1257603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125760940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125760e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1257613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125761930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125761e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1257623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125762920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125762e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1257633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125763910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125763e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1257643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125764900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125764e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1257653a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1257658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125765e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125766390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1257668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125766d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1257676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125767b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125768000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1257684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125768940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125768de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125769280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125769720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125769bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12576a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12576a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12576a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12576ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12576b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12576bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12576c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12576c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12576d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12576d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12576dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12576dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12576e390 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12683d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12683eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126840b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12685d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12683e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12683f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126841170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126811660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126813060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12681f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12685c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126841780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126809120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1268093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12685e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12685e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12685e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12685e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12685ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12685ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12685f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12685f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12685f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12685fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12685fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12685ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126860250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126860510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1268607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126860a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126860d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126861010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1268612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126861590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126861850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126861b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126861dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126862090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126862350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126862610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1268628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126862b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126862e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126863110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1268633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126863690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126863950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126863c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126863ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126864190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126864450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126864710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1268649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126864c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126864f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126865210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1268654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126865790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126865a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126865d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126865fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126866290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126866550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126866810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126866ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126866d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126867050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126867310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1268675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126867890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126867b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126867e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1268680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126868390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126868650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126868910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126868bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126868e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126869150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126869410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1268696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126869990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126869c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126869f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12686a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12686a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12686a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12686aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12686acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12686af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12686b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12686b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12686b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12686ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12686bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12686c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12686c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12686c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12686c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12686cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12686cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12686d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12686d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12686d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12686d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12686db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12686de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12686e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12686e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12686e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12686e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12686ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12686eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12686f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12686f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12686f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12686f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12686fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12686ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126870210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1268704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126870790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126870a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126870d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126870fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126871290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126871550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126871810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126871ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126871d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126872050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126872310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1268725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126872890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126872b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126872e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1268730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126873390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126873650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126873910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126873bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126873e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126874150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126874410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1268746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126874990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126874c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126874f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1268751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126875490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126875750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126875a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126875cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126875f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126876250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126876510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1268767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126876a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126876d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126877010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1268772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126877590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126877850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126877b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126877dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126878090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126878350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126878610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1268788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126878b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126878e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126879110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1268793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126879690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126879950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126879c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126879ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12687a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12687a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12687a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12687a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12687ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12687af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12687b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12687b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12687b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12687ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12687bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12687bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12687c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12687c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12687c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12687cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12687cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12687d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12687d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12687d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12687d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12687db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12687de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12687e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12687e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12687e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12687e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12687ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12687ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12687f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12687f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12687f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12687f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12687ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126880220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1268804e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1268807a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126880a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126880d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126880fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1268812a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126881560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126881820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126881ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126881da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126882060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126882320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1268825e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1268828a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126882b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126882e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1268830e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1268833a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126883660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126883920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126883e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1268843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126884910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126884e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1268853b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126885900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126885e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1268863a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1268868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126886e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126887390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1268878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126887e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126888380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1268888d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126888e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126889370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1268898c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126889e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12688a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12688a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12688ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12688b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12688b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12688bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12688c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12688c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12688cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12688d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12688d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12688ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12688e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12688e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12688edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12688f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12688f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12688f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12688fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126890290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126890790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126890c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126891190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126891690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126891b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126892090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126892590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126892a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126892f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126893490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126893990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126893e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1268948a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126894fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1268956e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126895e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1268960c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1268968b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126896b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126897180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.965s
user	0m0.238s
sys	0m0.193s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.44 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.68 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.12 sec*proc (2 tests)

Total Test time (real) =   2.13 sec
        2.15 real         0.52 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.23 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
