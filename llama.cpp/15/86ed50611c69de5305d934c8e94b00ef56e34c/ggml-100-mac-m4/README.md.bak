### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.64 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.64 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.30 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.93 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.78 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.26 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.18 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.19 sec*proc (28 tests)

Total Test time (real) = 221.20 sec

real	3m41.228s
user	7m35.005s
sys	0m6.279s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.45 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.33 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.40 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.16 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.57 sec*proc (28 tests)

Total Test time (real) =  51.58 sec

real	0m51.596s
user	1m11.745s
sys	0m5.555s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.092 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.927 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.878 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.887 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.889 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.889 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.890 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.891 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.892 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.892 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.893 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.893 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.899 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.900 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.901 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.901 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.902 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.902 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.903 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.674 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.676 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.677 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.677 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.678 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.678 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.678 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.679 I llama_model_loader: - type  f32:  124 tensors
0.00.025.679 I llama_model_loader: - type  f16:   73 tensors
0.00.025.680 I print_info: file format = GGUF V3 (latest)
0.00.025.683 I print_info: file type   = F16
0.00.025.684 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.818 I load: special tokens cache size = 5
0.00.031.789 I load: token to piece cache size = 0.2032 MB
0.00.031.815 I print_info: arch             = bert
0.00.031.816 I print_info: vocab_only       = 0
0.00.031.817 I print_info: n_ctx_train      = 512
0.00.031.817 I print_info: n_embd           = 384
0.00.031.817 I print_info: n_layer          = 12
0.00.031.821 I print_info: n_head           = 12
0.00.031.821 I print_info: n_head_kv        = 12
0.00.031.822 I print_info: n_rot            = 32
0.00.031.822 I print_info: n_swa            = 0
0.00.031.822 I print_info: n_embd_head_k    = 32
0.00.031.822 I print_info: n_embd_head_v    = 32
0.00.031.823 I print_info: n_gqa            = 1
0.00.031.824 I print_info: n_embd_k_gqa     = 384
0.00.031.824 I print_info: n_embd_v_gqa     = 384
0.00.031.825 I print_info: f_norm_eps       = 1.0e-12
0.00.031.826 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.826 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.826 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.826 I print_info: f_logit_scale    = 0.0e+00
0.00.031.827 I print_info: n_ff             = 1536
0.00.031.827 I print_info: n_expert         = 0
0.00.031.827 I print_info: n_expert_used    = 0
0.00.031.828 I print_info: causal attn      = 0
0.00.031.828 I print_info: pooling type     = 2
0.00.031.828 I print_info: rope type        = 2
0.00.031.828 I print_info: rope scaling     = linear
0.00.031.829 I print_info: freq_base_train  = 10000.0
0.00.031.829 I print_info: freq_scale_train = 1
0.00.031.829 I print_info: n_ctx_orig_yarn  = 512
0.00.031.829 I print_info: rope_finetuned   = unknown
0.00.031.830 I print_info: ssm_d_conv       = 0
0.00.031.830 I print_info: ssm_d_inner      = 0
0.00.031.830 I print_info: ssm_d_state      = 0
0.00.031.830 I print_info: ssm_dt_rank      = 0
0.00.031.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.830 I print_info: model type       = 33M
0.00.031.831 I print_info: model params     = 33.21 M
0.00.031.831 I print_info: general.name     = Bge Small
0.00.031.832 I print_info: vocab type       = WPM
0.00.031.832 I print_info: n_vocab          = 30522
0.00.031.832 I print_info: n_merges         = 0
0.00.031.832 I print_info: UNK token        = 100 '[UNK]'
0.00.031.833 I print_info: SEP token        = 102 '[SEP]'
0.00.031.833 I print_info: PAD token        = 0 '[PAD]'
0.00.031.833 I print_info: CLS token        = 101 '[CLS]'
0.00.031.833 I print_info: MASK token       = 103 '[MASK]'
0.00.031.833 I print_info: LF token         = 0 '[PAD]'
0.00.031.834 I print_info: max token length = 21
0.00.033.759 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.761 I load_tensors: offloading output layer to GPU
0.00.033.761 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.788 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.790 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.029 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.031 I llama_new_context_with_model: n_ctx         = 512
0.00.034.032 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.032 I llama_new_context_with_model: n_batch       = 2048
0.00.034.032 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.032 I llama_new_context_with_model: flash_attn    = 0
0.00.034.033 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.033 I llama_new_context_with_model: freq_scale    = 1
0.00.034.034 I ggml_metal_init: allocating
0.00.034.038 I ggml_metal_init: found device: Apple M4
0.00.034.041 I ggml_metal_init: picking default device: Apple M4
0.00.034.839 I ggml_metal_init: using embedded metal library
0.00.038.864 I ggml_metal_init: GPU name:   Apple M4
0.00.038.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.867 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.868 I ggml_metal_init: simdgroup reduction   = true
0.00.038.868 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.868 I ggml_metal_init: has bfloat            = true
0.00.038.868 I ggml_metal_init: use bfloat            = true
0.00.038.869 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.870 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.679 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.228 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.230 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.231 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.051.997 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.051.998 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.051.998 I llama_new_context_with_model: graph nodes  = 429
0.00.051.999 I llama_new_context_with_model: graph splits = 2
0.00.052.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.309 I 
0.00.059.323 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.946 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.118 I llama_perf_context_print:        load time =      44.38 ms
0.00.065.119 I llama_perf_context_print: prompt eval time =       5.03 ms /     9 tokens (    0.56 ms per token,  1788.20 tokens per second)
0.00.065.119 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.120 I llama_perf_context_print:       total time =       5.81 ms /    10 tokens
0.00.065.253 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.047s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.257 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.926 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.931 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.932 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.933 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.933 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.934 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.934 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.934 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.935 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.935 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.937 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.937 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.938 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.938 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.938 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.938 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.366 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.037 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.038 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.038 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.039 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.039 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.039 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.040 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.040 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.040 I llama_model_loader: - type  f32:  124 tensors
0.00.015.040 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.041 I print_info: file format = GGUF V3 (latest)
0.00.015.042 I print_info: file type   = Q8_0
0.00.015.042 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.532 I load: special tokens cache size = 5
0.00.018.785 I load: token to piece cache size = 0.2032 MB
0.00.018.796 I print_info: arch             = bert
0.00.018.797 I print_info: vocab_only       = 0
0.00.018.797 I print_info: n_ctx_train      = 512
0.00.018.797 I print_info: n_embd           = 384
0.00.018.797 I print_info: n_layer          = 12
0.00.018.800 I print_info: n_head           = 12
0.00.018.801 I print_info: n_head_kv        = 12
0.00.018.801 I print_info: n_rot            = 32
0.00.018.801 I print_info: n_swa            = 0
0.00.018.801 I print_info: n_embd_head_k    = 32
0.00.018.801 I print_info: n_embd_head_v    = 32
0.00.018.802 I print_info: n_gqa            = 1
0.00.018.803 I print_info: n_embd_k_gqa     = 384
0.00.018.803 I print_info: n_embd_v_gqa     = 384
0.00.018.804 I print_info: f_norm_eps       = 1.0e-12
0.00.018.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.804 I print_info: f_logit_scale    = 0.0e+00
0.00.018.805 I print_info: n_ff             = 1536
0.00.018.805 I print_info: n_expert         = 0
0.00.018.805 I print_info: n_expert_used    = 0
0.00.018.806 I print_info: causal attn      = 0
0.00.018.806 I print_info: pooling type     = 2
0.00.018.806 I print_info: rope type        = 2
0.00.018.806 I print_info: rope scaling     = linear
0.00.018.806 I print_info: freq_base_train  = 10000.0
0.00.018.807 I print_info: freq_scale_train = 1
0.00.018.807 I print_info: n_ctx_orig_yarn  = 512
0.00.018.807 I print_info: rope_finetuned   = unknown
0.00.018.807 I print_info: ssm_d_conv       = 0
0.00.018.807 I print_info: ssm_d_inner      = 0
0.00.018.807 I print_info: ssm_d_state      = 0
0.00.018.807 I print_info: ssm_dt_rank      = 0
0.00.018.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.808 I print_info: model type       = 33M
0.00.018.808 I print_info: model params     = 33.21 M
0.00.018.808 I print_info: general.name     = Bge Small
0.00.018.809 I print_info: vocab type       = WPM
0.00.018.809 I print_info: n_vocab          = 30522
0.00.018.809 I print_info: n_merges         = 0
0.00.018.809 I print_info: UNK token        = 100 '[UNK]'
0.00.018.809 I print_info: SEP token        = 102 '[SEP]'
0.00.018.810 I print_info: PAD token        = 0 '[PAD]'
0.00.018.810 I print_info: CLS token        = 101 '[CLS]'
0.00.018.810 I print_info: MASK token       = 103 '[MASK]'
0.00.018.810 I print_info: LF token         = 0 '[PAD]'
0.00.018.810 I print_info: max token length = 21
0.00.020.082 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.082 I load_tensors: offloading output layer to GPU
0.00.020.082 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.090 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.091 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.248 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.249 I llama_new_context_with_model: n_ctx         = 512
0.00.020.249 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.249 I llama_new_context_with_model: n_batch       = 2048
0.00.020.249 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.249 I llama_new_context_with_model: flash_attn    = 0
0.00.020.250 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.250 I llama_new_context_with_model: freq_scale    = 1
0.00.020.251 I ggml_metal_init: allocating
0.00.020.254 I ggml_metal_init: found device: Apple M4
0.00.020.256 I ggml_metal_init: picking default device: Apple M4
0.00.020.895 I ggml_metal_init: using embedded metal library
0.00.023.460 I ggml_metal_init: GPU name:   Apple M4
0.00.023.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.463 I ggml_metal_init: simdgroup reduction   = true
0.00.023.463 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.463 I ggml_metal_init: has bfloat            = true
0.00.023.463 I ggml_metal_init: use bfloat            = true
0.00.023.464 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.765 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.255 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.257 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.259 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.932 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.933 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.933 I llama_new_context_with_model: graph nodes  = 429
0.00.034.934 I llama_new_context_with_model: graph splits = 2
0.00.034.935 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.935 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.153 I 
0.00.040.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.738 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.262 I llama_perf_context_print:        load time =      30.89 ms
0.00.045.264 I llama_perf_context_print: prompt eval time =       4.39 ms /     9 tokens (    0.49 ms per token,  2048.25 tokens per second)
0.00.045.264 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.265 I llama_perf_context_print:       total time =       5.11 ms /    10 tokens
0.00.045.437 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.194 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.931 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.838 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.846 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.856 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.856 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.857 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.858 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.859 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.859 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.860 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.861 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.864 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.865 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.865 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.866 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.866 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.348 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.348 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.348 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.052.349 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.349 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.349 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.350 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.350 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.052.351 I llama_model_loader: - type  f32:   40 tensors
0.00.052.356 I llama_model_loader: - type  f16:   30 tensors
0.00.052.357 I print_info: file format = GGUF V3 (latest)
0.00.052.357 I print_info: file type   = F16
0.00.052.359 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.069.303 W load: empty token at index 5
0.00.073.939 W load: model vocab missing newline token, using special_pad_id instead
0.00.075.310 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.334 I load: special tokens cache size = 5
0.00.337.263 I load: token to piece cache size = 1.5060 MB
0.00.337.301 I print_info: arch             = jina-bert-v2
0.00.337.302 I print_info: vocab_only       = 0
0.00.337.303 I print_info: n_ctx_train      = 8192
0.00.337.303 I print_info: n_embd           = 384
0.00.337.303 I print_info: n_layer          = 4
0.00.337.309 I print_info: n_head           = 12
0.00.337.310 I print_info: n_head_kv        = 12
0.00.337.310 I print_info: n_rot            = 32
0.00.337.310 I print_info: n_swa            = 0
0.00.337.317 I print_info: n_embd_head_k    = 32
0.00.337.317 I print_info: n_embd_head_v    = 32
0.00.337.318 I print_info: n_gqa            = 1
0.00.337.318 I print_info: n_embd_k_gqa     = 384
0.00.337.319 I print_info: n_embd_v_gqa     = 384
0.00.337.320 I print_info: f_norm_eps       = 1.0e-12
0.00.337.320 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.337.321 I print_info: f_clamp_kqv      = 0.0e+00
0.00.337.321 I print_info: f_max_alibi_bias = 8.0e+00
0.00.337.321 I print_info: f_logit_scale    = 0.0e+00
0.00.337.322 I print_info: n_ff             = 1536
0.00.337.322 I print_info: n_expert         = 0
0.00.337.323 I print_info: n_expert_used    = 0
0.00.337.323 I print_info: causal attn      = 0
0.00.337.323 I print_info: pooling type     = -1
0.00.337.323 I print_info: rope type        = -1
0.00.337.323 I print_info: rope scaling     = linear
0.00.337.324 I print_info: freq_base_train  = 10000.0
0.00.337.324 I print_info: freq_scale_train = 1
0.00.337.324 I print_info: n_ctx_orig_yarn  = 8192
0.00.337.324 I print_info: rope_finetuned   = unknown
0.00.337.324 I print_info: ssm_d_conv       = 0
0.00.337.324 I print_info: ssm_d_inner      = 0
0.00.337.325 I print_info: ssm_d_state      = 0
0.00.337.325 I print_info: ssm_dt_rank      = 0
0.00.337.325 I print_info: ssm_dt_b_c_rms   = 0
0.00.337.325 I print_info: model type       = 33M
0.00.337.329 I print_info: model params     = 32.90 M
0.00.337.329 I print_info: general.name     = Jina Bert Implementation
0.00.337.331 I print_info: vocab type       = BPE
0.00.337.331 I print_info: n_vocab          = 61056
0.00.337.331 I print_info: n_merges         = 39382
0.00.337.332 I print_info: BOS token        = 0 '<s>'
0.00.337.332 I print_info: EOS token        = 2 '</s>'
0.00.337.332 I print_info: UNK token        = 3 '<unk>'
0.00.337.332 I print_info: SEP token        = 2 '</s>'
0.00.337.332 I print_info: PAD token        = 1 '<pad>'
0.00.337.333 I print_info: CLS token        = 0 '<s>'
0.00.337.333 I print_info: MASK token       = 4 '<mask>'
0.00.337.334 I print_info: EOG token        = 2 '</s>'
0.00.337.334 I print_info: max token length = 45
0.00.338.624 I load_tensors: offloading 4 repeating layers to GPU
0.00.338.624 I load_tensors: offloading output layer to GPU
0.00.338.624 I load_tensors: offloaded 5/5 layers to GPU
0.00.338.649 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.650 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.339.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.339.148 I llama_new_context_with_model: n_ctx         = 8192
0.00.339.148 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.339.149 I llama_new_context_with_model: n_batch       = 2048
0.00.339.149 I llama_new_context_with_model: n_ubatch      = 2048
0.00.339.149 I llama_new_context_with_model: flash_attn    = 0
0.00.339.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.339.150 I llama_new_context_with_model: freq_scale    = 1
0.00.339.150 I ggml_metal_init: allocating
0.00.339.154 I ggml_metal_init: found device: Apple M4
0.00.339.155 I ggml_metal_init: picking default device: Apple M4
0.00.340.033 I ggml_metal_init: using embedded metal library
0.00.342.936 I ggml_metal_init: GPU name:   Apple M4
0.00.342.938 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.939 I ggml_metal_init: simdgroup reduction   = true
0.00.342.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.939 I ggml_metal_init: has bfloat            = true
0.00.342.939 I ggml_metal_init: use bfloat            = true
0.00.342.940 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.909 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.371.658 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.371.660 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.371.663 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.372.327 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.372.328 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.372.328 I llama_new_context_with_model: graph nodes  = 154
0.00.372.328 I llama_new_context_with_model: graph splits = 2
0.00.372.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.372.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.384.054 I 
0.00.384.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.384.362 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.384.363 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.384.372 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.384.372 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.384.374 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.384.374 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.384.864 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.388.334 I llama_perf_context_print:        load time =     360.12 ms
0.00.388.335 I llama_perf_context_print: prompt eval time =       3.46 ms /    62 tokens (    0.06 ms per token, 17913.90 tokens per second)
0.00.388.336 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.388.338 I llama_perf_context_print:       total time =       4.28 ms /    63 tokens
0.00.388.599 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.344s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.175 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.285 I main: llama backend init
0.00.000.290 I main: load the model and apply lora adapter, if any
0.00.030.854 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.372 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.387 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.394 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.394 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.395 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.397 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.398 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.400 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.401 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.401 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.409 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.580 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.581 I llama_model_loader: - type  f32:  194 tensors
0.00.061.582 I llama_model_loader: - type  f16:   98 tensors
0.00.061.589 I print_info: file format = GGUF V3 (latest)
0.00.061.591 I print_info: file type   = all F32 (guessed)
0.00.061.592 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.219 I load: special tokens cache size = 25
0.00.098.264 I load: token to piece cache size = 0.2984 MB
0.00.098.287 I print_info: arch             = gptneox
0.00.098.288 I print_info: vocab_only       = 0
0.00.098.288 I print_info: n_ctx_train      = 2048
0.00.098.288 I print_info: n_embd           = 2048
0.00.098.289 I print_info: n_layer          = 24
0.00.098.292 I print_info: n_head           = 16
0.00.098.294 I print_info: n_head_kv        = 16
0.00.098.295 I print_info: n_rot            = 32
0.00.098.295 I print_info: n_swa            = 0
0.00.098.295 I print_info: n_embd_head_k    = 128
0.00.098.295 I print_info: n_embd_head_v    = 128
0.00.098.296 I print_info: n_gqa            = 1
0.00.098.302 I print_info: n_embd_k_gqa     = 2048
0.00.098.310 I print_info: n_embd_v_gqa     = 2048
0.00.098.311 I print_info: f_norm_eps       = 1.0e-05
0.00.098.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.313 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.313 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.313 I print_info: f_logit_scale    = 0.0e+00
0.00.098.314 I print_info: n_ff             = 8192
0.00.098.314 I print_info: n_expert         = 0
0.00.098.314 I print_info: n_expert_used    = 0
0.00.098.314 I print_info: causal attn      = 1
0.00.098.314 I print_info: pooling type     = 0
0.00.098.315 I print_info: rope type        = 2
0.00.098.315 I print_info: rope scaling     = linear
0.00.098.315 I print_info: freq_base_train  = 10000.0
0.00.098.316 I print_info: freq_scale_train = 1
0.00.098.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.316 I print_info: rope_finetuned   = unknown
0.00.098.316 I print_info: ssm_d_conv       = 0
0.00.098.317 I print_info: ssm_d_inner      = 0
0.00.098.317 I print_info: ssm_d_state      = 0
0.00.098.317 I print_info: ssm_dt_rank      = 0
0.00.098.317 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.317 I print_info: model type       = 1.4B
0.00.098.318 I print_info: model params     = 1.41 B
0.00.098.318 I print_info: general.name     = 1.4B
0.00.098.318 I print_info: vocab type       = BPE
0.00.098.319 I print_info: n_vocab          = 50304
0.00.098.320 I print_info: n_merges         = 50009
0.00.098.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.321 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.321 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.321 I print_info: LF token         = 128 ''
0.00.098.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.322 I print_info: max token length = 1024
0.00.100.966 I load_tensors: offloading 24 repeating layers to GPU
0.00.100.966 I load_tensors: offloading output layer to GPU
0.00.100.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.100.985 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.987 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.101.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.297 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.298 I llama_new_context_with_model: n_batch       = 2048
0.00.101.298 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.298 I llama_new_context_with_model: flash_attn    = 0
0.00.101.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.299 I llama_new_context_with_model: freq_scale    = 1
0.00.101.299 I ggml_metal_init: allocating
0.00.101.303 I ggml_metal_init: found device: Apple M4
0.00.101.305 I ggml_metal_init: picking default device: Apple M4
0.00.101.997 I ggml_metal_init: using embedded metal library
0.00.119.244 I ggml_metal_init: GPU name:   Apple M4
0.00.119.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.247 I ggml_metal_init: simdgroup reduction   = true
0.00.119.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.247 I ggml_metal_init: has bfloat            = true
0.00.119.247 I ggml_metal_init: use bfloat            = true
0.00.119.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.143.968 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.166.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.166.354 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.378 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.167.368 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.167.370 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.167.371 I llama_new_context_with_model: graph nodes  = 967
0.00.167.371 I llama_new_context_with_model: graph splits = 2
0.00.167.374 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.399 I main: llama threadpool init, n_threads = 4
0.00.249.443 I 
0.00.249.468 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.468 I 
0.00.249.543 I sampler seed: 1234
0.00.249.548 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.249.584 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.249.585 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.249.585 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.206 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.02.102.206 I llama_perf_context_print:        load time =     218.53 ms
0.02.102.209 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.02.102.210 I llama_perf_context_print:        eval time =    1795.32 ms /    63 runs   (   28.50 ms per token,    35.09 tokens per second)
0.02.102.210 I llama_perf_context_print:       total time =    1852.81 ms /    70 tokens
0.02.102.441 I ggml_metal_free: deallocating

real	0m2.419s
user	0m0.144s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.535 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.663 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.969 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.977 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.982 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.983 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.984 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.988 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.548 I llama_model_loader: - type  f32:  194 tensors
0.00.055.548 I llama_model_loader: - type  f16:   98 tensors
0.00.055.549 I print_info: file format = GGUF V3 (latest)
0.00.055.550 I print_info: file type   = all F32 (guessed)
0.00.055.552 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.669 I load: special tokens cache size = 25
0.00.090.541 I load: token to piece cache size = 0.2984 MB
0.00.090.555 I print_info: arch             = gptneox
0.00.090.556 I print_info: vocab_only       = 0
0.00.090.557 I print_info: n_ctx_train      = 2048
0.00.090.557 I print_info: n_embd           = 2048
0.00.090.557 I print_info: n_layer          = 24
0.00.090.560 I print_info: n_head           = 16
0.00.090.561 I print_info: n_head_kv        = 16
0.00.090.561 I print_info: n_rot            = 32
0.00.090.562 I print_info: n_swa            = 0
0.00.090.562 I print_info: n_embd_head_k    = 128
0.00.090.562 I print_info: n_embd_head_v    = 128
0.00.090.563 I print_info: n_gqa            = 1
0.00.090.564 I print_info: n_embd_k_gqa     = 2048
0.00.090.564 I print_info: n_embd_v_gqa     = 2048
0.00.090.565 I print_info: f_norm_eps       = 1.0e-05
0.00.090.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.566 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.566 I print_info: f_logit_scale    = 0.0e+00
0.00.090.566 I print_info: n_ff             = 8192
0.00.090.567 I print_info: n_expert         = 0
0.00.090.567 I print_info: n_expert_used    = 0
0.00.090.567 I print_info: causal attn      = 1
0.00.090.567 I print_info: pooling type     = 0
0.00.090.567 I print_info: rope type        = 2
0.00.090.569 I print_info: rope scaling     = linear
0.00.090.569 I print_info: freq_base_train  = 10000.0
0.00.090.569 I print_info: freq_scale_train = 1
0.00.090.570 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.570 I print_info: rope_finetuned   = unknown
0.00.090.570 I print_info: ssm_d_conv       = 0
0.00.090.570 I print_info: ssm_d_inner      = 0
0.00.090.570 I print_info: ssm_d_state      = 0
0.00.090.570 I print_info: ssm_dt_rank      = 0
0.00.090.571 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.571 I print_info: model type       = 1.4B
0.00.090.571 I print_info: model params     = 1.41 B
0.00.090.571 I print_info: general.name     = 1.4B
0.00.090.572 I print_info: vocab type       = BPE
0.00.090.572 I print_info: n_vocab          = 50304
0.00.090.572 I print_info: n_merges         = 50009
0.00.090.572 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.573 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.573 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.573 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.574 I print_info: LF token         = 128 ''
0.00.090.575 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.575 I print_info: max token length = 1024
0.00.093.180 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.180 I load_tensors: offloading output layer to GPU
0.00.093.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.191 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.192 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.476 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.477 I llama_new_context_with_model: n_ctx         = 128
0.00.093.478 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.478 I llama_new_context_with_model: n_batch       = 128
0.00.093.478 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.478 I llama_new_context_with_model: flash_attn    = 0
0.00.093.478 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.479 I llama_new_context_with_model: freq_scale    = 1
0.00.093.479 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.479 I ggml_metal_init: allocating
0.00.093.483 I ggml_metal_init: found device: Apple M4
0.00.093.485 I ggml_metal_init: picking default device: Apple M4
0.00.094.084 I ggml_metal_init: using embedded metal library
0.00.096.661 I ggml_metal_init: GPU name:   Apple M4
0.00.096.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.663 I ggml_metal_init: simdgroup reduction   = true
0.00.096.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.664 I ggml_metal_init: has bfloat            = true
0.00.096.664 I ggml_metal_init: use bfloat            = true
0.00.096.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.665 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.478 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.750 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.752 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.766 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.627 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.628 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.628 I llama_new_context_with_model: graph nodes  = 967
0.00.108.629 I llama_new_context_with_model: graph splits = 2
0.00.108.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.144.896 I 
0.01.144.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.144.964 I perplexity: tokenizing the input ..
0.01.158.407 I perplexity: tokenization took 13.442 ms
0.01.158.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.280.430 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.282.151 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.282.201 I llama_perf_context_print:        load time =    1120.20 ms
0.01.282.202 I llama_perf_context_print: prompt eval time =     121.09 ms /   128 tokens (    0.95 ms per token,  1057.10 tokens per second)
0.01.282.204 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.282.204 I llama_perf_context_print:       total time =     137.31 ms /   129 tokens
0.01.282.979 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.124s
sys	0m0.218s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.924 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.903 I llama_model_loader: - type  f32:  194 tensors
0.00.028.903 I llama_model_loader: - type q8_0:   98 tensors
0.00.028.904 I print_info: file format = GGUF V3 (latest)
0.00.028.904 I print_info: file type   = Q8_0
0.00.028.905 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.423 I load: special tokens cache size = 25
0.00.054.349 I load: token to piece cache size = 0.2984 MB
0.00.054.369 I print_info: arch             = gptneox
0.00.054.370 I print_info: vocab_only       = 0
0.00.054.370 I print_info: n_ctx_train      = 2048
0.00.054.370 I print_info: n_embd           = 2048
0.00.054.370 I print_info: n_layer          = 24
0.00.054.378 I print_info: n_head           = 16
0.00.054.379 I print_info: n_head_kv        = 16
0.00.054.379 I print_info: n_rot            = 32
0.00.054.379 I print_info: n_swa            = 0
0.00.054.379 I print_info: n_embd_head_k    = 128
0.00.054.379 I print_info: n_embd_head_v    = 128
0.00.054.380 I print_info: n_gqa            = 1
0.00.054.381 I print_info: n_embd_k_gqa     = 2048
0.00.054.382 I print_info: n_embd_v_gqa     = 2048
0.00.054.382 I print_info: f_norm_eps       = 1.0e-05
0.00.054.383 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.383 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.383 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.383 I print_info: f_logit_scale    = 0.0e+00
0.00.054.386 I print_info: n_ff             = 8192
0.00.054.386 I print_info: n_expert         = 0
0.00.054.386 I print_info: n_expert_used    = 0
0.00.054.386 I print_info: causal attn      = 1
0.00.054.386 I print_info: pooling type     = 0
0.00.054.386 I print_info: rope type        = 2
0.00.054.387 I print_info: rope scaling     = linear
0.00.054.387 I print_info: freq_base_train  = 10000.0
0.00.054.387 I print_info: freq_scale_train = 1
0.00.054.388 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.390 I print_info: rope_finetuned   = unknown
0.00.054.390 I print_info: ssm_d_conv       = 0
0.00.054.390 I print_info: ssm_d_inner      = 0
0.00.054.390 I print_info: ssm_d_state      = 0
0.00.054.390 I print_info: ssm_dt_rank      = 0
0.00.054.391 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.391 I print_info: model type       = 1.4B
0.00.054.391 I print_info: model params     = 1.41 B
0.00.054.391 I print_info: general.name     = 1.4B
0.00.054.392 I print_info: vocab type       = BPE
0.00.054.392 I print_info: n_vocab          = 50304
0.00.054.394 I print_info: n_merges         = 50009
0.00.054.394 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.394 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.394 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.394 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.395 I print_info: LF token         = 128 ''
0.00.054.395 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.395 I print_info: max token length = 1024
0.00.056.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.811 I load_tensors: offloading output layer to GPU
0.00.056.812 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.823 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.056.824 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.057.162 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.162 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.163 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.163 I llama_new_context_with_model: n_batch       = 2048
0.00.057.163 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.163 I llama_new_context_with_model: flash_attn    = 0
0.00.057.164 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.164 I llama_new_context_with_model: freq_scale    = 1
0.00.057.164 I ggml_metal_init: allocating
0.00.057.167 I ggml_metal_init: found device: Apple M4
0.00.057.169 I ggml_metal_init: picking default device: Apple M4
0.00.057.901 I ggml_metal_init: using embedded metal library
0.00.060.462 I ggml_metal_init: GPU name:   Apple M4
0.00.060.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.464 I ggml_metal_init: simdgroup reduction   = true
0.00.060.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.465 I ggml_metal_init: has bfloat            = true
0.00.060.465 I ggml_metal_init: use bfloat            = true
0.00.060.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.573 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.859 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.860 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.861 I llama_new_context_with_model: graph nodes  = 967
0.00.097.861 I llama_new_context_with_model: graph splits = 2
0.00.097.864 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.993 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.287.628 I main: llama threadpool init, n_threads = 4
0.01.287.663 I 
0.01.287.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.287.685 I 
0.01.287.878 I sampler seed: 1234
0.01.287.883 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.287.918 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.287.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.287.929 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.375.351 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.02.375.352 I llama_perf_context_print:        load time =    1277.70 ms
0.02.375.352 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.69 tokens per second)
0.02.375.353 I llama_perf_context_print:        eval time =    1044.70 ms /    63 runs   (   16.58 ms per token,    60.30 tokens per second)
0.02.375.354 I llama_perf_context_print:       total time =    1087.73 ms /    70 tokens
0.02.375.571 I ggml_metal_free: deallocating

real	0m2.392s
user	0m0.111s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.407 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.422 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.424 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.425 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.180 I llama_model_loader: - type  f32:  194 tensors
0.00.033.180 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.181 I print_info: file format = GGUF V3 (latest)
0.00.033.181 I print_info: file type   = Q8_0
0.00.033.183 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.236 I load: special tokens cache size = 25
0.00.062.175 I load: token to piece cache size = 0.2984 MB
0.00.062.190 I print_info: arch             = gptneox
0.00.062.191 I print_info: vocab_only       = 0
0.00.062.192 I print_info: n_ctx_train      = 2048
0.00.062.192 I print_info: n_embd           = 2048
0.00.062.192 I print_info: n_layer          = 24
0.00.062.196 I print_info: n_head           = 16
0.00.062.197 I print_info: n_head_kv        = 16
0.00.062.197 I print_info: n_rot            = 32
0.00.062.197 I print_info: n_swa            = 0
0.00.062.198 I print_info: n_embd_head_k    = 128
0.00.062.198 I print_info: n_embd_head_v    = 128
0.00.062.199 I print_info: n_gqa            = 1
0.00.062.200 I print_info: n_embd_k_gqa     = 2048
0.00.062.200 I print_info: n_embd_v_gqa     = 2048
0.00.062.201 I print_info: f_norm_eps       = 1.0e-05
0.00.062.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.206 I print_info: f_logit_scale    = 0.0e+00
0.00.062.207 I print_info: n_ff             = 8192
0.00.062.207 I print_info: n_expert         = 0
0.00.062.209 I print_info: n_expert_used    = 0
0.00.062.209 I print_info: causal attn      = 1
0.00.062.209 I print_info: pooling type     = 0
0.00.062.209 I print_info: rope type        = 2
0.00.062.209 I print_info: rope scaling     = linear
0.00.062.209 I print_info: freq_base_train  = 10000.0
0.00.062.210 I print_info: freq_scale_train = 1
0.00.062.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.210 I print_info: rope_finetuned   = unknown
0.00.062.210 I print_info: ssm_d_conv       = 0
0.00.062.210 I print_info: ssm_d_inner      = 0
0.00.062.210 I print_info: ssm_d_state      = 0
0.00.062.210 I print_info: ssm_dt_rank      = 0
0.00.062.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.211 I print_info: model type       = 1.4B
0.00.062.211 I print_info: model params     = 1.41 B
0.00.062.211 I print_info: general.name     = 1.4B
0.00.062.212 I print_info: vocab type       = BPE
0.00.062.212 I print_info: n_vocab          = 50304
0.00.062.212 I print_info: n_merges         = 50009
0.00.062.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.214 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.214 I print_info: LF token         = 128 ''
0.00.062.215 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.215 I print_info: max token length = 1024
0.00.064.527 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.527 I load_tensors: offloading output layer to GPU
0.00.064.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.538 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.540 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.846 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.847 I llama_new_context_with_model: n_ctx         = 128
0.00.064.847 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.847 I llama_new_context_with_model: n_batch       = 128
0.00.064.847 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.847 I llama_new_context_with_model: flash_attn    = 0
0.00.064.848 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.848 I llama_new_context_with_model: freq_scale    = 1
0.00.064.848 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.849 I ggml_metal_init: allocating
0.00.064.852 I ggml_metal_init: found device: Apple M4
0.00.064.854 I ggml_metal_init: picking default device: Apple M4
0.00.065.490 I ggml_metal_init: using embedded metal library
0.00.068.068 I ggml_metal_init: GPU name:   Apple M4
0.00.068.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.070 I ggml_metal_init: simdgroup reduction   = true
0.00.068.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.071 I ggml_metal_init: has bfloat            = true
0.00.068.071 I ggml_metal_init: use bfloat            = true
0.00.068.071 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.885 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.218 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.222 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.237 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.115 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.116 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.116 I llama_new_context_with_model: graph nodes  = 967
0.00.080.116 I llama_new_context_with_model: graph splits = 2
0.00.080.121 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.940.562 I 
0.00.940.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.940.593 I perplexity: tokenizing the input ..
0.00.948.711 I perplexity: tokenization took 8.116 ms
0.00.948.715 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.354 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.520 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.543 I llama_perf_context_print:        load time =     929.15 ms
0.01.074.545 I llama_perf_context_print: prompt eval time =     124.38 ms /   128 tokens (    0.97 ms per token,  1029.07 tokens per second)
0.01.074.546 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.546 I llama_perf_context_print:       total time =     133.98 ms /   129 tokens
0.01.075.030 I ggml_metal_free: deallocating

real	0m1.093s
user	0m0.090s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.016.095 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.254 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.728 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.729 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.730 I llama_model_loader: - type  f32:  194 tensors
0.00.042.730 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.730 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.731 I print_info: file format = GGUF V3 (latest)
0.00.042.732 I print_info: file type   = Q4_0
0.00.042.733 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.069.052 I load: special tokens cache size = 25
0.00.079.367 I load: token to piece cache size = 0.2984 MB
0.00.079.380 I print_info: arch             = gptneox
0.00.079.381 I print_info: vocab_only       = 0
0.00.079.382 I print_info: n_ctx_train      = 2048
0.00.079.382 I print_info: n_embd           = 2048
0.00.079.382 I print_info: n_layer          = 24
0.00.079.387 I print_info: n_head           = 16
0.00.079.388 I print_info: n_head_kv        = 16
0.00.079.388 I print_info: n_rot            = 32
0.00.079.389 I print_info: n_swa            = 0
0.00.079.389 I print_info: n_embd_head_k    = 128
0.00.079.390 I print_info: n_embd_head_v    = 128
0.00.079.391 I print_info: n_gqa            = 1
0.00.079.393 I print_info: n_embd_k_gqa     = 2048
0.00.079.394 I print_info: n_embd_v_gqa     = 2048
0.00.079.395 I print_info: f_norm_eps       = 1.0e-05
0.00.079.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.396 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.396 I print_info: f_logit_scale    = 0.0e+00
0.00.079.397 I print_info: n_ff             = 8192
0.00.079.398 I print_info: n_expert         = 0
0.00.079.398 I print_info: n_expert_used    = 0
0.00.079.401 I print_info: causal attn      = 1
0.00.079.401 I print_info: pooling type     = 0
0.00.079.401 I print_info: rope type        = 2
0.00.079.401 I print_info: rope scaling     = linear
0.00.079.402 I print_info: freq_base_train  = 10000.0
0.00.079.402 I print_info: freq_scale_train = 1
0.00.079.403 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.403 I print_info: rope_finetuned   = unknown
0.00.079.403 I print_info: ssm_d_conv       = 0
0.00.079.404 I print_info: ssm_d_inner      = 0
0.00.079.404 I print_info: ssm_d_state      = 0
0.00.079.404 I print_info: ssm_dt_rank      = 0
0.00.079.404 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.405 I print_info: model type       = 1.4B
0.00.079.405 I print_info: model params     = 1.41 B
0.00.079.406 I print_info: general.name     = 1.4B
0.00.079.406 I print_info: vocab type       = BPE
0.00.079.407 I print_info: n_vocab          = 50304
0.00.079.407 I print_info: n_merges         = 50009
0.00.079.408 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.408 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.408 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.409 I print_info: LF token         = 128 ''
0.00.079.409 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.410 I print_info: max token length = 1024
0.00.082.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.082.450 I load_tensors: offloading output layer to GPU
0.00.082.450 I load_tensors: offloaded 25/25 layers to GPU
0.00.082.457 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.082.458 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.082.929 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.931 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.931 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.931 I llama_new_context_with_model: n_batch       = 2048
0.00.082.932 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.932 I llama_new_context_with_model: flash_attn    = 0
0.00.082.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.933 I llama_new_context_with_model: freq_scale    = 1
0.00.082.934 I ggml_metal_init: allocating
0.00.082.938 I ggml_metal_init: found device: Apple M4
0.00.082.941 I ggml_metal_init: picking default device: Apple M4
0.00.083.958 I ggml_metal_init: using embedded metal library
0.00.088.162 I ggml_metal_init: GPU name:   Apple M4
0.00.088.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.166 I ggml_metal_init: simdgroup reduction   = true
0.00.088.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.167 I ggml_metal_init: has bfloat            = true
0.00.088.167 I ggml_metal_init: use bfloat            = true
0.00.088.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.593 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.127.473 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.487 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.509 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.128.646 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.128.647 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.128.648 I llama_new_context_with_model: graph nodes  = 967
0.00.128.648 I llama_new_context_with_model: graph splits = 2
0.00.128.659 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.128.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.145 I main: llama threadpool init, n_threads = 4
0.00.760.196 I 
0.00.760.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.222 I 
0.00.760.496 I sampler seed: 1234
0.00.760.501 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.538 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.538 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.438.760 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.438.760 I llama_perf_context_print:        load time =     744.04 ms
0.01.438.761 I llama_perf_context_print: prompt eval time =      47.33 ms /     7 tokens (    6.76 ms per token,   147.91 tokens per second)
0.01.438.762 I llama_perf_context_print:        eval time =     627.88 ms /    63 runs   (    9.97 ms per token,   100.34 tokens per second)
0.01.438.763 I llama_perf_context_print:       total time =     678.62 ms /    70 tokens
0.01.439.035 I ggml_metal_free: deallocating

real	0m1.463s
user	0m0.135s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.539 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.335 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.340 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.341 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.342 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.822 I llama_model_loader: - type  f32:  194 tensors
0.00.024.822 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.823 I print_info: file format = GGUF V3 (latest)
0.00.024.824 I print_info: file type   = Q4_0
0.00.024.825 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.693 I load: special tokens cache size = 25
0.00.049.629 I load: token to piece cache size = 0.2984 MB
0.00.049.643 I print_info: arch             = gptneox
0.00.049.644 I print_info: vocab_only       = 0
0.00.049.644 I print_info: n_ctx_train      = 2048
0.00.049.645 I print_info: n_embd           = 2048
0.00.049.645 I print_info: n_layer          = 24
0.00.049.648 I print_info: n_head           = 16
0.00.049.649 I print_info: n_head_kv        = 16
0.00.049.649 I print_info: n_rot            = 32
0.00.049.649 I print_info: n_swa            = 0
0.00.049.649 I print_info: n_embd_head_k    = 128
0.00.049.649 I print_info: n_embd_head_v    = 128
0.00.049.650 I print_info: n_gqa            = 1
0.00.049.651 I print_info: n_embd_k_gqa     = 2048
0.00.049.651 I print_info: n_embd_v_gqa     = 2048
0.00.049.652 I print_info: f_norm_eps       = 1.0e-05
0.00.049.652 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.653 I print_info: f_logit_scale    = 0.0e+00
0.00.049.654 I print_info: n_ff             = 8192
0.00.049.656 I print_info: n_expert         = 0
0.00.049.656 I print_info: n_expert_used    = 0
0.00.049.656 I print_info: causal attn      = 1
0.00.049.656 I print_info: pooling type     = 0
0.00.049.656 I print_info: rope type        = 2
0.00.049.656 I print_info: rope scaling     = linear
0.00.049.657 I print_info: freq_base_train  = 10000.0
0.00.049.657 I print_info: freq_scale_train = 1
0.00.049.657 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.657 I print_info: rope_finetuned   = unknown
0.00.049.657 I print_info: ssm_d_conv       = 0
0.00.049.657 I print_info: ssm_d_inner      = 0
0.00.049.657 I print_info: ssm_d_state      = 0
0.00.049.658 I print_info: ssm_dt_rank      = 0
0.00.049.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.658 I print_info: model type       = 1.4B
0.00.049.658 I print_info: model params     = 1.41 B
0.00.049.658 I print_info: general.name     = 1.4B
0.00.049.659 I print_info: vocab type       = BPE
0.00.049.659 I print_info: n_vocab          = 50304
0.00.049.659 I print_info: n_merges         = 50009
0.00.049.659 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: LF token         = 128 ''
0.00.049.660 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: max token length = 1024
0.00.051.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.602 I load_tensors: offloading output layer to GPU
0.00.051.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.613 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.614 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.899 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.900 I llama_new_context_with_model: n_ctx         = 128
0.00.051.900 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.900 I llama_new_context_with_model: n_batch       = 128
0.00.051.901 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.901 I llama_new_context_with_model: flash_attn    = 0
0.00.051.901 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.901 I llama_new_context_with_model: freq_scale    = 1
0.00.051.902 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.902 I ggml_metal_init: allocating
0.00.051.905 I ggml_metal_init: found device: Apple M4
0.00.051.907 I ggml_metal_init: picking default device: Apple M4
0.00.052.466 I ggml_metal_init: using embedded metal library
0.00.054.803 I ggml_metal_init: GPU name:   Apple M4
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.805 I ggml_metal_init: simdgroup reduction   = true
0.00.054.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.805 I ggml_metal_init: has bfloat            = true
0.00.054.805 I ggml_metal_init: use bfloat            = true
0.00.054.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.869 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.112 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.129 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.007 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.008 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.008 I llama_new_context_with_model: graph nodes  = 967
0.00.067.008 I llama_new_context_with_model: graph splits = 2
0.00.067.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.283 I 
0.00.611.316 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.328 I perplexity: tokenizing the input ..
0.00.619.091 I perplexity: tokenization took 7.761 ms
0.00.619.095 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.899 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.085 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.111 I llama_perf_context_print:        load time =     601.74 ms
0.00.743.112 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.23 tokens per second)
0.00.743.113 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.113 I llama_perf_context_print:       total time =     131.83 ms /   129 tokens
0.00.743.547 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.077s
sys	0m0.091s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.639 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.885 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.896 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.897 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.897 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.900 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.901 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.901 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.904 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.905 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.628 I llama_model_loader: - type  f32:  194 tensors
0.00.026.628 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.628 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.629 I print_info: file format = GGUF V3 (latest)
0.00.026.629 I print_info: file type   = Q4_1
0.00.026.630 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.046.403 I load: special tokens cache size = 25
0.00.052.329 I load: token to piece cache size = 0.2984 MB
0.00.052.344 I print_info: arch             = gptneox
0.00.052.345 I print_info: vocab_only       = 0
0.00.052.345 I print_info: n_ctx_train      = 2048
0.00.052.346 I print_info: n_embd           = 2048
0.00.052.346 I print_info: n_layer          = 24
0.00.052.349 I print_info: n_head           = 16
0.00.052.350 I print_info: n_head_kv        = 16
0.00.052.350 I print_info: n_rot            = 32
0.00.052.350 I print_info: n_swa            = 0
0.00.052.350 I print_info: n_embd_head_k    = 128
0.00.052.352 I print_info: n_embd_head_v    = 128
0.00.052.353 I print_info: n_gqa            = 1
0.00.052.354 I print_info: n_embd_k_gqa     = 2048
0.00.052.354 I print_info: n_embd_v_gqa     = 2048
0.00.052.355 I print_info: f_norm_eps       = 1.0e-05
0.00.052.355 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.356 I print_info: f_logit_scale    = 0.0e+00
0.00.052.356 I print_info: n_ff             = 8192
0.00.052.356 I print_info: n_expert         = 0
0.00.052.357 I print_info: n_expert_used    = 0
0.00.052.357 I print_info: causal attn      = 1
0.00.052.357 I print_info: pooling type     = 0
0.00.052.357 I print_info: rope type        = 2
0.00.052.358 I print_info: rope scaling     = linear
0.00.052.358 I print_info: freq_base_train  = 10000.0
0.00.052.359 I print_info: freq_scale_train = 1
0.00.052.359 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.360 I print_info: rope_finetuned   = unknown
0.00.052.360 I print_info: ssm_d_conv       = 0
0.00.052.360 I print_info: ssm_d_inner      = 0
0.00.052.360 I print_info: ssm_d_state      = 0
0.00.052.360 I print_info: ssm_dt_rank      = 0
0.00.052.361 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.361 I print_info: model type       = 1.4B
0.00.052.361 I print_info: model params     = 1.41 B
0.00.052.361 I print_info: general.name     = 1.4B
0.00.052.362 I print_info: vocab type       = BPE
0.00.052.362 I print_info: n_vocab          = 50304
0.00.052.362 I print_info: n_merges         = 50009
0.00.052.362 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.362 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.362 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.363 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.363 I print_info: LF token         = 128 ''
0.00.052.363 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.363 I print_info: max token length = 1024
0.00.054.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.395 I load_tensors: offloading output layer to GPU
0.00.054.395 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.406 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.407 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.054.712 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.713 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.713 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.713 I llama_new_context_with_model: n_batch       = 2048
0.00.054.713 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.714 I llama_new_context_with_model: flash_attn    = 0
0.00.054.714 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.714 I llama_new_context_with_model: freq_scale    = 1
0.00.054.715 I ggml_metal_init: allocating
0.00.054.718 I ggml_metal_init: found device: Apple M4
0.00.054.720 I ggml_metal_init: picking default device: Apple M4
0.00.055.371 I ggml_metal_init: using embedded metal library
0.00.057.815 I ggml_metal_init: GPU name:   Apple M4
0.00.057.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.817 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.818 I ggml_metal_init: simdgroup reduction   = true
0.00.057.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.818 I ggml_metal_init: has bfloat            = true
0.00.057.818 I ggml_metal_init: use bfloat            = true
0.00.057.819 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.836 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.825 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.830 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.849 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.948 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.949 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.949 I llama_new_context_with_model: graph nodes  = 967
0.00.089.950 I llama_new_context_with_model: graph splits = 2
0.00.089.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.869 I main: llama threadpool init, n_threads = 4
0.00.713.905 I 
0.00.713.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.928 I 
0.00.714.161 I sampler seed: 1234
0.00.714.166 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.181 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.183 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.183 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.435.076 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65257.35 tokens per second)
0.01.435.076 I llama_perf_context_print:        load time =     704.23 ms
0.01.435.077 I llama_perf_context_print: prompt eval time =      39.59 ms /     7 tokens (    5.66 ms per token,   176.81 tokens per second)
0.01.435.078 I llama_perf_context_print:        eval time =     678.45 ms /    63 runs   (   10.77 ms per token,    92.86 tokens per second)
0.01.435.078 I llama_perf_context_print:       total time =     721.21 ms /    70 tokens
0.01.435.299 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.898 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.736 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.736 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.457 I llama_model_loader: - type  f32:  194 tensors
0.00.024.457 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.458 I print_info: file format = GGUF V3 (latest)
0.00.024.458 I print_info: file type   = Q4_1
0.00.024.459 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.261 I load: special tokens cache size = 25
0.00.048.955 I load: token to piece cache size = 0.2984 MB
0.00.048.969 I print_info: arch             = gptneox
0.00.048.970 I print_info: vocab_only       = 0
0.00.048.970 I print_info: n_ctx_train      = 2048
0.00.048.970 I print_info: n_embd           = 2048
0.00.048.971 I print_info: n_layer          = 24
0.00.048.974 I print_info: n_head           = 16
0.00.048.974 I print_info: n_head_kv        = 16
0.00.048.974 I print_info: n_rot            = 32
0.00.048.975 I print_info: n_swa            = 0
0.00.048.975 I print_info: n_embd_head_k    = 128
0.00.048.975 I print_info: n_embd_head_v    = 128
0.00.048.976 I print_info: n_gqa            = 1
0.00.048.979 I print_info: n_embd_k_gqa     = 2048
0.00.048.979 I print_info: n_embd_v_gqa     = 2048
0.00.048.980 I print_info: f_norm_eps       = 1.0e-05
0.00.048.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.980 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.981 I print_info: f_logit_scale    = 0.0e+00
0.00.048.981 I print_info: n_ff             = 8192
0.00.048.981 I print_info: n_expert         = 0
0.00.048.982 I print_info: n_expert_used    = 0
0.00.048.982 I print_info: causal attn      = 1
0.00.048.982 I print_info: pooling type     = 0
0.00.048.982 I print_info: rope type        = 2
0.00.048.982 I print_info: rope scaling     = linear
0.00.048.983 I print_info: freq_base_train  = 10000.0
0.00.048.984 I print_info: freq_scale_train = 1
0.00.048.984 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.984 I print_info: rope_finetuned   = unknown
0.00.048.985 I print_info: ssm_d_conv       = 0
0.00.048.985 I print_info: ssm_d_inner      = 0
0.00.048.985 I print_info: ssm_d_state      = 0
0.00.048.985 I print_info: ssm_dt_rank      = 0
0.00.048.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.985 I print_info: model type       = 1.4B
0.00.048.986 I print_info: model params     = 1.41 B
0.00.048.986 I print_info: general.name     = 1.4B
0.00.048.987 I print_info: vocab type       = BPE
0.00.048.987 I print_info: n_vocab          = 50304
0.00.048.987 I print_info: n_merges         = 50009
0.00.048.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.988 I print_info: LF token         = 128 ''
0.00.048.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.988 I print_info: max token length = 1024
0.00.050.540 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.540 I load_tensors: offloading output layer to GPU
0.00.050.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.550 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.551 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.050.820 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.821 I llama_new_context_with_model: n_ctx         = 128
0.00.050.821 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.821 I llama_new_context_with_model: n_batch       = 128
0.00.050.821 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.822 I llama_new_context_with_model: flash_attn    = 0
0.00.050.822 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.822 I llama_new_context_with_model: freq_scale    = 1
0.00.050.823 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.823 I ggml_metal_init: allocating
0.00.050.826 I ggml_metal_init: found device: Apple M4
0.00.050.828 I ggml_metal_init: picking default device: Apple M4
0.00.051.394 I ggml_metal_init: using embedded metal library
0.00.053.761 I ggml_metal_init: GPU name:   Apple M4
0.00.053.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.763 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.763 I ggml_metal_init: simdgroup reduction   = true
0.00.053.763 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.764 I ggml_metal_init: has bfloat            = true
0.00.053.764 I ggml_metal_init: use bfloat            = true
0.00.053.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.383 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.748 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.752 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.680 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.682 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.682 I llama_new_context_with_model: graph nodes  = 967
0.00.065.682 I llama_new_context_with_model: graph splits = 2
0.00.065.683 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.602 I 
0.00.661.637 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.655 I perplexity: tokenizing the input ..
0.00.669.660 I perplexity: tokenization took 8.004 ms
0.00.669.664 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.536 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.793.704 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.793.735 I llama_perf_context_print:        load time =     652.70 ms
0.00.793.736 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.65 tokens per second)
0.00.793.737 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.737 I llama_perf_context_print:       total time =     132.14 ms /   129 tokens
0.00.794.250 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.077s
sys	0m0.106s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.824 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.318 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.320 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.330 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.218 I llama_model_loader: - type  f32:  194 tensors
0.00.027.218 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.219 I print_info: file format = GGUF V3 (latest)
0.00.027.219 I print_info: file type   = Q5_0
0.00.027.220 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.213 I load: special tokens cache size = 25
0.00.051.930 I load: token to piece cache size = 0.2984 MB
0.00.051.945 I print_info: arch             = gptneox
0.00.051.946 I print_info: vocab_only       = 0
0.00.051.946 I print_info: n_ctx_train      = 2048
0.00.051.946 I print_info: n_embd           = 2048
0.00.051.946 I print_info: n_layer          = 24
0.00.051.949 I print_info: n_head           = 16
0.00.051.950 I print_info: n_head_kv        = 16
0.00.051.950 I print_info: n_rot            = 32
0.00.051.950 I print_info: n_swa            = 0
0.00.051.950 I print_info: n_embd_head_k    = 128
0.00.051.950 I print_info: n_embd_head_v    = 128
0.00.051.951 I print_info: n_gqa            = 1
0.00.051.952 I print_info: n_embd_k_gqa     = 2048
0.00.051.954 I print_info: n_embd_v_gqa     = 2048
0.00.051.955 I print_info: f_norm_eps       = 1.0e-05
0.00.051.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.960 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.960 I print_info: f_logit_scale    = 0.0e+00
0.00.051.966 I print_info: n_ff             = 8192
0.00.051.967 I print_info: n_expert         = 0
0.00.051.967 I print_info: n_expert_used    = 0
0.00.051.967 I print_info: causal attn      = 1
0.00.051.969 I print_info: pooling type     = 0
0.00.051.969 I print_info: rope type        = 2
0.00.051.969 I print_info: rope scaling     = linear
0.00.051.969 I print_info: freq_base_train  = 10000.0
0.00.051.970 I print_info: freq_scale_train = 1
0.00.051.970 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.970 I print_info: rope_finetuned   = unknown
0.00.051.970 I print_info: ssm_d_conv       = 0
0.00.051.970 I print_info: ssm_d_inner      = 0
0.00.051.970 I print_info: ssm_d_state      = 0
0.00.051.970 I print_info: ssm_dt_rank      = 0
0.00.051.971 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.971 I print_info: model type       = 1.4B
0.00.051.971 I print_info: model params     = 1.41 B
0.00.051.971 I print_info: general.name     = 1.4B
0.00.051.972 I print_info: vocab type       = BPE
0.00.051.972 I print_info: n_vocab          = 50304
0.00.051.972 I print_info: n_merges         = 50009
0.00.051.973 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.973 I print_info: LF token         = 128 ''
0.00.051.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.975 I print_info: max token length = 1024
0.00.054.014 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.014 I load_tensors: offloading output layer to GPU
0.00.054.014 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.025 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.026 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.370 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.371 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.371 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.372 I llama_new_context_with_model: n_batch       = 2048
0.00.054.372 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.372 I llama_new_context_with_model: flash_attn    = 0
0.00.054.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.373 I llama_new_context_with_model: freq_scale    = 1
0.00.054.373 I ggml_metal_init: allocating
0.00.054.376 I ggml_metal_init: found device: Apple M4
0.00.054.378 I ggml_metal_init: picking default device: Apple M4
0.00.054.961 I ggml_metal_init: using embedded metal library
0.00.057.322 I ggml_metal_init: GPU name:   Apple M4
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.324 I ggml_metal_init: simdgroup reduction   = true
0.00.057.325 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.325 I ggml_metal_init: has bfloat            = true
0.00.057.325 I ggml_metal_init: use bfloat            = true
0.00.057.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.071 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.849 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.904 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.905 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.906 I llama_new_context_with_model: graph nodes  = 967
0.00.087.906 I llama_new_context_with_model: graph splits = 2
0.00.087.909 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.818 I main: llama threadpool init, n_threads = 4
0.00.740.859 I 
0.00.740.888 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.889 I 
0.00.741.120 I sampler seed: 1234
0.00.741.128 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.143 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.144 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.144 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.535.934 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49685.09 tokens per second)
0.01.535.934 I llama_perf_context_print:        load time =     729.99 ms
0.01.535.935 I llama_perf_context_print: prompt eval time =      42.79 ms /     7 tokens (    6.11 ms per token,   163.60 tokens per second)
0.01.535.938 I llama_perf_context_print:        eval time =     749.56 ms /    63 runs   (   11.90 ms per token,    84.05 tokens per second)
0.01.535.938 I llama_perf_context_print:       total time =     795.12 ms /    70 tokens
0.01.536.222 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.109s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.064 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.099 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.738 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.739 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.740 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.741 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.741 I llama_model_loader: - type  f32:  194 tensors
0.00.025.742 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.742 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.742 I print_info: file format = GGUF V3 (latest)
0.00.025.743 I print_info: file type   = Q5_0
0.00.025.744 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.506 I load: special tokens cache size = 25
0.00.050.507 I load: token to piece cache size = 0.2984 MB
0.00.050.521 I print_info: arch             = gptneox
0.00.050.522 I print_info: vocab_only       = 0
0.00.050.522 I print_info: n_ctx_train      = 2048
0.00.050.523 I print_info: n_embd           = 2048
0.00.050.523 I print_info: n_layer          = 24
0.00.050.526 I print_info: n_head           = 16
0.00.050.526 I print_info: n_head_kv        = 16
0.00.050.527 I print_info: n_rot            = 32
0.00.050.527 I print_info: n_swa            = 0
0.00.050.527 I print_info: n_embd_head_k    = 128
0.00.050.527 I print_info: n_embd_head_v    = 128
0.00.050.528 I print_info: n_gqa            = 1
0.00.050.529 I print_info: n_embd_k_gqa     = 2048
0.00.050.529 I print_info: n_embd_v_gqa     = 2048
0.00.050.530 I print_info: f_norm_eps       = 1.0e-05
0.00.050.531 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.531 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.531 I print_info: f_logit_scale    = 0.0e+00
0.00.050.532 I print_info: n_ff             = 8192
0.00.050.532 I print_info: n_expert         = 0
0.00.050.532 I print_info: n_expert_used    = 0
0.00.050.532 I print_info: causal attn      = 1
0.00.050.532 I print_info: pooling type     = 0
0.00.050.533 I print_info: rope type        = 2
0.00.050.533 I print_info: rope scaling     = linear
0.00.050.533 I print_info: freq_base_train  = 10000.0
0.00.050.533 I print_info: freq_scale_train = 1
0.00.050.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.534 I print_info: rope_finetuned   = unknown
0.00.050.534 I print_info: ssm_d_conv       = 0
0.00.050.534 I print_info: ssm_d_inner      = 0
0.00.050.535 I print_info: ssm_d_state      = 0
0.00.050.535 I print_info: ssm_dt_rank      = 0
0.00.050.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.536 I print_info: model type       = 1.4B
0.00.050.536 I print_info: model params     = 1.41 B
0.00.050.536 I print_info: general.name     = 1.4B
0.00.050.537 I print_info: vocab type       = BPE
0.00.050.537 I print_info: n_vocab          = 50304
0.00.050.537 I print_info: n_merges         = 50009
0.00.050.537 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.537 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.538 I print_info: LF token         = 128 ''
0.00.050.538 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.538 I print_info: max token length = 1024
0.00.052.517 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.517 I load_tensors: offloading output layer to GPU
0.00.052.517 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.528 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.529 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.811 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.812 I llama_new_context_with_model: n_ctx         = 128
0.00.052.812 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.812 I llama_new_context_with_model: n_batch       = 128
0.00.052.812 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.813 I llama_new_context_with_model: flash_attn    = 0
0.00.052.813 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.813 I llama_new_context_with_model: freq_scale    = 1
0.00.052.814 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.814 I ggml_metal_init: allocating
0.00.052.817 I ggml_metal_init: found device: Apple M4
0.00.052.819 I ggml_metal_init: picking default device: Apple M4
0.00.053.373 I ggml_metal_init: using embedded metal library
0.00.055.734 I ggml_metal_init: GPU name:   Apple M4
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.737 I ggml_metal_init: simdgroup reduction   = true
0.00.055.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.737 I ggml_metal_init: has bfloat            = true
0.00.055.737 I ggml_metal_init: use bfloat            = true
0.00.055.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.594 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.596 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.610 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.535 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.536 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.536 I llama_new_context_with_model: graph nodes  = 967
0.00.067.537 I llama_new_context_with_model: graph splits = 2
0.00.067.538 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.253 I 
0.00.687.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.295 I perplexity: tokenizing the input ..
0.00.695.628 I perplexity: tokenization took 8.331 ms
0.00.695.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.995 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.832.265 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.832.293 I llama_perf_context_print:        load time =     677.19 ms
0.00.832.294 I llama_perf_context_print: prompt eval time =     135.13 ms /   128 tokens (    1.06 ms per token,   947.24 tokens per second)
0.00.832.295 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.832.295 I llama_perf_context_print:       total time =     145.04 ms /   129 tokens
0.00.832.698 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.077s
sys	0m0.115s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.492 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.261 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.262 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.262 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.263 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.263 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.263 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.266 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.266 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.266 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.984 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.988 I llama_model_loader: - type  f32:  194 tensors
0.00.025.988 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.988 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.989 I print_info: file format = GGUF V3 (latest)
0.00.025.989 I print_info: file type   = Q5_1
0.00.025.991 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.592 I load: special tokens cache size = 25
0.00.052.481 I load: token to piece cache size = 0.2984 MB
0.00.052.499 I print_info: arch             = gptneox
0.00.052.501 I print_info: vocab_only       = 0
0.00.052.501 I print_info: n_ctx_train      = 2048
0.00.052.501 I print_info: n_embd           = 2048
0.00.052.501 I print_info: n_layer          = 24
0.00.052.506 I print_info: n_head           = 16
0.00.052.506 I print_info: n_head_kv        = 16
0.00.052.507 I print_info: n_rot            = 32
0.00.052.507 I print_info: n_swa            = 0
0.00.052.507 I print_info: n_embd_head_k    = 128
0.00.052.507 I print_info: n_embd_head_v    = 128
0.00.052.508 I print_info: n_gqa            = 1
0.00.052.508 I print_info: n_embd_k_gqa     = 2048
0.00.052.509 I print_info: n_embd_v_gqa     = 2048
0.00.052.509 I print_info: f_norm_eps       = 1.0e-05
0.00.052.511 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.512 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.512 I print_info: f_logit_scale    = 0.0e+00
0.00.052.514 I print_info: n_ff             = 8192
0.00.052.515 I print_info: n_expert         = 0
0.00.052.515 I print_info: n_expert_used    = 0
0.00.052.515 I print_info: causal attn      = 1
0.00.052.515 I print_info: pooling type     = 0
0.00.052.515 I print_info: rope type        = 2
0.00.052.515 I print_info: rope scaling     = linear
0.00.052.516 I print_info: freq_base_train  = 10000.0
0.00.052.516 I print_info: freq_scale_train = 1
0.00.052.516 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.516 I print_info: rope_finetuned   = unknown
0.00.052.516 I print_info: ssm_d_conv       = 0
0.00.052.516 I print_info: ssm_d_inner      = 0
0.00.052.517 I print_info: ssm_d_state      = 0
0.00.052.517 I print_info: ssm_dt_rank      = 0
0.00.052.517 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.517 I print_info: model type       = 1.4B
0.00.052.517 I print_info: model params     = 1.41 B
0.00.052.517 I print_info: general.name     = 1.4B
0.00.052.518 I print_info: vocab type       = BPE
0.00.052.518 I print_info: n_vocab          = 50304
0.00.052.518 I print_info: n_merges         = 50009
0.00.052.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.519 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.519 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.519 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.519 I print_info: LF token         = 128 ''
0.00.052.519 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.520 I print_info: max token length = 1024
0.00.054.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.545 I load_tensors: offloading output layer to GPU
0.00.054.546 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.556 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.558 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.054.858 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.858 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.859 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.859 I llama_new_context_with_model: n_batch       = 2048
0.00.054.859 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.859 I llama_new_context_with_model: flash_attn    = 0
0.00.054.860 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.860 I llama_new_context_with_model: freq_scale    = 1
0.00.054.860 I ggml_metal_init: allocating
0.00.054.864 I ggml_metal_init: found device: Apple M4
0.00.054.866 I ggml_metal_init: picking default device: Apple M4
0.00.055.490 I ggml_metal_init: using embedded metal library
0.00.058.029 I ggml_metal_init: GPU name:   Apple M4
0.00.058.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.032 I ggml_metal_init: simdgroup reduction   = true
0.00.058.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.032 I ggml_metal_init: has bfloat            = true
0.00.058.033 I ggml_metal_init: use bfloat            = true
0.00.058.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.225 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.924 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.933 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.956 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.034 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.036 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.036 I llama_new_context_with_model: graph nodes  = 967
0.00.090.037 I llama_new_context_with_model: graph splits = 2
0.00.090.040 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.171 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.263 I main: llama threadpool init, n_threads = 4
0.00.722.303 I 
0.00.722.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.324 I 
0.00.722.562 I sampler seed: 1234
0.00.722.566 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.608 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.609 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.609 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.551.864 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.551.865 I llama_perf_context_print:        load time =     712.76 ms
0.01.551.866 I llama_perf_context_print: prompt eval time =      42.32 ms /     7 tokens (    6.05 ms per token,   165.42 tokens per second)
0.01.551.866 I llama_perf_context_print:        eval time =     784.00 ms /    63 runs   (   12.44 ms per token,    80.36 tokens per second)
0.01.551.867 I llama_perf_context_print:       total time =     829.60 ms /    70 tokens
0.01.552.116 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.112s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.528 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.187 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.188 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.189 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.189 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.190 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.003 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.786 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.788 I llama_model_loader: - type  f32:  194 tensors
0.00.023.788 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.788 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.789 I print_info: file format = GGUF V3 (latest)
0.00.023.789 I print_info: file type   = Q5_1
0.00.023.790 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.635 I load: special tokens cache size = 25
0.00.048.559 I load: token to piece cache size = 0.2984 MB
0.00.048.568 I print_info: arch             = gptneox
0.00.048.569 I print_info: vocab_only       = 0
0.00.048.570 I print_info: n_ctx_train      = 2048
0.00.048.570 I print_info: n_embd           = 2048
0.00.048.570 I print_info: n_layer          = 24
0.00.048.573 I print_info: n_head           = 16
0.00.048.574 I print_info: n_head_kv        = 16
0.00.048.574 I print_info: n_rot            = 32
0.00.048.575 I print_info: n_swa            = 0
0.00.048.575 I print_info: n_embd_head_k    = 128
0.00.048.575 I print_info: n_embd_head_v    = 128
0.00.048.576 I print_info: n_gqa            = 1
0.00.048.576 I print_info: n_embd_k_gqa     = 2048
0.00.048.577 I print_info: n_embd_v_gqa     = 2048
0.00.048.578 I print_info: f_norm_eps       = 1.0e-05
0.00.048.578 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.578 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.578 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.579 I print_info: f_logit_scale    = 0.0e+00
0.00.048.580 I print_info: n_ff             = 8192
0.00.048.580 I print_info: n_expert         = 0
0.00.048.580 I print_info: n_expert_used    = 0
0.00.048.581 I print_info: causal attn      = 1
0.00.048.581 I print_info: pooling type     = 0
0.00.048.581 I print_info: rope type        = 2
0.00.048.581 I print_info: rope scaling     = linear
0.00.048.583 I print_info: freq_base_train  = 10000.0
0.00.048.585 I print_info: freq_scale_train = 1
0.00.048.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.585 I print_info: rope_finetuned   = unknown
0.00.048.585 I print_info: ssm_d_conv       = 0
0.00.048.585 I print_info: ssm_d_inner      = 0
0.00.048.585 I print_info: ssm_d_state      = 0
0.00.048.585 I print_info: ssm_dt_rank      = 0
0.00.048.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.587 I print_info: model type       = 1.4B
0.00.048.587 I print_info: model params     = 1.41 B
0.00.048.587 I print_info: general.name     = 1.4B
0.00.048.588 I print_info: vocab type       = BPE
0.00.048.588 I print_info: n_vocab          = 50304
0.00.048.588 I print_info: n_merges         = 50009
0.00.048.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.589 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.589 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.589 I print_info: LF token         = 128 ''
0.00.048.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.589 I print_info: max token length = 1024
0.00.050.644 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.644 I load_tensors: offloading output layer to GPU
0.00.050.644 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.655 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.656 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.047 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.048 I llama_new_context_with_model: n_ctx         = 128
0.00.051.048 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.048 I llama_new_context_with_model: n_batch       = 128
0.00.051.048 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.049 I llama_new_context_with_model: flash_attn    = 0
0.00.051.049 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.049 I llama_new_context_with_model: freq_scale    = 1
0.00.051.049 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.050 I ggml_metal_init: allocating
0.00.051.053 I ggml_metal_init: found device: Apple M4
0.00.051.054 I ggml_metal_init: picking default device: Apple M4
0.00.051.645 I ggml_metal_init: using embedded metal library
0.00.054.034 I ggml_metal_init: GPU name:   Apple M4
0.00.054.036 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.037 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.037 I ggml_metal_init: simdgroup reduction   = true
0.00.054.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.037 I ggml_metal_init: has bfloat            = true
0.00.054.038 I ggml_metal_init: use bfloat            = true
0.00.054.038 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.731 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.998 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.000 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.015 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.970 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.971 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.971 I llama_new_context_with_model: graph nodes  = 967
0.00.065.971 I llama_new_context_with_model: graph splits = 2
0.00.065.972 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.805 I 
0.00.648.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.845 I perplexity: tokenizing the input ..
0.00.656.865 I perplexity: tokenization took 8.018 ms
0.00.656.874 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.939 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.793.118 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.793.139 I llama_perf_context_print:        load time =     640.27 ms
0.00.793.140 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.29 tokens per second)
0.00.793.141 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.141 I llama_perf_context_print:       total time =     144.34 ms /   129 tokens
0.00.793.513 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.127s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.883 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.276 I llama_model_loader: - type  f32:  194 tensors
0.00.025.276 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.277 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.277 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.277 I print_info: file format = GGUF V3 (latest)
0.00.025.278 I print_info: file type   = Q2_K - Medium
0.00.025.279 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.157 I load: special tokens cache size = 25
0.00.050.119 I load: token to piece cache size = 0.2984 MB
0.00.050.132 I print_info: arch             = gptneox
0.00.050.134 I print_info: vocab_only       = 0
0.00.050.134 I print_info: n_ctx_train      = 2048
0.00.050.134 I print_info: n_embd           = 2048
0.00.050.134 I print_info: n_layer          = 24
0.00.050.137 I print_info: n_head           = 16
0.00.050.138 I print_info: n_head_kv        = 16
0.00.050.138 I print_info: n_rot            = 32
0.00.050.138 I print_info: n_swa            = 0
0.00.050.138 I print_info: n_embd_head_k    = 128
0.00.050.139 I print_info: n_embd_head_v    = 128
0.00.050.139 I print_info: n_gqa            = 1
0.00.050.140 I print_info: n_embd_k_gqa     = 2048
0.00.050.143 I print_info: n_embd_v_gqa     = 2048
0.00.050.143 I print_info: f_norm_eps       = 1.0e-05
0.00.050.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.144 I print_info: f_logit_scale    = 0.0e+00
0.00.050.145 I print_info: n_ff             = 8192
0.00.050.145 I print_info: n_expert         = 0
0.00.050.145 I print_info: n_expert_used    = 0
0.00.050.145 I print_info: causal attn      = 1
0.00.050.145 I print_info: pooling type     = 0
0.00.050.145 I print_info: rope type        = 2
0.00.050.145 I print_info: rope scaling     = linear
0.00.050.146 I print_info: freq_base_train  = 10000.0
0.00.050.146 I print_info: freq_scale_train = 1
0.00.050.146 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.146 I print_info: rope_finetuned   = unknown
0.00.050.147 I print_info: ssm_d_conv       = 0
0.00.050.147 I print_info: ssm_d_inner      = 0
0.00.050.147 I print_info: ssm_d_state      = 0
0.00.050.147 I print_info: ssm_dt_rank      = 0
0.00.050.150 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.151 I print_info: model type       = 1.4B
0.00.050.151 I print_info: model params     = 1.41 B
0.00.050.151 I print_info: general.name     = 1.4B
0.00.050.151 I print_info: vocab type       = BPE
0.00.050.152 I print_info: n_vocab          = 50304
0.00.050.152 I print_info: n_merges         = 50009
0.00.050.152 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.153 I print_info: LF token         = 128 ''
0.00.050.153 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.153 I print_info: max token length = 1024
0.00.051.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.731 I load_tensors: offloading output layer to GPU
0.00.051.732 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.742 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.743 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.025 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.026 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.026 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.026 I llama_new_context_with_model: n_batch       = 2048
0.00.052.026 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.026 I llama_new_context_with_model: flash_attn    = 0
0.00.052.027 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.027 I llama_new_context_with_model: freq_scale    = 1
0.00.052.027 I ggml_metal_init: allocating
0.00.052.030 I ggml_metal_init: found device: Apple M4
0.00.052.032 I ggml_metal_init: picking default device: Apple M4
0.00.052.594 I ggml_metal_init: using embedded metal library
0.00.054.990 I ggml_metal_init: GPU name:   Apple M4
0.00.054.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.992 I ggml_metal_init: simdgroup reduction   = true
0.00.054.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.992 I ggml_metal_init: has bfloat            = true
0.00.054.992 I ggml_metal_init: use bfloat            = true
0.00.054.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.756 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.728 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.736 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.760 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.839 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.841 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.841 I llama_new_context_with_model: graph nodes  = 967
0.00.084.842 I llama_new_context_with_model: graph splits = 2
0.00.084.845 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.546 I main: llama threadpool init, n_threads = 4
0.00.433.583 I 
0.00.433.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.623 I 
0.00.433.865 I sampler seed: 1234
0.00.433.871 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.433.905 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.433.906 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.433.906 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.109.428 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.109.429 I llama_perf_context_print:        load time =     423.66 ms
0.01.109.429 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.84 tokens per second)
0.01.109.430 I llama_perf_context_print:        eval time =     636.85 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.109.430 I llama_perf_context_print:       total time =     675.89 ms /    70 tokens
0.01.109.648 I ggml_metal_free: deallocating

real	0m1.125s
user	0m0.109s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.005 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.965 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.981 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.983 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.688 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.689 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.690 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.690 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.691 I llama_model_loader: - type  f32:  194 tensors
0.00.025.691 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.691 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.692 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.692 I print_info: file format = GGUF V3 (latest)
0.00.025.693 I print_info: file type   = Q2_K - Medium
0.00.025.693 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.189 I load: special tokens cache size = 25
0.00.051.239 I load: token to piece cache size = 0.2984 MB
0.00.051.253 I print_info: arch             = gptneox
0.00.051.254 I print_info: vocab_only       = 0
0.00.051.255 I print_info: n_ctx_train      = 2048
0.00.051.255 I print_info: n_embd           = 2048
0.00.051.255 I print_info: n_layer          = 24
0.00.051.259 I print_info: n_head           = 16
0.00.051.259 I print_info: n_head_kv        = 16
0.00.051.260 I print_info: n_rot            = 32
0.00.051.260 I print_info: n_swa            = 0
0.00.051.260 I print_info: n_embd_head_k    = 128
0.00.051.265 I print_info: n_embd_head_v    = 128
0.00.051.268 I print_info: n_gqa            = 1
0.00.051.270 I print_info: n_embd_k_gqa     = 2048
0.00.051.270 I print_info: n_embd_v_gqa     = 2048
0.00.051.271 I print_info: f_norm_eps       = 1.0e-05
0.00.051.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.272 I print_info: f_logit_scale    = 0.0e+00
0.00.051.275 I print_info: n_ff             = 8192
0.00.051.275 I print_info: n_expert         = 0
0.00.051.275 I print_info: n_expert_used    = 0
0.00.051.275 I print_info: causal attn      = 1
0.00.051.275 I print_info: pooling type     = 0
0.00.051.275 I print_info: rope type        = 2
0.00.051.276 I print_info: rope scaling     = linear
0.00.051.277 I print_info: freq_base_train  = 10000.0
0.00.051.277 I print_info: freq_scale_train = 1
0.00.051.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.277 I print_info: rope_finetuned   = unknown
0.00.051.277 I print_info: ssm_d_conv       = 0
0.00.051.278 I print_info: ssm_d_inner      = 0
0.00.051.278 I print_info: ssm_d_state      = 0
0.00.051.278 I print_info: ssm_dt_rank      = 0
0.00.051.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.278 I print_info: model type       = 1.4B
0.00.051.279 I print_info: model params     = 1.41 B
0.00.051.279 I print_info: general.name     = 1.4B
0.00.051.282 I print_info: vocab type       = BPE
0.00.051.282 I print_info: n_vocab          = 50304
0.00.051.282 I print_info: n_merges         = 50009
0.00.051.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.283 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.283 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: LF token         = 128 ''
0.00.051.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.284 I print_info: max token length = 1024
0.00.053.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.125 I load_tensors: offloading output layer to GPU
0.00.053.125 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.136 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.137 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.416 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.417 I llama_new_context_with_model: n_ctx         = 128
0.00.053.417 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.417 I llama_new_context_with_model: n_batch       = 128
0.00.053.417 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.417 I llama_new_context_with_model: flash_attn    = 0
0.00.053.418 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.418 I llama_new_context_with_model: freq_scale    = 1
0.00.053.418 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.419 I ggml_metal_init: allocating
0.00.053.421 I ggml_metal_init: found device: Apple M4
0.00.053.423 I ggml_metal_init: picking default device: Apple M4
0.00.053.967 I ggml_metal_init: using embedded metal library
0.00.056.250 I ggml_metal_init: GPU name:   Apple M4
0.00.056.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.253 I ggml_metal_init: simdgroup reduction   = true
0.00.056.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.253 I ggml_metal_init: has bfloat            = true
0.00.056.253 I ggml_metal_init: use bfloat            = true
0.00.056.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.616 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.888 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.904 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.770 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.771 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.772 I llama_new_context_with_model: graph nodes  = 967
0.00.067.772 I llama_new_context_with_model: graph splits = 2
0.00.067.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.379.832 I 
0.00.379.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.379.878 I perplexity: tokenizing the input ..
0.00.387.510 I perplexity: tokenization took 7.63 ms
0.00.387.513 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.520.125 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.521.370 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.521.391 I llama_perf_context_print:        load time =     369.82 ms
0.00.521.393 I llama_perf_context_print: prompt eval time =     132.38 ms /   128 tokens (    1.03 ms per token,   966.88 tokens per second)
0.00.521.394 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.521.394 I llama_perf_context_print:       total time =     141.56 ms /   129 tokens
0.00.521.874 I ggml_metal_free: deallocating

real	0m0.537s
user	0m0.077s
sys	0m0.066s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.410 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.073 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.963 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.964 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.965 I llama_model_loader: - type  f32:  194 tensors
0.00.024.965 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.965 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.966 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.966 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.966 I print_info: file format = GGUF V3 (latest)
0.00.024.967 I print_info: file type   = Q3_K - Medium
0.00.024.968 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.801 I load: special tokens cache size = 25
0.00.049.780 I load: token to piece cache size = 0.2984 MB
0.00.049.794 I print_info: arch             = gptneox
0.00.049.795 I print_info: vocab_only       = 0
0.00.049.795 I print_info: n_ctx_train      = 2048
0.00.049.795 I print_info: n_embd           = 2048
0.00.049.795 I print_info: n_layer          = 24
0.00.049.798 I print_info: n_head           = 16
0.00.049.799 I print_info: n_head_kv        = 16
0.00.049.799 I print_info: n_rot            = 32
0.00.049.799 I print_info: n_swa            = 0
0.00.049.799 I print_info: n_embd_head_k    = 128
0.00.049.799 I print_info: n_embd_head_v    = 128
0.00.049.800 I print_info: n_gqa            = 1
0.00.049.801 I print_info: n_embd_k_gqa     = 2048
0.00.049.802 I print_info: n_embd_v_gqa     = 2048
0.00.049.802 I print_info: f_norm_eps       = 1.0e-05
0.00.049.803 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.803 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.803 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.803 I print_info: f_logit_scale    = 0.0e+00
0.00.049.804 I print_info: n_ff             = 8192
0.00.049.805 I print_info: n_expert         = 0
0.00.049.807 I print_info: n_expert_used    = 0
0.00.049.807 I print_info: causal attn      = 1
0.00.049.807 I print_info: pooling type     = 0
0.00.049.807 I print_info: rope type        = 2
0.00.049.808 I print_info: rope scaling     = linear
0.00.049.808 I print_info: freq_base_train  = 10000.0
0.00.049.808 I print_info: freq_scale_train = 1
0.00.049.808 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.808 I print_info: rope_finetuned   = unknown
0.00.049.808 I print_info: ssm_d_conv       = 0
0.00.049.809 I print_info: ssm_d_inner      = 0
0.00.049.809 I print_info: ssm_d_state      = 0
0.00.049.809 I print_info: ssm_dt_rank      = 0
0.00.049.809 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.809 I print_info: model type       = 1.4B
0.00.049.809 I print_info: model params     = 1.41 B
0.00.049.810 I print_info: general.name     = 1.4B
0.00.049.810 I print_info: vocab type       = BPE
0.00.049.810 I print_info: n_vocab          = 50304
0.00.049.810 I print_info: n_merges         = 50009
0.00.049.811 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.811 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: LF token         = 128 ''
0.00.049.815 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.816 I print_info: max token length = 1024
0.00.051.756 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.756 I load_tensors: offloading output layer to GPU
0.00.051.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.767 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.768 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.056 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.057 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.057 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.057 I llama_new_context_with_model: n_batch       = 2048
0.00.052.057 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.057 I llama_new_context_with_model: flash_attn    = 0
0.00.052.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.058 I llama_new_context_with_model: freq_scale    = 1
0.00.052.059 I ggml_metal_init: allocating
0.00.052.062 I ggml_metal_init: found device: Apple M4
0.00.052.064 I ggml_metal_init: picking default device: Apple M4
0.00.052.654 I ggml_metal_init: using embedded metal library
0.00.054.986 I ggml_metal_init: GPU name:   Apple M4
0.00.054.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.988 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.988 I ggml_metal_init: simdgroup reduction   = true
0.00.054.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.989 I ggml_metal_init: has bfloat            = true
0.00.054.989 I ggml_metal_init: use bfloat            = true
0.00.054.989 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.748 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.629 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.639 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.672 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.607 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.609 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.609 I llama_new_context_with_model: graph nodes  = 967
0.00.085.609 I llama_new_context_with_model: graph splits = 2
0.00.085.612 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.966 I main: llama threadpool init, n_threads = 4
0.00.536.004 I 
0.00.536.026 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.026 I 
0.00.536.248 I sampler seed: 1234
0.00.536.252 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.536.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.536.268 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.536.268 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.276.732 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.276.732 I llama_perf_context_print:        load time =     526.55 ms
0.01.276.733 I llama_perf_context_print: prompt eval time =      40.44 ms /     7 tokens (    5.78 ms per token,   173.08 tokens per second)
0.01.276.734 I llama_perf_context_print:        eval time =     696.91 ms /    63 runs   (   11.06 ms per token,    90.40 tokens per second)
0.01.276.734 I llama_perf_context_print:       total time =     740.77 ms /    70 tokens
0.01.276.991 I ggml_metal_free: deallocating

real	0m1.293s
user	0m0.109s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.151 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.196 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.204 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.215 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.216 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.217 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.893 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.893 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.894 I llama_model_loader: - type  f32:  194 tensors
0.00.024.895 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.895 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.895 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.895 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.896 I print_info: file format = GGUF V3 (latest)
0.00.024.900 I print_info: file type   = Q3_K - Medium
0.00.024.901 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.436 I load: special tokens cache size = 25
0.00.050.336 I load: token to piece cache size = 0.2984 MB
0.00.050.350 I print_info: arch             = gptneox
0.00.050.351 I print_info: vocab_only       = 0
0.00.050.351 I print_info: n_ctx_train      = 2048
0.00.050.352 I print_info: n_embd           = 2048
0.00.050.352 I print_info: n_layer          = 24
0.00.050.355 I print_info: n_head           = 16
0.00.050.356 I print_info: n_head_kv        = 16
0.00.050.358 I print_info: n_rot            = 32
0.00.050.358 I print_info: n_swa            = 0
0.00.050.358 I print_info: n_embd_head_k    = 128
0.00.050.358 I print_info: n_embd_head_v    = 128
0.00.050.363 I print_info: n_gqa            = 1
0.00.050.363 I print_info: n_embd_k_gqa     = 2048
0.00.050.365 I print_info: n_embd_v_gqa     = 2048
0.00.050.365 I print_info: f_norm_eps       = 1.0e-05
0.00.050.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.366 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.368 I print_info: f_logit_scale    = 0.0e+00
0.00.050.369 I print_info: n_ff             = 8192
0.00.050.369 I print_info: n_expert         = 0
0.00.050.369 I print_info: n_expert_used    = 0
0.00.050.369 I print_info: causal attn      = 1
0.00.050.369 I print_info: pooling type     = 0
0.00.050.369 I print_info: rope type        = 2
0.00.050.370 I print_info: rope scaling     = linear
0.00.050.370 I print_info: freq_base_train  = 10000.0
0.00.050.370 I print_info: freq_scale_train = 1
0.00.050.370 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.373 I print_info: rope_finetuned   = unknown
0.00.050.373 I print_info: ssm_d_conv       = 0
0.00.050.373 I print_info: ssm_d_inner      = 0
0.00.050.373 I print_info: ssm_d_state      = 0
0.00.050.373 I print_info: ssm_dt_rank      = 0
0.00.050.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.374 I print_info: model type       = 1.4B
0.00.050.374 I print_info: model params     = 1.41 B
0.00.050.374 I print_info: general.name     = 1.4B
0.00.050.375 I print_info: vocab type       = BPE
0.00.050.375 I print_info: n_vocab          = 50304
0.00.050.375 I print_info: n_merges         = 50009
0.00.050.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.376 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.376 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.376 I print_info: LF token         = 128 ''
0.00.050.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.377 I print_info: max token length = 1024
0.00.052.421 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.421 I load_tensors: offloading output layer to GPU
0.00.052.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.432 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.433 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.766 I llama_new_context_with_model: n_ctx         = 128
0.00.052.766 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.766 I llama_new_context_with_model: n_batch       = 128
0.00.052.767 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.767 I llama_new_context_with_model: flash_attn    = 0
0.00.052.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.767 I llama_new_context_with_model: freq_scale    = 1
0.00.052.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.768 I ggml_metal_init: allocating
0.00.052.771 I ggml_metal_init: found device: Apple M4
0.00.052.773 I ggml_metal_init: picking default device: Apple M4
0.00.053.354 I ggml_metal_init: using embedded metal library
0.00.055.732 I ggml_metal_init: GPU name:   Apple M4
0.00.055.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.735 I ggml_metal_init: simdgroup reduction   = true
0.00.055.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.735 I ggml_metal_init: has bfloat            = true
0.00.055.735 I ggml_metal_init: use bfloat            = true
0.00.055.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.666 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.044 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.048 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.064 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.948 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.949 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.949 I llama_new_context_with_model: graph nodes  = 967
0.00.067.950 I llama_new_context_with_model: graph splits = 2
0.00.067.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.976 I 
0.00.474.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.011 I perplexity: tokenizing the input ..
0.00.482.021 I perplexity: tokenization took 8.008 ms
0.00.482.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.870 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.062 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.089 I llama_perf_context_print:        load time =     464.82 ms
0.00.615.090 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.50 tokens per second)
0.00.615.091 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.091 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.00.615.588 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.079s
sys	0m0.084s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.020 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.180 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.181 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.056 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.057 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.057 I llama_model_loader: - type  f32:  194 tensors
0.00.025.058 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.058 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.058 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.059 I print_info: file format = GGUF V3 (latest)
0.00.025.059 I print_info: file type   = Q4_K - Medium
0.00.025.060 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.949 I load: special tokens cache size = 25
0.00.049.821 I load: token to piece cache size = 0.2984 MB
0.00.049.835 I print_info: arch             = gptneox
0.00.049.836 I print_info: vocab_only       = 0
0.00.049.836 I print_info: n_ctx_train      = 2048
0.00.049.837 I print_info: n_embd           = 2048
0.00.049.837 I print_info: n_layer          = 24
0.00.049.840 I print_info: n_head           = 16
0.00.049.840 I print_info: n_head_kv        = 16
0.00.049.841 I print_info: n_rot            = 32
0.00.049.841 I print_info: n_swa            = 0
0.00.049.841 I print_info: n_embd_head_k    = 128
0.00.049.841 I print_info: n_embd_head_v    = 128
0.00.049.842 I print_info: n_gqa            = 1
0.00.049.843 I print_info: n_embd_k_gqa     = 2048
0.00.049.843 I print_info: n_embd_v_gqa     = 2048
0.00.049.844 I print_info: f_norm_eps       = 1.0e-05
0.00.049.845 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.846 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.846 I print_info: f_logit_scale    = 0.0e+00
0.00.049.846 I print_info: n_ff             = 8192
0.00.049.848 I print_info: n_expert         = 0
0.00.049.848 I print_info: n_expert_used    = 0
0.00.049.849 I print_info: causal attn      = 1
0.00.049.849 I print_info: pooling type     = 0
0.00.049.849 I print_info: rope type        = 2
0.00.049.849 I print_info: rope scaling     = linear
0.00.049.849 I print_info: freq_base_train  = 10000.0
0.00.049.850 I print_info: freq_scale_train = 1
0.00.049.850 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.850 I print_info: rope_finetuned   = unknown
0.00.049.850 I print_info: ssm_d_conv       = 0
0.00.049.850 I print_info: ssm_d_inner      = 0
0.00.049.850 I print_info: ssm_d_state      = 0
0.00.049.850 I print_info: ssm_dt_rank      = 0
0.00.049.851 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.852 I print_info: model type       = 1.4B
0.00.049.852 I print_info: model params     = 1.41 B
0.00.049.852 I print_info: general.name     = 1.4B
0.00.049.853 I print_info: vocab type       = BPE
0.00.049.853 I print_info: n_vocab          = 50304
0.00.049.853 I print_info: n_merges         = 50009
0.00.049.853 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.853 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.854 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.854 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.854 I print_info: LF token         = 128 ''
0.00.049.854 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.854 I print_info: max token length = 1024
0.00.051.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.849 I load_tensors: offloading output layer to GPU
0.00.051.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.860 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.861 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.150 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.151 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.151 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.151 I llama_new_context_with_model: n_batch       = 2048
0.00.052.151 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.151 I llama_new_context_with_model: flash_attn    = 0
0.00.052.152 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.152 I llama_new_context_with_model: freq_scale    = 1
0.00.052.153 I ggml_metal_init: allocating
0.00.052.155 I ggml_metal_init: found device: Apple M4
0.00.052.157 I ggml_metal_init: picking default device: Apple M4
0.00.052.734 I ggml_metal_init: using embedded metal library
0.00.055.098 I ggml_metal_init: GPU name:   Apple M4
0.00.055.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.100 I ggml_metal_init: simdgroup reduction   = true
0.00.055.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.100 I ggml_metal_init: has bfloat            = true
0.00.055.100 I ggml_metal_init: use bfloat            = true
0.00.055.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.880 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.094 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.050 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.052 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.052 I llama_new_context_with_model: graph nodes  = 967
0.00.086.052 I llama_new_context_with_model: graph splits = 2
0.00.086.055 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.094 I main: llama threadpool init, n_threads = 4
0.00.617.136 I 
0.00.617.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.170 I 
0.00.617.408 I sampler seed: 1234
0.00.617.412 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.457 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.461 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.461 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.374.794 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.374.795 I llama_perf_context_print:        load time =     608.07 ms
0.01.374.796 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.80 tokens per second)
0.01.374.796 I llama_perf_context_print:        eval time =     707.41 ms /    63 runs   (   11.23 ms per token,    89.06 tokens per second)
0.01.374.797 I llama_perf_context_print:       total time =     757.70 ms /    70 tokens
0.01.375.007 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.109s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.626 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.638 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.178 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.179 I llama_model_loader: - type  f32:  194 tensors
0.00.024.179 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.179 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.180 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.180 I print_info: file format = GGUF V3 (latest)
0.00.024.181 I print_info: file type   = Q4_K - Medium
0.00.024.182 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.866 I load: special tokens cache size = 25
0.00.048.852 I load: token to piece cache size = 0.2984 MB
0.00.048.867 I print_info: arch             = gptneox
0.00.048.868 I print_info: vocab_only       = 0
0.00.048.868 I print_info: n_ctx_train      = 2048
0.00.048.868 I print_info: n_embd           = 2048
0.00.048.868 I print_info: n_layer          = 24
0.00.048.872 I print_info: n_head           = 16
0.00.048.872 I print_info: n_head_kv        = 16
0.00.048.873 I print_info: n_rot            = 32
0.00.048.873 I print_info: n_swa            = 0
0.00.048.873 I print_info: n_embd_head_k    = 128
0.00.048.875 I print_info: n_embd_head_v    = 128
0.00.048.876 I print_info: n_gqa            = 1
0.00.048.877 I print_info: n_embd_k_gqa     = 2048
0.00.048.878 I print_info: n_embd_v_gqa     = 2048
0.00.048.878 I print_info: f_norm_eps       = 1.0e-05
0.00.048.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.879 I print_info: f_logit_scale    = 0.0e+00
0.00.048.880 I print_info: n_ff             = 8192
0.00.048.880 I print_info: n_expert         = 0
0.00.048.880 I print_info: n_expert_used    = 0
0.00.048.880 I print_info: causal attn      = 1
0.00.048.880 I print_info: pooling type     = 0
0.00.048.880 I print_info: rope type        = 2
0.00.048.880 I print_info: rope scaling     = linear
0.00.048.881 I print_info: freq_base_train  = 10000.0
0.00.048.882 I print_info: freq_scale_train = 1
0.00.048.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.882 I print_info: rope_finetuned   = unknown
0.00.048.882 I print_info: ssm_d_conv       = 0
0.00.048.882 I print_info: ssm_d_inner      = 0
0.00.048.883 I print_info: ssm_d_state      = 0
0.00.048.883 I print_info: ssm_dt_rank      = 0
0.00.048.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.883 I print_info: model type       = 1.4B
0.00.048.883 I print_info: model params     = 1.41 B
0.00.048.883 I print_info: general.name     = 1.4B
0.00.048.884 I print_info: vocab type       = BPE
0.00.048.884 I print_info: n_vocab          = 50304
0.00.048.884 I print_info: n_merges         = 50009
0.00.048.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.890 I print_info: LF token         = 128 ''
0.00.048.891 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.891 I print_info: max token length = 1024
0.00.050.806 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.807 I load_tensors: offloading output layer to GPU
0.00.050.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.817 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.818 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.099 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.100 I llama_new_context_with_model: n_ctx         = 128
0.00.051.100 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.100 I llama_new_context_with_model: n_batch       = 128
0.00.051.100 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.100 I llama_new_context_with_model: flash_attn    = 0
0.00.051.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.101 I llama_new_context_with_model: freq_scale    = 1
0.00.051.101 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.102 I ggml_metal_init: allocating
0.00.051.105 I ggml_metal_init: found device: Apple M4
0.00.051.106 I ggml_metal_init: picking default device: Apple M4
0.00.051.662 I ggml_metal_init: using embedded metal library
0.00.053.967 I ggml_metal_init: GPU name:   Apple M4
0.00.053.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.970 I ggml_metal_init: simdgroup reduction   = true
0.00.053.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.970 I ggml_metal_init: has bfloat            = true
0.00.053.970 I ggml_metal_init: use bfloat            = true
0.00.053.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.466 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.681 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.685 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.700 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.571 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.572 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.572 I llama_new_context_with_model: graph nodes  = 967
0.00.065.573 I llama_new_context_with_model: graph splits = 2
0.00.065.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.967 I 
0.00.539.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.065 I perplexity: tokenizing the input ..
0.00.547.133 I perplexity: tokenization took 8.066 ms
0.00.547.137 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.681.569 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.682.835 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.682.865 I llama_perf_context_print:        load time =     530.09 ms
0.00.682.866 I llama_perf_context_print: prompt eval time =     134.20 ms /   128 tokens (    1.05 ms per token,   953.81 tokens per second)
0.00.682.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.682.867 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.683.220 I ggml_metal_free: deallocating

real	0m0.697s
user	0m0.076s
sys	0m0.091s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.535 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.111 I llama_model_loader: - type  f32:  194 tensors
0.00.027.111 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.112 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.112 I print_info: file format = GGUF V3 (latest)
0.00.027.113 I print_info: file type   = Q5_K - Medium
0.00.027.113 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.935 I load: special tokens cache size = 25
0.00.053.010 I load: token to piece cache size = 0.2984 MB
0.00.053.021 I print_info: arch             = gptneox
0.00.053.022 I print_info: vocab_only       = 0
0.00.053.022 I print_info: n_ctx_train      = 2048
0.00.053.022 I print_info: n_embd           = 2048
0.00.053.023 I print_info: n_layer          = 24
0.00.053.025 I print_info: n_head           = 16
0.00.053.026 I print_info: n_head_kv        = 16
0.00.053.026 I print_info: n_rot            = 32
0.00.053.026 I print_info: n_swa            = 0
0.00.053.026 I print_info: n_embd_head_k    = 128
0.00.053.027 I print_info: n_embd_head_v    = 128
0.00.053.027 I print_info: n_gqa            = 1
0.00.053.028 I print_info: n_embd_k_gqa     = 2048
0.00.053.029 I print_info: n_embd_v_gqa     = 2048
0.00.053.029 I print_info: f_norm_eps       = 1.0e-05
0.00.053.030 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.030 I print_info: f_logit_scale    = 0.0e+00
0.00.053.031 I print_info: n_ff             = 8192
0.00.053.031 I print_info: n_expert         = 0
0.00.053.031 I print_info: n_expert_used    = 0
0.00.053.032 I print_info: causal attn      = 1
0.00.053.032 I print_info: pooling type     = 0
0.00.053.032 I print_info: rope type        = 2
0.00.053.032 I print_info: rope scaling     = linear
0.00.053.032 I print_info: freq_base_train  = 10000.0
0.00.053.033 I print_info: freq_scale_train = 1
0.00.053.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.033 I print_info: rope_finetuned   = unknown
0.00.053.033 I print_info: ssm_d_conv       = 0
0.00.053.033 I print_info: ssm_d_inner      = 0
0.00.053.035 I print_info: ssm_d_state      = 0
0.00.053.035 I print_info: ssm_dt_rank      = 0
0.00.053.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.035 I print_info: model type       = 1.4B
0.00.053.035 I print_info: model params     = 1.41 B
0.00.053.036 I print_info: general.name     = 1.4B
0.00.053.036 I print_info: vocab type       = BPE
0.00.053.036 I print_info: n_vocab          = 50304
0.00.053.036 I print_info: n_merges         = 50009
0.00.053.036 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.037 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.037 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.039 I print_info: LF token         = 128 ''
0.00.053.039 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.039 I print_info: max token length = 1024
0.00.054.911 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.911 I load_tensors: offloading output layer to GPU
0.00.054.911 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.917 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.918 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.305 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.305 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.306 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.306 I llama_new_context_with_model: n_batch       = 2048
0.00.055.306 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.306 I llama_new_context_with_model: flash_attn    = 0
0.00.055.306 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.307 I llama_new_context_with_model: freq_scale    = 1
0.00.055.307 I ggml_metal_init: allocating
0.00.055.311 I ggml_metal_init: found device: Apple M4
0.00.055.312 I ggml_metal_init: picking default device: Apple M4
0.00.055.926 I ggml_metal_init: using embedded metal library
0.00.058.375 I ggml_metal_init: GPU name:   Apple M4
0.00.058.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.378 I ggml_metal_init: simdgroup reduction   = true
0.00.058.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.378 I ggml_metal_init: has bfloat            = true
0.00.058.378 I ggml_metal_init: use bfloat            = true
0.00.058.379 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.435 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.370 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.375 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.394 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.504 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.505 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.506 I llama_new_context_with_model: graph nodes  = 967
0.00.089.506 I llama_new_context_with_model: graph splits = 2
0.00.089.509 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.375 I main: llama threadpool init, n_threads = 4
0.00.694.415 I 
0.00.694.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.438 I 
0.00.694.668 I sampler seed: 1234
0.00.694.674 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.691 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.544.826 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.544.826 I llama_perf_context_print:        load time =     682.83 ms
0.01.544.827 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.65 tokens per second)
0.01.544.828 I llama_perf_context_print:        eval time =     795.47 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.544.828 I llama_perf_context_print:       total time =     850.46 ms /    70 tokens
0.01.545.068 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.194 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.479 I llama_model_loader: - type  f32:  194 tensors
0.00.025.479 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.479 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.480 I print_info: file format = GGUF V3 (latest)
0.00.025.480 I print_info: file type   = Q5_K - Medium
0.00.025.485 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.866 I load: special tokens cache size = 25
0.00.050.811 I load: token to piece cache size = 0.2984 MB
0.00.050.825 I print_info: arch             = gptneox
0.00.050.826 I print_info: vocab_only       = 0
0.00.050.827 I print_info: n_ctx_train      = 2048
0.00.050.827 I print_info: n_embd           = 2048
0.00.050.827 I print_info: n_layer          = 24
0.00.050.830 I print_info: n_head           = 16
0.00.050.831 I print_info: n_head_kv        = 16
0.00.050.831 I print_info: n_rot            = 32
0.00.050.831 I print_info: n_swa            = 0
0.00.050.832 I print_info: n_embd_head_k    = 128
0.00.050.832 I print_info: n_embd_head_v    = 128
0.00.050.833 I print_info: n_gqa            = 1
0.00.050.834 I print_info: n_embd_k_gqa     = 2048
0.00.050.834 I print_info: n_embd_v_gqa     = 2048
0.00.050.835 I print_info: f_norm_eps       = 1.0e-05
0.00.050.835 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.835 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.835 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.836 I print_info: f_logit_scale    = 0.0e+00
0.00.050.838 I print_info: n_ff             = 8192
0.00.050.838 I print_info: n_expert         = 0
0.00.050.838 I print_info: n_expert_used    = 0
0.00.050.838 I print_info: causal attn      = 1
0.00.050.838 I print_info: pooling type     = 0
0.00.050.839 I print_info: rope type        = 2
0.00.050.839 I print_info: rope scaling     = linear
0.00.050.839 I print_info: freq_base_train  = 10000.0
0.00.050.839 I print_info: freq_scale_train = 1
0.00.050.839 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.840 I print_info: rope_finetuned   = unknown
0.00.050.840 I print_info: ssm_d_conv       = 0
0.00.050.844 I print_info: ssm_d_inner      = 0
0.00.050.846 I print_info: ssm_d_state      = 0
0.00.050.846 I print_info: ssm_dt_rank      = 0
0.00.050.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.847 I print_info: model type       = 1.4B
0.00.050.848 I print_info: model params     = 1.41 B
0.00.050.848 I print_info: general.name     = 1.4B
0.00.050.849 I print_info: vocab type       = BPE
0.00.050.849 I print_info: n_vocab          = 50304
0.00.050.849 I print_info: n_merges         = 50009
0.00.050.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.852 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.852 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.852 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.852 I print_info: LF token         = 128 ''
0.00.050.852 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.853 I print_info: max token length = 1024
0.00.052.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.933 I load_tensors: offloading output layer to GPU
0.00.052.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.944 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.945 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.256 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.257 I llama_new_context_with_model: n_ctx         = 128
0.00.053.257 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.257 I llama_new_context_with_model: n_batch       = 128
0.00.053.257 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.258 I llama_new_context_with_model: flash_attn    = 0
0.00.053.258 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.258 I llama_new_context_with_model: freq_scale    = 1
0.00.053.259 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.259 I ggml_metal_init: allocating
0.00.053.262 I ggml_metal_init: found device: Apple M4
0.00.053.264 I ggml_metal_init: picking default device: Apple M4
0.00.053.855 I ggml_metal_init: using embedded metal library
0.00.056.172 I ggml_metal_init: GPU name:   Apple M4
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.175 I ggml_metal_init: simdgroup reduction   = true
0.00.056.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.175 I ggml_metal_init: has bfloat            = true
0.00.056.175 I ggml_metal_init: use bfloat            = true
0.00.056.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.285 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.299 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.215 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.216 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.216 I llama_new_context_with_model: graph nodes  = 967
0.00.068.217 I llama_new_context_with_model: graph splits = 2
0.00.068.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.218 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.993 I 
0.00.633.021 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.032 I perplexity: tokenizing the input ..
0.00.641.269 I perplexity: tokenization took 8.235 ms
0.00.641.278 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.433 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.782.620 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.782.640 I llama_perf_context_print:        load time =     622.80 ms
0.00.782.641 I llama_perf_context_print: prompt eval time =     139.93 ms /   128 tokens (    1.09 ms per token,   914.76 tokens per second)
0.00.782.642 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.642 I llama_perf_context_print:       total time =     149.65 ms /   129 tokens
0.00.783.038 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.078s
sys	0m0.117s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.754 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.846 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.847 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.848 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.848 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.851 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.606 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.607 I llama_model_loader: - type  f32:  194 tensors
0.00.025.607 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.608 I print_info: file format = GGUF V3 (latest)
0.00.025.608 I print_info: file type   = Q6_K
0.00.025.609 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.242 I load: special tokens cache size = 25
0.00.051.164 I load: token to piece cache size = 0.2984 MB
0.00.051.179 I print_info: arch             = gptneox
0.00.051.180 I print_info: vocab_only       = 0
0.00.051.180 I print_info: n_ctx_train      = 2048
0.00.051.180 I print_info: n_embd           = 2048
0.00.051.180 I print_info: n_layer          = 24
0.00.051.184 I print_info: n_head           = 16
0.00.051.185 I print_info: n_head_kv        = 16
0.00.051.185 I print_info: n_rot            = 32
0.00.051.185 I print_info: n_swa            = 0
0.00.051.185 I print_info: n_embd_head_k    = 128
0.00.051.186 I print_info: n_embd_head_v    = 128
0.00.051.187 I print_info: n_gqa            = 1
0.00.051.188 I print_info: n_embd_k_gqa     = 2048
0.00.051.189 I print_info: n_embd_v_gqa     = 2048
0.00.051.190 I print_info: f_norm_eps       = 1.0e-05
0.00.051.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.190 I print_info: f_logit_scale    = 0.0e+00
0.00.051.191 I print_info: n_ff             = 8192
0.00.051.191 I print_info: n_expert         = 0
0.00.051.191 I print_info: n_expert_used    = 0
0.00.051.192 I print_info: causal attn      = 1
0.00.051.192 I print_info: pooling type     = 0
0.00.051.192 I print_info: rope type        = 2
0.00.051.192 I print_info: rope scaling     = linear
0.00.051.192 I print_info: freq_base_train  = 10000.0
0.00.051.193 I print_info: freq_scale_train = 1
0.00.051.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.193 I print_info: rope_finetuned   = unknown
0.00.051.194 I print_info: ssm_d_conv       = 0
0.00.051.194 I print_info: ssm_d_inner      = 0
0.00.051.195 I print_info: ssm_d_state      = 0
0.00.051.195 I print_info: ssm_dt_rank      = 0
0.00.051.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.195 I print_info: model type       = 1.4B
0.00.051.195 I print_info: model params     = 1.41 B
0.00.051.195 I print_info: general.name     = 1.4B
0.00.051.196 I print_info: vocab type       = BPE
0.00.051.196 I print_info: n_vocab          = 50304
0.00.051.196 I print_info: n_merges         = 50009
0.00.051.196 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.197 I print_info: LF token         = 128 ''
0.00.051.201 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.201 I print_info: max token length = 1024
0.00.053.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.237 I load_tensors: offloading output layer to GPU
0.00.053.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.248 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.249 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.540 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.541 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.541 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.542 I llama_new_context_with_model: n_batch       = 2048
0.00.053.542 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.542 I llama_new_context_with_model: flash_attn    = 0
0.00.053.542 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.543 I llama_new_context_with_model: freq_scale    = 1
0.00.053.543 I ggml_metal_init: allocating
0.00.053.546 I ggml_metal_init: found device: Apple M4
0.00.053.548 I ggml_metal_init: picking default device: Apple M4
0.00.054.149 I ggml_metal_init: using embedded metal library
0.00.056.542 I ggml_metal_init: GPU name:   Apple M4
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.545 I ggml_metal_init: simdgroup reduction   = true
0.00.056.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.545 I ggml_metal_init: has bfloat            = true
0.00.056.545 I ggml_metal_init: use bfloat            = true
0.00.056.546 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.587 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.684 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.689 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.710 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.777 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.778 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.779 I llama_new_context_with_model: graph nodes  = 967
0.00.087.779 I llama_new_context_with_model: graph splits = 2
0.00.087.782 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.938 I main: llama threadpool init, n_threads = 4
0.00.741.978 I 
0.00.742.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.024 I 
0.00.742.261 I sampler seed: 1234
0.00.742.265 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.310 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.314 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.315 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.630.670 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49067.04 tokens per second)
0.01.630.671 I llama_perf_context_print:        load time =     733.18 ms
0.01.630.671 I llama_perf_context_print: prompt eval time =      54.52 ms /     7 tokens (    7.79 ms per token,   128.39 tokens per second)
0.01.630.673 I llama_perf_context_print:        eval time =     831.16 ms /    63 runs   (   13.19 ms per token,    75.80 tokens per second)
0.01.630.673 I llama_perf_context_print:       total time =     888.74 ms /    70 tokens
0.01.630.938 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4464 (1586ed50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.763 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.610 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.611 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.612 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.384 I llama_model_loader: - type  f32:  194 tensors
0.00.024.384 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.385 I print_info: file format = GGUF V3 (latest)
0.00.024.386 I print_info: file type   = Q6_K
0.00.024.386 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.058 I load: special tokens cache size = 25
0.00.048.944 I load: token to piece cache size = 0.2984 MB
0.00.048.958 I print_info: arch             = gptneox
0.00.048.959 I print_info: vocab_only       = 0
0.00.048.959 I print_info: n_ctx_train      = 2048
0.00.048.959 I print_info: n_embd           = 2048
0.00.048.960 I print_info: n_layer          = 24
0.00.048.963 I print_info: n_head           = 16
0.00.048.964 I print_info: n_head_kv        = 16
0.00.048.964 I print_info: n_rot            = 32
0.00.048.964 I print_info: n_swa            = 0
0.00.048.964 I print_info: n_embd_head_k    = 128
0.00.048.964 I print_info: n_embd_head_v    = 128
0.00.048.967 I print_info: n_gqa            = 1
0.00.048.968 I print_info: n_embd_k_gqa     = 2048
0.00.048.968 I print_info: n_embd_v_gqa     = 2048
0.00.048.969 I print_info: f_norm_eps       = 1.0e-05
0.00.048.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.974 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.974 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.974 I print_info: f_logit_scale    = 0.0e+00
0.00.048.975 I print_info: n_ff             = 8192
0.00.048.975 I print_info: n_expert         = 0
0.00.048.975 I print_info: n_expert_used    = 0
0.00.048.975 I print_info: causal attn      = 1
0.00.048.975 I print_info: pooling type     = 0
0.00.048.975 I print_info: rope type        = 2
0.00.048.975 I print_info: rope scaling     = linear
0.00.048.976 I print_info: freq_base_train  = 10000.0
0.00.048.978 I print_info: freq_scale_train = 1
0.00.048.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.979 I print_info: rope_finetuned   = unknown
0.00.048.979 I print_info: ssm_d_conv       = 0
0.00.048.979 I print_info: ssm_d_inner      = 0
0.00.048.980 I print_info: ssm_d_state      = 0
0.00.048.980 I print_info: ssm_dt_rank      = 0
0.00.048.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.983 I print_info: model type       = 1.4B
0.00.048.983 I print_info: model params     = 1.41 B
0.00.048.983 I print_info: general.name     = 1.4B
0.00.048.984 I print_info: vocab type       = BPE
0.00.048.985 I print_info: n_vocab          = 50304
0.00.048.985 I print_info: n_merges         = 50009
0.00.048.985 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.985 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.986 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.986 I print_info: LF token         = 128 ''
0.00.048.986 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.994 I print_info: max token length = 1024
0.00.051.034 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.034 I load_tensors: offloading output layer to GPU
0.00.051.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.045 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.046 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.429 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.430 I llama_new_context_with_model: n_ctx         = 128
0.00.051.430 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.430 I llama_new_context_with_model: n_batch       = 128
0.00.051.431 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.431 I llama_new_context_with_model: flash_attn    = 0
0.00.051.431 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.431 I llama_new_context_with_model: freq_scale    = 1
0.00.051.432 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.432 I ggml_metal_init: allocating
0.00.051.435 I ggml_metal_init: found device: Apple M4
0.00.051.437 I ggml_metal_init: picking default device: Apple M4
0.00.052.006 I ggml_metal_init: using embedded metal library
0.00.054.332 I ggml_metal_init: GPU name:   Apple M4
0.00.054.334 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.335 I ggml_metal_init: simdgroup reduction   = true
0.00.054.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.335 I ggml_metal_init: has bfloat            = true
0.00.054.335 I ggml_metal_init: use bfloat            = true
0.00.054.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.864 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.091 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.104 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.958 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.958 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.959 I llama_new_context_with_model: graph nodes  = 967
0.00.065.959 I llama_new_context_with_model: graph splits = 2
0.00.065.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.522 I 
0.00.382.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.586 I perplexity: tokenizing the input ..
0.00.390.469 I perplexity: tokenization took 7.881 ms
0.00.390.473 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.529.831 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.531.390 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.531.415 I llama_perf_context_print:        load time =     373.75 ms
0.00.531.417 I llama_perf_context_print: prompt eval time =     139.12 ms /   128 tokens (    1.09 ms per token,   920.04 tokens per second)
0.00.531.418 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.531.418 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.531.797 I ggml_metal_free: deallocating

real	0m0.545s
user	0m0.077s
sys	0m0.060s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4464 (1586ed50)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118e04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118e04a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118e04e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118e052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118e05750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118e05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118e06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118e064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118e06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118e06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118e071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118e07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118e083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118e08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118e09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118e09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118e0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118e0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118e0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118e0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118e0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118e0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118e0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118e0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118e0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118e0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118e0e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118e0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118e0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118e0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118e10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118e10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118e10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118e11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118e11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118e11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118e11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118e12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118e124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118e12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118e12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118e13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118e13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118e13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118e13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118e14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118e14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118e150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118e15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118e159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118e15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118e16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118e166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118e16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118e17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118e17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118e17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118e18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118e18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118e19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118e19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118e1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118e1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118e1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118e1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118e1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118e1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118e1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118e1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118e1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118e1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118e1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118e1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118e1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118e1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118e1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118e1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118e20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118e20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118e212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118e21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118e21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118e223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118e22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118e22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118e234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118e23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118e245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118e14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118e24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118e251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118e25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118e25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118e26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118e26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118e26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118e27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118e27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118e28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118e28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118e28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118e294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118e29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118e2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118e2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118e2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118e2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118e2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118e2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118e2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118e2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118e2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118e2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118e2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118e2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118e2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118e2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118e2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118e2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118e30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118e30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118e30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118e31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118e31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118e31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118e32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118e32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118e32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118e33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118e33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118e34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118e34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118e34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118e35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118e35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118e35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118e36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118e36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118e36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118e37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118e37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118e37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118e38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118e38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118e39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118e39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118e39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118e39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118e3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118e3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118e3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118e3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118e3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118e3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118e3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118e3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118e3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118e3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118e3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118e3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118e3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118e3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118e3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118e3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118e3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118e3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118e40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118e40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118e40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118e41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118e41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118e41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118e42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118e42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118e42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118e43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118e435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118e43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118e44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118e446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118e44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118e452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118e458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118e460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118e46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118e46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118e47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118e47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118e480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118e48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118e48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118e491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118e49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118e49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118e4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118e4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118e4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118e4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118e4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118e4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118e4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118e4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118e4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118e4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118e4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118e4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118e4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118e4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118e4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118e4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118e4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118e50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118e506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118e50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118e51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118e516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118e51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118e52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118e526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118e52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118e53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118e53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118e53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118e54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118e54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118e54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118e55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118e55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118e55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118e56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118e56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118e56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118e57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118e57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118e57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118e580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118e58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118e58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118e590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118e59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118e59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118e5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118e5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118e5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118e5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118e5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118e5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118e5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118e5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118e5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118e5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118e5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118e5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118e5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118e5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118e5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118e5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118e5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118e5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118e5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118e5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118e600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118e60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118e60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118e61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118e61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118e62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118e62550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118e62d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118e63000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118e63610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.118.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118e0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118e1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118e1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118e220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118e1c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x118e242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x118e21b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x118e291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x118e28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x118e28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x118e23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x118e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x118e269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x118e43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x118e23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x118e1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x118e21560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x118e1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x118e26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x118e432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118e280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118e231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118e1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118e20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118e1f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118e25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118e27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118e22c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118e1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118e258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118e27540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118e22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118e20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118e26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x118e632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118e44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118e455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118e47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118e0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118e14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118e10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118e074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x118e0e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118e169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118e17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x118e62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118e24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118e47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x118e45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118e63a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118e63d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118e63ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x118e642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x118e64570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x118e64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x118e64af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118e64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118e65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x118e65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x118e655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x118e658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x118e65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x118e65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x118e660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118e663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118e66670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118e66930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118e66bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118e66eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x118e67170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118e67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118e676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118e679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118e67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118e67f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118e681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x118e684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118e68770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118e68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x118e68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118e68fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118e69270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118e69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118e697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118e69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118e69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118e6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118e6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118e6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118e6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118e6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118e6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118e6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118e6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118e6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118e6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x118e6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118e6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x118e6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x118e6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118e6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x118e6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x118e6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x118e6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x118e6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118e6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118e6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x118e6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118e6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x118e6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118e6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x118e6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x118e6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x118e6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118e6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118e6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118e6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118e6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118e6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118e6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118e6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118e70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118e70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118e705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118e708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118e70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118e70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118e710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118e713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x118e71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118e71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118e71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118e71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118e72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118e72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118e726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118e729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x118e72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118e72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118e731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x118e734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118e73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118e73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x118e73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118e73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118e74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118e74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118e747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118e74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118e74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x118e75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x118e752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x118e755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x118e75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x118e75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x118e75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118e760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118e76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118e76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118e768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x118e76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118e76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118e77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x118e773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x118e776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x118e77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x118e77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x118e77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118e781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x118e78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118e78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118e789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118e78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118e78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118e79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x118e794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118e797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x118e79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118e79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118e79ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x118e7a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118e7a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118e7a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118e7aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118e7adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x118e7b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118e7b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118e7b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x118e7b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118e7bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118e7be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x118e7c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118e7c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118e7c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x118e7c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x118e7cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118e7ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x118e7d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x118e7d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118e7d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118e7d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118e7dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118e7df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x118e7e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118e7e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118e7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x118e7ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x118e7ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118e7efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x118e7f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118e7f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118e7fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x118e802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x118e80830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x118e80d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118e812d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x118e81820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x118e81d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x118e822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x118e82810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118e82d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118e832b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118e83800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118e83d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118e842a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118e847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118e84d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118e85290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118e857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118e85d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118e86280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118e867d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118e86d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118e87270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118e877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118e87d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118e88260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118e887b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x118e88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118e89250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118e897a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118e89cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118e8a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118e8a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x118e8ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118e8b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118e8b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x118e8bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118e8c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118e8c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118e8ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x118e8d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118e8d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x118e8dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118e8e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118e8e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x118e8eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x118e8f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x118e8f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x118e8fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x118e901e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x118e90730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118e90c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118e911d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118e91720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118e91c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x118e91f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x118e921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118e924b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118e92920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118e92d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118e93200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118e93670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118e93ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x118e93f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118e943c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118e94830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x118e94ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118e95110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118e95580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118e959f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118e95e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118e962d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118e96fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118e976e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118e97e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118e980c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118e988b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x118e98b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118e99180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148e046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148e04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148e04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148e05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148e058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148e05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148e06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148e065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148e06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148e06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148e07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148e07aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148e085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148e08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148e09580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148e09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148e0a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148e0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148e0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148e0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148e0c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148e0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148e0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148e0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148e0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148e0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148e0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148e0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148e0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148e0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148e0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148e10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148e10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148e109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148e11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148e12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148e12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148e12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148e12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148e131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148e13ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148e13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148e143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148e14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148e14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148e150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148e15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148e159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148e162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148e16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148e16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148e17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148e18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148e18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148e190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148e19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148e19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148e1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148e1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148e1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148e1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148e1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148e1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148e1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148e1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148e1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148e1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148e1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148e1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148e1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148e1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148e1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148e1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148e1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148e1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148e20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148e20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148e21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148e215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148e21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148e21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148e22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148e22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148e22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148e23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148e234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148e24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148e251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148e25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148e25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148e26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148e26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148e27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148e28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148e29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148e29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148e298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148e29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148e2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148e2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148e2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148e2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148e2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148e2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148e2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148e2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148e2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148e2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148e2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148e2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148e2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148e2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148e2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148e2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148e2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148e2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148e2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148e30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148e307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148e30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148e310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148e31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148e31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148e31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e56420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e58cc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.822s
user	0m0.277s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4464 (1586ed50)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c810110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c810820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c810dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c811380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c811930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c812490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c812a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c812ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c8134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c8139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c813ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c814a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c8151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c8159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c8160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c816810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c816f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c817650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c817e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c818540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c818c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c819380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c819c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c81a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c81a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c81ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c81b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c81bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c81c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c81c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c81d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c81d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c81d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c81dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c81e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c81e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c81eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c81ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c81f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c81f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c81fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c820210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c8204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c820ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c8210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c821a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c822020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c822630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c822c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c823250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c823860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c823e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c824660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c824b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c824fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c825260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c825870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c826320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c8267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c826c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c827100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c8275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c827a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c827ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c828380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c828820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c829160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c829600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c829aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c829ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c82a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c82aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c82afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c82b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c82ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c82bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c82c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c82ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c82cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c82d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c82da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c82dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c82e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c82ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c82efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c82f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c82fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c82ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c8304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c830a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c830f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c8314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c831a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c821700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c832640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c832b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c8330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c833630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c833b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c8340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c834620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c8350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c835b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c8360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c836ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c837490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c837930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c837dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c838270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c838bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c839050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c8394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c839990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c839e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c83a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c83a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c83ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c83b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c83b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c83b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c83be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c83c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c83cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c83d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c83d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c83da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c83def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c83e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c83e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c83ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c83f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c83f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c83fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c83ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c8403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c840890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c840d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c8411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c841670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c841b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c841fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c842450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c8428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c842d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c843230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c8436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c843b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c844010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c8444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c844950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c844df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c845290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c845730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c845bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c846070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c846510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c8469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c846e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c8472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c847790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c847c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c8480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c848570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c848a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c8497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c849c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c84a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c84a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c84aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c84af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c84b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c84bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c84c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c84c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c84cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c84d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c84d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c84dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c84e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c84e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c84ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c84f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c84f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c84fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c850170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c850780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c850f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c851410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c8516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c851ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c8522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c852ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c852f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c853420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c8538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c854070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c8545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c854b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c855060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c8555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c855b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c856050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c8565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c856af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c857040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c857590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c857ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c858030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c858580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c858ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c859020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c859570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c859ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c85a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c85a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c85aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c85b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c85b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c85baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c85bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c85c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c85ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c85cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c85d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c85da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c85dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c85e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c85ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c85efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c85f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c85fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c85ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c860500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c860a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c860fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c8614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c861a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c861f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c8624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c862a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c862f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c8634d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c863a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c863f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c8644c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c864a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c864f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c8654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c865a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c865f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c8664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c8669f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c866e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c867330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c8677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c867c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c868110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c8685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c868a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c868ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c869390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c869830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c869cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c86a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c86a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c86aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c86af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c86b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c86bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c86c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c86ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c86d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c86d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c86dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c86de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c86e4a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c86e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c851990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c84f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c850430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c823510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c822f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c825520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c851fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c81a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c8213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c821cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c8222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c820790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c8228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c8198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c825b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c832150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c86d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c81caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c81cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c8525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c850a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c81aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c81b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c86e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c86ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c86ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c86f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c86f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c86f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c86f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c86fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c86ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c8701c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c870480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c870740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c870a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c870cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c870f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c871240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c871500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c8717c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c871a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c871d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c872000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c8722c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c872580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c872840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c872b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c872dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c873080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c873340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c873600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c8738c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c873b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c873e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c874100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c8743c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c874680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c874940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c874c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c874ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c875180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c875440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c875700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c8759c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c875c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c875f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c876200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c8764c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c876780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c876a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c876d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c876fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c877280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c877540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c877800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c877ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c877d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c878040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c878300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c8785c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c878880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c878b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c878e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c8790c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c879380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c879640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c879900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c879bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c879e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c87a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c87a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c87a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c87a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c87ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c87af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c87b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c87b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c87b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c87ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c87bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c87bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c87c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c87c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c87c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c87ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c87cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c87d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c87d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c87d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c87d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c87db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c87ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c87e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c87e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c87e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c87e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c87eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c87ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c87f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c87f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c87f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c87f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c87fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c87fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c880180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c880440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c880700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c8809c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c880c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c880f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c881200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c8814c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c881780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c881a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c881d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c881fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c882280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c882540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c882800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c882ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c882d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c883040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c883300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c8835c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c883880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c883b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c883e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c8840c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c884380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c884640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c884900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c884bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c884e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c885140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c885400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c8856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c885980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c885c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c885f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c8861c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c886480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c886740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c886a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c886cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c886f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c887240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c887500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c8877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c887a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c887d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c888000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c8882c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c888580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c888840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c888b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c888dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c889080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c889340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c889600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c8898c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c889b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c889e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c88a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c88a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c88a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c88a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c88ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c88aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c88b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c88b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c88b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c88b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c88bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c88bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c88c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c88c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c88c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c88ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c88cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c88cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c88d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c88d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c88d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c88dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c88dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c88e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c88e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c88e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c88eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c88ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c88f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c88f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c88fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c890010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c890480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c8908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c890d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c8911d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c891640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c891ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c891f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c892390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c892800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c892c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c8930e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c893550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c8939c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c893e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c8942a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c894710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c894b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c894ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c895460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c8958d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c895d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c8961b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c896620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c896a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c896f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c897370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c8977e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c897c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c8980c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c898530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c8989a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c898e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c899280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c8996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c899b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c899fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c89a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c89a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c89ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c89b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c89b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c89ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c89bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c89c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c89c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c89cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c89d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c89d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c89d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c89ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c89e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c89e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c89eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c89efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c89f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c89f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c89fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c8a0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c8a05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c8a0a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c8a0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c8a1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c8a17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c8a1c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c8a2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c8a24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c8a2f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c8a3680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c8a3da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c8a44c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c8a4780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c8a4f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c8a5230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c8a5840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c8a27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c8a54f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c8a4a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c8a5ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c8a5f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c8a6220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c8a64e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c8a67a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c8a6a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c8a6d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c8a6fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c8a72a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c8a7870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c8a7e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c8a8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c8a8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c8a89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c8a8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c8a8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c8a9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c8a94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c8a97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c8a9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c8a9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c8a9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c8aa2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c8aa570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c8aa830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c8aaaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c8aadb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c8ab070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c8ab330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c8ab5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c8ab8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c8abb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c8abe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c8ac0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c8ac3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c8ac670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c8ac930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c8acbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c8aceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c8ad170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c8ad430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c8ad6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c8ad9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c8adc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c8adf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c8ae1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c8ae4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c8ae770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c8aea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c8aecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c8aefb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c8af270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c8af530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c8af7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c8afab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c8afd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c8b0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c8b02f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c8b05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c8b0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c8b0b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c8b0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c8b10b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c8b1370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c8b1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c8b18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c8b1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c8b1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c8b2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c8b23f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c8b26b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c8b2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c8b2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c8b2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c8b31b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c8b3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c8b3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c8b39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c8b3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c8b3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c8b4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c8b44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c8b47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c8b4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c8b4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c8b4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c8b52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c8b5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c8b5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c8b5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c8b5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c8b6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c8b6330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c8b65f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c8b68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c8b6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c8b6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c8b70f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c8b73b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c8b7670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c8b7930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c8b7bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c8b7eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c8b8170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c8b8430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c8b86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c8b89b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c8b8c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c8b8f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c8b91f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c8b94b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c8b9770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c8b9a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c8b9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c8b9fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c8ba270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c8ba530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c8ba7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c8baab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c8bad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c8bb030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c8bb2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c8bb5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c8bb870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c8bbb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c8bbdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c8bc0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c8bc370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c8bc630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c8bc8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c8bcbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c8bce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c8bd130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c8bd3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c8bd6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c8bd970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c8bdc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c8bdef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c8be1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c8be470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c8be730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c8be9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c8becb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c8bef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c8bf230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c8bf4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c8bf7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c8bfa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c8bfd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c8bfff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c8c02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c8c0570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c8c0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c8c0af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c8c0db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c8c1070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c8c1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c8c15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c8c18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c8c1b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c8c1e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c8c20f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c8c23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c8c2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c8c2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c8c2bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c8c2eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c8c3170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c8c3430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c8c36f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c8c39b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c8c3c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c8c3f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c8c41f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c8c44b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c8c4770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c8c4a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c8c4cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c8c4fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c8c5270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c8c5530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c8c57f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c8c5ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c8c5d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c8c6030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c8c62f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c8c65b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c8c6870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c8c6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c8c6df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c8c70b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c8c7370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c8c7630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c8c78f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c8c7bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c8c7e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c8c8130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c8c83f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c8c86b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c8c8970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c8c8c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c8c8ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c8c91b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c8c9470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c8c9730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c8c99f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c8c9cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c8ca280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c8ca540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c8ca800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c8caac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c8cad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c8cb040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c8cb300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c8cb5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c8cb880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c8cbb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c8cbe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c8cc0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c8cc380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c8cc640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c8cc900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c8ccbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c8cce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c8cd140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c8cd400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c8cd6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c8cd980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c8cdc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c8cdf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c8ce1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c8ce480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c8ce740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c8cea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c8cecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c8cef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c8cf240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c8cf500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c8cf7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c8cfa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c8cfd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c8d0000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c8d02c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c8d0580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c8d0840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c8d0b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c8d0dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c8d1080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c8d1340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c8d1600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c8d18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c8d1b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c8d1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c8d2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c8d23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c8d2680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c8d2940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c8d2c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c8d2ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c8d3180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c8d3440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c8d3700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c8d39c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c8d3c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c8d3f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c8d4200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c8d44c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c8d4780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c8d4a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c8d4e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c8d5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c8d53c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c8d5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c8d5ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c8d6110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c8d6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c8d69f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c8d6e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c8d72d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c8d7740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c8d82b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c8d89d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c8d90f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c8d9810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c8d9ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c8d9d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c8da2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c8da730 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.911s
user	0m0.241s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
