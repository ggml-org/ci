+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/15/21f9eebc47e8bfced0ef193af2a4b77065fe9e/ggml-0-x86-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/15/21f9eebc47e8bfced0ef193af2a4b77065fe9e/ggml-0-x86-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.722s
user	0m0.554s
sys	0m0.170s
+ tee -a /home/ggml/results/llama.cpp/15/21f9eebc47e8bfced0ef193af2a4b77065fe9e/ggml-0-x86-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  3%] Built target sha256
[  3%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target sha1
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Linking CXX shared library libggml-base.so
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Built target build_info
[  7%] Built target ggml-base
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Built target llama-gguf-hash
[ 16%] Built target llama-gguf
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
/home/ggml/work/llama.cpp/src/llama-impl.cpp: In function ‘std::string llama_format_tensor_shape(const std::vector<long int>&)’:
/home/ggml/work/llama.cpp/src/llama-impl.cpp:99:24: error: ‘strlen’ was not declared in this scope
   99 |         snprintf(buf + strlen(buf), sizeof(buf) - strlen(buf), ", %5" PRId64, ne.at(i));
      |                        ^~~~~~
/home/ggml/work/llama.cpp/src/llama-impl.cpp:10:1: note: ‘strlen’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    9 | #include <sstream>
  +++ |+#include <cstring>
   10 | 
/home/ggml/work/llama.cpp/src/llama-impl.cpp: In function ‘std::string llama_format_tensor_shape(const ggml_tensor*)’:
/home/ggml/work/llama.cpp/src/llama-impl.cpp:108:24: error: ‘strlen’ was not declared in this scope
  108 |         snprintf(buf + strlen(buf), sizeof(buf) - strlen(buf), ", %5" PRId64, t->ne[i]);
      |                        ^~~~~~
/home/ggml/work/llama.cpp/src/llama-impl.cpp:108:24: note: ‘strlen’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
make[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-impl.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/ggml/work/llama.cpp/src/llama.cpp:9:
/home/ggml/work/llama.cpp/src/llama-model-loader.h: In constructor ‘llama_model_loader::llama_tensor_weight::llama_tensor_weight(const llama_file*, uint16_t, const gguf_context*, ggml_tensor*)’:
/home/ggml/work/llama.cpp/src/llama-model-loader.h:36:28: error: ‘runtime_error’ is not a member of ‘std’
   36 |                 throw std::runtime_error(format("tensor '%s' not found in the model", ggml_get_name(tensor)));
      |                            ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.h:41:28: error: ‘runtime_error’ is not a member of ‘std’
   41 |                 throw std::runtime_error(format("tensor '%s' data is not within the file bounds, model is corrupted or incomplete", ggml_get_name(tensor)));
      |                            ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:76: src/CMakeFiles/llama.dir/llama.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m3.484s
user	0m10.354s
sys	0m1.144s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/15/21f9eebc47e8bfced0ef193af2a4b77065fe9e/ggml-0-x86-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
