Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.574s
user	0m0.884s
sys	0m1.246s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target xxhash
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Built target llava_shared
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 51%] Linking CXX executable ../bin/test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Built target test-llama-grammar
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Built target test-arg-parser
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-gguf
[ 63%] Built target test-chat-template
[ 63%] Built target test-backend-ops
[ 63%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Built target test-quantize-fns
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-bench
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-parallel
[ 82%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Built target llama-cli
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Built target llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Built target llama-perplexity
[ 90%] Built target llama-quantize
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.960s
user	0m6.073s
sys	0m9.736s

main: quantize time =  5209.63 ms
main:    total time =  5209.63 ms

main: quantize time =  1943.32 ms
main:    total time =  1943.32 ms

main: quantize time =  1806.52 ms
main:    total time =  1806.52 ms

main: quantize time =  2363.97 ms
main:    total time =  2363.97 ms

main: quantize time =  2731.89 ms
main:    total time =  2731.89 ms

main: quantize time =  5349.15 ms
main:    total time =  5349.15 ms

main: quantize time =  5636.12 ms
main:    total time =  5636.12 ms

main: quantize time =  6639.51 ms
main:    total time =  6639.51 ms

main: quantize time =  5965.07 ms
main:    total time =  5965.07 ms

main: quantize time =  4461.22 ms
main:    total time =  4461.22 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.102 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.191 I main: llama backend init
0.00.000.196 I main: load the model and apply lora adapter, if any
0.00.029.071 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.624 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.651 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.654 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.655 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.656 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.659 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.660 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.334 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.191 I llama_model_loader: - type  f32:  194 tensors
0.00.051.191 I llama_model_loader: - type  f16:   98 tensors
0.00.072.082 I llm_load_vocab: special tokens cache size = 25
0.00.078.178 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.183 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.183 I llm_load_print_meta: arch             = gptneox
0.00.078.183 I llm_load_print_meta: vocab type       = BPE
0.00.078.184 I llm_load_print_meta: n_vocab          = 50304
0.00.078.184 I llm_load_print_meta: n_merges         = 50009
0.00.078.184 I llm_load_print_meta: vocab_only       = 0
0.00.078.184 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.184 I llm_load_print_meta: n_embd           = 2048
0.00.078.184 I llm_load_print_meta: n_layer          = 24
0.00.078.189 I llm_load_print_meta: n_head           = 16
0.00.078.189 I llm_load_print_meta: n_head_kv        = 16
0.00.078.190 I llm_load_print_meta: n_rot            = 32
0.00.078.190 I llm_load_print_meta: n_swa            = 0
0.00.078.190 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.190 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.191 I llm_load_print_meta: n_gqa            = 1
0.00.078.192 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.192 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.193 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.193 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.194 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.194 I llm_load_print_meta: n_ff             = 8192
0.00.078.194 I llm_load_print_meta: n_expert         = 0
0.00.078.195 I llm_load_print_meta: n_expert_used    = 0
0.00.078.195 I llm_load_print_meta: causal attn      = 1
0.00.078.195 I llm_load_print_meta: pooling type     = 0
0.00.078.195 I llm_load_print_meta: rope type        = 2
0.00.078.195 I llm_load_print_meta: rope scaling     = linear
0.00.078.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.196 I llm_load_print_meta: freq_scale_train = 1
0.00.078.196 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.196 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.196 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.197 I llm_load_print_meta: model type       = 1.4B
0.00.078.198 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.078.198 I llm_load_print_meta: model params     = 1.41 B
0.00.078.198 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.078.198 I llm_load_print_meta: general.name     = 1.4B
0.00.078.199 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.202 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.202 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.202 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.202 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.203 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.203 I llm_load_print_meta: max token length = 1024
0.00.080.950 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.950 I llm_load_tensors: offloading output layer to GPU
0.00.080.950 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.972 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.080.973 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.082.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.015 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.016 I llama_new_context_with_model: n_batch       = 2048
0.00.082.016 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.016 I llama_new_context_with_model: flash_attn    = 0
0.00.082.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.017 I llama_new_context_with_model: freq_scale    = 1
0.00.082.018 I ggml_metal_init: allocating
0.00.082.025 I ggml_metal_init: found device: Apple M4
0.00.082.029 I ggml_metal_init: picking default device: Apple M4
0.00.082.691 I ggml_metal_init: using embedded metal library
0.00.091.698 I ggml_metal_init: GPU name:   Apple M4
0.00.091.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.702 I ggml_metal_init: simdgroup reduction   = true
0.00.091.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.703 I ggml_metal_init: has bfloat            = true
0.00.091.703 I ggml_metal_init: use bfloat            = true
0.00.091.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.115.390 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.134.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.597 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.639 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.579 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.580 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.580 I llama_new_context_with_model: graph nodes  = 967
0.00.135.581 I llama_new_context_with_model: graph splits = 2
0.00.135.584 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.216.658 I main: llama threadpool init, n_threads = 4
0.00.216.697 I 
0.00.216.719 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.216.721 I 
0.00.216.800 I sampler seed: 1234
0.00.216.804 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.216.828 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.216.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.216.830 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.073.904 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.02.073.905 I llama_perf_context_print:        load time =     187.57 ms
0.02.073.906 I llama_perf_context_print: prompt eval time =      54.21 ms /     7 tokens (    7.74 ms per token,   129.13 tokens per second)
0.02.073.907 I llama_perf_context_print:        eval time =    1800.08 ms /    63 runs   (   28.57 ms per token,    35.00 tokens per second)
0.02.073.908 I llama_perf_context_print:       total time =    1857.25 ms /    70 tokens
0.02.074.155 I ggml_metal_free: deallocating

real	0m2.353s
user	0m0.125s
sys	0m0.092s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.519 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.520 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.521 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.522 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.522 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.523 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.523 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.013 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.013 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.014 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.014 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.015 I llama_model_loader: - type  f32:  194 tensors
0.00.039.015 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.383 I llm_load_vocab: special tokens cache size = 25
0.00.069.925 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.929 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.929 I llm_load_print_meta: arch             = gptneox
0.00.069.930 I llm_load_print_meta: vocab type       = BPE
0.00.069.930 I llm_load_print_meta: n_vocab          = 50304
0.00.069.930 I llm_load_print_meta: n_merges         = 50009
0.00.069.933 I llm_load_print_meta: vocab_only       = 0
0.00.069.933 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.933 I llm_load_print_meta: n_embd           = 2048
0.00.069.933 I llm_load_print_meta: n_layer          = 24
0.00.069.938 I llm_load_print_meta: n_head           = 16
0.00.069.939 I llm_load_print_meta: n_head_kv        = 16
0.00.069.940 I llm_load_print_meta: n_rot            = 32
0.00.069.940 I llm_load_print_meta: n_swa            = 0
0.00.069.941 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.941 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.941 I llm_load_print_meta: n_gqa            = 1
0.00.069.942 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.944 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.945 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.945 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.945 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.945 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.946 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.946 I llm_load_print_meta: n_ff             = 8192
0.00.069.947 I llm_load_print_meta: n_expert         = 0
0.00.069.948 I llm_load_print_meta: n_expert_used    = 0
0.00.069.948 I llm_load_print_meta: causal attn      = 1
0.00.069.948 I llm_load_print_meta: pooling type     = 0
0.00.069.948 I llm_load_print_meta: rope type        = 2
0.00.069.948 I llm_load_print_meta: rope scaling     = linear
0.00.069.949 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.949 I llm_load_print_meta: freq_scale_train = 1
0.00.069.949 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.949 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.949 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.950 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.950 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.950 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.951 I llm_load_print_meta: model type       = 1.4B
0.00.069.951 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.951 I llm_load_print_meta: model params     = 1.41 B
0.00.069.952 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.952 I llm_load_print_meta: general.name     = 1.4B
0.00.069.952 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.952 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.953 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.953 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.953 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.953 I llm_load_print_meta: max token length = 1024
0.00.072.489 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.489 I llm_load_tensors: offloading output layer to GPU
0.00.072.489 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.501 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.502 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.520 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.521 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.521 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.521 I llama_new_context_with_model: n_batch       = 2048
0.00.073.521 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.521 I llama_new_context_with_model: flash_attn    = 0
0.00.073.522 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.522 I llama_new_context_with_model: freq_scale    = 1
0.00.073.522 I ggml_metal_init: allocating
0.00.073.525 I ggml_metal_init: found device: Apple M4
0.00.073.527 I ggml_metal_init: picking default device: Apple M4
0.00.074.304 I ggml_metal_init: using embedded metal library
0.00.077.105 I ggml_metal_init: GPU name:   Apple M4
0.00.077.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.108 I ggml_metal_init: simdgroup reduction   = true
0.00.077.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.108 I ggml_metal_init: has bfloat            = true
0.00.077.108 I ggml_metal_init: use bfloat            = true
0.00.077.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.770 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.957 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.967 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.009 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.134 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.136 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.137 I llama_new_context_with_model: graph nodes  = 967
0.00.115.137 I llama_new_context_with_model: graph splits = 2
0.00.115.141 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.282 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.173.549 I main: llama threadpool init, n_threads = 4
0.01.173.579 I 
0.01.173.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.173.596 I 
0.01.173.824 I sampler seed: 1234
0.01.173.829 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.173.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.173.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.173.841 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.259.601 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.259.602 I llama_perf_context_print:        load time =    1163.69 ms
0.02.259.602 I llama_perf_context_print: prompt eval time =      39.96 ms /     7 tokens (    5.71 ms per token,   175.17 tokens per second)
0.02.259.603 I llama_perf_context_print:        eval time =    1042.82 ms /    63 runs   (   16.55 ms per token,    60.41 tokens per second)
0.02.259.604 I llama_perf_context_print:       total time =    1086.05 ms /    70 tokens
0.02.259.807 I ggml_metal_free: deallocating

real	0m2.280s
user	0m0.119s
sys	0m0.233s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.019.201 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.647 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.648 I llama_model_loader: - type  f32:  194 tensors
0.00.045.648 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.648 I llama_model_loader: - type q6_K:    1 tensors
0.00.072.861 I llm_load_vocab: special tokens cache size = 25
0.00.082.217 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.082.222 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.082.222 I llm_load_print_meta: arch             = gptneox
0.00.082.223 I llm_load_print_meta: vocab type       = BPE
0.00.082.223 I llm_load_print_meta: n_vocab          = 50304
0.00.082.223 I llm_load_print_meta: n_merges         = 50009
0.00.082.223 I llm_load_print_meta: vocab_only       = 0
0.00.082.224 I llm_load_print_meta: n_ctx_train      = 2048
0.00.082.224 I llm_load_print_meta: n_embd           = 2048
0.00.082.224 I llm_load_print_meta: n_layer          = 24
0.00.082.229 I llm_load_print_meta: n_head           = 16
0.00.082.232 I llm_load_print_meta: n_head_kv        = 16
0.00.082.233 I llm_load_print_meta: n_rot            = 32
0.00.082.233 I llm_load_print_meta: n_swa            = 0
0.00.082.233 I llm_load_print_meta: n_embd_head_k    = 128
0.00.082.234 I llm_load_print_meta: n_embd_head_v    = 128
0.00.082.235 I llm_load_print_meta: n_gqa            = 1
0.00.082.237 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.082.247 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.082.249 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.082.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.082.250 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.082.251 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.082.251 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.082.264 I llm_load_print_meta: n_ff             = 8192
0.00.082.264 I llm_load_print_meta: n_expert         = 0
0.00.082.265 I llm_load_print_meta: n_expert_used    = 0
0.00.082.265 I llm_load_print_meta: causal attn      = 1
0.00.082.267 I llm_load_print_meta: pooling type     = 0
0.00.082.267 I llm_load_print_meta: rope type        = 2
0.00.082.268 I llm_load_print_meta: rope scaling     = linear
0.00.082.268 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.082.269 I llm_load_print_meta: freq_scale_train = 1
0.00.082.270 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.082.270 I llm_load_print_meta: rope_finetuned   = unknown
0.00.082.270 I llm_load_print_meta: ssm_d_conv       = 0
0.00.082.270 I llm_load_print_meta: ssm_d_inner      = 0
0.00.082.271 I llm_load_print_meta: ssm_d_state      = 0
0.00.082.271 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.082.271 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.082.274 I llm_load_print_meta: model type       = 1.4B
0.00.082.275 I llm_load_print_meta: model ftype      = Q4_0
0.00.082.275 I llm_load_print_meta: model params     = 1.41 B
0.00.082.276 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.082.277 I llm_load_print_meta: general.name     = 1.4B
0.00.082.277 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.082.278 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.082.280 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.082.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.082.281 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.082.281 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.082.281 I llm_load_print_meta: max token length = 1024
0.00.085.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.617 I llm_load_tensors: offloading output layer to GPU
0.00.085.618 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.631 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.085.633 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.087.205 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.206 I llama_new_context_with_model: n_ctx         = 2048
0.00.087.206 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.087.207 I llama_new_context_with_model: n_batch       = 2048
0.00.087.207 I llama_new_context_with_model: n_ubatch      = 512
0.00.087.207 I llama_new_context_with_model: flash_attn    = 0
0.00.087.208 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.208 I llama_new_context_with_model: freq_scale    = 1
0.00.087.209 I ggml_metal_init: allocating
0.00.087.214 I ggml_metal_init: found device: Apple M4
0.00.087.217 I ggml_metal_init: picking default device: Apple M4
0.00.088.290 I ggml_metal_init: using embedded metal library
0.00.092.400 I ggml_metal_init: GPU name:   Apple M4
0.00.092.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.404 I ggml_metal_init: simdgroup reduction   = true
0.00.092.404 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.404 I ggml_metal_init: has bfloat            = true
0.00.092.404 I ggml_metal_init: use bfloat            = true
0.00.092.405 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.332 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.130.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.037 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.084 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.131.222 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.131.225 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.131.225 I llama_new_context_with_model: graph nodes  = 967
0.00.131.225 I llama_new_context_with_model: graph splits = 2
0.00.131.229 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.131.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.131.371 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.249 I main: llama threadpool init, n_threads = 4
0.00.718.288 I 
0.00.718.311 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.311 I 
0.00.718.551 I sampler seed: 1234
0.00.718.555 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.576 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.577 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.577 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.409.788 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.409.789 I llama_perf_context_print:        load time =     699.04 ms
0.01.409.789 I llama_perf_context_print: prompt eval time =      44.78 ms /     7 tokens (    6.40 ms per token,   156.33 tokens per second)
0.01.409.791 I llama_perf_context_print:        eval time =     643.37 ms /    63 runs   (   10.21 ms per token,    97.92 tokens per second)
0.01.409.791 I llama_perf_context_print:       total time =     691.54 ms /    70 tokens
0.01.410.021 I ggml_metal_free: deallocating

real	0m1.427s
user	0m0.130s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.614 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.312 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.262 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.218 I llama_model_loader: - type  f32:  194 tensors
0.00.024.218 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.232 I llm_load_vocab: special tokens cache size = 25
0.00.050.200 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.203 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.203 I llm_load_print_meta: arch             = gptneox
0.00.050.203 I llm_load_print_meta: vocab type       = BPE
0.00.050.204 I llm_load_print_meta: n_vocab          = 50304
0.00.050.204 I llm_load_print_meta: n_merges         = 50009
0.00.050.204 I llm_load_print_meta: vocab_only       = 0
0.00.050.204 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.204 I llm_load_print_meta: n_embd           = 2048
0.00.050.204 I llm_load_print_meta: n_layer          = 24
0.00.050.207 I llm_load_print_meta: n_head           = 16
0.00.050.208 I llm_load_print_meta: n_head_kv        = 16
0.00.050.208 I llm_load_print_meta: n_rot            = 32
0.00.050.208 I llm_load_print_meta: n_swa            = 0
0.00.050.209 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.209 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.210 I llm_load_print_meta: n_gqa            = 1
0.00.050.215 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.216 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.216 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.217 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.217 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.217 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.217 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.218 I llm_load_print_meta: n_ff             = 8192
0.00.050.218 I llm_load_print_meta: n_expert         = 0
0.00.050.219 I llm_load_print_meta: n_expert_used    = 0
0.00.050.219 I llm_load_print_meta: causal attn      = 1
0.00.050.219 I llm_load_print_meta: pooling type     = 0
0.00.050.219 I llm_load_print_meta: rope type        = 2
0.00.050.219 I llm_load_print_meta: rope scaling     = linear
0.00.050.220 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.220 I llm_load_print_meta: freq_scale_train = 1
0.00.050.220 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.220 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.221 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.221 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.221 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.221 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.221 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.222 I llm_load_print_meta: model type       = 1.4B
0.00.050.222 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.222 I llm_load_print_meta: model params     = 1.41 B
0.00.050.223 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.223 I llm_load_print_meta: general.name     = 1.4B
0.00.050.223 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.223 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.224 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.224 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.224 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.224 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.224 I llm_load_print_meta: max token length = 1024
0.00.052.135 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.135 I llm_load_tensors: offloading output layer to GPU
0.00.052.135 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.149 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.150 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.011 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.012 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.012 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.012 I llama_new_context_with_model: n_batch       = 2048
0.00.053.013 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.013 I llama_new_context_with_model: flash_attn    = 0
0.00.053.013 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.013 I llama_new_context_with_model: freq_scale    = 1
0.00.053.014 I ggml_metal_init: allocating
0.00.053.021 I ggml_metal_init: found device: Apple M4
0.00.053.023 I ggml_metal_init: picking default device: Apple M4
0.00.053.615 I ggml_metal_init: using embedded metal library
0.00.055.951 I ggml_metal_init: GPU name:   Apple M4
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.954 I ggml_metal_init: simdgroup reduction   = true
0.00.055.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.954 I ggml_metal_init: has bfloat            = true
0.00.055.954 I ggml_metal_init: use bfloat            = true
0.00.055.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.309 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.163 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.197 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.186 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.188 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.188 I llama_new_context_with_model: graph nodes  = 967
0.00.086.188 I llama_new_context_with_model: graph splits = 2
0.00.086.191 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.315 I main: llama threadpool init, n_threads = 4
0.00.671.350 I 
0.00.671.370 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.370 I 
0.00.671.530 I sampler seed: 1234
0.00.671.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.671.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.671.567 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.671.567 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.401.120 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.401.120 I llama_perf_context_print:        load time =     662.69 ms
0.01.401.121 I llama_perf_context_print: prompt eval time =      39.55 ms /     7 tokens (    5.65 ms per token,   176.98 tokens per second)
0.01.401.122 I llama_perf_context_print:        eval time =     687.07 ms /    63 runs   (   10.91 ms per token,    91.69 tokens per second)
0.01.401.123 I llama_perf_context_print:       total time =     729.81 ms /    70 tokens
0.01.401.387 I ggml_metal_free: deallocating

real	0m1.418s
user	0m0.108s
sys	0m0.134s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.915 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.467 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.470 I llama_model_loader: - type  f32:  194 tensors
0.00.025.470 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.472 I llm_load_vocab: special tokens cache size = 25
0.00.051.483 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.486 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.486 I llm_load_print_meta: arch             = gptneox
0.00.051.487 I llm_load_print_meta: vocab type       = BPE
0.00.051.487 I llm_load_print_meta: n_vocab          = 50304
0.00.051.487 I llm_load_print_meta: n_merges         = 50009
0.00.051.487 I llm_load_print_meta: vocab_only       = 0
0.00.051.488 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.488 I llm_load_print_meta: n_embd           = 2048
0.00.051.488 I llm_load_print_meta: n_layer          = 24
0.00.051.491 I llm_load_print_meta: n_head           = 16
0.00.051.494 I llm_load_print_meta: n_head_kv        = 16
0.00.051.494 I llm_load_print_meta: n_rot            = 32
0.00.051.495 I llm_load_print_meta: n_swa            = 0
0.00.051.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.496 I llm_load_print_meta: n_gqa            = 1
0.00.051.497 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.497 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.498 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.498 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.499 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.499 I llm_load_print_meta: n_ff             = 8192
0.00.051.499 I llm_load_print_meta: n_expert         = 0
0.00.051.500 I llm_load_print_meta: n_expert_used    = 0
0.00.051.501 I llm_load_print_meta: causal attn      = 1
0.00.051.502 I llm_load_print_meta: pooling type     = 0
0.00.051.502 I llm_load_print_meta: rope type        = 2
0.00.051.502 I llm_load_print_meta: rope scaling     = linear
0.00.051.502 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.503 I llm_load_print_meta: freq_scale_train = 1
0.00.051.503 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.503 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.503 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.503 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.504 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.504 I llm_load_print_meta: model type       = 1.4B
0.00.051.505 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.505 I llm_load_print_meta: model params     = 1.41 B
0.00.051.506 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.506 I llm_load_print_meta: general.name     = 1.4B
0.00.051.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.506 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.507 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.507 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.507 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.508 I llm_load_print_meta: max token length = 1024
0.00.053.443 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.443 I llm_load_tensors: offloading output layer to GPU
0.00.053.444 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.454 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.455 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.338 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.339 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.339 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.339 I llama_new_context_with_model: n_batch       = 2048
0.00.054.340 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.340 I llama_new_context_with_model: flash_attn    = 0
0.00.054.340 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.340 I llama_new_context_with_model: freq_scale    = 1
0.00.054.341 I ggml_metal_init: allocating
0.00.054.348 I ggml_metal_init: found device: Apple M4
0.00.054.350 I ggml_metal_init: picking default device: Apple M4
0.00.054.937 I ggml_metal_init: using embedded metal library
0.00.057.284 I ggml_metal_init: GPU name:   Apple M4
0.00.057.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.285 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.286 I ggml_metal_init: simdgroup reduction   = true
0.00.057.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.286 I ggml_metal_init: has bfloat            = true
0.00.057.286 I ggml_metal_init: use bfloat            = true
0.00.057.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.289 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.197 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.242 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.190 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.191 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.192 I llama_new_context_with_model: graph nodes  = 967
0.00.086.192 I llama_new_context_with_model: graph splits = 2
0.00.086.195 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.338 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.194 I main: llama threadpool init, n_threads = 4
0.00.760.245 I 
0.00.760.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.270 I 
0.00.760.504 I sampler seed: 1234
0.00.760.508 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.552 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.552 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.538.095 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.538.095 I llama_perf_context_print:        load time =     750.27 ms
0.01.538.096 I llama_perf_context_print: prompt eval time =      43.06 ms /     7 tokens (    6.15 ms per token,   162.56 tokens per second)
0.01.538.099 I llama_perf_context_print:        eval time =     731.52 ms /    63 runs   (   11.61 ms per token,    86.12 tokens per second)
0.01.538.099 I llama_perf_context_print:       total time =     777.91 ms /    70 tokens
0.01.538.308 I ggml_metal_free: deallocating

real	0m1.555s
user	0m0.107s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.702 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.640 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.641 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.644 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.645 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.650 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.397 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.398 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.398 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.399 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.399 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.400 I llama_model_loader: - type  f32:  194 tensors
0.00.024.400 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.400 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.456 I llm_load_vocab: special tokens cache size = 25
0.00.050.427 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.430 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.431 I llm_load_print_meta: arch             = gptneox
0.00.050.431 I llm_load_print_meta: vocab type       = BPE
0.00.050.431 I llm_load_print_meta: n_vocab          = 50304
0.00.050.432 I llm_load_print_meta: n_merges         = 50009
0.00.050.432 I llm_load_print_meta: vocab_only       = 0
0.00.050.432 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.432 I llm_load_print_meta: n_embd           = 2048
0.00.050.432 I llm_load_print_meta: n_layer          = 24
0.00.050.435 I llm_load_print_meta: n_head           = 16
0.00.050.436 I llm_load_print_meta: n_head_kv        = 16
0.00.050.436 I llm_load_print_meta: n_rot            = 32
0.00.050.436 I llm_load_print_meta: n_swa            = 0
0.00.050.437 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.437 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.438 I llm_load_print_meta: n_gqa            = 1
0.00.050.438 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.439 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.440 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.440 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.440 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.440 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.440 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.441 I llm_load_print_meta: n_ff             = 8192
0.00.050.441 I llm_load_print_meta: n_expert         = 0
0.00.050.441 I llm_load_print_meta: n_expert_used    = 0
0.00.050.443 I llm_load_print_meta: causal attn      = 1
0.00.050.445 I llm_load_print_meta: pooling type     = 0
0.00.050.445 I llm_load_print_meta: rope type        = 2
0.00.050.445 I llm_load_print_meta: rope scaling     = linear
0.00.050.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.446 I llm_load_print_meta: freq_scale_train = 1
0.00.050.446 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.446 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.449 I llm_load_print_meta: model type       = 1.4B
0.00.050.449 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.449 I llm_load_print_meta: model params     = 1.41 B
0.00.050.450 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.450 I llm_load_print_meta: general.name     = 1.4B
0.00.050.451 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.451 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.451 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.454 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.455 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.455 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.456 I llm_load_print_meta: max token length = 1024
0.00.052.472 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.472 I llm_load_tensors: offloading output layer to GPU
0.00.052.472 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.483 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.484 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.442 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.442 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.443 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.443 I llama_new_context_with_model: n_batch       = 2048
0.00.053.443 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.443 I llama_new_context_with_model: flash_attn    = 0
0.00.053.444 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.444 I llama_new_context_with_model: freq_scale    = 1
0.00.053.444 I ggml_metal_init: allocating
0.00.053.451 I ggml_metal_init: found device: Apple M4
0.00.053.453 I ggml_metal_init: picking default device: Apple M4
0.00.054.076 I ggml_metal_init: using embedded metal library
0.00.056.404 I ggml_metal_init: GPU name:   Apple M4
0.00.056.405 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.406 I ggml_metal_init: simdgroup reduction   = true
0.00.056.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.408 I ggml_metal_init: has bfloat            = true
0.00.056.408 I ggml_metal_init: use bfloat            = true
0.00.056.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.133 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.477 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.484 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.514 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.554 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.556 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.556 I llama_new_context_with_model: graph nodes  = 967
0.00.086.556 I llama_new_context_with_model: graph splits = 2
0.00.086.559 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.815.700 I main: llama threadpool init, n_threads = 4
0.00.815.747 I 
0.00.815.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.815.779 I 
0.00.816.052 I sampler seed: 1234
0.00.816.057 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.094 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.102 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.102 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.654.965 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.01.654.966 I llama_perf_context_print:        load time =     806.99 ms
0.01.654.966 I llama_perf_context_print: prompt eval time =      42.26 ms /     7 tokens (    6.04 ms per token,   165.63 tokens per second)
0.01.654.967 I llama_perf_context_print:        eval time =     793.57 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.654.967 I llama_perf_context_print:       total time =     839.27 ms /    70 tokens
0.01.655.226 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.808 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.151 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.153 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.955 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.956 I llama_model_loader: - type  f32:  194 tensors
0.00.023.956 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.957 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.957 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.935 I llm_load_vocab: special tokens cache size = 25
0.00.049.999 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.001 I llm_load_print_meta: arch             = gptneox
0.00.050.002 I llm_load_print_meta: vocab type       = BPE
0.00.050.002 I llm_load_print_meta: n_vocab          = 50304
0.00.050.002 I llm_load_print_meta: n_merges         = 50009
0.00.050.002 I llm_load_print_meta: vocab_only       = 0
0.00.050.002 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.003 I llm_load_print_meta: n_embd           = 2048
0.00.050.003 I llm_load_print_meta: n_layer          = 24
0.00.050.005 I llm_load_print_meta: n_head           = 16
0.00.050.006 I llm_load_print_meta: n_head_kv        = 16
0.00.050.006 I llm_load_print_meta: n_rot            = 32
0.00.050.006 I llm_load_print_meta: n_swa            = 0
0.00.050.007 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.007 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.007 I llm_load_print_meta: n_gqa            = 1
0.00.050.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.009 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.010 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.012 I llm_load_print_meta: n_ff             = 8192
0.00.050.012 I llm_load_print_meta: n_expert         = 0
0.00.050.012 I llm_load_print_meta: n_expert_used    = 0
0.00.050.012 I llm_load_print_meta: causal attn      = 1
0.00.050.013 I llm_load_print_meta: pooling type     = 0
0.00.050.014 I llm_load_print_meta: rope type        = 2
0.00.050.015 I llm_load_print_meta: rope scaling     = linear
0.00.050.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.015 I llm_load_print_meta: freq_scale_train = 1
0.00.050.015 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.016 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.016 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.016 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.016 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.016 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.016 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.017 I llm_load_print_meta: model type       = 1.4B
0.00.050.017 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.018 I llm_load_print_meta: model params     = 1.41 B
0.00.050.018 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.018 I llm_load_print_meta: general.name     = 1.4B
0.00.050.023 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.024 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.024 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.024 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.025 I llm_load_print_meta: max token length = 1024
0.00.051.862 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.862 I llm_load_tensors: offloading output layer to GPU
0.00.051.862 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.872 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.874 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.790 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.790 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.791 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.791 I llama_new_context_with_model: n_batch       = 2048
0.00.052.791 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.791 I llama_new_context_with_model: flash_attn    = 0
0.00.052.792 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.792 I llama_new_context_with_model: freq_scale    = 1
0.00.052.792 I ggml_metal_init: allocating
0.00.052.796 I ggml_metal_init: found device: Apple M4
0.00.052.798 I ggml_metal_init: picking default device: Apple M4
0.00.053.375 I ggml_metal_init: using embedded metal library
0.00.055.667 I ggml_metal_init: GPU name:   Apple M4
0.00.055.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.669 I ggml_metal_init: simdgroup reduction   = true
0.00.055.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.670 I ggml_metal_init: has bfloat            = true
0.00.055.670 I ggml_metal_init: use bfloat            = true
0.00.055.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.214 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.303 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.308 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.336 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.338 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.338 I llama_new_context_with_model: graph nodes  = 967
0.00.085.339 I llama_new_context_with_model: graph splits = 2
0.00.085.341 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.349 I main: llama threadpool init, n_threads = 4
0.00.506.399 I 
0.00.506.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.433 I 
0.00.506.666 I sampler seed: 1234
0.00.506.672 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.710 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.186.176 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.186.177 I llama_perf_context_print:        load time =     496.53 ms
0.01.186.178 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.51 tokens per second)
0.01.186.178 I llama_perf_context_print:        eval time =     640.63 ms /    63 runs   (   10.17 ms per token,    98.34 tokens per second)
0.01.186.179 I llama_perf_context_print:       total time =     679.83 ms /    70 tokens
0.01.186.401 I ggml_metal_free: deallocating

real	0m1.204s
user	0m0.109s
sys	0m0.113s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.617 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.904 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.904 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.909 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.912 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.855 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.824 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.826 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.826 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.827 I llama_model_loader: - type  f32:  194 tensors
0.00.023.828 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.828 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.828 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.828 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.680 I llm_load_vocab: special tokens cache size = 25
0.00.050.573 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.576 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.577 I llm_load_print_meta: arch             = gptneox
0.00.050.577 I llm_load_print_meta: vocab type       = BPE
0.00.050.577 I llm_load_print_meta: n_vocab          = 50304
0.00.050.577 I llm_load_print_meta: n_merges         = 50009
0.00.050.578 I llm_load_print_meta: vocab_only       = 0
0.00.050.578 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.578 I llm_load_print_meta: n_embd           = 2048
0.00.050.578 I llm_load_print_meta: n_layer          = 24
0.00.050.581 I llm_load_print_meta: n_head           = 16
0.00.050.582 I llm_load_print_meta: n_head_kv        = 16
0.00.050.582 I llm_load_print_meta: n_rot            = 32
0.00.050.582 I llm_load_print_meta: n_swa            = 0
0.00.050.582 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.583 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.585 I llm_load_print_meta: n_gqa            = 1
0.00.050.586 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.587 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.587 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.588 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.589 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.589 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.590 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.590 I llm_load_print_meta: n_ff             = 8192
0.00.050.591 I llm_load_print_meta: n_expert         = 0
0.00.050.591 I llm_load_print_meta: n_expert_used    = 0
0.00.050.591 I llm_load_print_meta: causal attn      = 1
0.00.050.591 I llm_load_print_meta: pooling type     = 0
0.00.050.591 I llm_load_print_meta: rope type        = 2
0.00.050.591 I llm_load_print_meta: rope scaling     = linear
0.00.050.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.592 I llm_load_print_meta: freq_scale_train = 1
0.00.050.592 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.593 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.593 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.593 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.595 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.595 I llm_load_print_meta: model type       = 1.4B
0.00.050.595 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.596 I llm_load_print_meta: model params     = 1.41 B
0.00.050.596 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.597 I llm_load_print_meta: general.name     = 1.4B
0.00.050.597 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.597 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.597 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.597 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.599 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.599 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.599 I llm_load_print_meta: max token length = 1024
0.00.052.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.571 I llm_load_tensors: offloading output layer to GPU
0.00.052.571 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.582 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.583 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.538 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.539 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.539 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.539 I llama_new_context_with_model: n_batch       = 2048
0.00.053.539 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.539 I llama_new_context_with_model: flash_attn    = 0
0.00.053.540 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.540 I llama_new_context_with_model: freq_scale    = 1
0.00.053.541 I ggml_metal_init: allocating
0.00.053.544 I ggml_metal_init: found device: Apple M4
0.00.053.546 I ggml_metal_init: picking default device: Apple M4
0.00.054.152 I ggml_metal_init: using embedded metal library
0.00.056.481 I ggml_metal_init: GPU name:   Apple M4
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.484 I ggml_metal_init: simdgroup reduction   = true
0.00.056.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.484 I ggml_metal_init: has bfloat            = true
0.00.056.484 I ggml_metal_init: use bfloat            = true
0.00.056.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.309 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.223 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.198 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.199 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.199 I llama_new_context_with_model: graph nodes  = 967
0.00.086.199 I llama_new_context_with_model: graph splits = 2
0.00.086.202 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.349 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.439 I main: llama threadpool init, n_threads = 4
0.00.532.477 I 
0.00.532.497 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.498 I 
0.00.532.722 I sampler seed: 1234
0.00.532.726 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.737 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.737 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.738 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.282.283 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.282.283 I llama_perf_context_print:        load time =     523.81 ms
0.01.282.284 I llama_perf_context_print: prompt eval time =      44.53 ms /     7 tokens (    6.36 ms per token,   157.18 tokens per second)
0.01.282.285 I llama_perf_context_print:        eval time =     701.90 ms /    63 runs   (   11.14 ms per token,    89.76 tokens per second)
0.01.282.285 I llama_perf_context_print:       total time =     749.85 ms /    70 tokens
0.01.282.478 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.092 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.948 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.949 I llama_model_loader: - type  f32:  194 tensors
0.00.022.949 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.949 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.949 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.737 I llm_load_vocab: special tokens cache size = 25
0.00.049.919 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.922 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.922 I llm_load_print_meta: arch             = gptneox
0.00.049.923 I llm_load_print_meta: vocab type       = BPE
0.00.049.923 I llm_load_print_meta: n_vocab          = 50304
0.00.049.923 I llm_load_print_meta: n_merges         = 50009
0.00.049.923 I llm_load_print_meta: vocab_only       = 0
0.00.049.923 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.923 I llm_load_print_meta: n_embd           = 2048
0.00.049.924 I llm_load_print_meta: n_layer          = 24
0.00.049.926 I llm_load_print_meta: n_head           = 16
0.00.049.927 I llm_load_print_meta: n_head_kv        = 16
0.00.049.927 I llm_load_print_meta: n_rot            = 32
0.00.049.927 I llm_load_print_meta: n_swa            = 0
0.00.049.928 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.928 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.928 I llm_load_print_meta: n_gqa            = 1
0.00.049.929 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.930 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.930 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.931 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.931 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.931 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.931 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.932 I llm_load_print_meta: n_ff             = 8192
0.00.049.932 I llm_load_print_meta: n_expert         = 0
0.00.049.932 I llm_load_print_meta: n_expert_used    = 0
0.00.049.933 I llm_load_print_meta: causal attn      = 1
0.00.049.933 I llm_load_print_meta: pooling type     = 0
0.00.049.933 I llm_load_print_meta: rope type        = 2
0.00.049.933 I llm_load_print_meta: rope scaling     = linear
0.00.049.933 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.934 I llm_load_print_meta: freq_scale_train = 1
0.00.049.934 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.934 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.934 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.935 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.935 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.935 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.937 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.937 I llm_load_print_meta: model type       = 1.4B
0.00.049.938 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.938 I llm_load_print_meta: model params     = 1.41 B
0.00.049.939 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.939 I llm_load_print_meta: general.name     = 1.4B
0.00.049.939 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.940 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.944 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.944 I llm_load_print_meta: max token length = 1024
0.00.051.915 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.916 I llm_load_tensors: offloading output layer to GPU
0.00.051.916 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.926 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.928 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.825 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.826 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.826 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.826 I llama_new_context_with_model: n_batch       = 2048
0.00.052.826 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.827 I llama_new_context_with_model: flash_attn    = 0
0.00.052.827 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.827 I llama_new_context_with_model: freq_scale    = 1
0.00.052.828 I ggml_metal_init: allocating
0.00.052.831 I ggml_metal_init: found device: Apple M4
0.00.052.833 I ggml_metal_init: picking default device: Apple M4
0.00.053.409 I ggml_metal_init: using embedded metal library
0.00.055.754 I ggml_metal_init: GPU name:   Apple M4
0.00.055.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.757 I ggml_metal_init: simdgroup reduction   = true
0.00.055.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.757 I ggml_metal_init: has bfloat            = true
0.00.055.757 I ggml_metal_init: use bfloat            = true
0.00.055.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.545 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.060 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.055 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.057 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.057 I llama_new_context_with_model: graph nodes  = 967
0.00.086.057 I llama_new_context_with_model: graph splits = 2
0.00.086.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.764 I main: llama threadpool init, n_threads = 4
0.00.618.811 I 
0.00.618.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.840 I 
0.00.619.110 I sampler seed: 1234
0.00.619.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.619.168 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.619.171 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.619.171 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.371.669 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47940.58 tokens per second)
0.01.371.670 I llama_perf_context_print:        load time =     610.08 ms
0.01.371.672 I llama_perf_context_print: prompt eval time =      47.16 ms /     7 tokens (    6.74 ms per token,   148.42 tokens per second)
0.01.371.673 I llama_perf_context_print:        eval time =     702.46 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.371.673 I llama_perf_context_print:       total time =     752.91 ms /    70 tokens
0.01.371.897 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.110s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.424 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.427 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.429 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.599 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.725 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.726 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.475 I llm_load_vocab: special tokens cache size = 25
0.00.051.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.641 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.642 I llm_load_print_meta: arch             = gptneox
0.00.051.642 I llm_load_print_meta: vocab type       = BPE
0.00.051.642 I llm_load_print_meta: n_vocab          = 50304
0.00.051.642 I llm_load_print_meta: n_merges         = 50009
0.00.051.643 I llm_load_print_meta: vocab_only       = 0
0.00.051.643 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.643 I llm_load_print_meta: n_embd           = 2048
0.00.051.643 I llm_load_print_meta: n_layer          = 24
0.00.051.646 I llm_load_print_meta: n_head           = 16
0.00.051.647 I llm_load_print_meta: n_head_kv        = 16
0.00.051.647 I llm_load_print_meta: n_rot            = 32
0.00.051.650 I llm_load_print_meta: n_swa            = 0
0.00.051.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.651 I llm_load_print_meta: n_gqa            = 1
0.00.051.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.652 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.653 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.653 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.653 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.654 I llm_load_print_meta: n_ff             = 8192
0.00.051.655 I llm_load_print_meta: n_expert         = 0
0.00.051.655 I llm_load_print_meta: n_expert_used    = 0
0.00.051.655 I llm_load_print_meta: causal attn      = 1
0.00.051.655 I llm_load_print_meta: pooling type     = 0
0.00.051.656 I llm_load_print_meta: rope type        = 2
0.00.051.656 I llm_load_print_meta: rope scaling     = linear
0.00.051.657 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.657 I llm_load_print_meta: freq_scale_train = 1
0.00.051.657 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.657 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.658 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.658 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.658 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.658 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.658 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.659 I llm_load_print_meta: model type       = 1.4B
0.00.051.659 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.660 I llm_load_print_meta: model params     = 1.41 B
0.00.051.660 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.660 I llm_load_print_meta: general.name     = 1.4B
0.00.051.661 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.662 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.662 I llm_load_print_meta: max token length = 1024
0.00.053.686 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.686 I llm_load_tensors: offloading output layer to GPU
0.00.053.686 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.697 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.698 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.624 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.624 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.624 I llama_new_context_with_model: n_batch       = 2048
0.00.054.624 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.625 I llama_new_context_with_model: flash_attn    = 0
0.00.054.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.625 I llama_new_context_with_model: freq_scale    = 1
0.00.054.626 I ggml_metal_init: allocating
0.00.054.632 I ggml_metal_init: found device: Apple M4
0.00.054.635 I ggml_metal_init: picking default device: Apple M4
0.00.055.219 I ggml_metal_init: using embedded metal library
0.00.057.532 I ggml_metal_init: GPU name:   Apple M4
0.00.057.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.534 I ggml_metal_init: simdgroup reduction   = true
0.00.057.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.535 I ggml_metal_init: has bfloat            = true
0.00.057.535 I ggml_metal_init: use bfloat            = true
0.00.057.535 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.536 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.135 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.304 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.310 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.338 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.289 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.291 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.291 I llama_new_context_with_model: graph nodes  = 967
0.00.087.291 I llama_new_context_with_model: graph splits = 2
0.00.087.294 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.321 I main: llama threadpool init, n_threads = 4
0.00.682.365 I 
0.00.682.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.386 I 
0.00.682.617 I sampler seed: 1234
0.00.682.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.633 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.633 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.633 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.531.572 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62776.30 tokens per second)
0.01.531.572 I llama_perf_context_print:        load time =     672.68 ms
0.01.531.573 I llama_perf_context_print: prompt eval time =      51.61 ms /     7 tokens (    7.37 ms per token,   135.64 tokens per second)
0.01.531.575 I llama_perf_context_print:        eval time =     794.41 ms /    63 runs   (   12.61 ms per token,    79.30 tokens per second)
0.01.531.575 I llama_perf_context_print:       total time =     849.26 ms /    70 tokens
0.01.531.770 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.110s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.206 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.872 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.873 I llama_model_loader: - type  f32:  194 tensors
0.00.022.873 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.909 I llm_load_vocab: special tokens cache size = 25
0.00.048.955 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.957 I llm_load_print_meta: arch             = gptneox
0.00.048.958 I llm_load_print_meta: vocab type       = BPE
0.00.048.958 I llm_load_print_meta: n_vocab          = 50304
0.00.048.958 I llm_load_print_meta: n_merges         = 50009
0.00.048.958 I llm_load_print_meta: vocab_only       = 0
0.00.048.959 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.959 I llm_load_print_meta: n_embd           = 2048
0.00.048.959 I llm_load_print_meta: n_layer          = 24
0.00.048.961 I llm_load_print_meta: n_head           = 16
0.00.048.962 I llm_load_print_meta: n_head_kv        = 16
0.00.048.962 I llm_load_print_meta: n_rot            = 32
0.00.048.963 I llm_load_print_meta: n_swa            = 0
0.00.048.963 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.963 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.965 I llm_load_print_meta: n_gqa            = 1
0.00.048.965 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.966 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.967 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.967 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.967 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.967 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.968 I llm_load_print_meta: n_ff             = 8192
0.00.048.968 I llm_load_print_meta: n_expert         = 0
0.00.048.968 I llm_load_print_meta: n_expert_used    = 0
0.00.048.969 I llm_load_print_meta: causal attn      = 1
0.00.048.969 I llm_load_print_meta: pooling type     = 0
0.00.048.969 I llm_load_print_meta: rope type        = 2
0.00.048.969 I llm_load_print_meta: rope scaling     = linear
0.00.048.969 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.970 I llm_load_print_meta: freq_scale_train = 1
0.00.048.970 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.970 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.970 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.970 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.971 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.973 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.973 I llm_load_print_meta: model type       = 1.4B
0.00.048.974 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.974 I llm_load_print_meta: model params     = 1.41 B
0.00.048.974 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.975 I llm_load_print_meta: general.name     = 1.4B
0.00.048.975 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.975 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.975 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.975 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.976 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.976 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.976 I llm_load_print_meta: max token length = 1024
0.00.050.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.962 I llm_load_tensors: offloading output layer to GPU
0.00.050.962 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.973 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.974 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.863 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.864 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.864 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.864 I llama_new_context_with_model: n_batch       = 2048
0.00.051.864 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.864 I llama_new_context_with_model: flash_attn    = 0
0.00.051.865 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.865 I llama_new_context_with_model: freq_scale    = 1
0.00.051.866 I ggml_metal_init: allocating
0.00.051.872 I ggml_metal_init: found device: Apple M4
0.00.051.874 I ggml_metal_init: picking default device: Apple M4
0.00.052.450 I ggml_metal_init: using embedded metal library
0.00.054.775 I ggml_metal_init: GPU name:   Apple M4
0.00.054.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.777 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.777 I ggml_metal_init: simdgroup reduction   = true
0.00.054.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.778 I ggml_metal_init: has bfloat            = true
0.00.054.778 I ggml_metal_init: use bfloat            = true
0.00.054.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.779 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.148 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.127 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.136 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.170 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.063 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.064 I llama_new_context_with_model: graph nodes  = 967
0.00.084.064 I llama_new_context_with_model: graph splits = 2
0.00.084.067 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.033 I main: llama threadpool init, n_threads = 4
0.00.736.069 I 
0.00.736.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.089 I 
0.00.736.250 I sampler seed: 1234
0.00.736.255 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.280 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.281 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.281 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.615.813 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.615.814 I llama_perf_context_print:        load time =     727.27 ms
0.01.615.815 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.63 tokens per second)
0.01.615.815 I llama_perf_context_print:        eval time =     822.06 ms /    63 runs   (   13.05 ms per token,    76.64 tokens per second)
0.01.615.816 I llama_perf_context_print:       total time =     879.78 ms /    70 tokens
0.01.616.042 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.109s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.592 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.515 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.632 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.642 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.643 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.644 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.647 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.647 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.648 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.961 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.962 I llama_model_loader: - type  f32:  194 tensors
0.00.051.962 I llama_model_loader: - type  f16:   98 tensors
0.00.080.441 I llm_load_vocab: special tokens cache size = 25
0.00.086.839 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.843 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.844 I llm_load_print_meta: arch             = gptneox
0.00.086.844 I llm_load_print_meta: vocab type       = BPE
0.00.086.844 I llm_load_print_meta: n_vocab          = 50304
0.00.086.849 I llm_load_print_meta: n_merges         = 50009
0.00.086.849 I llm_load_print_meta: vocab_only       = 0
0.00.086.850 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.850 I llm_load_print_meta: n_embd           = 2048
0.00.086.850 I llm_load_print_meta: n_layer          = 24
0.00.086.853 I llm_load_print_meta: n_head           = 16
0.00.086.853 I llm_load_print_meta: n_head_kv        = 16
0.00.086.854 I llm_load_print_meta: n_rot            = 32
0.00.086.854 I llm_load_print_meta: n_swa            = 0
0.00.086.854 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.854 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.855 I llm_load_print_meta: n_gqa            = 1
0.00.086.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.856 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.857 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.857 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.857 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.858 I llm_load_print_meta: n_ff             = 8192
0.00.086.858 I llm_load_print_meta: n_expert         = 0
0.00.086.858 I llm_load_print_meta: n_expert_used    = 0
0.00.086.858 I llm_load_print_meta: causal attn      = 1
0.00.086.858 I llm_load_print_meta: pooling type     = 0
0.00.086.858 I llm_load_print_meta: rope type        = 2
0.00.086.858 I llm_load_print_meta: rope scaling     = linear
0.00.086.859 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.859 I llm_load_print_meta: freq_scale_train = 1
0.00.086.859 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.859 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.859 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.860 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.860 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.860 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.860 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.860 I llm_load_print_meta: model type       = 1.4B
0.00.086.861 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.861 I llm_load_print_meta: model params     = 1.41 B
0.00.086.862 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.862 I llm_load_print_meta: general.name     = 1.4B
0.00.086.862 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.862 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.862 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.863 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.863 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.863 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.863 I llm_load_print_meta: max token length = 1024
0.00.089.375 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.376 I llm_load_tensors: offloading output layer to GPU
0.00.089.376 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.387 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.388 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.324 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.324 I llama_new_context_with_model: n_ctx         = 128
0.00.090.325 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.325 I llama_new_context_with_model: n_batch       = 128
0.00.090.325 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.325 I llama_new_context_with_model: flash_attn    = 0
0.00.090.326 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.326 I llama_new_context_with_model: freq_scale    = 1
0.00.090.326 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.327 I ggml_metal_init: allocating
0.00.090.331 I ggml_metal_init: found device: Apple M4
0.00.090.333 I ggml_metal_init: picking default device: Apple M4
0.00.090.922 I ggml_metal_init: using embedded metal library
0.00.093.438 I ggml_metal_init: GPU name:   Apple M4
0.00.093.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.441 I ggml_metal_init: simdgroup reduction   = true
0.00.093.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.441 I ggml_metal_init: has bfloat            = true
0.00.093.441 I ggml_metal_init: use bfloat            = true
0.00.093.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.053 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.355 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.357 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.383 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.277 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.278 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.278 I llama_new_context_with_model: graph nodes  = 967
0.00.104.278 I llama_new_context_with_model: graph splits = 2
0.00.104.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.220.931 I 
0.01.220.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.220.994 I perplexity: tokenizing the input ..
0.01.233.822 I perplexity: tokenization took 12.824 ms
0.01.233.827 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.355.963 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.357.795 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.357.825 I llama_perf_context_print:        load time =    1199.40 ms
0.01.357.827 I llama_perf_context_print: prompt eval time =     121.26 ms /   128 tokens (    0.95 ms per token,  1055.55 tokens per second)
0.01.357.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.357.829 I llama_perf_context_print:       total time =     136.89 ms /   129 tokens
0.01.358.698 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.123s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.295 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.404 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.764 I llama_model_loader: - type  f32:  194 tensors
0.00.032.764 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.883 I llm_load_vocab: special tokens cache size = 25
0.00.066.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.311 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.311 I llm_load_print_meta: arch             = gptneox
0.00.066.312 I llm_load_print_meta: vocab type       = BPE
0.00.066.312 I llm_load_print_meta: n_vocab          = 50304
0.00.066.312 I llm_load_print_meta: n_merges         = 50009
0.00.066.312 I llm_load_print_meta: vocab_only       = 0
0.00.066.312 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.312 I llm_load_print_meta: n_embd           = 2048
0.00.066.313 I llm_load_print_meta: n_layer          = 24
0.00.066.317 I llm_load_print_meta: n_head           = 16
0.00.066.321 I llm_load_print_meta: n_head_kv        = 16
0.00.066.321 I llm_load_print_meta: n_rot            = 32
0.00.066.321 I llm_load_print_meta: n_swa            = 0
0.00.066.321 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.322 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.322 I llm_load_print_meta: n_gqa            = 1
0.00.066.323 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.324 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.324 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.325 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.325 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.325 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.325 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.326 I llm_load_print_meta: n_ff             = 8192
0.00.066.326 I llm_load_print_meta: n_expert         = 0
0.00.066.326 I llm_load_print_meta: n_expert_used    = 0
0.00.066.327 I llm_load_print_meta: causal attn      = 1
0.00.066.328 I llm_load_print_meta: pooling type     = 0
0.00.066.328 I llm_load_print_meta: rope type        = 2
0.00.066.328 I llm_load_print_meta: rope scaling     = linear
0.00.066.328 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.328 I llm_load_print_meta: freq_scale_train = 1
0.00.066.328 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.329 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.330 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.330 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.330 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.330 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.330 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.331 I llm_load_print_meta: model type       = 1.4B
0.00.066.331 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.331 I llm_load_print_meta: model params     = 1.41 B
0.00.066.332 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.332 I llm_load_print_meta: general.name     = 1.4B
0.00.066.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.333 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.334 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.334 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.334 I llm_load_print_meta: max token length = 1024
0.00.068.740 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.740 I llm_load_tensors: offloading output layer to GPU
0.00.068.740 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.751 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.752 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.733 I llama_new_context_with_model: n_ctx         = 128
0.00.069.734 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.734 I llama_new_context_with_model: n_batch       = 128
0.00.069.734 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.734 I llama_new_context_with_model: flash_attn    = 0
0.00.069.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.735 I llama_new_context_with_model: freq_scale    = 1
0.00.069.736 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.736 I ggml_metal_init: allocating
0.00.069.742 I ggml_metal_init: found device: Apple M4
0.00.069.745 I ggml_metal_init: picking default device: Apple M4
0.00.070.410 I ggml_metal_init: using embedded metal library
0.00.073.014 I ggml_metal_init: GPU name:   Apple M4
0.00.073.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.017 I ggml_metal_init: simdgroup reduction   = true
0.00.073.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.017 I ggml_metal_init: has bfloat            = true
0.00.073.018 I ggml_metal_init: use bfloat            = true
0.00.073.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.722 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.059 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.063 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.090 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.954 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.955 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.955 I llama_new_context_with_model: graph nodes  = 967
0.00.084.955 I llama_new_context_with_model: graph splits = 2
0.00.084.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.957 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.848.701 I 
0.00.848.724 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.848.761 I perplexity: tokenizing the input ..
0.00.856.466 I perplexity: tokenization took 7.704 ms
0.00.856.470 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.980.987 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.982.144 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.982.162 I llama_perf_context_print:        load time =     837.39 ms
0.00.982.163 I llama_perf_context_print: prompt eval time =     124.29 ms /   128 tokens (    0.97 ms per token,  1029.83 tokens per second)
0.00.982.164 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.982.165 I llama_perf_context_print:       total time =     133.46 ms /   129 tokens
0.00.982.713 I ggml_metal_free: deallocating

real	0m1.001s
user	0m0.095s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.475 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.407 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.409 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.891 I llm_load_vocab: special tokens cache size = 25
0.00.050.789 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.792 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.792 I llm_load_print_meta: arch             = gptneox
0.00.050.793 I llm_load_print_meta: vocab type       = BPE
0.00.050.793 I llm_load_print_meta: n_vocab          = 50304
0.00.050.793 I llm_load_print_meta: n_merges         = 50009
0.00.050.793 I llm_load_print_meta: vocab_only       = 0
0.00.050.793 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.794 I llm_load_print_meta: n_embd           = 2048
0.00.050.794 I llm_load_print_meta: n_layer          = 24
0.00.050.796 I llm_load_print_meta: n_head           = 16
0.00.050.797 I llm_load_print_meta: n_head_kv        = 16
0.00.050.797 I llm_load_print_meta: n_rot            = 32
0.00.050.798 I llm_load_print_meta: n_swa            = 0
0.00.050.798 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.798 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.799 I llm_load_print_meta: n_gqa            = 1
0.00.050.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.800 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.803 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.804 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.804 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.804 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.804 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.805 I llm_load_print_meta: n_ff             = 8192
0.00.050.805 I llm_load_print_meta: n_expert         = 0
0.00.050.805 I llm_load_print_meta: n_expert_used    = 0
0.00.050.806 I llm_load_print_meta: causal attn      = 1
0.00.050.806 I llm_load_print_meta: pooling type     = 0
0.00.050.806 I llm_load_print_meta: rope type        = 2
0.00.050.806 I llm_load_print_meta: rope scaling     = linear
0.00.050.806 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.807 I llm_load_print_meta: freq_scale_train = 1
0.00.050.807 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.807 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.808 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.808 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.808 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.809 I llm_load_print_meta: model type       = 1.4B
0.00.050.809 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.809 I llm_load_print_meta: model params     = 1.41 B
0.00.050.810 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.810 I llm_load_print_meta: general.name     = 1.4B
0.00.050.811 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.811 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.811 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.811 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.811 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.812 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.813 I llm_load_print_meta: max token length = 1024
0.00.052.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.723 I llm_load_tensors: offloading output layer to GPU
0.00.052.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.734 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.735 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.612 I llama_new_context_with_model: n_ctx         = 128
0.00.053.612 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.612 I llama_new_context_with_model: n_batch       = 128
0.00.053.612 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.612 I llama_new_context_with_model: flash_attn    = 0
0.00.053.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.613 I llama_new_context_with_model: freq_scale    = 1
0.00.053.613 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.614 I ggml_metal_init: allocating
0.00.053.620 I ggml_metal_init: found device: Apple M4
0.00.053.622 I ggml_metal_init: picking default device: Apple M4
0.00.054.194 I ggml_metal_init: using embedded metal library
0.00.056.553 I ggml_metal_init: GPU name:   Apple M4
0.00.056.554 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.555 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.555 I ggml_metal_init: simdgroup reduction   = true
0.00.056.556 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.556 I ggml_metal_init: has bfloat            = true
0.00.056.556 I ggml_metal_init: use bfloat            = true
0.00.056.556 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.557 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.158 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.414 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.418 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.443 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.331 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.332 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.332 I llama_new_context_with_model: graph nodes  = 967
0.00.068.333 I llama_new_context_with_model: graph splits = 2
0.00.068.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.159 I 
0.00.601.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.601.215 I perplexity: tokenizing the input ..
0.00.608.605 I perplexity: tokenization took 7.389 ms
0.00.608.609 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.463 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.732.700 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.732.728 I llama_perf_context_print:        load time =     591.67 ms
0.00.732.729 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.82 tokens per second)
0.00.732.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.731 I llama_perf_context_print:       total time =     131.58 ms /   129 tokens
0.00.733.273 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.996 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.928 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.936 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.937 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.938 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.939 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.940 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.753 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.634 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.634 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.634 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.634 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.635 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.635 I llama_model_loader: - type  f32:  194 tensors
0.00.023.635 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.636 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.568 I llm_load_vocab: special tokens cache size = 25
0.00.049.751 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.753 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.754 I llm_load_print_meta: arch             = gptneox
0.00.049.754 I llm_load_print_meta: vocab type       = BPE
0.00.049.754 I llm_load_print_meta: n_vocab          = 50304
0.00.049.754 I llm_load_print_meta: n_merges         = 50009
0.00.049.754 I llm_load_print_meta: vocab_only       = 0
0.00.049.755 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.755 I llm_load_print_meta: n_embd           = 2048
0.00.049.755 I llm_load_print_meta: n_layer          = 24
0.00.049.758 I llm_load_print_meta: n_head           = 16
0.00.049.759 I llm_load_print_meta: n_head_kv        = 16
0.00.049.759 I llm_load_print_meta: n_rot            = 32
0.00.049.759 I llm_load_print_meta: n_swa            = 0
0.00.049.759 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.759 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.760 I llm_load_print_meta: n_gqa            = 1
0.00.049.761 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.762 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.762 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.762 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.763 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.763 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.763 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.763 I llm_load_print_meta: n_ff             = 8192
0.00.049.764 I llm_load_print_meta: n_expert         = 0
0.00.049.764 I llm_load_print_meta: n_expert_used    = 0
0.00.049.764 I llm_load_print_meta: causal attn      = 1
0.00.049.764 I llm_load_print_meta: pooling type     = 0
0.00.049.764 I llm_load_print_meta: rope type        = 2
0.00.049.764 I llm_load_print_meta: rope scaling     = linear
0.00.049.765 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.767 I llm_load_print_meta: freq_scale_train = 1
0.00.049.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.768 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.768 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.768 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.769 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.769 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.770 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.770 I llm_load_print_meta: model type       = 1.4B
0.00.049.770 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.771 I llm_load_print_meta: model params     = 1.41 B
0.00.049.771 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.772 I llm_load_print_meta: general.name     = 1.4B
0.00.049.772 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.772 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.772 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.777 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.777 I llm_load_print_meta: max token length = 1024
0.00.051.742 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.742 I llm_load_tensors: offloading output layer to GPU
0.00.051.742 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.753 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.754 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.658 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.659 I llama_new_context_with_model: n_ctx         = 128
0.00.052.659 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.660 I llama_new_context_with_model: n_batch       = 128
0.00.052.660 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.660 I llama_new_context_with_model: flash_attn    = 0
0.00.052.661 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.661 I llama_new_context_with_model: freq_scale    = 1
0.00.052.661 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.662 I ggml_metal_init: allocating
0.00.052.668 I ggml_metal_init: found device: Apple M4
0.00.052.671 I ggml_metal_init: picking default device: Apple M4
0.00.053.284 I ggml_metal_init: using embedded metal library
0.00.055.608 I ggml_metal_init: GPU name:   Apple M4
0.00.055.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.611 I ggml_metal_init: simdgroup reduction   = true
0.00.055.611 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.611 I ggml_metal_init: has bfloat            = true
0.00.055.611 I ggml_metal_init: use bfloat            = true
0.00.055.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.235 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.241 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.269 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.143 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.144 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.144 I llama_new_context_with_model: graph nodes  = 967
0.00.067.144 I llama_new_context_with_model: graph splits = 2
0.00.067.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.223 I 
0.00.620.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.268 I perplexity: tokenizing the input ..
0.00.627.859 I perplexity: tokenization took 7.589 ms
0.00.627.863 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.750.670 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.751.823 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.751.844 I llama_perf_context_print:        load time =     611.22 ms
0.00.751.849 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.29 tokens per second)
0.00.751.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.751.851 I llama_perf_context_print:       total time =     131.62 ms /   129 tokens
0.00.752.412 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.634 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.637 I llama_model_loader: - type  f32:  194 tensors
0.00.024.637 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.637 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.418 I llm_load_vocab: special tokens cache size = 25
0.00.051.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.408 I llm_load_print_meta: arch             = gptneox
0.00.051.408 I llm_load_print_meta: vocab type       = BPE
0.00.051.408 I llm_load_print_meta: n_vocab          = 50304
0.00.051.408 I llm_load_print_meta: n_merges         = 50009
0.00.051.409 I llm_load_print_meta: vocab_only       = 0
0.00.051.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.409 I llm_load_print_meta: n_embd           = 2048
0.00.051.409 I llm_load_print_meta: n_layer          = 24
0.00.051.412 I llm_load_print_meta: n_head           = 16
0.00.051.413 I llm_load_print_meta: n_head_kv        = 16
0.00.051.413 I llm_load_print_meta: n_rot            = 32
0.00.051.413 I llm_load_print_meta: n_swa            = 0
0.00.051.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.414 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.416 I llm_load_print_meta: n_gqa            = 1
0.00.051.417 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.418 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.421 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.421 I llm_load_print_meta: n_ff             = 8192
0.00.051.423 I llm_load_print_meta: n_expert         = 0
0.00.051.423 I llm_load_print_meta: n_expert_used    = 0
0.00.051.423 I llm_load_print_meta: causal attn      = 1
0.00.051.423 I llm_load_print_meta: pooling type     = 0
0.00.051.423 I llm_load_print_meta: rope type        = 2
0.00.051.424 I llm_load_print_meta: rope scaling     = linear
0.00.051.424 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.424 I llm_load_print_meta: freq_scale_train = 1
0.00.051.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.424 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.425 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.425 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.425 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.425 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.426 I llm_load_print_meta: model type       = 1.4B
0.00.051.426 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.427 I llm_load_print_meta: model params     = 1.41 B
0.00.051.428 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.428 I llm_load_print_meta: general.name     = 1.4B
0.00.051.428 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.429 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.429 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.429 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.429 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.430 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.430 I llm_load_print_meta: max token length = 1024
0.00.053.480 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.480 I llm_load_tensors: offloading output layer to GPU
0.00.053.480 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.491 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.492 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.474 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.475 I llama_new_context_with_model: n_ctx         = 128
0.00.054.475 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.475 I llama_new_context_with_model: n_batch       = 128
0.00.054.475 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.475 I llama_new_context_with_model: flash_attn    = 0
0.00.054.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.476 I llama_new_context_with_model: freq_scale    = 1
0.00.054.476 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.477 I ggml_metal_init: allocating
0.00.054.482 I ggml_metal_init: found device: Apple M4
0.00.054.484 I ggml_metal_init: picking default device: Apple M4
0.00.055.029 I ggml_metal_init: using embedded metal library
0.00.057.386 I ggml_metal_init: GPU name:   Apple M4
0.00.057.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.388 I ggml_metal_init: simdgroup reduction   = true
0.00.057.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.389 I ggml_metal_init: has bfloat            = true
0.00.057.389 I ggml_metal_init: use bfloat            = true
0.00.057.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.777 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.978 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.980 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.005 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.850 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.851 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.851 I llama_new_context_with_model: graph nodes  = 967
0.00.068.852 I llama_new_context_with_model: graph splits = 2
0.00.068.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.916 I 
0.00.700.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.031 I perplexity: tokenizing the input ..
0.00.714.583 I perplexity: tokenization took 14.549 ms
0.00.714.589 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.331 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.867.195 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.867.228 I llama_perf_context_print:        load time =     689.96 ms
0.00.867.229 I llama_perf_context_print: prompt eval time =     147.48 ms /   128 tokens (    1.15 ms per token,   867.90 tokens per second)
0.00.867.230 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.231 I llama_perf_context_print:       total time =     167.32 ms /   129 tokens
0.00.868.047 I ggml_metal_free: deallocating

real	0m0.890s
user	0m0.108s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.719 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.720 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.727 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.728 I llama_model_loader: - type  f32:  194 tensors
0.00.023.728 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.037 I llm_load_vocab: special tokens cache size = 25
0.00.051.132 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.136 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.136 I llm_load_print_meta: arch             = gptneox
0.00.051.137 I llm_load_print_meta: vocab type       = BPE
0.00.051.137 I llm_load_print_meta: n_vocab          = 50304
0.00.051.137 I llm_load_print_meta: n_merges         = 50009
0.00.051.137 I llm_load_print_meta: vocab_only       = 0
0.00.051.137 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.142 I llm_load_print_meta: n_embd           = 2048
0.00.051.143 I llm_load_print_meta: n_layer          = 24
0.00.051.146 I llm_load_print_meta: n_head           = 16
0.00.051.147 I llm_load_print_meta: n_head_kv        = 16
0.00.051.147 I llm_load_print_meta: n_rot            = 32
0.00.051.147 I llm_load_print_meta: n_swa            = 0
0.00.051.147 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.147 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.148 I llm_load_print_meta: n_gqa            = 1
0.00.051.149 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.149 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.150 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.150 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.150 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.150 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.150 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.151 I llm_load_print_meta: n_ff             = 8192
0.00.051.151 I llm_load_print_meta: n_expert         = 0
0.00.051.151 I llm_load_print_meta: n_expert_used    = 0
0.00.051.151 I llm_load_print_meta: causal attn      = 1
0.00.051.152 I llm_load_print_meta: pooling type     = 0
0.00.051.152 I llm_load_print_meta: rope type        = 2
0.00.051.152 I llm_load_print_meta: rope scaling     = linear
0.00.051.155 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.156 I llm_load_print_meta: freq_scale_train = 1
0.00.051.156 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.157 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.157 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.157 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.157 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.157 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.158 I llm_load_print_meta: model type       = 1.4B
0.00.051.158 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.158 I llm_load_print_meta: model params     = 1.41 B
0.00.051.159 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.159 I llm_load_print_meta: general.name     = 1.4B
0.00.051.159 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.160 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.163 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.163 I llm_load_print_meta: max token length = 1024
0.00.053.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.238 I llm_load_tensors: offloading output layer to GPU
0.00.053.238 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.249 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.250 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.139 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.140 I llama_new_context_with_model: n_ctx         = 128
0.00.054.140 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.140 I llama_new_context_with_model: n_batch       = 128
0.00.054.141 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.141 I llama_new_context_with_model: flash_attn    = 0
0.00.054.141 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.141 I llama_new_context_with_model: freq_scale    = 1
0.00.054.142 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.142 I ggml_metal_init: allocating
0.00.054.146 I ggml_metal_init: found device: Apple M4
0.00.054.148 I ggml_metal_init: picking default device: Apple M4
0.00.054.785 I ggml_metal_init: using embedded metal library
0.00.057.370 I ggml_metal_init: GPU name:   Apple M4
0.00.057.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.373 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.373 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.374 I ggml_metal_init: simdgroup reduction   = true
0.00.057.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.374 I ggml_metal_init: has bfloat            = true
0.00.057.374 I ggml_metal_init: use bfloat            = true
0.00.057.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.004 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.367 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.373 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.401 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.287 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.288 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.289 I llama_new_context_with_model: graph nodes  = 967
0.00.069.289 I llama_new_context_with_model: graph splits = 2
0.00.069.290 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.926 I 
0.00.778.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.974 I perplexity: tokenizing the input ..
0.00.788.041 I perplexity: tokenization took 9.066 ms
0.00.788.045 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.167 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.923.480 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.923.495 I llama_perf_context_print:        load time =     770.17 ms
0.00.923.496 I llama_perf_context_print: prompt eval time =     133.89 ms /   128 tokens (    1.05 ms per token,   955.98 tokens per second)
0.00.923.496 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.497 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.923.873 I ggml_metal_free: deallocating

real	0m0.946s
user	0m0.084s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.521 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.186 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.951 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.952 I llama_model_loader: - type  f32:  194 tensors
0.00.023.952 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.952 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.980 I llm_load_vocab: special tokens cache size = 25
0.00.050.080 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.084 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.084 I llm_load_print_meta: arch             = gptneox
0.00.050.085 I llm_load_print_meta: vocab type       = BPE
0.00.050.085 I llm_load_print_meta: n_vocab          = 50304
0.00.050.085 I llm_load_print_meta: n_merges         = 50009
0.00.050.085 I llm_load_print_meta: vocab_only       = 0
0.00.050.085 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.087 I llm_load_print_meta: n_embd           = 2048
0.00.050.087 I llm_load_print_meta: n_layer          = 24
0.00.050.090 I llm_load_print_meta: n_head           = 16
0.00.050.091 I llm_load_print_meta: n_head_kv        = 16
0.00.050.091 I llm_load_print_meta: n_rot            = 32
0.00.050.091 I llm_load_print_meta: n_swa            = 0
0.00.050.091 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.092 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.092 I llm_load_print_meta: n_gqa            = 1
0.00.050.093 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.094 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.095 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.095 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.095 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.096 I llm_load_print_meta: n_ff             = 8192
0.00.050.096 I llm_load_print_meta: n_expert         = 0
0.00.050.096 I llm_load_print_meta: n_expert_used    = 0
0.00.050.096 I llm_load_print_meta: causal attn      = 1
0.00.050.097 I llm_load_print_meta: pooling type     = 0
0.00.050.097 I llm_load_print_meta: rope type        = 2
0.00.050.097 I llm_load_print_meta: rope scaling     = linear
0.00.050.097 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.098 I llm_load_print_meta: freq_scale_train = 1
0.00.050.098 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.098 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.098 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.098 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.098 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.099 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.099 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.099 I llm_load_print_meta: model type       = 1.4B
0.00.050.100 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.100 I llm_load_print_meta: model params     = 1.41 B
0.00.050.101 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.101 I llm_load_print_meta: general.name     = 1.4B
0.00.050.101 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.101 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.102 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.102 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.102 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.103 I llm_load_print_meta: max token length = 1024
0.00.051.957 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.957 I llm_load_tensors: offloading output layer to GPU
0.00.051.957 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.968 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.969 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.871 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.871 I llama_new_context_with_model: n_ctx         = 128
0.00.052.872 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.872 I llama_new_context_with_model: n_batch       = 128
0.00.052.872 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.872 I llama_new_context_with_model: flash_attn    = 0
0.00.052.873 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.873 I llama_new_context_with_model: freq_scale    = 1
0.00.052.873 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.874 I ggml_metal_init: allocating
0.00.052.880 I ggml_metal_init: found device: Apple M4
0.00.052.883 I ggml_metal_init: picking default device: Apple M4
0.00.053.463 I ggml_metal_init: using embedded metal library
0.00.055.861 I ggml_metal_init: GPU name:   Apple M4
0.00.055.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.864 I ggml_metal_init: simdgroup reduction   = true
0.00.055.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.864 I ggml_metal_init: has bfloat            = true
0.00.055.864 I ggml_metal_init: use bfloat            = true
0.00.055.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.364 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.725 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.596 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.597 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.598 I llama_new_context_with_model: graph nodes  = 967
0.00.067.598 I llama_new_context_with_model: graph splits = 2
0.00.067.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.017 I 
0.00.446.058 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.073 I perplexity: tokenizing the input ..
0.00.453.874 I perplexity: tokenization took 7.8 ms
0.00.453.882 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.854 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.587.051 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.587.068 I llama_perf_context_print:        load time =     436.48 ms
0.00.587.070 I llama_perf_context_print: prompt eval time =     131.75 ms /   128 tokens (    1.03 ms per token,   971.57 tokens per second)
0.00.587.071 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.072 I llama_perf_context_print:       total time =     141.06 ms /   129 tokens
0.00.587.508 I ggml_metal_free: deallocating

real	0m0.604s
user	0m0.078s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.864 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.643 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.644 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.645 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.550 I llama_model_loader: - type  f32:  194 tensors
0.00.023.550 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.551 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.551 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.551 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.363 I llm_load_vocab: special tokens cache size = 25
0.00.049.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.436 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.436 I llm_load_print_meta: arch             = gptneox
0.00.049.437 I llm_load_print_meta: vocab type       = BPE
0.00.049.437 I llm_load_print_meta: n_vocab          = 50304
0.00.049.437 I llm_load_print_meta: n_merges         = 50009
0.00.049.437 I llm_load_print_meta: vocab_only       = 0
0.00.049.437 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.438 I llm_load_print_meta: n_embd           = 2048
0.00.049.438 I llm_load_print_meta: n_layer          = 24
0.00.049.441 I llm_load_print_meta: n_head           = 16
0.00.049.441 I llm_load_print_meta: n_head_kv        = 16
0.00.049.441 I llm_load_print_meta: n_rot            = 32
0.00.049.442 I llm_load_print_meta: n_swa            = 0
0.00.049.442 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.442 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.443 I llm_load_print_meta: n_gqa            = 1
0.00.049.444 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.446 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.446 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.447 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.447 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.449 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.450 I llm_load_print_meta: n_ff             = 8192
0.00.049.450 I llm_load_print_meta: n_expert         = 0
0.00.049.450 I llm_load_print_meta: n_expert_used    = 0
0.00.049.450 I llm_load_print_meta: causal attn      = 1
0.00.049.450 I llm_load_print_meta: pooling type     = 0
0.00.049.450 I llm_load_print_meta: rope type        = 2
0.00.049.451 I llm_load_print_meta: rope scaling     = linear
0.00.049.451 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.451 I llm_load_print_meta: freq_scale_train = 1
0.00.049.452 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.452 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.452 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.454 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.454 I llm_load_print_meta: model type       = 1.4B
0.00.049.455 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.455 I llm_load_print_meta: model params     = 1.41 B
0.00.049.456 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.456 I llm_load_print_meta: general.name     = 1.4B
0.00.049.456 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.457 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.458 I llm_load_print_meta: max token length = 1024
0.00.051.197 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.198 I llm_load_tensors: offloading output layer to GPU
0.00.051.198 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.204 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.204 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.118 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.119 I llama_new_context_with_model: n_ctx         = 128
0.00.052.119 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.119 I llama_new_context_with_model: n_batch       = 128
0.00.052.120 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.120 I llama_new_context_with_model: flash_attn    = 0
0.00.052.120 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.120 I llama_new_context_with_model: freq_scale    = 1
0.00.052.121 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.121 I ggml_metal_init: allocating
0.00.052.124 I ggml_metal_init: found device: Apple M4
0.00.052.126 I ggml_metal_init: picking default device: Apple M4
0.00.052.698 I ggml_metal_init: using embedded metal library
0.00.055.019 I ggml_metal_init: GPU name:   Apple M4
0.00.055.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.022 I ggml_metal_init: simdgroup reduction   = true
0.00.055.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.022 I ggml_metal_init: has bfloat            = true
0.00.055.022 I ggml_metal_init: use bfloat            = true
0.00.055.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.460 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.693 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.702 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.734 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.686 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.687 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.687 I llama_new_context_with_model: graph nodes  = 967
0.00.066.688 I llama_new_context_with_model: graph splits = 2
0.00.066.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.050 I 
0.00.473.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.083 I perplexity: tokenizing the input ..
0.00.480.843 I perplexity: tokenization took 7.758 ms
0.00.480.846 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.916 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.111 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.127 I llama_perf_context_print:        load time =     464.18 ms
0.00.614.129 I llama_perf_context_print: prompt eval time =     131.84 ms /   128 tokens (    1.03 ms per token,   970.84 tokens per second)
0.00.614.129 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.130 I llama_perf_context_print:       total time =     141.08 ms /   129 tokens
0.00.614.529 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.218 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.218 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.219 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.222 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.008 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.010 I llama_model_loader: - type  f32:  194 tensors
0.00.023.010 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.010 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.010 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.856 I llm_load_vocab: special tokens cache size = 25
0.00.048.792 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.794 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.795 I llm_load_print_meta: arch             = gptneox
0.00.048.795 I llm_load_print_meta: vocab type       = BPE
0.00.048.795 I llm_load_print_meta: n_vocab          = 50304
0.00.048.795 I llm_load_print_meta: n_merges         = 50009
0.00.048.796 I llm_load_print_meta: vocab_only       = 0
0.00.048.796 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.796 I llm_load_print_meta: n_embd           = 2048
0.00.048.796 I llm_load_print_meta: n_layer          = 24
0.00.048.798 I llm_load_print_meta: n_head           = 16
0.00.048.799 I llm_load_print_meta: n_head_kv        = 16
0.00.048.799 I llm_load_print_meta: n_rot            = 32
0.00.048.801 I llm_load_print_meta: n_swa            = 0
0.00.048.802 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.802 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.802 I llm_load_print_meta: n_gqa            = 1
0.00.048.803 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.808 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.809 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.809 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.809 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.810 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.810 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.811 I llm_load_print_meta: n_ff             = 8192
0.00.048.811 I llm_load_print_meta: n_expert         = 0
0.00.048.812 I llm_load_print_meta: n_expert_used    = 0
0.00.048.812 I llm_load_print_meta: causal attn      = 1
0.00.048.812 I llm_load_print_meta: pooling type     = 0
0.00.048.812 I llm_load_print_meta: rope type        = 2
0.00.048.812 I llm_load_print_meta: rope scaling     = linear
0.00.048.813 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.813 I llm_load_print_meta: freq_scale_train = 1
0.00.048.813 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.817 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.817 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.817 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.817 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.818 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.818 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.818 I llm_load_print_meta: model type       = 1.4B
0.00.048.819 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.819 I llm_load_print_meta: model params     = 1.41 B
0.00.048.820 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.820 I llm_load_print_meta: general.name     = 1.4B
0.00.048.820 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.820 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.820 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.820 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.821 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.821 I llm_load_print_meta: max token length = 1024
0.00.050.758 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.759 I llm_load_tensors: offloading output layer to GPU
0.00.050.759 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.769 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.770 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.646 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.647 I llama_new_context_with_model: n_ctx         = 128
0.00.051.647 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.647 I llama_new_context_with_model: n_batch       = 128
0.00.051.647 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.648 I llama_new_context_with_model: flash_attn    = 0
0.00.051.648 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.648 I llama_new_context_with_model: freq_scale    = 1
0.00.051.649 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.649 I ggml_metal_init: allocating
0.00.051.652 I ggml_metal_init: found device: Apple M4
0.00.051.654 I ggml_metal_init: picking default device: Apple M4
0.00.052.203 I ggml_metal_init: using embedded metal library
0.00.054.483 I ggml_metal_init: GPU name:   Apple M4
0.00.054.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.485 I ggml_metal_init: simdgroup reduction   = true
0.00.054.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.485 I ggml_metal_init: has bfloat            = true
0.00.054.485 I ggml_metal_init: use bfloat            = true
0.00.054.486 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.848 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.095 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.099 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.127 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.012 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.013 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.013 I llama_new_context_with_model: graph nodes  = 967
0.00.066.013 I llama_new_context_with_model: graph splits = 2
0.00.066.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.798 I 
0.00.561.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.840 I perplexity: tokenizing the input ..
0.00.569.467 I perplexity: tokenization took 7.625 ms
0.00.569.471 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.704.137 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.705.410 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.705.429 I llama_perf_context_print:        load time =     553.02 ms
0.00.705.430 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.32 tokens per second)
0.00.705.431 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.705.431 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.705.914 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.076s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.801 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.301 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.316 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.242 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.295 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.339 I llama_model_loader: - type  f32:  194 tensors
0.00.024.339 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.339 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.902 I llm_load_vocab: special tokens cache size = 25
0.00.050.886 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.889 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.889 I llm_load_print_meta: arch             = gptneox
0.00.050.889 I llm_load_print_meta: vocab type       = BPE
0.00.050.889 I llm_load_print_meta: n_vocab          = 50304
0.00.050.890 I llm_load_print_meta: n_merges         = 50009
0.00.050.890 I llm_load_print_meta: vocab_only       = 0
0.00.050.890 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.890 I llm_load_print_meta: n_embd           = 2048
0.00.050.890 I llm_load_print_meta: n_layer          = 24
0.00.050.893 I llm_load_print_meta: n_head           = 16
0.00.050.894 I llm_load_print_meta: n_head_kv        = 16
0.00.050.894 I llm_load_print_meta: n_rot            = 32
0.00.050.894 I llm_load_print_meta: n_swa            = 0
0.00.050.895 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.895 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.896 I llm_load_print_meta: n_gqa            = 1
0.00.050.897 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.897 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.898 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.898 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.898 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.898 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.899 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.899 I llm_load_print_meta: n_ff             = 8192
0.00.050.899 I llm_load_print_meta: n_expert         = 0
0.00.050.900 I llm_load_print_meta: n_expert_used    = 0
0.00.050.900 I llm_load_print_meta: causal attn      = 1
0.00.050.900 I llm_load_print_meta: pooling type     = 0
0.00.050.900 I llm_load_print_meta: rope type        = 2
0.00.050.900 I llm_load_print_meta: rope scaling     = linear
0.00.050.901 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.901 I llm_load_print_meta: freq_scale_train = 1
0.00.050.901 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.901 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.902 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.904 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.904 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.904 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.904 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.905 I llm_load_print_meta: model type       = 1.4B
0.00.050.905 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.906 I llm_load_print_meta: model params     = 1.41 B
0.00.050.910 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.911 I llm_load_print_meta: general.name     = 1.4B
0.00.050.911 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.912 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: max token length = 1024
0.00.052.952 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.952 I llm_load_tensors: offloading output layer to GPU
0.00.052.952 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.963 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.964 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.870 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.871 I llama_new_context_with_model: n_ctx         = 128
0.00.053.871 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.871 I llama_new_context_with_model: n_batch       = 128
0.00.053.871 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.871 I llama_new_context_with_model: flash_attn    = 0
0.00.053.872 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.872 I llama_new_context_with_model: freq_scale    = 1
0.00.053.872 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.873 I ggml_metal_init: allocating
0.00.053.879 I ggml_metal_init: found device: Apple M4
0.00.053.881 I ggml_metal_init: picking default device: Apple M4
0.00.054.452 I ggml_metal_init: using embedded metal library
0.00.056.807 I ggml_metal_init: GPU name:   Apple M4
0.00.056.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.809 I ggml_metal_init: simdgroup reduction   = true
0.00.056.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.810 I ggml_metal_init: has bfloat            = true
0.00.056.810 I ggml_metal_init: use bfloat            = true
0.00.056.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.252 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.603 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.611 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.512 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.513 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.513 I llama_new_context_with_model: graph nodes  = 967
0.00.068.513 I llama_new_context_with_model: graph splits = 2
0.00.068.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.515 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.596 I 
0.00.626.624 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.637 I perplexity: tokenizing the input ..
0.00.634.137 I perplexity: tokenization took 7.498 ms
0.00.634.140 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.759 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.775.906 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.775.928 I llama_perf_context_print:        load time =     616.78 ms
0.00.775.929 I llama_perf_context_print: prompt eval time =     140.39 ms /   128 tokens (    1.10 ms per token,   911.73 tokens per second)
0.00.775.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.775.930 I llama_perf_context_print:       total time =     149.33 ms /   129 tokens
0.00.776.469 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.078s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.802 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.855 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.904 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.658 I llama_model_loader: - type  f32:  194 tensors
0.00.022.658 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.378 I llm_load_vocab: special tokens cache size = 25
0.00.048.412 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.414 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.414 I llm_load_print_meta: arch             = gptneox
0.00.048.415 I llm_load_print_meta: vocab type       = BPE
0.00.048.415 I llm_load_print_meta: n_vocab          = 50304
0.00.048.415 I llm_load_print_meta: n_merges         = 50009
0.00.048.415 I llm_load_print_meta: vocab_only       = 0
0.00.048.415 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.416 I llm_load_print_meta: n_embd           = 2048
0.00.048.416 I llm_load_print_meta: n_layer          = 24
0.00.048.419 I llm_load_print_meta: n_head           = 16
0.00.048.420 I llm_load_print_meta: n_head_kv        = 16
0.00.048.420 I llm_load_print_meta: n_rot            = 32
0.00.048.420 I llm_load_print_meta: n_swa            = 0
0.00.048.420 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.420 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.421 I llm_load_print_meta: n_gqa            = 1
0.00.048.422 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.422 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.423 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.423 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.423 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.426 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.426 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.426 I llm_load_print_meta: n_ff             = 8192
0.00.048.427 I llm_load_print_meta: n_expert         = 0
0.00.048.427 I llm_load_print_meta: n_expert_used    = 0
0.00.048.427 I llm_load_print_meta: causal attn      = 1
0.00.048.427 I llm_load_print_meta: pooling type     = 0
0.00.048.427 I llm_load_print_meta: rope type        = 2
0.00.048.428 I llm_load_print_meta: rope scaling     = linear
0.00.048.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.428 I llm_load_print_meta: freq_scale_train = 1
0.00.048.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.431 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.431 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.431 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.431 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.432 I llm_load_print_meta: model type       = 1.4B
0.00.048.432 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.432 I llm_load_print_meta: model params     = 1.41 B
0.00.048.433 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.433 I llm_load_print_meta: general.name     = 1.4B
0.00.048.433 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.434 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.434 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.435 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.438 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.438 I llm_load_print_meta: max token length = 1024
0.00.050.422 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.422 I llm_load_tensors: offloading output layer to GPU
0.00.050.423 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.433 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.434 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.321 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.322 I llama_new_context_with_model: n_ctx         = 128
0.00.051.322 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.322 I llama_new_context_with_model: n_batch       = 128
0.00.051.322 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.322 I llama_new_context_with_model: flash_attn    = 0
0.00.051.323 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.323 I llama_new_context_with_model: freq_scale    = 1
0.00.051.323 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.324 I ggml_metal_init: allocating
0.00.051.330 I ggml_metal_init: found device: Apple M4
0.00.051.332 I ggml_metal_init: picking default device: Apple M4
0.00.051.909 I ggml_metal_init: using embedded metal library
0.00.054.229 I ggml_metal_init: GPU name:   Apple M4
0.00.054.230 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.231 I ggml_metal_init: simdgroup reduction   = true
0.00.054.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.232 I ggml_metal_init: has bfloat            = true
0.00.054.232 I ggml_metal_init: use bfloat            = true
0.00.054.232 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.085 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.088 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.111 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.963 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.964 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.965 I llama_new_context_with_model: graph nodes  = 967
0.00.065.965 I llama_new_context_with_model: graph splits = 2
0.00.065.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.444 I 
0.00.496.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.495 I perplexity: tokenizing the input ..
0.00.504.225 I perplexity: tokenization took 7.727 ms
0.00.504.228 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.644.585 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.645.805 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.645.827 I llama_perf_context_print:        load time =     487.63 ms
0.00.645.827 I llama_perf_context_print: prompt eval time =     140.12 ms /   128 tokens (    1.09 ms per token,   913.50 tokens per second)
0.00.645.828 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.829 I llama_perf_context_print:       total time =     149.39 ms /   129 tokens
0.00.646.291 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.076s
sys	0m0.099s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.209 I build: 4426 (1521f9ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.348 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.393 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.964 I llama_model_loader: - type  f32:  194 tensors
0.00.052.964 I llama_model_loader: - type  f16:   98 tensors
0.00.081.859 I llm_load_vocab: special tokens cache size = 25
0.00.088.567 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.570 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.571 I llm_load_print_meta: arch             = gptneox
0.00.088.571 I llm_load_print_meta: vocab type       = BPE
0.00.088.571 I llm_load_print_meta: n_vocab          = 50304
0.00.088.571 I llm_load_print_meta: n_merges         = 50009
0.00.088.571 I llm_load_print_meta: vocab_only       = 0
0.00.088.572 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.572 I llm_load_print_meta: n_embd           = 2048
0.00.088.572 I llm_load_print_meta: n_layer          = 24
0.00.088.575 I llm_load_print_meta: n_head           = 16
0.00.088.576 I llm_load_print_meta: n_head_kv        = 16
0.00.088.576 I llm_load_print_meta: n_rot            = 32
0.00.088.576 I llm_load_print_meta: n_swa            = 0
0.00.088.576 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.576 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.577 I llm_load_print_meta: n_gqa            = 1
0.00.088.578 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.578 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.579 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.579 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.579 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.579 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.580 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.580 I llm_load_print_meta: n_ff             = 8192
0.00.088.581 I llm_load_print_meta: n_expert         = 0
0.00.088.581 I llm_load_print_meta: n_expert_used    = 0
0.00.088.581 I llm_load_print_meta: causal attn      = 1
0.00.088.581 I llm_load_print_meta: pooling type     = 0
0.00.088.581 I llm_load_print_meta: rope type        = 2
0.00.088.582 I llm_load_print_meta: rope scaling     = linear
0.00.088.582 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.582 I llm_load_print_meta: freq_scale_train = 1
0.00.088.582 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.583 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.583 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.583 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.583 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.583 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.583 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.584 I llm_load_print_meta: model type       = 1.4B
0.00.088.584 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.585 I llm_load_print_meta: model params     = 1.41 B
0.00.088.585 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.585 I llm_load_print_meta: general.name     = 1.4B
0.00.088.586 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.586 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.586 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.587 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.588 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.588 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.588 I llm_load_print_meta: max token length = 1024
0.00.091.157 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.157 I llm_load_tensors: offloading output layer to GPU
0.00.091.158 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.168 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.169 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.118 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.119 I llama_new_context_with_model: n_ctx         = 128
0.00.092.119 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.120 I llama_new_context_with_model: n_batch       = 128
0.00.092.120 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.120 I llama_new_context_with_model: flash_attn    = 0
0.00.092.120 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.121 I llama_new_context_with_model: freq_scale    = 1
0.00.092.121 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.122 I ggml_metal_init: allocating
0.00.092.129 I ggml_metal_init: found device: Apple M4
0.00.092.132 I ggml_metal_init: picking default device: Apple M4
0.00.092.778 I ggml_metal_init: using embedded metal library
0.00.095.330 I ggml_metal_init: GPU name:   Apple M4
0.00.095.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.333 I ggml_metal_init: simdgroup reduction   = true
0.00.095.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.333 I ggml_metal_init: has bfloat            = true
0.00.095.333 I ggml_metal_init: use bfloat            = true
0.00.095.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.230 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.463 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.466 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.333 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.334 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.335 I llama_new_context_with_model: graph nodes  = 967
0.00.106.335 I llama_new_context_with_model: graph splits = 2
0.00.106.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.336 I 
0.00.106.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.363 I compute_imatrix: tokenizing the input ..
0.00.113.193 I compute_imatrix: tokenization took 6.83 ms
0.00.113.195 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.618.531 I compute_imatrix: 1.51 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.620.901 I llama_perf_context_print:        load time =    1595.16 ms
0.01.620.903 I llama_perf_context_print: prompt eval time =    1504.69 ms /   128 tokens (   11.76 ms per token,    85.07 tokens per second)
0.01.620.904 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.620.904 I llama_perf_context_print:       total time =    1597.52 ms /   129 tokens
0.01.621.481 I ggml_metal_free: deallocating

real	0m1.809s
user	0m0.169s
sys	0m0.242s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4426 (1521f9ee)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14210a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14210a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14210aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14210b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14210ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14210bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14210c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14210cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14210d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14210d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14210daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14210dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14210eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14210f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14210fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1421101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142110910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142111030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142111750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142111f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142112640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142112d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142113480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142113d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142114440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142114700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142114d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142115980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142115ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142116180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142116620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1421168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142117170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1421176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142117970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142117e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1421182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142118750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142118bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142119090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142119530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1421199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142119e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14211a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14211a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14211abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14211b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14211bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14211c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14211c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14211cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14211d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14211d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14211df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14211e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14211ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14211f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14211f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14211f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142120160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142120420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1421208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142120d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142121200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1421216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142121b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142121fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142122480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142122920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142122dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142123260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142123700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142123ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1421240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142124640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142124b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1421250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142125630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142125b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1421260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142126620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142126b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1421270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142127610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142127b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1421280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142128600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142128b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1421290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1421295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142129b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14212a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14212a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14212ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14212b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14212b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14212bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14211b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14212bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14212c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14212cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14212d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14212d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14212dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14212e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14212e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14212ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14212f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14212f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14212fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1421301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142130700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142130c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1421310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142131590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142131a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142131ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142132370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142132810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142132cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142133150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1421335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142133a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142133f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1421343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142134870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142134d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1421351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142135650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142135af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142135f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142136430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1421368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142136d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142137210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1421376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142137b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142137ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142138490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142138930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142138dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142139270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142139710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142139bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14213a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14213a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14213a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14213ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14213b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14213b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14213bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14213c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14213c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14213c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14213ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14213d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14213d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14213dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14213e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14213e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14213ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14213eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14213f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14213f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14213fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142140170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142140610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142140ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142140f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1421413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142141890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142141d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1421421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142142670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142142b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142142fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142143450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1421438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142143d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142144230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1421446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142144b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142145010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1421454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142145950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142145df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142146290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142146730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142146bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142147070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142147510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1421479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142147e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1421483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1421488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142148e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142149390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142149650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142149c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14214a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14214a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14214b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14214b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14214b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14214bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14214c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14214cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14214d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14214d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14214d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14214e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14214e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14214ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14214f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14214f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14214fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142150150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1421506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142150bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142151140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142151690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142151be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142152130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142152680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142152bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142153120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142153670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142153bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142154110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142154660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142154bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142155100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142155650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142155ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1421560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142156640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142156b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1421570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142157630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142157b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1421580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142158620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142158b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1421590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142159610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142159b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14215a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14215a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14215ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14215b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14215b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14215bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14215c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14215c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14215cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14215d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14215d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14215db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14215e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14215e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14215eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14215f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14215f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14215fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142160050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1421605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142160af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142160f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142161430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1421618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142161d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142162210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1421626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142162b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142162ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142163490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142163930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142163dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142164270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142164710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142164bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142165050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1421655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142165cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1421663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142166b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142167220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1421674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142167cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142167f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1421685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105104bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105105030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1051054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105105910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105105d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1051061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105106660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105106ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105106f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1051073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105107820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105107ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105108a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1051091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1051099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10510a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10510a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10510af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10510b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10510be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10510c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10510cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10510d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10510da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10510e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10510e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10510e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10510eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10510f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10510f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10510f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10510fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105110290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105110550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1051109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105110e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1051112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105111710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105111b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105111ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105112460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1051128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105112d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1051131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105113620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105113a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105113f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105114370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1051147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105114c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1051150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105115530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1051159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105115e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105116280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1051166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105116c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105117160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1051175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105117a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105117eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105118320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105118790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105118c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105119070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1051194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105119950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105119dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10511a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10511a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10511ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10511af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10511b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10511b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10511bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10511c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10511c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10511ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10511ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10511d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10511d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10511dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10511e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10511e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10511e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10511eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10511f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10511f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10511faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10511ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1051203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105120840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105120cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105121120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105121590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105121a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105121e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1051222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105122750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105122bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105123030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1051234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105123910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105123d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1051241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105124660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105124ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105124f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1051253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105125820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105125c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105126100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105126570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1051269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105126e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1051272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105127730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105127ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105128010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105128480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1051288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105128d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1051291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105129640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105129ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105129f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10512a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10512a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10512ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10512b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10512b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10512b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10512be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10512c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10512c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10512cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10512cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10512d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10512d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10512dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10512e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10512e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10512ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10512ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10512f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10512f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10512fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1051300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105130530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1051309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105130e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105131280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1051316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105131b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105131fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105132440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1051328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105132d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105133190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105133600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105133a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105133ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105134350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1051347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105134c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1051350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105135cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105135f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105136250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1051366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105136b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105136fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105137410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105137880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105137cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105138160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1051385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105138a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105138eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105139320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105139790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105139c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10513a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10513a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10513a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10513adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10513b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10513b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10513bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10513bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10513c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10513c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10513ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10513d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10513d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10513da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10513de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10513e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10513e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10513ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10513f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10513f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10513fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10513ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1051403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105140810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105140c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1051410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105141610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105141b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105142690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105142950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105142f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1051434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105143a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105144050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105144610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105144bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105145190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105145750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105145d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1051462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105146890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105146e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105147410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1051479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105147f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105148550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105148b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1051490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105149690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105149c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10514a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10514a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10514ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10514b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10514b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10514bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10514c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10514ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10514d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10514d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10514db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10514e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10514e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10514ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10514f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10514f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10514fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1051503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105150990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105150f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105151510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105151ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105152090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105152650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105152c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1051531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105153790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105153d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105154310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1051548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105154e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105155450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105155a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105155fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105156590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105156b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105157050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105157550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105157a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105157f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105158450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105158950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105158e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105159350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105159850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105159d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10515a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10515a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10515ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10515b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10515b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10515c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10515c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10515cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10515d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10515d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10515e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10515e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10515e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142168250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142149f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142149910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14214a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14211d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14211d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14211f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14214c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1421149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14211b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14211bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14211c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14211a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14211c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1421139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14211fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14212c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1421677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142116ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142116e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14214c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14214ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142114fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142115290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142115550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142168a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142168cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142168f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142169240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142169500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1421697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142169a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142169d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14216a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14216a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14216a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14216a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14216ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14216adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14216b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14216b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14216b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14216b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14216bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14216be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14216c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14216c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14216c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14216c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14216cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14216cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14216d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14216d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14216d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14216d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14216dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14216df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14216e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14216e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14216e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14216ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14216ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14216efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14216f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14216f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14216f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14216fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14216fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142170040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142170300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1421705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142170880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142170b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142170e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1421710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142171380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142171640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142171900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142171bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142171e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142172140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142172400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1421726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142172980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142172c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142172f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1421731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142173480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142173740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142173a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142173cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142173f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142174240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142174500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1421747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142174a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142174d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142175000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1421752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142175580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142175840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142175b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142175dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142176080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142176340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142176600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1421768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142176b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142176e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142177100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1421773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142177680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142177940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142177c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142177ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142178180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142178440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142178700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1421789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142178c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142178f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142179200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1421794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142179780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142179a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142179d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142179fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14217a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14217a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14217a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14217aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14217ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14217b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14217b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14217b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14217b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14217bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14217be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14217c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14217c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14217c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14217c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14217cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14217ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14217d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14217d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14217d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14217d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14217dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14217df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14217e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14217e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14217e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14217ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14217ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14217ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14217f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14217f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14217f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14217fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14217fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142180000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1421802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142180580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142180840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142180b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142180dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142181080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142181340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142181600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1421818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142181b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142181e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142182100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1421823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142182680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142182940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142182c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142182ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142183180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142183440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142183700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1421839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142183c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142183f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142184200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1421844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142184780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x142184a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142184d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142184fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142185280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142185540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142185800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142185ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142185d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x142186040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142186300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1421865c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142186880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142186b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142186e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1421870c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142187380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142187640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142187900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142187bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142187e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142188140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142188400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1421889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142188f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142189470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1421899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142189f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14218a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14218a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14218af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14218b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14218b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14218bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14218c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14218c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14218cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14218d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14218d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14218ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14218e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14218e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14218eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14218f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14218f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14218feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142190400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142190950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142190ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1421913f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142191940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142191e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1421923e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142192930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142192e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1421933d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142193920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142193e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1421943c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142194910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142194e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1421953b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142195900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142195e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1421963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1421968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142196e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142197390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1421978e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142197e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142198380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1421988d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142198e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142199370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1421998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142199e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14219a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14219a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14219ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14219b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14219b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14219b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14219bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14219c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14219c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14219c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14219cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14219d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14219d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14219daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14219df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14219e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14219e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14219ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14219f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14219f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14219f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1421a06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1421a0dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1421a14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1421a17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1421a1c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1421a2210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1421a2820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.787s
user	0m0.294s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4426 (1521f9ee)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158e0e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158e0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158e0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158e0f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158e0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158e102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158e10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158e10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158e113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158e118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158e11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158e122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158e12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158e13580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158e13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158e144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158e14bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158e152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158e15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158e161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158e16900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158e17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158e17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158e17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158e18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158e189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158e18fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158e19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158e1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158e1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158e1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158e1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158e1b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158e1b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158e1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158e1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158e1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158e1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158e1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158e1d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158e1dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158e1e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158e1e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158e1e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158e1eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158e1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158e1fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158e203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158e209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158e21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158e21610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158e21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158e22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158e22a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158e22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158e23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158e23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158e23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158e24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158e246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158e24b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158e25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158e254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158e25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158e25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158e262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158e26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158e27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158e27520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158e279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158e27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158e283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158e28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158e28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158e293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158e298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158e29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158e2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158e2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158e2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158e2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158e2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158e2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158e2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158e2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158e2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158e2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158e2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158e2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158e2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158e2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158e2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158e2f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158e2fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158e30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158e30a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158e30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158e314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158e319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158e31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158e32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158e329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158e32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158e33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158e339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158e33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158e34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158e349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158e34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158e353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158e35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158e35cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158e36190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158e36630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158e36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158e36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158e37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158e378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158e37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158e381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158e38690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158e38b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158e38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158e39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158e39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158e39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158e3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158e3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158e3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158e3b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158e3b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158e3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158e3be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158e3c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158e3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158e3cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158e3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158e3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158e3d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158e3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158e3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158e3e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158e3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158e3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158e3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158e3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158e3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158e40370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158e40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158e40cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158e41150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158e415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158e41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158e41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158e423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158e42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158e42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158e431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158e43650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158e43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158e43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158e44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158e448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158e44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158e45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158e456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158e45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158e45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158e46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158e46930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158e46dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158e47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158e47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158e47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158e48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158e484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158e48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158e48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158e492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158e49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158e49c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158e4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158e4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158e4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158e4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158e4b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158e4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158e4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158e4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158e4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158e4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158e4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158e4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158e4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158e4e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158e4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158e4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158e4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158e4fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158e500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158e506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158e50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158e51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158e517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158e51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158e52430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158e52980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158e52ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158e53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158e53970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158e53ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158e54410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158e54960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158e54eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158e55400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158e55950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158e55ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158e563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158e56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158e56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158e573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158e57930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158e57e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158e583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158e58920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158e58e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158e593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158e59910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158e59e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158e5a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158e5a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158e5ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158e5b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158e5b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158e5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158e5c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158e5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158e5ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158e5d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158e5d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158e5de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158e5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158e5e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158e5ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158e5f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158e5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158e5fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158e60350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158e608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158e60df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158e61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158e61890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158e61de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158e62330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158e62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158e62dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158e63320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158e63870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158e63dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158e64310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158e64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158e64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158e65250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158e656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158e65b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158e66030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158e664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158e66970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158e66e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158e672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158e67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158e67bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158e68090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158e68530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158e689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158e68e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158e69310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158e69860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158e69f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158e6a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158e6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158e6b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158e6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158e6bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158e6c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158e6c860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.658 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.667 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158e6c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158e4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158e4dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158e4e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158e218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158e212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158e238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158e50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158e18c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158e1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158e20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158e206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158e1eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158e20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158e17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158e0db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158e224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158e23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158e30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158e6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158e1ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158e1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158e50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158e4ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158e19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158e19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158e19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158e6ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158e6cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158e6d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158e6d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158e6d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158e6da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158e6dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158e6e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158e6e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158e6e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158e6e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158e6eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158e6edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158e6f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158e6f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158e6f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158e6f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158e6fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158e6fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158e70100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158e703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158e70680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158e70940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158e70c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158e70ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158e71180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158e71440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158e71700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158e719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158e71c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158e71f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158e72200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158e724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158e72780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158e72a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158e72d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158e72fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158e73280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158e73540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158e73800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158e73ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158e73d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158e74040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158e74300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158e745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158e74880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158e74b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158e74e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158e750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158e75380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158e75640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158e75900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158e75bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158e75e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158e76140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158e76400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158e766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158e76980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158e76c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158e76f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158e771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158e77480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158e77740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158e77a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158e77cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158e77f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158e78240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158e78500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158e787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158e78a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158e78d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158e79000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158e792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158e79580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158e79840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158e79b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158e79dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158e7a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158e7a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158e7a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158e7a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158e7ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158e7ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158e7b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158e7b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158e7b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158e7b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158e7bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158e7bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158e7c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158e7c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158e7c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158e7c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158e7cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158e7cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158e7d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158e7d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158e7d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158e7da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158e7dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158e7dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158e7e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158e7e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158e7e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158e7eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158e7ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158e7f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158e7f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158e7f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158e7f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158e7fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158e7fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158e800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158e80380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158e80640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158e80900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158e80bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158e80e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158e81140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158e81400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158e816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158e81980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158e81c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158e81f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158e821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158e82480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158e82740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158e82a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158e82cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158e82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158e83240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158e83500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158e837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158e83a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158e83d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158e84000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158e842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158e84580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158e84840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158e84b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158e84dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158e85080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158e85340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158e85600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158e858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158e85b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158e85e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158e86100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158e863c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158e86680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158e86940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158e86c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158e86ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158e87180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158e87440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158e87700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158e879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158e87c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158e87f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158e88200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158e884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158e88780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158e88a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158e88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158e88fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158e89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158e89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158e89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158e89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158e89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158e8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158e8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158e8a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158e8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158e8ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158e8ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158e8b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158e8b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158e8b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158e8b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158e8bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158e8be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158e8c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158e8c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158e8c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158e8cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158e8cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158e8d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158e8d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158e8d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158e8da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158e8dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158e8dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158e8e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158e8e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158e8e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158e8ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158e8ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158e8f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158e8f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158e8f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158e8f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158e8fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158e8fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158e900d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158e90390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158e90650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158e90910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158e90bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158e90e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158e91150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158e91410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158e91960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158e91eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158e92400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158e92950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158e92ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158e933f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158e93940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158e93e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158e943e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158e94930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158e94e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158e953d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158e95920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158e95e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158e963c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158e96910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158e96e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158e973b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158e97900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158e97e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158e983a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158e988f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158e98e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158e99390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158e998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158e99e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158e9a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158e9a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158e9ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158e9ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158e9b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158e9b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158e9bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158e9c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158e9c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158e9cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158e9d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158e9d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158e9db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158e9e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158e9e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158e9ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158e9ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158e9f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158e9fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158ea0580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158ea0ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158ea13c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158ea1680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158ea1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158ea2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158ea2740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a0046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a0058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a0065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a007a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a008530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a008ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a0094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a009c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a00a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a00aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a00b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a00b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a00c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a00c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a00cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a00d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a00dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a00dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a00e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a00e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a00eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a00efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a00f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a00f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a00fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a010080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a0104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a010960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a010dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a011240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a0116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a011b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a011f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a012400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a012870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a012ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a013150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a0135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a013a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a013ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a014310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a014780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a014bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a015060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a0154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a015940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a016220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a016c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a017100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a017570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a0179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a017e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a0182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a018730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a018ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a019010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a019480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a0198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a019d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a01a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a01a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a01aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a01af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a01b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a01b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a01bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a01c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a01c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a01c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a01ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a01d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a01d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a01db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a01dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a01e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a01e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a01ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a01f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a01f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a01fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a01ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a020370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a0207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a020c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a0210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a021530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a0219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a021e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a022280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a0226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a022b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a022fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a023440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a023cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a023f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a024400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a024870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a024ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a025150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a0255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a025ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a026780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a026bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a0274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a027940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a027db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a028220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a028690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a028b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a028f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a0293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a029850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a029cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a02a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a02a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a02aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a02ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a02b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a02b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a02bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a02c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a02c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a02c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a02cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a02d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a02d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a02dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a02df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a02e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a02e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a02eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a02f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a02f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a02f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a02fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a0302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a030740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a030bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a031020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a031490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a031900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a031d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a0321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a032650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a032ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a032f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a0333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a033810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a033c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a0340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a034560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a0349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a0352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a035b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a036000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a036470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a0368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a036d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a0371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a037630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a037aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a037f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a038380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a0387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a038c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a0390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a039540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a0399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a039e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a03a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a03a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a03ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a03afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a03b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a03b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a03bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a03c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a03c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a03ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a03cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a03d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a03d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a03dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a03e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a03e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a03e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a03ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a03f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a03f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a03fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a03ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a040430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a0408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a040d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a041180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a041d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a042280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a0426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a042b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a042fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a043440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a0438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a043d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a044600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a044ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a045350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a0457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a0460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a046510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a046980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a046df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a0476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a047b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a047fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a049170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a0495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a049a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a04a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a04a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a04ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a04b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a04b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a04b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a04bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a04c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a04c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a04cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a04cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a04d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a04d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a04dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a04e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a04e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a04ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a04eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a04f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a04f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a04fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a050060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a0504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a050940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a050db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a051220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a051690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a051b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a051f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a0523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a052cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a053130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a0535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a053a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a053e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a0542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a054760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a054bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a055040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a0554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a055920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a056390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a056ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a0571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a0578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a057bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a058020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a058620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a058c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.905s
user	0m0.243s
sys	0m0.129s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
