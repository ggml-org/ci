### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.09 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.86 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.94 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  190.87 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 251.46 sec*proc (29 tests)

Total Test time (real) = 251.48 sec

real	4m11.549s
user	8m29.672s
sys	0m7.163s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.77 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.38 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.99 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.40 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.11 sec*proc (29 tests)

Total Test time (real) =  55.12 sec

real	0m55.137s
user	1m17.077s
sys	0m6.507s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.205 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.867 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.029.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.877 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.029.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.879 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.029.880 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.029.880 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.029.882 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.029.883 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.029.883 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.029.884 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.029.885 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.029.888 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.029.889 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.029.893 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.029.893 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.029.894 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.029.895 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.029.895 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.035.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.036.383 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.386 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.036.386 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.036.387 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.036.387 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.036.388 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.036.389 I llama_model_loader: - type  f32:  124 tensors
0.00.036.389 I llama_model_loader: - type  f16:   73 tensors
0.00.036.390 I print_info: file format = GGUF V3 (latest)
0.00.036.391 I print_info: file type   = F16
0.00.036.392 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.041.191 I load: special tokens cache size = 5
0.00.043.408 I load: token to piece cache size = 0.2032 MB
0.00.043.412 I print_info: arch             = bert
0.00.043.413 I print_info: vocab_only       = 0
0.00.043.413 I print_info: n_ctx_train      = 512
0.00.043.413 I print_info: n_embd           = 384
0.00.043.413 I print_info: n_layer          = 12
0.00.043.417 I print_info: n_head           = 12
0.00.043.418 I print_info: n_head_kv        = 12
0.00.043.418 I print_info: n_rot            = 32
0.00.043.418 I print_info: n_swa            = 0
0.00.043.419 I print_info: n_embd_head_k    = 32
0.00.043.419 I print_info: n_embd_head_v    = 32
0.00.043.420 I print_info: n_gqa            = 1
0.00.043.421 I print_info: n_embd_k_gqa     = 384
0.00.043.421 I print_info: n_embd_v_gqa     = 384
0.00.043.423 I print_info: f_norm_eps       = 1.0e-12
0.00.043.423 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.426 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.426 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.426 I print_info: f_logit_scale    = 0.0e+00
0.00.043.427 I print_info: n_ff             = 1536
0.00.043.427 I print_info: n_expert         = 0
0.00.043.427 I print_info: n_expert_used    = 0
0.00.043.427 I print_info: causal attn      = 0
0.00.043.436 I print_info: pooling type     = 2
0.00.043.439 I print_info: rope type        = 2
0.00.043.439 I print_info: rope scaling     = linear
0.00.043.442 I print_info: freq_base_train  = 10000.0
0.00.043.442 I print_info: freq_scale_train = 1
0.00.043.442 I print_info: n_ctx_orig_yarn  = 512
0.00.043.443 I print_info: rope_finetuned   = unknown
0.00.043.443 I print_info: ssm_d_conv       = 0
0.00.043.443 I print_info: ssm_d_inner      = 0
0.00.043.443 I print_info: ssm_d_state      = 0
0.00.043.444 I print_info: ssm_dt_rank      = 0
0.00.043.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.444 I print_info: model type       = 33M
0.00.043.445 I print_info: model params     = 33.21 M
0.00.043.445 I print_info: general.name     = Bge Small
0.00.043.446 I print_info: vocab type       = WPM
0.00.043.446 I print_info: n_vocab          = 30522
0.00.043.448 I print_info: n_merges         = 0
0.00.043.448 I print_info: BOS token        = 101 '[CLS]'
0.00.043.449 I print_info: UNK token        = 100 '[UNK]'
0.00.043.449 I print_info: SEP token        = 102 '[SEP]'
0.00.043.449 I print_info: PAD token        = 0 '[PAD]'
0.00.043.450 I print_info: MASK token       = 103 '[MASK]'
0.00.043.450 I print_info: LF token         = 0 '[PAD]'
0.00.043.451 I print_info: max token length = 21
0.00.046.611 I load_tensors: offloading 12 repeating layers to GPU
0.00.046.612 I load_tensors: offloading output layer to GPU
0.00.046.613 I load_tensors: offloaded 13/13 layers to GPU
0.00.046.638 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.046.640 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.046.976 I llama_init_from_model: n_seq_max     = 1
0.00.046.978 I llama_init_from_model: n_ctx         = 512
0.00.046.978 I llama_init_from_model: n_ctx_per_seq = 512
0.00.046.978 I llama_init_from_model: n_batch       = 2048
0.00.046.979 I llama_init_from_model: n_ubatch      = 2048
0.00.046.979 I llama_init_from_model: flash_attn    = 0
0.00.046.979 I llama_init_from_model: freq_base     = 10000.0
0.00.046.980 I llama_init_from_model: freq_scale    = 1
0.00.046.980 I ggml_metal_init: allocating
0.00.046.994 I ggml_metal_init: found device: Apple M4
0.00.047.001 I ggml_metal_init: picking default device: Apple M4
0.00.047.788 I ggml_metal_init: using embedded metal library
0.00.052.244 I ggml_metal_init: GPU name:   Apple M4
0.00.052.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.248 I ggml_metal_init: simdgroup reduction   = true
0.00.052.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.248 I ggml_metal_init: has residency sets    = true
0.00.052.249 I ggml_metal_init: has bfloat            = true
0.00.052.249 I ggml_metal_init: use bfloat            = true
0.00.052.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.683 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.065.382 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.065.384 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.065.407 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.066.706 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.066.708 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.066.708 I llama_init_from_model: graph nodes  = 429
0.00.066.708 I llama_init_from_model: graph splits = 2
0.00.066.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.066.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.072.301 I 
0.00.072.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.073.057 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.918 I llama_perf_context_print:        load time =      48.64 ms
0.00.076.919 I llama_perf_context_print: prompt eval time =       3.72 ms /     9 tokens (    0.41 ms per token,  2418.70 tokens per second)
0.00.076.919 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.920 I llama_perf_context_print:       total time =       4.62 ms /    10 tokens
0.00.077.070 I ggml_metal_free: deallocating

real	0m0.266s
user	0m0.053s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.155 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.972 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.977 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.978 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.978 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.978 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.979 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.980 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.980 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.980 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.981 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.982 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.983 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.983 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.984 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.985 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.985 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.524 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.227 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.228 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.228 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.229 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.229 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.229 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.229 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.230 I llama_model_loader: - type  f32:  124 tensors
0.00.015.230 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.231 I print_info: file format = GGUF V3 (latest)
0.00.015.231 I print_info: file type   = Q8_0
0.00.015.232 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.927 I load: special tokens cache size = 5
0.00.019.304 I load: token to piece cache size = 0.2032 MB
0.00.019.307 I print_info: arch             = bert
0.00.019.307 I print_info: vocab_only       = 0
0.00.019.308 I print_info: n_ctx_train      = 512
0.00.019.308 I print_info: n_embd           = 384
0.00.019.308 I print_info: n_layer          = 12
0.00.019.311 I print_info: n_head           = 12
0.00.019.311 I print_info: n_head_kv        = 12
0.00.019.311 I print_info: n_rot            = 32
0.00.019.311 I print_info: n_swa            = 0
0.00.019.311 I print_info: n_embd_head_k    = 32
0.00.019.313 I print_info: n_embd_head_v    = 32
0.00.019.313 I print_info: n_gqa            = 1
0.00.019.314 I print_info: n_embd_k_gqa     = 384
0.00.019.314 I print_info: n_embd_v_gqa     = 384
0.00.019.315 I print_info: f_norm_eps       = 1.0e-12
0.00.019.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.316 I print_info: f_logit_scale    = 0.0e+00
0.00.019.316 I print_info: n_ff             = 1536
0.00.019.317 I print_info: n_expert         = 0
0.00.019.317 I print_info: n_expert_used    = 0
0.00.019.317 I print_info: causal attn      = 0
0.00.019.317 I print_info: pooling type     = 2
0.00.019.317 I print_info: rope type        = 2
0.00.019.317 I print_info: rope scaling     = linear
0.00.019.318 I print_info: freq_base_train  = 10000.0
0.00.019.318 I print_info: freq_scale_train = 1
0.00.019.318 I print_info: n_ctx_orig_yarn  = 512
0.00.019.318 I print_info: rope_finetuned   = unknown
0.00.019.319 I print_info: ssm_d_conv       = 0
0.00.019.319 I print_info: ssm_d_inner      = 0
0.00.019.319 I print_info: ssm_d_state      = 0
0.00.019.319 I print_info: ssm_dt_rank      = 0
0.00.019.319 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.319 I print_info: model type       = 33M
0.00.019.320 I print_info: model params     = 33.21 M
0.00.019.320 I print_info: general.name     = Bge Small
0.00.019.320 I print_info: vocab type       = WPM
0.00.019.320 I print_info: n_vocab          = 30522
0.00.019.321 I print_info: n_merges         = 0
0.00.019.323 I print_info: BOS token        = 101 '[CLS]'
0.00.019.323 I print_info: UNK token        = 100 '[UNK]'
0.00.019.323 I print_info: SEP token        = 102 '[SEP]'
0.00.019.324 I print_info: PAD token        = 0 '[PAD]'
0.00.019.324 I print_info: MASK token       = 103 '[MASK]'
0.00.019.324 I print_info: LF token         = 0 '[PAD]'
0.00.019.324 I print_info: max token length = 21
0.00.021.136 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.137 I load_tensors: offloading output layer to GPU
0.00.021.137 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.144 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.144 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.359 I llama_init_from_model: n_seq_max     = 1
0.00.021.360 I llama_init_from_model: n_ctx         = 512
0.00.021.360 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.361 I llama_init_from_model: n_batch       = 2048
0.00.021.361 I llama_init_from_model: n_ubatch      = 2048
0.00.021.361 I llama_init_from_model: flash_attn    = 0
0.00.021.361 I llama_init_from_model: freq_base     = 10000.0
0.00.021.362 I llama_init_from_model: freq_scale    = 1
0.00.021.362 I ggml_metal_init: allocating
0.00.021.374 I ggml_metal_init: found device: Apple M4
0.00.021.379 I ggml_metal_init: picking default device: Apple M4
0.00.021.886 I ggml_metal_init: using embedded metal library
0.00.024.524 I ggml_metal_init: GPU name:   Apple M4
0.00.024.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.527 I ggml_metal_init: simdgroup reduction   = true
0.00.024.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.528 I ggml_metal_init: has residency sets    = true
0.00.024.528 I ggml_metal_init: has bfloat            = true
0.00.024.528 I ggml_metal_init: use bfloat            = true
0.00.024.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.580 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.201 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.203 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.219 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.282 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.283 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.283 I llama_init_from_model: graph nodes  = 429
0.00.035.284 I llama_init_from_model: graph splits = 2
0.00.035.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.856 I 
0.00.038.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.445 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.712 I llama_perf_context_print:        load time =      29.70 ms
0.00.042.713 I llama_perf_context_print: prompt eval time =       3.13 ms /     9 tokens (    0.35 ms per token,  2873.56 tokens per second)
0.00.042.714 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.714 I llama_perf_context_print:       total time =       3.86 ms /    10 tokens
0.00.042.925 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.300 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.325 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.274 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.282 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.291 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.292 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.292 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.294 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.295 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.295 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.296 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.296 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.300 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.300 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.301 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.093 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.094 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.094 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.094 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.095 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.095 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.095 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.096 I llama_model_loader: - type  f32:   40 tensors
0.00.049.096 I llama_model_loader: - type  f16:   30 tensors
0.00.049.097 I print_info: file format = GGUF V3 (latest)
0.00.049.098 I print_info: file type   = F16
0.00.049.099 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.450 W load: empty token at index 5
0.00.058.497 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.029 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.064 I load: special tokens cache size = 5
0.00.318.383 I load: token to piece cache size = 1.5060 MB
0.00.318.389 I print_info: arch             = jina-bert-v2
0.00.318.389 I print_info: vocab_only       = 0
0.00.318.389 I print_info: n_ctx_train      = 8192
0.00.318.390 I print_info: n_embd           = 384
0.00.318.390 I print_info: n_layer          = 4
0.00.318.397 I print_info: n_head           = 12
0.00.318.398 I print_info: n_head_kv        = 12
0.00.318.398 I print_info: n_rot            = 32
0.00.318.398 I print_info: n_swa            = 0
0.00.318.398 I print_info: n_embd_head_k    = 32
0.00.318.399 I print_info: n_embd_head_v    = 32
0.00.318.399 I print_info: n_gqa            = 1
0.00.318.399 I print_info: n_embd_k_gqa     = 384
0.00.318.400 I print_info: n_embd_v_gqa     = 384
0.00.318.401 I print_info: f_norm_eps       = 1.0e-12
0.00.318.402 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.318.402 I print_info: f_clamp_kqv      = 0.0e+00
0.00.318.402 I print_info: f_max_alibi_bias = 8.0e+00
0.00.318.402 I print_info: f_logit_scale    = 0.0e+00
0.00.318.403 I print_info: n_ff             = 1536
0.00.318.403 I print_info: n_expert         = 0
0.00.318.403 I print_info: n_expert_used    = 0
0.00.318.404 I print_info: causal attn      = 0
0.00.318.404 I print_info: pooling type     = -1
0.00.318.404 I print_info: rope type        = -1
0.00.318.407 I print_info: rope scaling     = linear
0.00.318.408 I print_info: freq_base_train  = 10000.0
0.00.318.408 I print_info: freq_scale_train = 1
0.00.318.408 I print_info: n_ctx_orig_yarn  = 8192
0.00.318.408 I print_info: rope_finetuned   = unknown
0.00.318.409 I print_info: ssm_d_conv       = 0
0.00.318.409 I print_info: ssm_d_inner      = 0
0.00.318.409 I print_info: ssm_d_state      = 0
0.00.318.409 I print_info: ssm_dt_rank      = 0
0.00.318.409 I print_info: ssm_dt_b_c_rms   = 0
0.00.318.410 I print_info: model type       = 33M
0.00.318.410 I print_info: model params     = 32.90 M
0.00.318.410 I print_info: general.name     = Jina Bert Implementation
0.00.318.411 I print_info: vocab type       = BPE
0.00.318.411 I print_info: n_vocab          = 61056
0.00.318.412 I print_info: n_merges         = 39382
0.00.318.412 I print_info: BOS token        = 0 '<s>'
0.00.318.412 I print_info: EOS token        = 2 '</s>'
0.00.318.412 I print_info: UNK token        = 3 '<unk>'
0.00.318.412 I print_info: SEP token        = 2 '</s>'
0.00.318.412 I print_info: PAD token        = 1 '<pad>'
0.00.318.413 I print_info: MASK token       = 4 '<mask>'
0.00.318.413 I print_info: EOG token        = 2 '</s>'
0.00.318.413 I print_info: max token length = 45
0.00.320.483 I load_tensors: offloading 4 repeating layers to GPU
0.00.320.484 I load_tensors: offloading output layer to GPU
0.00.320.484 I load_tensors: offloaded 5/5 layers to GPU
0.00.320.509 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.510 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.320.957 I llama_init_from_model: n_seq_max     = 1
0.00.320.958 I llama_init_from_model: n_ctx         = 8192
0.00.320.959 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.320.959 I llama_init_from_model: n_batch       = 2048
0.00.320.959 I llama_init_from_model: n_ubatch      = 2048
0.00.320.960 I llama_init_from_model: flash_attn    = 0
0.00.320.960 I llama_init_from_model: freq_base     = 10000.0
0.00.320.960 I llama_init_from_model: freq_scale    = 1
0.00.320.961 I ggml_metal_init: allocating
0.00.320.965 I ggml_metal_init: found device: Apple M4
0.00.320.969 I ggml_metal_init: picking default device: Apple M4
0.00.321.596 I ggml_metal_init: using embedded metal library
0.00.324.059 I ggml_metal_init: GPU name:   Apple M4
0.00.324.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.324.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.324.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.324.061 I ggml_metal_init: simdgroup reduction   = true
0.00.324.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.324.062 I ggml_metal_init: has residency sets    = true
0.00.324.062 I ggml_metal_init: has bfloat            = true
0.00.324.062 I ggml_metal_init: use bfloat            = true
0.00.324.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.324.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.333.540 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.336.574 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.336.576 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.336.596 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.343.156 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.343.157 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.343.157 I llama_init_from_model: graph nodes  = 154
0.00.343.158 I llama_init_from_model: graph splits = 2
0.00.343.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.343.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.350.447 I 
0.00.350.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.350.672 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.350.673 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.350.676 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.350.676 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.350.679 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.350.680 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.351.178 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.045 I llama_perf_context_print:        load time =     328.11 ms
0.00.355.047 I llama_perf_context_print: prompt eval time =       3.86 ms /    62 tokens (    0.06 ms per token, 16078.84 tokens per second)
0.00.355.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.048 I llama_perf_context_print:       total time =       4.60 ms /    63 tokens
0.00.355.322 I ggml_metal_free: deallocating

real	0m1.072s
user	0m0.325s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.087 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.219 I main: llama backend init
0.00.000.224 I main: load the model and apply lora adapter, if any
0.00.054.624 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.418 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.433 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.075.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.082.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.025 I llama_model_loader: - type  f32:  194 tensors
0.00.082.026 I llama_model_loader: - type  f16:   98 tensors
0.00.082.027 I print_info: file format = GGUF V3 (latest)
0.00.082.028 I print_info: file type   = all F32 (guessed)
0.00.082.030 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.225 I load: special tokens cache size = 25
0.00.104.885 I load: token to piece cache size = 0.2984 MB
0.00.104.889 I print_info: arch             = gptneox
0.00.104.890 I print_info: vocab_only       = 0
0.00.104.890 I print_info: n_ctx_train      = 2048
0.00.104.892 I print_info: n_embd           = 2048
0.00.104.892 I print_info: n_layer          = 24
0.00.104.897 I print_info: n_head           = 16
0.00.104.897 I print_info: n_head_kv        = 16
0.00.104.897 I print_info: n_rot            = 32
0.00.104.898 I print_info: n_swa            = 0
0.00.104.898 I print_info: n_embd_head_k    = 128
0.00.104.898 I print_info: n_embd_head_v    = 128
0.00.104.899 I print_info: n_gqa            = 1
0.00.104.899 I print_info: n_embd_k_gqa     = 2048
0.00.104.902 I print_info: n_embd_v_gqa     = 2048
0.00.104.903 I print_info: f_norm_eps       = 1.0e-05
0.00.104.904 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.904 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.904 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.904 I print_info: f_logit_scale    = 0.0e+00
0.00.104.905 I print_info: n_ff             = 8192
0.00.104.905 I print_info: n_expert         = 0
0.00.104.905 I print_info: n_expert_used    = 0
0.00.104.905 I print_info: causal attn      = 1
0.00.104.906 I print_info: pooling type     = 0
0.00.104.906 I print_info: rope type        = 2
0.00.104.907 I print_info: rope scaling     = linear
0.00.104.908 I print_info: freq_base_train  = 10000.0
0.00.104.908 I print_info: freq_scale_train = 1
0.00.104.908 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.909 I print_info: rope_finetuned   = unknown
0.00.104.909 I print_info: ssm_d_conv       = 0
0.00.104.909 I print_info: ssm_d_inner      = 0
0.00.104.909 I print_info: ssm_d_state      = 0
0.00.104.909 I print_info: ssm_dt_rank      = 0
0.00.104.909 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.909 I print_info: model type       = 1.4B
0.00.104.910 I print_info: model params     = 1.41 B
0.00.104.910 I print_info: general.name     = 1.4B
0.00.104.911 I print_info: vocab type       = BPE
0.00.104.911 I print_info: n_vocab          = 50304
0.00.104.911 I print_info: n_merges         = 50009
0.00.104.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.913 I print_info: LF token         = 187 'Ċ'
0.00.104.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.914 I print_info: max token length = 1024
0.00.142.723 I load_tensors: offloading 24 repeating layers to GPU
0.00.142.727 I load_tensors: offloading output layer to GPU
0.00.142.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.753 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.754 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.143.138 I llama_init_from_model: n_seq_max     = 1
0.00.143.139 I llama_init_from_model: n_ctx         = 2048
0.00.143.139 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.143.139 I llama_init_from_model: n_batch       = 2048
0.00.143.140 I llama_init_from_model: n_ubatch      = 512
0.00.143.140 I llama_init_from_model: flash_attn    = 0
0.00.143.140 I llama_init_from_model: freq_base     = 10000.0
0.00.143.140 I llama_init_from_model: freq_scale    = 1
0.00.143.141 I ggml_metal_init: allocating
0.00.143.161 I ggml_metal_init: found device: Apple M4
0.00.143.166 I ggml_metal_init: picking default device: Apple M4
0.00.143.870 I ggml_metal_init: using embedded metal library
0.00.153.293 I ggml_metal_init: GPU name:   Apple M4
0.00.153.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.153.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.153.298 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.153.299 I ggml_metal_init: simdgroup reduction   = true
0.00.153.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.153.299 I ggml_metal_init: has residency sets    = true
0.00.153.299 I ggml_metal_init: has bfloat            = true
0.00.153.299 I ggml_metal_init: use bfloat            = true
0.00.153.300 I ggml_metal_init: hasUnifiedMemory      = true
0.00.153.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.180.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.209.218 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.209.223 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.209.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.212.800 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.212.802 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.212.803 I llama_init_from_model: graph nodes  = 967
0.00.212.803 I llama_init_from_model: graph splits = 2
0.00.212.806 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.212.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.212.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.279.787 I main: llama threadpool init, n_threads = 4
0.00.279.830 I 
0.00.279.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.279.864 I 
0.00.279.908 I sampler seed: 1234
0.00.279.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.279.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.279.939 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.279.939 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.114.718 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.02.114.719 I llama_perf_context_print:        load time =     224.37 ms
0.02.114.720 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.92 tokens per second)
0.02.114.720 I llama_perf_context_print:        eval time =    1788.18 ms /    63 runs   (   28.38 ms per token,    35.23 tokens per second)
0.02.114.721 I llama_perf_context_print:       total time =    1835.72 ms /    70 tokens
0.02.114.996 I ggml_metal_free: deallocating

real	0m2.415s
user	0m0.126s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.766 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.924 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.547 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.026 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.030 I llama_model_loader: - type  f32:  194 tensors
0.00.053.030 I llama_model_loader: - type  f16:   98 tensors
0.00.053.031 I print_info: file format = GGUF V3 (latest)
0.00.053.031 I print_info: file type   = all F32 (guessed)
0.00.053.032 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.822 I load: special tokens cache size = 25
0.00.072.572 I load: token to piece cache size = 0.2984 MB
0.00.072.575 I print_info: arch             = gptneox
0.00.072.575 I print_info: vocab_only       = 0
0.00.072.576 I print_info: n_ctx_train      = 2048
0.00.072.576 I print_info: n_embd           = 2048
0.00.072.576 I print_info: n_layer          = 24
0.00.072.580 I print_info: n_head           = 16
0.00.072.581 I print_info: n_head_kv        = 16
0.00.072.583 I print_info: n_rot            = 32
0.00.072.583 I print_info: n_swa            = 0
0.00.072.583 I print_info: n_embd_head_k    = 128
0.00.072.583 I print_info: n_embd_head_v    = 128
0.00.072.584 I print_info: n_gqa            = 1
0.00.072.585 I print_info: n_embd_k_gqa     = 2048
0.00.072.586 I print_info: n_embd_v_gqa     = 2048
0.00.072.586 I print_info: f_norm_eps       = 1.0e-05
0.00.072.587 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.587 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.588 I print_info: f_logit_scale    = 0.0e+00
0.00.072.589 I print_info: n_ff             = 8192
0.00.072.589 I print_info: n_expert         = 0
0.00.072.590 I print_info: n_expert_used    = 0
0.00.072.590 I print_info: causal attn      = 1
0.00.072.590 I print_info: pooling type     = 0
0.00.072.590 I print_info: rope type        = 2
0.00.072.590 I print_info: rope scaling     = linear
0.00.072.591 I print_info: freq_base_train  = 10000.0
0.00.072.592 I print_info: freq_scale_train = 1
0.00.072.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.593 I print_info: rope_finetuned   = unknown
0.00.072.593 I print_info: ssm_d_conv       = 0
0.00.072.593 I print_info: ssm_d_inner      = 0
0.00.072.593 I print_info: ssm_d_state      = 0
0.00.072.593 I print_info: ssm_dt_rank      = 0
0.00.072.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.593 I print_info: model type       = 1.4B
0.00.072.594 I print_info: model params     = 1.41 B
0.00.072.594 I print_info: general.name     = 1.4B
0.00.072.595 I print_info: vocab type       = BPE
0.00.072.595 I print_info: n_vocab          = 50304
0.00.072.595 I print_info: n_merges         = 50009
0.00.072.597 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.597 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.597 I print_info: LF token         = 187 'Ċ'
0.00.072.597 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.598 I print_info: max token length = 1024
0.01.058.084 I load_tensors: offloading 24 repeating layers to GPU
0.01.058.089 I load_tensors: offloading output layer to GPU
0.01.058.090 I load_tensors: offloaded 25/25 layers to GPU
0.01.058.121 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.058.123 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.058.999 I llama_init_from_model: n_seq_max     = 1
0.01.059.000 I llama_init_from_model: n_ctx         = 128
0.01.059.000 I llama_init_from_model: n_ctx_per_seq = 128
0.01.059.001 I llama_init_from_model: n_batch       = 128
0.01.059.001 I llama_init_from_model: n_ubatch      = 128
0.01.059.001 I llama_init_from_model: flash_attn    = 0
0.01.059.002 I llama_init_from_model: freq_base     = 10000.0
0.01.059.002 I llama_init_from_model: freq_scale    = 1
0.01.059.002 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.059.004 I ggml_metal_init: allocating
0.01.059.083 I ggml_metal_init: found device: Apple M4
0.01.059.092 I ggml_metal_init: picking default device: Apple M4
0.01.060.226 I ggml_metal_init: using embedded metal library
0.01.064.059 I ggml_metal_init: GPU name:   Apple M4
0.01.064.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.064.062 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.064.062 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.064.063 I ggml_metal_init: simdgroup reduction   = true
0.01.064.063 I ggml_metal_init: simdgroup matrix mul. = true
0.01.064.063 I ggml_metal_init: has residency sets    = true
0.01.064.063 I ggml_metal_init: has bfloat            = true
0.01.064.063 I ggml_metal_init: use bfloat            = true
0.01.064.064 I ggml_metal_init: hasUnifiedMemory      = true
0.01.064.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.074.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.076.311 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.076.313 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.076.346 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.077.957 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.077.959 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.077.959 I llama_init_from_model: graph nodes  = 967
0.01.077.959 I llama_init_from_model: graph splits = 2
0.01.077.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.077.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.113.641 I 
0.01.113.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.113.699 I perplexity: tokenizing the input ..
0.01.118.673 I perplexity: tokenization took 4.972 ms
0.01.118.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.237.598 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.238.948 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.238.971 I llama_perf_context_print:        load time =    1091.71 ms
0.01.238.972 I llama_perf_context_print: prompt eval time =     118.60 ms /   128 tokens (    0.93 ms per token,  1079.26 tokens per second)
0.01.238.972 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.238.973 I llama_perf_context_print:       total time =     125.33 ms /   129 tokens
0.01.239.410 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.096s
sys	0m0.216s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.098.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.098.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.098.728 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.098.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.098.729 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.098.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.098.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.098.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.098.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.098.733 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.098.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.098.734 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.098.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.098.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.098.739 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.098.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.098.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.105.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.107.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.114.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.114.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.114.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.114.686 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.114.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.114.688 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.114.689 I llama_model_loader: - type  f32:  194 tensors
0.00.114.690 I llama_model_loader: - type q8_0:   98 tensors
0.00.114.691 I print_info: file format = GGUF V3 (latest)
0.00.114.692 I print_info: file type   = Q8_0
0.00.114.694 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.128.046 I load: special tokens cache size = 25
0.00.133.933 I load: token to piece cache size = 0.2984 MB
0.00.133.938 I print_info: arch             = gptneox
0.00.133.938 I print_info: vocab_only       = 0
0.00.133.938 I print_info: n_ctx_train      = 2048
0.00.133.939 I print_info: n_embd           = 2048
0.00.133.941 I print_info: n_layer          = 24
0.00.133.946 I print_info: n_head           = 16
0.00.133.947 I print_info: n_head_kv        = 16
0.00.133.947 I print_info: n_rot            = 32
0.00.133.948 I print_info: n_swa            = 0
0.00.133.948 I print_info: n_embd_head_k    = 128
0.00.133.948 I print_info: n_embd_head_v    = 128
0.00.133.951 I print_info: n_gqa            = 1
0.00.133.952 I print_info: n_embd_k_gqa     = 2048
0.00.133.953 I print_info: n_embd_v_gqa     = 2048
0.00.133.953 I print_info: f_norm_eps       = 1.0e-05
0.00.133.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.133.954 I print_info: f_clamp_kqv      = 0.0e+00
0.00.133.954 I print_info: f_max_alibi_bias = 0.0e+00
0.00.133.954 I print_info: f_logit_scale    = 0.0e+00
0.00.133.955 I print_info: n_ff             = 8192
0.00.133.955 I print_info: n_expert         = 0
0.00.133.956 I print_info: n_expert_used    = 0
0.00.133.956 I print_info: causal attn      = 1
0.00.133.956 I print_info: pooling type     = 0
0.00.133.956 I print_info: rope type        = 2
0.00.133.956 I print_info: rope scaling     = linear
0.00.133.957 I print_info: freq_base_train  = 10000.0
0.00.133.957 I print_info: freq_scale_train = 1
0.00.133.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.133.957 I print_info: rope_finetuned   = unknown
0.00.133.957 I print_info: ssm_d_conv       = 0
0.00.133.958 I print_info: ssm_d_inner      = 0
0.00.133.958 I print_info: ssm_d_state      = 0
0.00.133.958 I print_info: ssm_dt_rank      = 0
0.00.133.958 I print_info: ssm_dt_b_c_rms   = 0
0.00.133.958 I print_info: model type       = 1.4B
0.00.133.959 I print_info: model params     = 1.41 B
0.00.133.959 I print_info: general.name     = 1.4B
0.00.133.960 I print_info: vocab type       = BPE
0.00.133.960 I print_info: n_vocab          = 50304
0.00.133.960 I print_info: n_merges         = 50009
0.00.133.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.133.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.133.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.133.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.133.961 I print_info: LF token         = 187 'Ċ'
0.00.133.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.133.961 I print_info: max token length = 1024
0.01.408.806 I load_tensors: offloading 24 repeating layers to GPU
0.01.408.814 I load_tensors: offloading output layer to GPU
0.01.408.816 I load_tensors: offloaded 25/25 layers to GPU
0.01.408.842 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.408.844 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.409.834 I llama_init_from_model: n_seq_max     = 1
0.01.409.836 I llama_init_from_model: n_ctx         = 2048
0.01.409.837 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.409.837 I llama_init_from_model: n_batch       = 2048
0.01.409.838 I llama_init_from_model: n_ubatch      = 512
0.01.409.838 I llama_init_from_model: flash_attn    = 0
0.01.409.839 I llama_init_from_model: freq_base     = 10000.0
0.01.409.839 I llama_init_from_model: freq_scale    = 1
0.01.409.841 I ggml_metal_init: allocating
0.01.409.872 I ggml_metal_init: found device: Apple M4
0.01.409.881 I ggml_metal_init: picking default device: Apple M4
0.01.411.106 I ggml_metal_init: using embedded metal library
0.01.416.568 I ggml_metal_init: GPU name:   Apple M4
0.01.416.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.416.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.416.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.416.573 I ggml_metal_init: simdgroup reduction   = true
0.01.416.574 I ggml_metal_init: simdgroup matrix mul. = true
0.01.416.574 I ggml_metal_init: has residency sets    = true
0.01.416.574 I ggml_metal_init: has bfloat            = true
0.01.416.574 I ggml_metal_init: use bfloat            = true
0.01.416.575 I ggml_metal_init: hasUnifiedMemory      = true
0.01.416.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.433.061 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.489.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.489.672 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.489.712 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.494.150 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.494.152 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.494.153 I llama_init_from_model: graph nodes  = 967
0.01.494.153 I llama_init_from_model: graph splits = 2
0.01.494.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.494.290 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.494.291 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.546.786 I main: llama threadpool init, n_threads = 4
0.01.546.825 I 
0.01.546.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.546.851 I 
0.01.546.970 I sampler seed: 1234
0.01.546.974 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.546.984 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.546.984 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.546.984 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.642.833 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.02.642.834 I llama_perf_context_print:        load time =    1536.26 ms
0.02.642.835 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.01 tokens per second)
0.02.642.835 I llama_perf_context_print:        eval time =    1043.95 ms /    63 runs   (   16.57 ms per token,    60.35 tokens per second)
0.02.642.837 I llama_perf_context_print:       total time =    1096.70 ms /    70 tokens
0.02.643.054 I ggml_metal_free: deallocating

real	0m2.664s
user	0m0.124s
sys	0m0.256s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.259 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.471 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.216 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.216 I llama_model_loader: - type  f32:  194 tensors
0.00.025.217 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.218 I print_info: file format = GGUF V3 (latest)
0.00.025.218 I print_info: file type   = Q8_0
0.00.025.219 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.713 I load: special tokens cache size = 25
0.00.039.954 I load: token to piece cache size = 0.2984 MB
0.00.039.958 I print_info: arch             = gptneox
0.00.039.958 I print_info: vocab_only       = 0
0.00.039.958 I print_info: n_ctx_train      = 2048
0.00.039.958 I print_info: n_embd           = 2048
0.00.039.959 I print_info: n_layer          = 24
0.00.039.963 I print_info: n_head           = 16
0.00.039.963 I print_info: n_head_kv        = 16
0.00.039.964 I print_info: n_rot            = 32
0.00.039.964 I print_info: n_swa            = 0
0.00.039.964 I print_info: n_embd_head_k    = 128
0.00.039.964 I print_info: n_embd_head_v    = 128
0.00.039.965 I print_info: n_gqa            = 1
0.00.039.966 I print_info: n_embd_k_gqa     = 2048
0.00.039.966 I print_info: n_embd_v_gqa     = 2048
0.00.039.967 I print_info: f_norm_eps       = 1.0e-05
0.00.039.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.969 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.970 I print_info: f_logit_scale    = 0.0e+00
0.00.039.970 I print_info: n_ff             = 8192
0.00.039.970 I print_info: n_expert         = 0
0.00.039.970 I print_info: n_expert_used    = 0
0.00.039.970 I print_info: causal attn      = 1
0.00.039.971 I print_info: pooling type     = 0
0.00.039.973 I print_info: rope type        = 2
0.00.039.973 I print_info: rope scaling     = linear
0.00.039.973 I print_info: freq_base_train  = 10000.0
0.00.039.973 I print_info: freq_scale_train = 1
0.00.039.974 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.974 I print_info: rope_finetuned   = unknown
0.00.039.974 I print_info: ssm_d_conv       = 0
0.00.039.974 I print_info: ssm_d_inner      = 0
0.00.039.974 I print_info: ssm_d_state      = 0
0.00.039.974 I print_info: ssm_dt_rank      = 0
0.00.039.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.975 I print_info: model type       = 1.4B
0.00.039.975 I print_info: model params     = 1.41 B
0.00.039.975 I print_info: general.name     = 1.4B
0.00.039.976 I print_info: vocab type       = BPE
0.00.039.976 I print_info: n_vocab          = 50304
0.00.039.977 I print_info: n_merges         = 50009
0.00.039.978 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.978 I print_info: LF token         = 187 'Ċ'
0.00.039.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.979 I print_info: max token length = 1024
0.00.870.090 I load_tensors: offloading 24 repeating layers to GPU
0.00.870.094 I load_tensors: offloading output layer to GPU
0.00.870.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.870.122 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.870.125 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.871.428 I llama_init_from_model: n_seq_max     = 1
0.00.871.430 I llama_init_from_model: n_ctx         = 128
0.00.871.430 I llama_init_from_model: n_ctx_per_seq = 128
0.00.871.431 I llama_init_from_model: n_batch       = 128
0.00.871.434 I llama_init_from_model: n_ubatch      = 128
0.00.871.435 I llama_init_from_model: flash_attn    = 0
0.00.871.435 I llama_init_from_model: freq_base     = 10000.0
0.00.871.436 I llama_init_from_model: freq_scale    = 1
0.00.871.437 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.871.438 I ggml_metal_init: allocating
0.00.871.501 I ggml_metal_init: found device: Apple M4
0.00.871.509 I ggml_metal_init: picking default device: Apple M4
0.00.872.796 I ggml_metal_init: using embedded metal library
0.00.877.998 I ggml_metal_init: GPU name:   Apple M4
0.00.878.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.878.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.878.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.878.004 I ggml_metal_init: simdgroup reduction   = true
0.00.878.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.878.004 I ggml_metal_init: has residency sets    = true
0.00.878.005 I ggml_metal_init: has bfloat            = true
0.00.878.005 I ggml_metal_init: use bfloat            = true
0.00.878.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.878.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.894.165 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.897.000 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.897.006 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.897.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.899.600 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.899.602 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.899.602 I llama_init_from_model: graph nodes  = 967
0.00.899.602 I llama_init_from_model: graph splits = 2
0.00.899.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.899.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.241 I 
0.00.926.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.926.315 I perplexity: tokenizing the input ..
0.00.933.281 I perplexity: tokenization took 6.964 ms
0.00.933.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.058.201 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.059.637 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.059.650 I llama_perf_context_print:        load time =     916.98 ms
0.01.059.651 I llama_perf_context_print: prompt eval time =     124.46 ms /   128 tokens (    0.97 ms per token,  1028.47 tokens per second)
0.01.059.652 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.059.652 I llama_perf_context_print:       total time =     133.41 ms /   129 tokens
0.01.060.067 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.077s
sys	0m0.163s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.014.542 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.698 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.698 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.021 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.388 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.392 I llama_model_loader: - type  f32:  194 tensors
0.00.045.392 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.392 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.393 I print_info: file format = GGUF V3 (latest)
0.00.045.394 I print_info: file type   = Q4_0
0.00.045.395 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.056.438 I load: special tokens cache size = 25
0.00.065.853 I load: token to piece cache size = 0.2984 MB
0.00.065.857 I print_info: arch             = gptneox
0.00.065.857 I print_info: vocab_only       = 0
0.00.065.858 I print_info: n_ctx_train      = 2048
0.00.065.858 I print_info: n_embd           = 2048
0.00.065.858 I print_info: n_layer          = 24
0.00.065.863 I print_info: n_head           = 16
0.00.065.864 I print_info: n_head_kv        = 16
0.00.065.864 I print_info: n_rot            = 32
0.00.065.864 I print_info: n_swa            = 0
0.00.065.865 I print_info: n_embd_head_k    = 128
0.00.065.865 I print_info: n_embd_head_v    = 128
0.00.065.866 I print_info: n_gqa            = 1
0.00.065.867 I print_info: n_embd_k_gqa     = 2048
0.00.065.867 I print_info: n_embd_v_gqa     = 2048
0.00.065.868 I print_info: f_norm_eps       = 1.0e-05
0.00.065.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.869 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.869 I print_info: f_logit_scale    = 0.0e+00
0.00.065.870 I print_info: n_ff             = 8192
0.00.065.871 I print_info: n_expert         = 0
0.00.065.871 I print_info: n_expert_used    = 0
0.00.065.871 I print_info: causal attn      = 1
0.00.065.871 I print_info: pooling type     = 0
0.00.065.871 I print_info: rope type        = 2
0.00.065.872 I print_info: rope scaling     = linear
0.00.065.872 I print_info: freq_base_train  = 10000.0
0.00.065.873 I print_info: freq_scale_train = 1
0.00.065.873 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.873 I print_info: rope_finetuned   = unknown
0.00.065.876 I print_info: ssm_d_conv       = 0
0.00.065.876 I print_info: ssm_d_inner      = 0
0.00.065.876 I print_info: ssm_d_state      = 0
0.00.065.877 I print_info: ssm_dt_rank      = 0
0.00.065.877 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.877 I print_info: model type       = 1.4B
0.00.065.877 I print_info: model params     = 1.41 B
0.00.065.877 I print_info: general.name     = 1.4B
0.00.065.878 I print_info: vocab type       = BPE
0.00.065.879 I print_info: n_vocab          = 50304
0.00.065.879 I print_info: n_merges         = 50009
0.00.065.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.885 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.885 I print_info: LF token         = 187 'Ċ'
0.00.065.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.886 I print_info: max token length = 1024
0.00.642.603 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.616 I load_tensors: offloading output layer to GPU
0.00.642.617 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.646 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.642.647 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.644.116 I llama_init_from_model: n_seq_max     = 1
0.00.644.123 I llama_init_from_model: n_ctx         = 2048
0.00.644.123 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.644.124 I llama_init_from_model: n_batch       = 2048
0.00.644.124 I llama_init_from_model: n_ubatch      = 512
0.00.644.124 I llama_init_from_model: flash_attn    = 0
0.00.644.127 I llama_init_from_model: freq_base     = 10000.0
0.00.644.127 I llama_init_from_model: freq_scale    = 1
0.00.644.134 I ggml_metal_init: allocating
0.00.644.228 I ggml_metal_init: found device: Apple M4
0.00.644.242 I ggml_metal_init: picking default device: Apple M4
0.00.646.032 I ggml_metal_init: using embedded metal library
0.00.651.513 I ggml_metal_init: GPU name:   Apple M4
0.00.651.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.526 I ggml_metal_init: simdgroup reduction   = true
0.00.651.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.526 I ggml_metal_init: has residency sets    = true
0.00.651.527 I ggml_metal_init: has bfloat            = true
0.00.651.527 I ggml_metal_init: use bfloat            = true
0.00.651.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.453 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.919 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.926 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.960 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.437 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.438 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.438 I llama_init_from_model: graph nodes  = 967
0.00.741.439 I llama_init_from_model: graph splits = 2
0.00.741.445 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.357 I main: llama threadpool init, n_threads = 4
0.00.795.400 I 
0.00.795.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.425 I 
0.00.795.601 I sampler seed: 1234
0.00.795.606 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.629 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.630 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.630 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.477.250 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.477.250 I llama_perf_context_print:        load time =     780.17 ms
0.01.477.251 I llama_perf_context_print: prompt eval time =      48.24 ms /     7 tokens (    6.89 ms per token,   145.12 tokens per second)
0.01.477.252 I llama_perf_context_print:        eval time =     630.53 ms /    63 runs   (   10.01 ms per token,    99.92 tokens per second)
0.01.477.252 I llama_perf_context_print:       total time =     682.54 ms /    70 tokens
0.01.477.463 I ggml_metal_free: deallocating

real	0m1.502s
user	0m0.122s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.356 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.679 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.686 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.688 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.689 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.689 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.690 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.691 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.691 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.691 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.692 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.609 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.505 I llama_model_loader: - type  f32:  194 tensors
0.00.035.506 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.506 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.507 I print_info: file format = GGUF V3 (latest)
0.00.035.507 I print_info: file type   = Q4_0
0.00.035.508 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.411 I load: special tokens cache size = 25
0.00.051.350 I load: token to piece cache size = 0.2984 MB
0.00.051.354 I print_info: arch             = gptneox
0.00.051.354 I print_info: vocab_only       = 0
0.00.051.354 I print_info: n_ctx_train      = 2048
0.00.051.354 I print_info: n_embd           = 2048
0.00.051.354 I print_info: n_layer          = 24
0.00.051.359 I print_info: n_head           = 16
0.00.051.360 I print_info: n_head_kv        = 16
0.00.051.360 I print_info: n_rot            = 32
0.00.051.360 I print_info: n_swa            = 0
0.00.051.360 I print_info: n_embd_head_k    = 128
0.00.051.360 I print_info: n_embd_head_v    = 128
0.00.051.361 I print_info: n_gqa            = 1
0.00.051.362 I print_info: n_embd_k_gqa     = 2048
0.00.051.362 I print_info: n_embd_v_gqa     = 2048
0.00.051.363 I print_info: f_norm_eps       = 1.0e-05
0.00.051.363 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.364 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.364 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.364 I print_info: f_logit_scale    = 0.0e+00
0.00.051.364 I print_info: n_ff             = 8192
0.00.051.365 I print_info: n_expert         = 0
0.00.051.368 I print_info: n_expert_used    = 0
0.00.051.368 I print_info: causal attn      = 1
0.00.051.368 I print_info: pooling type     = 0
0.00.051.368 I print_info: rope type        = 2
0.00.051.368 I print_info: rope scaling     = linear
0.00.051.369 I print_info: freq_base_train  = 10000.0
0.00.051.369 I print_info: freq_scale_train = 1
0.00.051.369 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.370 I print_info: rope_finetuned   = unknown
0.00.051.370 I print_info: ssm_d_conv       = 0
0.00.051.370 I print_info: ssm_d_inner      = 0
0.00.051.370 I print_info: ssm_d_state      = 0
0.00.051.370 I print_info: ssm_dt_rank      = 0
0.00.051.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.370 I print_info: model type       = 1.4B
0.00.051.371 I print_info: model params     = 1.41 B
0.00.051.371 I print_info: general.name     = 1.4B
0.00.051.376 I print_info: vocab type       = BPE
0.00.051.376 I print_info: n_vocab          = 50304
0.00.051.376 I print_info: n_merges         = 50009
0.00.051.376 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.385 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.385 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: LF token         = 187 'Ċ'
0.00.051.386 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.386 I print_info: max token length = 1024
0.00.630.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.279 I load_tensors: offloading output layer to GPU
0.00.630.280 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.312 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.630.313 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.631.707 I llama_init_from_model: n_seq_max     = 1
0.00.631.713 I llama_init_from_model: n_ctx         = 128
0.00.631.713 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.714 I llama_init_from_model: n_batch       = 128
0.00.631.717 I llama_init_from_model: n_ubatch      = 128
0.00.631.718 I llama_init_from_model: flash_attn    = 0
0.00.631.721 I llama_init_from_model: freq_base     = 10000.0
0.00.631.722 I llama_init_from_model: freq_scale    = 1
0.00.631.722 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.725 I ggml_metal_init: allocating
0.00.631.819 I ggml_metal_init: found device: Apple M4
0.00.631.833 I ggml_metal_init: picking default device: Apple M4
0.00.633.701 I ggml_metal_init: using embedded metal library
0.00.639.024 I ggml_metal_init: GPU name:   Apple M4
0.00.639.042 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.044 I ggml_metal_init: simdgroup reduction   = true
0.00.639.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.045 I ggml_metal_init: has residency sets    = true
0.00.639.045 I ggml_metal_init: has bfloat            = true
0.00.639.045 I ggml_metal_init: use bfloat            = true
0.00.639.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.186 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.743 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.753 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.102 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.104 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.105 I llama_init_from_model: graph nodes  = 967
0.00.666.105 I llama_init_from_model: graph splits = 2
0.00.666.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.108 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.355 I 
0.00.691.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.457 I perplexity: tokenizing the input ..
0.00.699.084 I perplexity: tokenization took 7.624 ms
0.00.699.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.303 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.837.635 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.837.648 I llama_perf_context_print:        load time =     677.99 ms
0.00.837.649 I llama_perf_context_print: prompt eval time =     136.27 ms /   128 tokens (    1.06 ms per token,   939.29 tokens per second)
0.00.837.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.650 I llama_perf_context_print:       total time =     146.30 ms /   129 tokens
0.00.838.031 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.083s
sys	0m0.130s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.013.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.172 I llama_model_loader: - type  f32:  194 tensors
0.00.029.173 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.174 I print_info: file format = GGUF V3 (latest)
0.00.029.174 I print_info: file type   = Q4_1
0.00.029.175 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.960 I load: special tokens cache size = 25
0.00.042.984 I load: token to piece cache size = 0.2984 MB
0.00.042.987 I print_info: arch             = gptneox
0.00.042.987 I print_info: vocab_only       = 0
0.00.042.987 I print_info: n_ctx_train      = 2048
0.00.042.988 I print_info: n_embd           = 2048
0.00.042.988 I print_info: n_layer          = 24
0.00.042.991 I print_info: n_head           = 16
0.00.042.991 I print_info: n_head_kv        = 16
0.00.042.992 I print_info: n_rot            = 32
0.00.042.992 I print_info: n_swa            = 0
0.00.042.992 I print_info: n_embd_head_k    = 128
0.00.042.992 I print_info: n_embd_head_v    = 128
0.00.042.993 I print_info: n_gqa            = 1
0.00.042.996 I print_info: n_embd_k_gqa     = 2048
0.00.042.997 I print_info: n_embd_v_gqa     = 2048
0.00.042.997 I print_info: f_norm_eps       = 1.0e-05
0.00.042.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.998 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.998 I print_info: f_logit_scale    = 0.0e+00
0.00.043.000 I print_info: n_ff             = 8192
0.00.043.000 I print_info: n_expert         = 0
0.00.043.000 I print_info: n_expert_used    = 0
0.00.043.000 I print_info: causal attn      = 1
0.00.043.002 I print_info: pooling type     = 0
0.00.043.002 I print_info: rope type        = 2
0.00.043.002 I print_info: rope scaling     = linear
0.00.043.002 I print_info: freq_base_train  = 10000.0
0.00.043.003 I print_info: freq_scale_train = 1
0.00.043.003 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.003 I print_info: rope_finetuned   = unknown
0.00.043.003 I print_info: ssm_d_conv       = 0
0.00.043.003 I print_info: ssm_d_inner      = 0
0.00.043.004 I print_info: ssm_d_state      = 0
0.00.043.004 I print_info: ssm_dt_rank      = 0
0.00.043.004 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.004 I print_info: model type       = 1.4B
0.00.043.004 I print_info: model params     = 1.41 B
0.00.043.005 I print_info: general.name     = 1.4B
0.00.043.005 I print_info: vocab type       = BPE
0.00.043.005 I print_info: n_vocab          = 50304
0.00.043.005 I print_info: n_merges         = 50009
0.00.043.006 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.006 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.007 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.010 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.011 I print_info: LF token         = 187 'Ċ'
0.00.043.011 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.011 I print_info: max token length = 1024
0.00.666.542 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.558 I load_tensors: offloading output layer to GPU
0.00.666.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.595 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.666.596 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.668.055 I llama_init_from_model: n_seq_max     = 1
0.00.668.060 I llama_init_from_model: n_ctx         = 2048
0.00.668.061 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.061 I llama_init_from_model: n_batch       = 2048
0.00.668.062 I llama_init_from_model: n_ubatch      = 512
0.00.668.062 I llama_init_from_model: flash_attn    = 0
0.00.668.064 I llama_init_from_model: freq_base     = 10000.0
0.00.668.065 I llama_init_from_model: freq_scale    = 1
0.00.668.067 I ggml_metal_init: allocating
0.00.668.139 I ggml_metal_init: found device: Apple M4
0.00.668.154 I ggml_metal_init: picking default device: Apple M4
0.00.669.967 I ggml_metal_init: using embedded metal library
0.00.675.706 I ggml_metal_init: GPU name:   Apple M4
0.00.675.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.712 I ggml_metal_init: simdgroup reduction   = true
0.00.675.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.713 I ggml_metal_init: has residency sets    = true
0.00.675.713 I ggml_metal_init: has bfloat            = true
0.00.675.714 I ggml_metal_init: use bfloat            = true
0.00.675.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.136 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.064 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.750.072 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.750.112 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.533 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.754.535 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.754.535 I llama_init_from_model: graph nodes  = 967
0.00.754.535 I llama_init_from_model: graph splits = 2
0.00.754.542 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.872 I main: llama threadpool init, n_threads = 4
0.00.809.916 I 
0.00.809.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.943 I 
0.00.810.094 I sampler seed: 1234
0.00.810.099 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.141 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.144 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.144 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.543.919 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.543.919 I llama_perf_context_print:        load time =     796.17 ms
0.01.543.920 I llama_perf_context_print: prompt eval time =      44.74 ms /     7 tokens (    6.39 ms per token,   156.47 tokens per second)
0.01.543.921 I llama_perf_context_print:        eval time =     686.22 ms /    63 runs   (   10.89 ms per token,    91.81 tokens per second)
0.01.543.921 I llama_perf_context_print:       total time =     734.74 ms /    70 tokens
0.01.544.205 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.908 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.922 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.924 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.555 I llama_model_loader: - type  f32:  194 tensors
0.00.028.555 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.556 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.556 I print_info: file format = GGUF V3 (latest)
0.00.028.557 I print_info: file type   = Q4_1
0.00.028.558 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.408 I load: special tokens cache size = 25
0.00.042.371 I load: token to piece cache size = 0.2984 MB
0.00.042.374 I print_info: arch             = gptneox
0.00.042.374 I print_info: vocab_only       = 0
0.00.042.375 I print_info: n_ctx_train      = 2048
0.00.042.375 I print_info: n_embd           = 2048
0.00.042.375 I print_info: n_layer          = 24
0.00.042.379 I print_info: n_head           = 16
0.00.042.379 I print_info: n_head_kv        = 16
0.00.042.382 I print_info: n_rot            = 32
0.00.042.382 I print_info: n_swa            = 0
0.00.042.382 I print_info: n_embd_head_k    = 128
0.00.042.384 I print_info: n_embd_head_v    = 128
0.00.042.385 I print_info: n_gqa            = 1
0.00.042.386 I print_info: n_embd_k_gqa     = 2048
0.00.042.387 I print_info: n_embd_v_gqa     = 2048
0.00.042.388 I print_info: f_norm_eps       = 1.0e-05
0.00.042.388 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.389 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.389 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.389 I print_info: f_logit_scale    = 0.0e+00
0.00.042.389 I print_info: n_ff             = 8192
0.00.042.390 I print_info: n_expert         = 0
0.00.042.390 I print_info: n_expert_used    = 0
0.00.042.390 I print_info: causal attn      = 1
0.00.042.390 I print_info: pooling type     = 0
0.00.042.390 I print_info: rope type        = 2
0.00.042.390 I print_info: rope scaling     = linear
0.00.042.391 I print_info: freq_base_train  = 10000.0
0.00.042.392 I print_info: freq_scale_train = 1
0.00.042.396 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.396 I print_info: rope_finetuned   = unknown
0.00.042.398 I print_info: ssm_d_conv       = 0
0.00.042.398 I print_info: ssm_d_inner      = 0
0.00.042.399 I print_info: ssm_d_state      = 0
0.00.042.399 I print_info: ssm_dt_rank      = 0
0.00.042.399 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.399 I print_info: model type       = 1.4B
0.00.042.399 I print_info: model params     = 1.41 B
0.00.042.400 I print_info: general.name     = 1.4B
0.00.042.402 I print_info: vocab type       = BPE
0.00.042.402 I print_info: n_vocab          = 50304
0.00.042.402 I print_info: n_merges         = 50009
0.00.042.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.403 I print_info: LF token         = 187 'Ċ'
0.00.042.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.403 I print_info: max token length = 1024
0.00.741.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.741.465 I load_tensors: offloading output layer to GPU
0.00.741.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.741.498 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.741.499 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.743.124 I llama_init_from_model: n_seq_max     = 1
0.00.743.129 I llama_init_from_model: n_ctx         = 128
0.00.743.130 I llama_init_from_model: n_ctx_per_seq = 128
0.00.743.131 I llama_init_from_model: n_batch       = 128
0.00.743.131 I llama_init_from_model: n_ubatch      = 128
0.00.743.132 I llama_init_from_model: flash_attn    = 0
0.00.743.134 I llama_init_from_model: freq_base     = 10000.0
0.00.743.135 I llama_init_from_model: freq_scale    = 1
0.00.743.135 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.743.137 I ggml_metal_init: allocating
0.00.743.200 I ggml_metal_init: found device: Apple M4
0.00.743.215 I ggml_metal_init: picking default device: Apple M4
0.00.744.951 I ggml_metal_init: using embedded metal library
0.00.751.722 I ggml_metal_init: GPU name:   Apple M4
0.00.751.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.751.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.751.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.751.729 I ggml_metal_init: simdgroup reduction   = true
0.00.751.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.751.729 I ggml_metal_init: has residency sets    = true
0.00.751.729 I ggml_metal_init: has bfloat            = true
0.00.751.730 I ggml_metal_init: use bfloat            = true
0.00.751.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.751.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.769.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.773.393 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.773.397 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.773.439 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.776.755 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.776.757 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.776.757 I llama_init_from_model: graph nodes  = 967
0.00.776.758 I llama_init_from_model: graph splits = 2
0.00.776.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.776.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.128 I 
0.00.803.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.228 I perplexity: tokenizing the input ..
0.00.810.438 I perplexity: tokenization took 7.207 ms
0.00.810.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.946.135 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.947.593 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.947.605 I llama_perf_context_print:        load time =     794.26 ms
0.00.947.606 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.30 tokens per second)
0.00.947.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.947.608 I llama_perf_context_print:       total time =     144.48 ms /   129 tokens
0.00.947.961 I ggml_metal_free: deallocating

real	0m0.961s
user	0m0.078s
sys	0m0.124s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.984 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.994 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.994 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.996 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.996 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.997 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.001 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.798 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.520 I llama_model_loader: - type  f32:  194 tensors
0.00.028.520 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.521 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.521 I print_info: file format = GGUF V3 (latest)
0.00.028.522 I print_info: file type   = Q5_0
0.00.028.522 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.282 I load: special tokens cache size = 25
0.00.042.022 I load: token to piece cache size = 0.2984 MB
0.00.042.025 I print_info: arch             = gptneox
0.00.042.025 I print_info: vocab_only       = 0
0.00.042.025 I print_info: n_ctx_train      = 2048
0.00.042.025 I print_info: n_embd           = 2048
0.00.042.025 I print_info: n_layer          = 24
0.00.042.028 I print_info: n_head           = 16
0.00.042.029 I print_info: n_head_kv        = 16
0.00.042.029 I print_info: n_rot            = 32
0.00.042.029 I print_info: n_swa            = 0
0.00.042.030 I print_info: n_embd_head_k    = 128
0.00.042.030 I print_info: n_embd_head_v    = 128
0.00.042.030 I print_info: n_gqa            = 1
0.00.042.031 I print_info: n_embd_k_gqa     = 2048
0.00.042.032 I print_info: n_embd_v_gqa     = 2048
0.00.042.032 I print_info: f_norm_eps       = 1.0e-05
0.00.042.033 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.034 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.034 I print_info: f_logit_scale    = 0.0e+00
0.00.042.035 I print_info: n_ff             = 8192
0.00.042.035 I print_info: n_expert         = 0
0.00.042.036 I print_info: n_expert_used    = 0
0.00.042.036 I print_info: causal attn      = 1
0.00.042.036 I print_info: pooling type     = 0
0.00.042.036 I print_info: rope type        = 2
0.00.042.038 I print_info: rope scaling     = linear
0.00.042.038 I print_info: freq_base_train  = 10000.0
0.00.042.039 I print_info: freq_scale_train = 1
0.00.042.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.039 I print_info: rope_finetuned   = unknown
0.00.042.039 I print_info: ssm_d_conv       = 0
0.00.042.039 I print_info: ssm_d_inner      = 0
0.00.042.039 I print_info: ssm_d_state      = 0
0.00.042.040 I print_info: ssm_dt_rank      = 0
0.00.042.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.040 I print_info: model type       = 1.4B
0.00.042.040 I print_info: model params     = 1.41 B
0.00.042.040 I print_info: general.name     = 1.4B
0.00.042.041 I print_info: vocab type       = BPE
0.00.042.041 I print_info: n_vocab          = 50304
0.00.042.041 I print_info: n_merges         = 50009
0.00.042.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.042 I print_info: LF token         = 187 'Ċ'
0.00.042.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.046 I print_info: max token length = 1024
0.00.845.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.845.418 I load_tensors: offloading output layer to GPU
0.00.845.419 I load_tensors: offloaded 25/25 layers to GPU
0.00.845.454 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.845.455 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.846.833 I llama_init_from_model: n_seq_max     = 1
0.00.846.837 I llama_init_from_model: n_ctx         = 2048
0.00.846.837 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.846.838 I llama_init_from_model: n_batch       = 2048
0.00.846.838 I llama_init_from_model: n_ubatch      = 512
0.00.846.839 I llama_init_from_model: flash_attn    = 0
0.00.846.841 I llama_init_from_model: freq_base     = 10000.0
0.00.846.842 I llama_init_from_model: freq_scale    = 1
0.00.846.844 I ggml_metal_init: allocating
0.00.846.903 I ggml_metal_init: found device: Apple M4
0.00.846.917 I ggml_metal_init: picking default device: Apple M4
0.00.848.452 I ggml_metal_init: using embedded metal library
0.00.854.841 I ggml_metal_init: GPU name:   Apple M4
0.00.854.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.854.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.854.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.854.847 I ggml_metal_init: simdgroup reduction   = true
0.00.854.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.854.847 I ggml_metal_init: has residency sets    = true
0.00.854.847 I ggml_metal_init: has bfloat            = true
0.00.854.848 I ggml_metal_init: use bfloat            = true
0.00.854.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.854.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.872.041 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.928.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.928.606 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.928.648 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.933.889 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.933.891 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.933.891 I llama_init_from_model: graph nodes  = 967
0.00.933.891 I llama_init_from_model: graph splits = 2
0.00.933.897 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.934.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.934.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.989.183 I main: llama threadpool init, n_threads = 4
0.00.989.231 I 
0.00.989.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.989.258 I 
0.00.989.413 I sampler seed: 1234
0.00.989.417 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.989.462 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.989.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.989.465 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.778.660 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.778.661 I llama_perf_context_print:        load time =     979.73 ms
0.01.778.661 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.47 tokens per second)
0.01.778.662 I llama_perf_context_print:        eval time =     743.22 ms /    63 runs   (   11.80 ms per token,    84.77 tokens per second)
0.01.778.662 I llama_perf_context_print:       total time =     790.12 ms /    70 tokens
0.01.778.879 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.609 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.027.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.082 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.991 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.991 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.991 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.035.992 I llama_model_loader: - type  f32:  194 tensors
0.00.035.992 I llama_model_loader: - type q5_0:   97 tensors
0.00.035.992 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.993 I print_info: file format = GGUF V3 (latest)
0.00.035.993 I print_info: file type   = Q5_0
0.00.035.994 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.721 I load: special tokens cache size = 25
0.00.051.459 I load: token to piece cache size = 0.2984 MB
0.00.051.463 I print_info: arch             = gptneox
0.00.051.463 I print_info: vocab_only       = 0
0.00.051.463 I print_info: n_ctx_train      = 2048
0.00.051.463 I print_info: n_embd           = 2048
0.00.051.464 I print_info: n_layer          = 24
0.00.051.467 I print_info: n_head           = 16
0.00.051.468 I print_info: n_head_kv        = 16
0.00.051.468 I print_info: n_rot            = 32
0.00.051.468 I print_info: n_swa            = 0
0.00.051.468 I print_info: n_embd_head_k    = 128
0.00.051.469 I print_info: n_embd_head_v    = 128
0.00.051.471 I print_info: n_gqa            = 1
0.00.051.472 I print_info: n_embd_k_gqa     = 2048
0.00.051.473 I print_info: n_embd_v_gqa     = 2048
0.00.051.473 I print_info: f_norm_eps       = 1.0e-05
0.00.051.474 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.475 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.475 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.476 I print_info: f_logit_scale    = 0.0e+00
0.00.051.476 I print_info: n_ff             = 8192
0.00.051.477 I print_info: n_expert         = 0
0.00.051.477 I print_info: n_expert_used    = 0
0.00.051.477 I print_info: causal attn      = 1
0.00.051.478 I print_info: pooling type     = 0
0.00.051.478 I print_info: rope type        = 2
0.00.051.479 I print_info: rope scaling     = linear
0.00.051.479 I print_info: freq_base_train  = 10000.0
0.00.051.479 I print_info: freq_scale_train = 1
0.00.051.479 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.480 I print_info: rope_finetuned   = unknown
0.00.051.480 I print_info: ssm_d_conv       = 0
0.00.051.480 I print_info: ssm_d_inner      = 0
0.00.051.480 I print_info: ssm_d_state      = 0
0.00.051.480 I print_info: ssm_dt_rank      = 0
0.00.051.480 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.481 I print_info: model type       = 1.4B
0.00.051.481 I print_info: model params     = 1.41 B
0.00.051.481 I print_info: general.name     = 1.4B
0.00.051.482 I print_info: vocab type       = BPE
0.00.051.482 I print_info: n_vocab          = 50304
0.00.051.482 I print_info: n_merges         = 50009
0.00.051.482 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.483 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.483 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.483 I print_info: LF token         = 187 'Ċ'
0.00.051.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.484 I print_info: max token length = 1024
0.00.888.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.888.077 I load_tensors: offloading output layer to GPU
0.00.888.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.888.094 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.888.095 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.888.938 I llama_init_from_model: n_seq_max     = 1
0.00.888.944 I llama_init_from_model: n_ctx         = 128
0.00.888.945 I llama_init_from_model: n_ctx_per_seq = 128
0.00.888.945 I llama_init_from_model: n_batch       = 128
0.00.888.946 I llama_init_from_model: n_ubatch      = 128
0.00.888.946 I llama_init_from_model: flash_attn    = 0
0.00.888.947 I llama_init_from_model: freq_base     = 10000.0
0.00.888.948 I llama_init_from_model: freq_scale    = 1
0.00.888.948 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.888.950 I ggml_metal_init: allocating
0.00.888.981 I ggml_metal_init: found device: Apple M4
0.00.888.992 I ggml_metal_init: picking default device: Apple M4
0.00.890.076 I ggml_metal_init: using embedded metal library
0.00.894.363 I ggml_metal_init: GPU name:   Apple M4
0.00.894.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.894.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.894.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.894.371 I ggml_metal_init: simdgroup reduction   = true
0.00.894.372 I ggml_metal_init: simdgroup matrix mul. = true
0.00.894.372 I ggml_metal_init: has residency sets    = true
0.00.894.372 I ggml_metal_init: has bfloat            = true
0.00.894.372 I ggml_metal_init: use bfloat            = true
0.00.894.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.894.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.909.655 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.911.319 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.911.321 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.911.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.912.896 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.912.897 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.912.897 I llama_init_from_model: graph nodes  = 967
0.00.912.898 I llama_init_from_model: graph splits = 2
0.00.912.899 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.912.899 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.937.386 I 
0.00.937.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.937.432 I perplexity: tokenizing the input ..
0.00.941.342 I perplexity: tokenization took 3.908 ms
0.00.941.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.075.502 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.01.076.913 I Final estimate: PPL = 10.0972 +/- 3.20136

0.01.076.930 I llama_perf_context_print:        load time =     922.77 ms
0.01.076.931 I llama_perf_context_print: prompt eval time =     133.92 ms /   128 tokens (    1.05 ms per token,   955.78 tokens per second)
0.01.076.931 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.076.932 I llama_perf_context_print:       total time =     139.55 ms /   129 tokens
0.01.077.290 I ggml_metal_free: deallocating

real	0m1.099s
user	0m0.071s
sys	0m0.141s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.541 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.821 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.444 I llama_model_loader: - type  f32:  194 tensors
0.00.025.444 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.445 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.445 I print_info: file format = GGUF V3 (latest)
0.00.025.446 I print_info: file type   = Q5_1
0.00.025.447 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.483 I load: special tokens cache size = 25
0.00.039.448 I load: token to piece cache size = 0.2984 MB
0.00.039.451 I print_info: arch             = gptneox
0.00.039.451 I print_info: vocab_only       = 0
0.00.039.451 I print_info: n_ctx_train      = 2048
0.00.039.452 I print_info: n_embd           = 2048
0.00.039.452 I print_info: n_layer          = 24
0.00.039.455 I print_info: n_head           = 16
0.00.039.455 I print_info: n_head_kv        = 16
0.00.039.456 I print_info: n_rot            = 32
0.00.039.456 I print_info: n_swa            = 0
0.00.039.456 I print_info: n_embd_head_k    = 128
0.00.039.456 I print_info: n_embd_head_v    = 128
0.00.039.457 I print_info: n_gqa            = 1
0.00.039.458 I print_info: n_embd_k_gqa     = 2048
0.00.039.459 I print_info: n_embd_v_gqa     = 2048
0.00.039.459 I print_info: f_norm_eps       = 1.0e-05
0.00.039.462 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.462 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.462 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.462 I print_info: f_logit_scale    = 0.0e+00
0.00.039.463 I print_info: n_ff             = 8192
0.00.039.463 I print_info: n_expert         = 0
0.00.039.463 I print_info: n_expert_used    = 0
0.00.039.463 I print_info: causal attn      = 1
0.00.039.464 I print_info: pooling type     = 0
0.00.039.464 I print_info: rope type        = 2
0.00.039.464 I print_info: rope scaling     = linear
0.00.039.466 I print_info: freq_base_train  = 10000.0
0.00.039.466 I print_info: freq_scale_train = 1
0.00.039.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.467 I print_info: rope_finetuned   = unknown
0.00.039.467 I print_info: ssm_d_conv       = 0
0.00.039.467 I print_info: ssm_d_inner      = 0
0.00.039.467 I print_info: ssm_d_state      = 0
0.00.039.467 I print_info: ssm_dt_rank      = 0
0.00.039.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.467 I print_info: model type       = 1.4B
0.00.039.468 I print_info: model params     = 1.41 B
0.00.039.468 I print_info: general.name     = 1.4B
0.00.039.468 I print_info: vocab type       = BPE
0.00.039.469 I print_info: n_vocab          = 50304
0.00.039.473 I print_info: n_merges         = 50009
0.00.039.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.475 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.475 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.475 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.475 I print_info: LF token         = 187 'Ċ'
0.00.039.475 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: max token length = 1024
0.00.597.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.165 I load_tensors: offloading output layer to GPU
0.00.597.166 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.202 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.597.203 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.598.155 I llama_init_from_model: n_seq_max     = 1
0.00.598.159 I llama_init_from_model: n_ctx         = 2048
0.00.598.160 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.598.160 I llama_init_from_model: n_batch       = 2048
0.00.598.161 I llama_init_from_model: n_ubatch      = 512
0.00.598.161 I llama_init_from_model: flash_attn    = 0
0.00.598.163 I llama_init_from_model: freq_base     = 10000.0
0.00.598.163 I llama_init_from_model: freq_scale    = 1
0.00.598.173 I ggml_metal_init: allocating
0.00.598.256 I ggml_metal_init: found device: Apple M4
0.00.598.271 I ggml_metal_init: picking default device: Apple M4
0.00.600.057 I ggml_metal_init: using embedded metal library
0.00.606.373 I ggml_metal_init: GPU name:   Apple M4
0.00.606.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.380 I ggml_metal_init: simdgroup reduction   = true
0.00.606.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.380 I ggml_metal_init: has residency sets    = true
0.00.606.380 I ggml_metal_init: has bfloat            = true
0.00.606.381 I ggml_metal_init: use bfloat            = true
0.00.606.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.868 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.875 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.603 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.605 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.605 I llama_init_from_model: graph nodes  = 967
0.00.685.606 I llama_init_from_model: graph splits = 2
0.00.685.616 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.041 I main: llama threadpool init, n_threads = 4
0.00.741.087 I 
0.00.741.111 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.113 I 
0.00.741.295 I sampler seed: 1234
0.00.741.299 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.324 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.326 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.326 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.583.195 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49168.98 tokens per second)
0.01.583.196 I llama_perf_context_print:        load time =     730.84 ms
0.01.583.197 I llama_perf_context_print: prompt eval time =      42.13 ms /     7 tokens (    6.02 ms per token,   166.17 tokens per second)
0.01.583.198 I llama_perf_context_print:        eval time =     796.84 ms /    63 runs   (   12.65 ms per token,    79.06 tokens per second)
0.01.583.198 I llama_perf_context_print:       total time =     842.81 ms /    70 tokens
0.01.583.464 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.108s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.622 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.622 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.624 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.626 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.626 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.484 I print_info: file format = GGUF V3 (latest)
0.00.024.484 I print_info: file type   = Q5_1
0.00.024.490 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.730 I load: special tokens cache size = 25
0.00.038.985 I load: token to piece cache size = 0.2984 MB
0.00.038.996 I print_info: arch             = gptneox
0.00.038.996 I print_info: vocab_only       = 0
0.00.038.996 I print_info: n_ctx_train      = 2048
0.00.038.996 I print_info: n_embd           = 2048
0.00.038.996 I print_info: n_layer          = 24
0.00.039.000 I print_info: n_head           = 16
0.00.039.001 I print_info: n_head_kv        = 16
0.00.039.001 I print_info: n_rot            = 32
0.00.039.001 I print_info: n_swa            = 0
0.00.039.001 I print_info: n_embd_head_k    = 128
0.00.039.001 I print_info: n_embd_head_v    = 128
0.00.039.002 I print_info: n_gqa            = 1
0.00.039.002 I print_info: n_embd_k_gqa     = 2048
0.00.039.003 I print_info: n_embd_v_gqa     = 2048
0.00.039.003 I print_info: f_norm_eps       = 1.0e-05
0.00.039.004 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.004 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.004 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.004 I print_info: f_logit_scale    = 0.0e+00
0.00.039.006 I print_info: n_ff             = 8192
0.00.039.007 I print_info: n_expert         = 0
0.00.039.007 I print_info: n_expert_used    = 0
0.00.039.008 I print_info: causal attn      = 1
0.00.039.008 I print_info: pooling type     = 0
0.00.039.008 I print_info: rope type        = 2
0.00.039.008 I print_info: rope scaling     = linear
0.00.039.008 I print_info: freq_base_train  = 10000.0
0.00.039.009 I print_info: freq_scale_train = 1
0.00.039.009 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.009 I print_info: rope_finetuned   = unknown
0.00.039.009 I print_info: ssm_d_conv       = 0
0.00.039.009 I print_info: ssm_d_inner      = 0
0.00.039.010 I print_info: ssm_d_state      = 0
0.00.039.010 I print_info: ssm_dt_rank      = 0
0.00.039.010 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.010 I print_info: model type       = 1.4B
0.00.039.011 I print_info: model params     = 1.41 B
0.00.039.011 I print_info: general.name     = 1.4B
0.00.039.012 I print_info: vocab type       = BPE
0.00.039.012 I print_info: n_vocab          = 50304
0.00.039.012 I print_info: n_merges         = 50009
0.00.039.013 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.013 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.013 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.014 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.014 I print_info: LF token         = 187 'Ċ'
0.00.039.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.015 I print_info: max token length = 1024
0.00.811.711 I load_tensors: offloading 24 repeating layers to GPU
0.00.811.728 I load_tensors: offloading output layer to GPU
0.00.811.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.811.764 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.811.766 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.813.053 I llama_init_from_model: n_seq_max     = 1
0.00.813.058 I llama_init_from_model: n_ctx         = 128
0.00.813.058 I llama_init_from_model: n_ctx_per_seq = 128
0.00.813.063 I llama_init_from_model: n_batch       = 128
0.00.813.064 I llama_init_from_model: n_ubatch      = 128
0.00.813.064 I llama_init_from_model: flash_attn    = 0
0.00.813.065 I llama_init_from_model: freq_base     = 10000.0
0.00.813.068 I llama_init_from_model: freq_scale    = 1
0.00.813.069 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.813.071 I ggml_metal_init: allocating
0.00.813.118 I ggml_metal_init: found device: Apple M4
0.00.813.129 I ggml_metal_init: picking default device: Apple M4
0.00.814.678 I ggml_metal_init: using embedded metal library
0.00.820.925 I ggml_metal_init: GPU name:   Apple M4
0.00.820.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.820.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.820.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.820.932 I ggml_metal_init: simdgroup reduction   = true
0.00.820.932 I ggml_metal_init: simdgroup matrix mul. = true
0.00.820.932 I ggml_metal_init: has residency sets    = true
0.00.820.932 I ggml_metal_init: has bfloat            = true
0.00.820.933 I ggml_metal_init: use bfloat            = true
0.00.820.933 I ggml_metal_init: hasUnifiedMemory      = true
0.00.820.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.837.540 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.840.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.840.934 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.840.976 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.844.274 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.844.276 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.844.276 I llama_init_from_model: graph nodes  = 967
0.00.844.277 I llama_init_from_model: graph splits = 2
0.00.844.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.844.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.873.848 I 
0.00.873.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.873.906 I perplexity: tokenizing the input ..
0.00.880.019 I perplexity: tokenization took 6.112 ms
0.00.880.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.027.032 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.01.028.579 I Final estimate: PPL = 10.1971 +/- 3.18866

0.01.028.593 I llama_perf_context_print:        load time =     864.94 ms
0.01.028.595 I llama_perf_context_print: prompt eval time =     146.62 ms /   128 tokens (    1.15 ms per token,   873.03 tokens per second)
0.01.028.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.028.596 I llama_perf_context_print:       total time =     154.75 ms /   129 tokens
0.01.028.963 I ggml_metal_free: deallocating

real	0m1.044s
user	0m0.077s
sys	0m0.128s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.012.663 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.311 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.313 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.313 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.316 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.878 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.879 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.880 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.880 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.880 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.881 I llama_model_loader: - type  f32:  194 tensors
0.00.027.882 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.882 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.882 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.883 I print_info: file format = GGUF V3 (latest)
0.00.027.884 I print_info: file type   = Q2_K - Medium
0.00.027.892 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.036.170 I load: special tokens cache size = 25
0.00.042.316 I load: token to piece cache size = 0.2984 MB
0.00.042.323 I print_info: arch             = gptneox
0.00.042.323 I print_info: vocab_only       = 0
0.00.042.324 I print_info: n_ctx_train      = 2048
0.00.042.324 I print_info: n_embd           = 2048
0.00.042.324 I print_info: n_layer          = 24
0.00.042.328 I print_info: n_head           = 16
0.00.042.329 I print_info: n_head_kv        = 16
0.00.042.329 I print_info: n_rot            = 32
0.00.042.329 I print_info: n_swa            = 0
0.00.042.330 I print_info: n_embd_head_k    = 128
0.00.042.330 I print_info: n_embd_head_v    = 128
0.00.042.330 I print_info: n_gqa            = 1
0.00.042.331 I print_info: n_embd_k_gqa     = 2048
0.00.042.331 I print_info: n_embd_v_gqa     = 2048
0.00.042.332 I print_info: f_norm_eps       = 1.0e-05
0.00.042.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.336 I print_info: f_logit_scale    = 0.0e+00
0.00.042.336 I print_info: n_ff             = 8192
0.00.042.336 I print_info: n_expert         = 0
0.00.042.336 I print_info: n_expert_used    = 0
0.00.042.338 I print_info: causal attn      = 1
0.00.042.338 I print_info: pooling type     = 0
0.00.042.338 I print_info: rope type        = 2
0.00.042.338 I print_info: rope scaling     = linear
0.00.042.338 I print_info: freq_base_train  = 10000.0
0.00.042.339 I print_info: freq_scale_train = 1
0.00.042.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.339 I print_info: rope_finetuned   = unknown
0.00.042.339 I print_info: ssm_d_conv       = 0
0.00.042.339 I print_info: ssm_d_inner      = 0
0.00.042.339 I print_info: ssm_d_state      = 0
0.00.042.339 I print_info: ssm_dt_rank      = 0
0.00.042.340 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.341 I print_info: model type       = 1.4B
0.00.042.341 I print_info: model params     = 1.41 B
0.00.042.341 I print_info: general.name     = 1.4B
0.00.042.342 I print_info: vocab type       = BPE
0.00.042.342 I print_info: n_vocab          = 50304
0.00.042.342 I print_info: n_merges         = 50009
0.00.042.342 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.342 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.344 I print_info: LF token         = 187 'Ċ'
0.00.042.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.344 I print_info: max token length = 1024
0.00.328.718 I load_tensors: offloading 24 repeating layers to GPU
0.00.328.726 I load_tensors: offloading output layer to GPU
0.00.328.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.328.744 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.328.745 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.329.518 I llama_init_from_model: n_seq_max     = 1
0.00.329.523 I llama_init_from_model: n_ctx         = 2048
0.00.329.524 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.329.524 I llama_init_from_model: n_batch       = 2048
0.00.329.524 I llama_init_from_model: n_ubatch      = 512
0.00.329.525 I llama_init_from_model: flash_attn    = 0
0.00.329.526 I llama_init_from_model: freq_base     = 10000.0
0.00.329.526 I llama_init_from_model: freq_scale    = 1
0.00.329.527 I ggml_metal_init: allocating
0.00.329.572 I ggml_metal_init: found device: Apple M4
0.00.329.584 I ggml_metal_init: picking default device: Apple M4
0.00.330.632 I ggml_metal_init: using embedded metal library
0.00.334.746 I ggml_metal_init: GPU name:   Apple M4
0.00.334.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.334.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.334.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.334.752 I ggml_metal_init: simdgroup reduction   = true
0.00.334.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.334.753 I ggml_metal_init: has residency sets    = true
0.00.334.753 I ggml_metal_init: has bfloat            = true
0.00.334.753 I ggml_metal_init: use bfloat            = true
0.00.334.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.334.757 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.351.775 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.381.292 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.381.298 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.381.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.385.933 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.385.935 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.385.935 I llama_init_from_model: graph nodes  = 967
0.00.385.935 I llama_init_from_model: graph splits = 2
0.00.385.940 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.386.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.386.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.491 I main: llama threadpool init, n_threads = 4
0.00.445.538 I 
0.00.445.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.559 I 
0.00.445.707 I sampler seed: 1234
0.00.445.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.445.752 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.445.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.445.769 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.115.188 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.115.190 I llama_perf_context_print:        load time =     432.20 ms
0.01.115.190 I llama_perf_context_print: prompt eval time =      44.14 ms /     7 tokens (    6.31 ms per token,   158.57 tokens per second)
0.01.115.191 I llama_perf_context_print:        eval time =     622.98 ms /    63 runs   (    9.89 ms per token,   101.13 tokens per second)
0.01.115.191 I llama_perf_context_print:       total time =     670.32 ms /    70 tokens
0.01.115.404 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.106s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.366 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.026.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.697 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.698 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.700 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.700 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.727 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.017 I llama_model_loader: - type  f32:  194 tensors
0.00.036.017 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.017 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.018 I print_info: file format = GGUF V3 (latest)
0.00.036.019 I print_info: file type   = Q2_K - Medium
0.00.036.020 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.227 I load: special tokens cache size = 25
0.00.054.265 I load: token to piece cache size = 0.2984 MB
0.00.054.268 I print_info: arch             = gptneox
0.00.054.269 I print_info: vocab_only       = 0
0.00.054.269 I print_info: n_ctx_train      = 2048
0.00.054.269 I print_info: n_embd           = 2048
0.00.054.269 I print_info: n_layer          = 24
0.00.054.273 I print_info: n_head           = 16
0.00.054.274 I print_info: n_head_kv        = 16
0.00.054.274 I print_info: n_rot            = 32
0.00.054.275 I print_info: n_swa            = 0
0.00.054.275 I print_info: n_embd_head_k    = 128
0.00.054.275 I print_info: n_embd_head_v    = 128
0.00.054.277 I print_info: n_gqa            = 1
0.00.054.278 I print_info: n_embd_k_gqa     = 2048
0.00.054.279 I print_info: n_embd_v_gqa     = 2048
0.00.054.280 I print_info: f_norm_eps       = 1.0e-05
0.00.054.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.282 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.282 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.282 I print_info: f_logit_scale    = 0.0e+00
0.00.054.283 I print_info: n_ff             = 8192
0.00.054.283 I print_info: n_expert         = 0
0.00.054.283 I print_info: n_expert_used    = 0
0.00.054.283 I print_info: causal attn      = 1
0.00.054.284 I print_info: pooling type     = 0
0.00.054.284 I print_info: rope type        = 2
0.00.054.284 I print_info: rope scaling     = linear
0.00.054.285 I print_info: freq_base_train  = 10000.0
0.00.054.285 I print_info: freq_scale_train = 1
0.00.054.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.286 I print_info: rope_finetuned   = unknown
0.00.054.286 I print_info: ssm_d_conv       = 0
0.00.054.286 I print_info: ssm_d_inner      = 0
0.00.054.286 I print_info: ssm_d_state      = 0
0.00.054.286 I print_info: ssm_dt_rank      = 0
0.00.054.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.287 I print_info: model type       = 1.4B
0.00.054.287 I print_info: model params     = 1.41 B
0.00.054.287 I print_info: general.name     = 1.4B
0.00.054.290 I print_info: vocab type       = BPE
0.00.054.290 I print_info: n_vocab          = 50304
0.00.054.290 I print_info: n_merges         = 50009
0.00.054.291 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.291 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.291 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.293 I print_info: LF token         = 187 'Ċ'
0.00.054.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.294 I print_info: max token length = 1024
0.00.395.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.395.310 I load_tensors: offloading output layer to GPU
0.00.395.311 I load_tensors: offloaded 25/25 layers to GPU
0.00.395.346 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.395.348 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.396.561 I llama_init_from_model: n_seq_max     = 1
0.00.396.565 I llama_init_from_model: n_ctx         = 128
0.00.396.566 I llama_init_from_model: n_ctx_per_seq = 128
0.00.396.566 I llama_init_from_model: n_batch       = 128
0.00.396.567 I llama_init_from_model: n_ubatch      = 128
0.00.396.567 I llama_init_from_model: flash_attn    = 0
0.00.396.569 I llama_init_from_model: freq_base     = 10000.0
0.00.396.570 I llama_init_from_model: freq_scale    = 1
0.00.396.570 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.396.573 I ggml_metal_init: allocating
0.00.396.670 I ggml_metal_init: found device: Apple M4
0.00.396.685 I ggml_metal_init: picking default device: Apple M4
0.00.398.502 I ggml_metal_init: using embedded metal library
0.00.403.977 I ggml_metal_init: GPU name:   Apple M4
0.00.403.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.403.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.403.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.403.992 I ggml_metal_init: simdgroup reduction   = true
0.00.403.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.403.992 I ggml_metal_init: has residency sets    = true
0.00.403.992 I ggml_metal_init: has bfloat            = true
0.00.403.993 I ggml_metal_init: use bfloat            = true
0.00.403.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.404.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.425.672 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.270 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.429.277 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.429.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.432.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.432.690 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.432.690 I llama_init_from_model: graph nodes  = 967
0.00.432.691 I llama_init_from_model: graph splits = 2
0.00.432.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.432.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.232 I 
0.00.465.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.336 I perplexity: tokenizing the input ..
0.00.472.854 I perplexity: tokenization took 7.514 ms
0.00.472.873 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.492 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.620.834 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.620.846 I llama_perf_context_print:        load time =     448.86 ms
0.00.620.847 I llama_perf_context_print: prompt eval time =     145.69 ms /   128 tokens (    1.14 ms per token,   878.57 tokens per second)
0.00.620.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.620.848 I llama_perf_context_print:       total time =     155.62 ms /   129 tokens
0.00.621.218 I ggml_metal_free: deallocating

real	0m0.652s
user	0m0.087s
sys	0m0.104s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.943 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.557 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.558 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.558 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.559 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.559 I llama_model_loader: - type  f32:  194 tensors
0.00.025.560 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.560 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.560 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.560 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.561 I print_info: file format = GGUF V3 (latest)
0.00.025.561 I print_info: file type   = Q3_K - Medium
0.00.025.563 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.456 I load: special tokens cache size = 25
0.00.039.521 I load: token to piece cache size = 0.2984 MB
0.00.039.523 I print_info: arch             = gptneox
0.00.039.523 I print_info: vocab_only       = 0
0.00.039.524 I print_info: n_ctx_train      = 2048
0.00.039.524 I print_info: n_embd           = 2048
0.00.039.524 I print_info: n_layer          = 24
0.00.039.526 I print_info: n_head           = 16
0.00.039.527 I print_info: n_head_kv        = 16
0.00.039.527 I print_info: n_rot            = 32
0.00.039.527 I print_info: n_swa            = 0
0.00.039.528 I print_info: n_embd_head_k    = 128
0.00.039.530 I print_info: n_embd_head_v    = 128
0.00.039.530 I print_info: n_gqa            = 1
0.00.039.531 I print_info: n_embd_k_gqa     = 2048
0.00.039.532 I print_info: n_embd_v_gqa     = 2048
0.00.039.536 I print_info: f_norm_eps       = 1.0e-05
0.00.039.537 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.539 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.539 I print_info: f_logit_scale    = 0.0e+00
0.00.039.540 I print_info: n_ff             = 8192
0.00.039.540 I print_info: n_expert         = 0
0.00.039.540 I print_info: n_expert_used    = 0
0.00.039.540 I print_info: causal attn      = 1
0.00.039.540 I print_info: pooling type     = 0
0.00.039.540 I print_info: rope type        = 2
0.00.039.541 I print_info: rope scaling     = linear
0.00.039.541 I print_info: freq_base_train  = 10000.0
0.00.039.541 I print_info: freq_scale_train = 1
0.00.039.541 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.542 I print_info: rope_finetuned   = unknown
0.00.039.542 I print_info: ssm_d_conv       = 0
0.00.039.542 I print_info: ssm_d_inner      = 0
0.00.039.542 I print_info: ssm_d_state      = 0
0.00.039.542 I print_info: ssm_dt_rank      = 0
0.00.039.542 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.543 I print_info: model type       = 1.4B
0.00.039.543 I print_info: model params     = 1.41 B
0.00.039.543 I print_info: general.name     = 1.4B
0.00.039.543 I print_info: vocab type       = BPE
0.00.039.544 I print_info: n_vocab          = 50304
0.00.039.544 I print_info: n_merges         = 50009
0.00.039.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.544 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: LF token         = 187 'Ċ'
0.00.039.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: max token length = 1024
0.00.432.768 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.785 I load_tensors: offloading output layer to GPU
0.00.432.785 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.819 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.827 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.434.071 I llama_init_from_model: n_seq_max     = 1
0.00.434.078 I llama_init_from_model: n_ctx         = 2048
0.00.434.079 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.079 I llama_init_from_model: n_batch       = 2048
0.00.434.080 I llama_init_from_model: n_ubatch      = 512
0.00.434.080 I llama_init_from_model: flash_attn    = 0
0.00.434.082 I llama_init_from_model: freq_base     = 10000.0
0.00.434.082 I llama_init_from_model: freq_scale    = 1
0.00.434.084 I ggml_metal_init: allocating
0.00.434.160 I ggml_metal_init: found device: Apple M4
0.00.434.174 I ggml_metal_init: picking default device: Apple M4
0.00.436.013 I ggml_metal_init: using embedded metal library
0.00.440.671 I ggml_metal_init: GPU name:   Apple M4
0.00.440.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.440.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.440.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.440.684 I ggml_metal_init: simdgroup reduction   = true
0.00.440.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.440.684 I ggml_metal_init: has residency sets    = true
0.00.440.685 I ggml_metal_init: has bfloat            = true
0.00.440.685 I ggml_metal_init: use bfloat            = true
0.00.440.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.440.689 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.453.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.486.240 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.486.247 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.486.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.490.728 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.490.731 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.490.731 I llama_init_from_model: graph nodes  = 967
0.00.490.731 I llama_init_from_model: graph splits = 2
0.00.490.738 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.490.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.490.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.176 I main: llama threadpool init, n_threads = 4
0.00.547.224 I 
0.00.547.249 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.249 I 
0.00.547.426 I sampler seed: 1234
0.00.547.430 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.441 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.443 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.443 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.298.592 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.298.592 I llama_perf_context_print:        load time =     537.14 ms
0.01.298.593 I llama_perf_context_print: prompt eval time =      49.72 ms /     7 tokens (    7.10 ms per token,   140.79 tokens per second)
0.01.298.594 I llama_perf_context_print:        eval time =     698.50 ms /    63 runs   (   11.09 ms per token,    90.19 tokens per second)
0.01.298.594 I llama_perf_context_print:       total time =     752.06 ms /    70 tokens
0.01.298.806 I ggml_metal_free: deallocating

real	0m1.316s
user	0m0.103s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.338 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.339 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.342 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.352 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.028.818 I llama_model_loader: - type  f32:  194 tensors
0.00.028.818 I llama_model_loader: - type q3_K:   25 tensors
0.00.028.818 I llama_model_loader: - type q4_K:   71 tensors
0.00.028.819 I llama_model_loader: - type q5_K:    1 tensors
0.00.028.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.820 I print_info: file format = GGUF V3 (latest)
0.00.028.820 I print_info: file type   = Q3_K - Medium
0.00.028.821 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.036.632 I load: special tokens cache size = 25
0.00.042.559 I load: token to piece cache size = 0.2984 MB
0.00.042.562 I print_info: arch             = gptneox
0.00.042.562 I print_info: vocab_only       = 0
0.00.042.563 I print_info: n_ctx_train      = 2048
0.00.042.563 I print_info: n_embd           = 2048
0.00.042.563 I print_info: n_layer          = 24
0.00.042.566 I print_info: n_head           = 16
0.00.042.567 I print_info: n_head_kv        = 16
0.00.042.567 I print_info: n_rot            = 32
0.00.042.567 I print_info: n_swa            = 0
0.00.042.567 I print_info: n_embd_head_k    = 128
0.00.042.567 I print_info: n_embd_head_v    = 128
0.00.042.568 I print_info: n_gqa            = 1
0.00.042.569 I print_info: n_embd_k_gqa     = 2048
0.00.042.570 I print_info: n_embd_v_gqa     = 2048
0.00.042.570 I print_info: f_norm_eps       = 1.0e-05
0.00.042.570 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.571 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.571 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.571 I print_info: f_logit_scale    = 0.0e+00
0.00.042.572 I print_info: n_ff             = 8192
0.00.042.572 I print_info: n_expert         = 0
0.00.042.572 I print_info: n_expert_used    = 0
0.00.042.572 I print_info: causal attn      = 1
0.00.042.572 I print_info: pooling type     = 0
0.00.042.574 I print_info: rope type        = 2
0.00.042.576 I print_info: rope scaling     = linear
0.00.042.576 I print_info: freq_base_train  = 10000.0
0.00.042.576 I print_info: freq_scale_train = 1
0.00.042.577 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.577 I print_info: rope_finetuned   = unknown
0.00.042.577 I print_info: ssm_d_conv       = 0
0.00.042.577 I print_info: ssm_d_inner      = 0
0.00.042.577 I print_info: ssm_d_state      = 0
0.00.042.577 I print_info: ssm_dt_rank      = 0
0.00.042.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.578 I print_info: model type       = 1.4B
0.00.042.578 I print_info: model params     = 1.41 B
0.00.042.578 I print_info: general.name     = 1.4B
0.00.042.579 I print_info: vocab type       = BPE
0.00.042.579 I print_info: n_vocab          = 50304
0.00.042.579 I print_info: n_merges         = 50009
0.00.042.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.580 I print_info: LF token         = 187 'Ċ'
0.00.042.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.580 I print_info: max token length = 1024
0.00.512.115 I load_tensors: offloading 24 repeating layers to GPU
0.00.512.130 I load_tensors: offloading output layer to GPU
0.00.512.130 I load_tensors: offloaded 25/25 layers to GPU
0.00.512.164 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.512.165 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.513.698 I llama_init_from_model: n_seq_max     = 1
0.00.513.703 I llama_init_from_model: n_ctx         = 128
0.00.513.703 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.704 I llama_init_from_model: n_batch       = 128
0.00.513.704 I llama_init_from_model: n_ubatch      = 128
0.00.513.704 I llama_init_from_model: flash_attn    = 0
0.00.513.706 I llama_init_from_model: freq_base     = 10000.0
0.00.513.707 I llama_init_from_model: freq_scale    = 1
0.00.513.712 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.720 I ggml_metal_init: allocating
0.00.513.792 I ggml_metal_init: found device: Apple M4
0.00.513.805 I ggml_metal_init: picking default device: Apple M4
0.00.515.742 I ggml_metal_init: using embedded metal library
0.00.521.234 I ggml_metal_init: GPU name:   Apple M4
0.00.521.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.240 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.241 I ggml_metal_init: simdgroup reduction   = true
0.00.521.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.242 I ggml_metal_init: has residency sets    = true
0.00.521.242 I ggml_metal_init: has bfloat            = true
0.00.521.242 I ggml_metal_init: use bfloat            = true
0.00.521.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.251 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.776 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.779 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.975 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.977 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.977 I llama_init_from_model: graph nodes  = 967
0.00.546.978 I llama_init_from_model: graph splits = 2
0.00.546.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.158 I 
0.00.576.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.266 I perplexity: tokenizing the input ..
0.00.583.427 I perplexity: tokenization took 7.157 ms
0.00.583.454 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.730.087 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.731.425 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.731.437 I llama_perf_context_print:        load time =     567.39 ms
0.00.731.438 I llama_perf_context_print: prompt eval time =     145.78 ms /   128 tokens (    1.14 ms per token,   878.04 tokens per second)
0.00.731.439 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.439 I llama_perf_context_print:       total time =     155.28 ms /   129 tokens
0.00.731.818 I ggml_metal_free: deallocating

real	0m0.746s
user	0m0.078s
sys	0m0.105s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.119 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.974 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.975 I llama_model_loader: - type  f32:  194 tensors
0.00.025.976 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.976 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.976 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.977 I print_info: file format = GGUF V3 (latest)
0.00.025.977 I print_info: file type   = Q4_K - Medium
0.00.025.978 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.772 I load: special tokens cache size = 25
0.00.039.704 I load: token to piece cache size = 0.2984 MB
0.00.039.708 I print_info: arch             = gptneox
0.00.039.708 I print_info: vocab_only       = 0
0.00.039.708 I print_info: n_ctx_train      = 2048
0.00.039.708 I print_info: n_embd           = 2048
0.00.039.709 I print_info: n_layer          = 24
0.00.039.711 I print_info: n_head           = 16
0.00.039.712 I print_info: n_head_kv        = 16
0.00.039.712 I print_info: n_rot            = 32
0.00.039.712 I print_info: n_swa            = 0
0.00.039.714 I print_info: n_embd_head_k    = 128
0.00.039.714 I print_info: n_embd_head_v    = 128
0.00.039.714 I print_info: n_gqa            = 1
0.00.039.715 I print_info: n_embd_k_gqa     = 2048
0.00.039.716 I print_info: n_embd_v_gqa     = 2048
0.00.039.717 I print_info: f_norm_eps       = 1.0e-05
0.00.039.717 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.717 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.717 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.717 I print_info: f_logit_scale    = 0.0e+00
0.00.039.718 I print_info: n_ff             = 8192
0.00.039.718 I print_info: n_expert         = 0
0.00.039.718 I print_info: n_expert_used    = 0
0.00.039.719 I print_info: causal attn      = 1
0.00.039.722 I print_info: pooling type     = 0
0.00.039.722 I print_info: rope type        = 2
0.00.039.722 I print_info: rope scaling     = linear
0.00.039.723 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.723 I print_info: rope_finetuned   = unknown
0.00.039.723 I print_info: ssm_d_conv       = 0
0.00.039.724 I print_info: ssm_d_inner      = 0
0.00.039.724 I print_info: ssm_d_state      = 0
0.00.039.724 I print_info: ssm_dt_rank      = 0
0.00.039.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.724 I print_info: model type       = 1.4B
0.00.039.725 I print_info: model params     = 1.41 B
0.00.039.725 I print_info: general.name     = 1.4B
0.00.039.725 I print_info: vocab type       = BPE
0.00.039.725 I print_info: n_vocab          = 50304
0.00.039.726 I print_info: n_merges         = 50009
0.00.039.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.727 I print_info: LF token         = 187 'Ċ'
0.00.039.728 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: max token length = 1024
0.00.521.052 I load_tensors: offloading 24 repeating layers to GPU
0.00.521.068 I load_tensors: offloading output layer to GPU
0.00.521.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.521.119 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.521.123 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.522.727 I llama_init_from_model: n_seq_max     = 1
0.00.522.732 I llama_init_from_model: n_ctx         = 2048
0.00.522.733 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.522.733 I llama_init_from_model: n_batch       = 2048
0.00.522.734 I llama_init_from_model: n_ubatch      = 512
0.00.522.734 I llama_init_from_model: flash_attn    = 0
0.00.522.736 I llama_init_from_model: freq_base     = 10000.0
0.00.522.741 I llama_init_from_model: freq_scale    = 1
0.00.522.747 I ggml_metal_init: allocating
0.00.522.820 I ggml_metal_init: found device: Apple M4
0.00.522.835 I ggml_metal_init: picking default device: Apple M4
0.00.524.762 I ggml_metal_init: using embedded metal library
0.00.531.349 I ggml_metal_init: GPU name:   Apple M4
0.00.531.352 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.353 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.355 I ggml_metal_init: simdgroup reduction   = true
0.00.531.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.355 I ggml_metal_init: has residency sets    = true
0.00.531.356 I ggml_metal_init: has bfloat            = true
0.00.531.356 I ggml_metal_init: use bfloat            = true
0.00.531.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.606.101 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.606.107 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.606.141 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.951 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.610.953 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.610.953 I llama_init_from_model: graph nodes  = 967
0.00.610.953 I llama_init_from_model: graph splits = 2
0.00.610.959 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.611.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.611.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.758 I main: llama threadpool init, n_threads = 4
0.00.666.804 I 
0.00.666.830 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.831 I 
0.00.667.009 I sampler seed: 1234
0.00.667.014 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.034 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.034 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.034 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.422.920 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.422.921 I llama_perf_context_print:        load time =     656.00 ms
0.01.422.921 I llama_perf_context_print: prompt eval time =      46.75 ms /     7 tokens (    6.68 ms per token,   149.72 tokens per second)
0.01.422.922 I llama_perf_context_print:        eval time =     706.28 ms /    63 runs   (   11.21 ms per token,    89.20 tokens per second)
0.01.422.926 I llama_perf_context_print:       total time =     756.80 ms /    70 tokens
0.01.423.191 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.265 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.428 I llama_model_loader: - type  f32:  194 tensors
0.00.025.429 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.429 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.429 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.429 I print_info: file format = GGUF V3 (latest)
0.00.025.430 I print_info: file type   = Q4_K - Medium
0.00.025.430 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.156 I load: special tokens cache size = 25
0.00.038.901 I load: token to piece cache size = 0.2984 MB
0.00.038.903 I print_info: arch             = gptneox
0.00.038.904 I print_info: vocab_only       = 0
0.00.038.904 I print_info: n_ctx_train      = 2048
0.00.038.904 I print_info: n_embd           = 2048
0.00.038.904 I print_info: n_layer          = 24
0.00.038.907 I print_info: n_head           = 16
0.00.038.908 I print_info: n_head_kv        = 16
0.00.038.910 I print_info: n_rot            = 32
0.00.038.910 I print_info: n_swa            = 0
0.00.038.910 I print_info: n_embd_head_k    = 128
0.00.038.910 I print_info: n_embd_head_v    = 128
0.00.038.911 I print_info: n_gqa            = 1
0.00.038.912 I print_info: n_embd_k_gqa     = 2048
0.00.038.917 I print_info: n_embd_v_gqa     = 2048
0.00.038.917 I print_info: f_norm_eps       = 1.0e-05
0.00.038.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.919 I print_info: f_logit_scale    = 0.0e+00
0.00.038.920 I print_info: n_ff             = 8192
0.00.038.920 I print_info: n_expert         = 0
0.00.038.920 I print_info: n_expert_used    = 0
0.00.038.920 I print_info: causal attn      = 1
0.00.038.920 I print_info: pooling type     = 0
0.00.038.920 I print_info: rope type        = 2
0.00.038.921 I print_info: rope scaling     = linear
0.00.038.921 I print_info: freq_base_train  = 10000.0
0.00.038.925 I print_info: freq_scale_train = 1
0.00.038.925 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.925 I print_info: rope_finetuned   = unknown
0.00.038.925 I print_info: ssm_d_conv       = 0
0.00.038.925 I print_info: ssm_d_inner      = 0
0.00.038.927 I print_info: ssm_d_state      = 0
0.00.038.927 I print_info: ssm_dt_rank      = 0
0.00.038.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.927 I print_info: model type       = 1.4B
0.00.038.927 I print_info: model params     = 1.41 B
0.00.038.927 I print_info: general.name     = 1.4B
0.00.038.928 I print_info: vocab type       = BPE
0.00.038.928 I print_info: n_vocab          = 50304
0.00.038.928 I print_info: n_merges         = 50009
0.00.038.929 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.929 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.929 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.929 I print_info: LF token         = 187 'Ċ'
0.00.038.930 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.930 I print_info: max token length = 1024
0.00.517.720 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.732 I load_tensors: offloading output layer to GPU
0.00.517.733 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.766 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.768 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.519.290 I llama_init_from_model: n_seq_max     = 1
0.00.519.295 I llama_init_from_model: n_ctx         = 128
0.00.519.295 I llama_init_from_model: n_ctx_per_seq = 128
0.00.519.296 I llama_init_from_model: n_batch       = 128
0.00.519.296 I llama_init_from_model: n_ubatch      = 128
0.00.519.297 I llama_init_from_model: flash_attn    = 0
0.00.519.299 I llama_init_from_model: freq_base     = 10000.0
0.00.519.300 I llama_init_from_model: freq_scale    = 1
0.00.519.300 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.519.303 I ggml_metal_init: allocating
0.00.519.349 I ggml_metal_init: found device: Apple M4
0.00.519.363 I ggml_metal_init: picking default device: Apple M4
0.00.521.045 I ggml_metal_init: using embedded metal library
0.00.526.553 I ggml_metal_init: GPU name:   Apple M4
0.00.526.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.573 I ggml_metal_init: simdgroup reduction   = true
0.00.526.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.574 I ggml_metal_init: has residency sets    = true
0.00.526.574 I ggml_metal_init: has bfloat            = true
0.00.526.574 I ggml_metal_init: use bfloat            = true
0.00.526.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.583 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.823 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.506 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.550.516 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.591 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.553.934 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.553.937 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.553.937 I llama_init_from_model: graph nodes  = 967
0.00.553.938 I llama_init_from_model: graph splits = 2
0.00.553.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.553.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.137 I 
0.00.583.206 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.229 I perplexity: tokenizing the input ..
0.00.590.311 I perplexity: tokenization took 7.08 ms
0.00.590.329 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.843 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.727.399 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.727.413 I llama_perf_context_print:        load time =     572.87 ms
0.00.727.414 I llama_perf_context_print: prompt eval time =     134.55 ms /   128 tokens (    1.05 ms per token,   951.34 tokens per second)
0.00.727.414 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.415 I llama_perf_context_print:       total time =     144.28 ms /   129 tokens
0.00.727.785 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.078s
sys	0m0.129s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.018 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.023 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.033 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.033 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.034 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.034 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.034 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.037 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.038 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.039 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.040 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.815 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.794 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.532 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.533 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.534 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.535 I llama_model_loader: - type  f32:  194 tensors
0.00.026.535 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.535 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.536 I print_info: file format = GGUF V3 (latest)
0.00.026.536 I print_info: file type   = Q5_K - Medium
0.00.026.537 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.397 I load: special tokens cache size = 25
0.00.040.416 I load: token to piece cache size = 0.2984 MB
0.00.040.419 I print_info: arch             = gptneox
0.00.040.419 I print_info: vocab_only       = 0
0.00.040.419 I print_info: n_ctx_train      = 2048
0.00.040.419 I print_info: n_embd           = 2048
0.00.040.419 I print_info: n_layer          = 24
0.00.040.422 I print_info: n_head           = 16
0.00.040.423 I print_info: n_head_kv        = 16
0.00.040.423 I print_info: n_rot            = 32
0.00.040.423 I print_info: n_swa            = 0
0.00.040.423 I print_info: n_embd_head_k    = 128
0.00.040.424 I print_info: n_embd_head_v    = 128
0.00.040.426 I print_info: n_gqa            = 1
0.00.040.427 I print_info: n_embd_k_gqa     = 2048
0.00.040.427 I print_info: n_embd_v_gqa     = 2048
0.00.040.428 I print_info: f_norm_eps       = 1.0e-05
0.00.040.428 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.428 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.429 I print_info: f_logit_scale    = 0.0e+00
0.00.040.429 I print_info: n_ff             = 8192
0.00.040.430 I print_info: n_expert         = 0
0.00.040.430 I print_info: n_expert_used    = 0
0.00.040.430 I print_info: causal attn      = 1
0.00.040.432 I print_info: pooling type     = 0
0.00.040.432 I print_info: rope type        = 2
0.00.040.432 I print_info: rope scaling     = linear
0.00.040.432 I print_info: freq_base_train  = 10000.0
0.00.040.433 I print_info: freq_scale_train = 1
0.00.040.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.433 I print_info: rope_finetuned   = unknown
0.00.040.433 I print_info: ssm_d_conv       = 0
0.00.040.433 I print_info: ssm_d_inner      = 0
0.00.040.433 I print_info: ssm_d_state      = 0
0.00.040.433 I print_info: ssm_dt_rank      = 0
0.00.040.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.434 I print_info: model type       = 1.4B
0.00.040.434 I print_info: model params     = 1.41 B
0.00.040.434 I print_info: general.name     = 1.4B
0.00.040.435 I print_info: vocab type       = BPE
0.00.040.435 I print_info: n_vocab          = 50304
0.00.040.435 I print_info: n_merges         = 50009
0.00.040.435 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.437 I print_info: LF token         = 187 'Ċ'
0.00.040.438 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.438 I print_info: max token length = 1024
0.00.594.495 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.508 I load_tensors: offloading output layer to GPU
0.00.594.509 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.544 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.546 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.923 I llama_init_from_model: n_seq_max     = 1
0.00.595.928 I llama_init_from_model: n_ctx         = 2048
0.00.595.928 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.929 I llama_init_from_model: n_batch       = 2048
0.00.595.929 I llama_init_from_model: n_ubatch      = 512
0.00.595.930 I llama_init_from_model: flash_attn    = 0
0.00.595.932 I llama_init_from_model: freq_base     = 10000.0
0.00.595.932 I llama_init_from_model: freq_scale    = 1
0.00.595.934 I ggml_metal_init: allocating
0.00.595.999 I ggml_metal_init: found device: Apple M4
0.00.596.012 I ggml_metal_init: picking default device: Apple M4
0.00.597.840 I ggml_metal_init: using embedded metal library
0.00.604.198 I ggml_metal_init: GPU name:   Apple M4
0.00.604.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.204 I ggml_metal_init: simdgroup reduction   = true
0.00.604.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.204 I ggml_metal_init: has residency sets    = true
0.00.604.205 I ggml_metal_init: has bfloat            = true
0.00.604.205 I ggml_metal_init: use bfloat            = true
0.00.604.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.185 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.678.035 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.115 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.323 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.325 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.326 I llama_init_from_model: graph nodes  = 967
0.00.682.326 I llama_init_from_model: graph splits = 2
0.00.682.335 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.472 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.285 I main: llama threadpool init, n_threads = 4
0.00.745.329 I 
0.00.745.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.354 I 
0.00.745.507 I sampler seed: 1234
0.00.745.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.523 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.523 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.590.239 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.590.240 I llama_perf_context_print:        load time =     735.74 ms
0.01.590.240 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.75 tokens per second)
0.01.590.242 I llama_perf_context_print:        eval time =     790.69 ms /    63 runs   (   12.55 ms per token,    79.68 tokens per second)
0.01.590.242 I llama_perf_context_print:       total time =     845.61 ms /    70 tokens
0.01.590.496 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.138 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.139 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.532 I llama_model_loader: - type  f32:  194 tensors
0.00.024.532 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.533 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.534 I print_info: file format = GGUF V3 (latest)
0.00.024.539 I print_info: file type   = Q5_K - Medium
0.00.024.540 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.328 I load: special tokens cache size = 25
0.00.038.159 I load: token to piece cache size = 0.2984 MB
0.00.038.163 I print_info: arch             = gptneox
0.00.038.163 I print_info: vocab_only       = 0
0.00.038.163 I print_info: n_ctx_train      = 2048
0.00.038.163 I print_info: n_embd           = 2048
0.00.038.163 I print_info: n_layer          = 24
0.00.038.167 I print_info: n_head           = 16
0.00.038.167 I print_info: n_head_kv        = 16
0.00.038.168 I print_info: n_rot            = 32
0.00.038.168 I print_info: n_swa            = 0
0.00.038.168 I print_info: n_embd_head_k    = 128
0.00.038.168 I print_info: n_embd_head_v    = 128
0.00.038.169 I print_info: n_gqa            = 1
0.00.038.170 I print_info: n_embd_k_gqa     = 2048
0.00.038.170 I print_info: n_embd_v_gqa     = 2048
0.00.038.171 I print_info: f_norm_eps       = 1.0e-05
0.00.038.171 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.172 I print_info: f_logit_scale    = 0.0e+00
0.00.038.172 I print_info: n_ff             = 8192
0.00.038.173 I print_info: n_expert         = 0
0.00.038.173 I print_info: n_expert_used    = 0
0.00.038.173 I print_info: causal attn      = 1
0.00.038.173 I print_info: pooling type     = 0
0.00.038.173 I print_info: rope type        = 2
0.00.038.173 I print_info: rope scaling     = linear
0.00.038.175 I print_info: freq_base_train  = 10000.0
0.00.038.175 I print_info: freq_scale_train = 1
0.00.038.175 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.176 I print_info: rope_finetuned   = unknown
0.00.038.176 I print_info: ssm_d_conv       = 0
0.00.038.176 I print_info: ssm_d_inner      = 0
0.00.038.176 I print_info: ssm_d_state      = 0
0.00.038.177 I print_info: ssm_dt_rank      = 0
0.00.038.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.177 I print_info: model type       = 1.4B
0.00.038.178 I print_info: model params     = 1.41 B
0.00.038.178 I print_info: general.name     = 1.4B
0.00.038.178 I print_info: vocab type       = BPE
0.00.038.179 I print_info: n_vocab          = 50304
0.00.038.179 I print_info: n_merges         = 50009
0.00.038.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.180 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.180 I print_info: LF token         = 187 'Ċ'
0.00.038.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.180 I print_info: max token length = 1024
0.00.588.417 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.431 I load_tensors: offloading output layer to GPU
0.00.588.432 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.463 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.588.464 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.589.844 I llama_init_from_model: n_seq_max     = 1
0.00.589.849 I llama_init_from_model: n_ctx         = 128
0.00.589.850 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.855 I llama_init_from_model: n_batch       = 128
0.00.589.856 I llama_init_from_model: n_ubatch      = 128
0.00.589.857 I llama_init_from_model: flash_attn    = 0
0.00.589.865 I llama_init_from_model: freq_base     = 10000.0
0.00.589.866 I llama_init_from_model: freq_scale    = 1
0.00.589.866 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.869 I ggml_metal_init: allocating
0.00.589.949 I ggml_metal_init: found device: Apple M4
0.00.589.963 I ggml_metal_init: picking default device: Apple M4
0.00.591.843 I ggml_metal_init: using embedded metal library
0.00.598.470 I ggml_metal_init: GPU name:   Apple M4
0.00.598.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.476 I ggml_metal_init: simdgroup reduction   = true
0.00.598.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.476 I ggml_metal_init: has residency sets    = true
0.00.598.476 I ggml_metal_init: has bfloat            = true
0.00.598.477 I ggml_metal_init: use bfloat            = true
0.00.598.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.226 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.229 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.295 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.539 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.542 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.542 I llama_init_from_model: graph nodes  = 967
0.00.622.542 I llama_init_from_model: graph splits = 2
0.00.622.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.047 I 
0.00.657.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.153 I perplexity: tokenizing the input ..
0.00.663.926 I perplexity: tokenization took 6.77 ms
0.00.663.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.799 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.806.121 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.806.136 I llama_perf_context_print:        load time =     647.95 ms
0.00.806.137 I llama_perf_context_print: prompt eval time =     140.30 ms /   128 tokens (    1.10 ms per token,   912.32 tokens per second)
0.00.806.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.138 I llama_perf_context_print:       total time =     149.09 ms /   129 tokens
0.00.806.495 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.078s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.863 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.870 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.874 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.875 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.876 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.243 I llama_model_loader: - type  f32:  194 tensors
0.00.026.243 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.244 I print_info: file format = GGUF V3 (latest)
0.00.026.245 I print_info: file type   = Q6_K
0.00.026.245 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.074 I load: special tokens cache size = 25
0.00.039.835 I load: token to piece cache size = 0.2984 MB
0.00.039.837 I print_info: arch             = gptneox
0.00.039.838 I print_info: vocab_only       = 0
0.00.039.838 I print_info: n_ctx_train      = 2048
0.00.039.838 I print_info: n_embd           = 2048
0.00.039.838 I print_info: n_layer          = 24
0.00.039.841 I print_info: n_head           = 16
0.00.039.841 I print_info: n_head_kv        = 16
0.00.039.842 I print_info: n_rot            = 32
0.00.039.842 I print_info: n_swa            = 0
0.00.039.843 I print_info: n_embd_head_k    = 128
0.00.039.843 I print_info: n_embd_head_v    = 128
0.00.039.845 I print_info: n_gqa            = 1
0.00.039.846 I print_info: n_embd_k_gqa     = 2048
0.00.039.846 I print_info: n_embd_v_gqa     = 2048
0.00.039.847 I print_info: f_norm_eps       = 1.0e-05
0.00.039.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.847 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.848 I print_info: f_logit_scale    = 0.0e+00
0.00.039.848 I print_info: n_ff             = 8192
0.00.039.849 I print_info: n_expert         = 0
0.00.039.849 I print_info: n_expert_used    = 0
0.00.039.849 I print_info: causal attn      = 1
0.00.039.849 I print_info: pooling type     = 0
0.00.039.849 I print_info: rope type        = 2
0.00.039.850 I print_info: rope scaling     = linear
0.00.039.850 I print_info: freq_base_train  = 10000.0
0.00.039.850 I print_info: freq_scale_train = 1
0.00.039.850 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.851 I print_info: rope_finetuned   = unknown
0.00.039.851 I print_info: ssm_d_conv       = 0
0.00.039.851 I print_info: ssm_d_inner      = 0
0.00.039.852 I print_info: ssm_d_state      = 0
0.00.039.853 I print_info: ssm_dt_rank      = 0
0.00.039.853 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.853 I print_info: model type       = 1.4B
0.00.039.853 I print_info: model params     = 1.41 B
0.00.039.854 I print_info: general.name     = 1.4B
0.00.039.854 I print_info: vocab type       = BPE
0.00.039.854 I print_info: n_vocab          = 50304
0.00.039.854 I print_info: n_merges         = 50009
0.00.039.855 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.855 I print_info: LF token         = 187 'Ċ'
0.00.039.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: max token length = 1024
0.00.649.429 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.434 I load_tensors: offloading output layer to GPU
0.00.649.435 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.457 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.459 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.650.823 I llama_init_from_model: n_seq_max     = 1
0.00.650.826 I llama_init_from_model: n_ctx         = 2048
0.00.650.826 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.826 I llama_init_from_model: n_batch       = 2048
0.00.650.827 I llama_init_from_model: n_ubatch      = 512
0.00.650.827 I llama_init_from_model: flash_attn    = 0
0.00.650.828 I llama_init_from_model: freq_base     = 10000.0
0.00.650.829 I llama_init_from_model: freq_scale    = 1
0.00.650.830 I ggml_metal_init: allocating
0.00.650.843 I ggml_metal_init: found device: Apple M4
0.00.650.852 I ggml_metal_init: picking default device: Apple M4
0.00.652.227 I ggml_metal_init: using embedded metal library
0.00.658.145 I ggml_metal_init: GPU name:   Apple M4
0.00.658.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.151 I ggml_metal_init: simdgroup reduction   = true
0.00.658.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.151 I ggml_metal_init: has residency sets    = true
0.00.658.151 I ggml_metal_init: has bfloat            = true
0.00.658.152 I ggml_metal_init: use bfloat            = true
0.00.658.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.154 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.825 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.046 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.726.055 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.726.089 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.161 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.163 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.163 I llama_init_from_model: graph nodes  = 967
0.00.730.163 I llama_init_from_model: graph splits = 2
0.00.730.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.312 I main: llama threadpool init, n_threads = 4
0.00.800.361 I 
0.00.800.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.389 I 
0.00.800.558 I sampler seed: 1234
0.00.800.564 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.607 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.611 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.611 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.676.988 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.676.989 I llama_perf_context_print:        load time =     789.75 ms
0.01.676.990 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.58 tokens per second)
0.01.676.990 I llama_perf_context_print:        eval time =     819.00 ms /    63 runs   (   13.00 ms per token,    76.92 tokens per second)
0.01.676.991 I llama_perf_context_print:       total time =     877.38 ms /    70 tokens
0.01.677.269 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.106s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4639 (3ec9fd4b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.232 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.980 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.982 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.983 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.725 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.547 I llama_model_loader: - type  f32:  194 tensors
0.00.025.547 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.548 I print_info: file format = GGUF V3 (latest)
0.00.025.548 I print_info: file type   = Q6_K
0.00.025.549 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.657 I load: special tokens cache size = 25
0.00.039.638 I load: token to piece cache size = 0.2984 MB
0.00.039.640 I print_info: arch             = gptneox
0.00.039.641 I print_info: vocab_only       = 0
0.00.039.641 I print_info: n_ctx_train      = 2048
0.00.039.641 I print_info: n_embd           = 2048
0.00.039.641 I print_info: n_layer          = 24
0.00.039.644 I print_info: n_head           = 16
0.00.039.645 I print_info: n_head_kv        = 16
0.00.039.645 I print_info: n_rot            = 32
0.00.039.645 I print_info: n_swa            = 0
0.00.039.645 I print_info: n_embd_head_k    = 128
0.00.039.645 I print_info: n_embd_head_v    = 128
0.00.039.646 I print_info: n_gqa            = 1
0.00.039.647 I print_info: n_embd_k_gqa     = 2048
0.00.039.648 I print_info: n_embd_v_gqa     = 2048
0.00.039.648 I print_info: f_norm_eps       = 1.0e-05
0.00.039.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.651 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.651 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.652 I print_info: f_logit_scale    = 0.0e+00
0.00.039.652 I print_info: n_ff             = 8192
0.00.039.652 I print_info: n_expert         = 0
0.00.039.653 I print_info: n_expert_used    = 0
0.00.039.653 I print_info: causal attn      = 1
0.00.039.653 I print_info: pooling type     = 0
0.00.039.653 I print_info: rope type        = 2
0.00.039.653 I print_info: rope scaling     = linear
0.00.039.654 I print_info: freq_base_train  = 10000.0
0.00.039.654 I print_info: freq_scale_train = 1
0.00.039.654 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.654 I print_info: rope_finetuned   = unknown
0.00.039.655 I print_info: ssm_d_conv       = 0
0.00.039.655 I print_info: ssm_d_inner      = 0
0.00.039.655 I print_info: ssm_d_state      = 0
0.00.039.655 I print_info: ssm_dt_rank      = 0
0.00.039.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.657 I print_info: model type       = 1.4B
0.00.039.657 I print_info: model params     = 1.41 B
0.00.039.657 I print_info: general.name     = 1.4B
0.00.039.658 I print_info: vocab type       = BPE
0.00.039.658 I print_info: n_vocab          = 50304
0.00.039.658 I print_info: n_merges         = 50009
0.00.039.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: LF token         = 187 'Ċ'
0.00.039.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.659 I print_info: max token length = 1024
0.00.294.474 I load_tensors: offloading 24 repeating layers to GPU
0.00.294.480 I load_tensors: offloading output layer to GPU
0.00.294.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.294.505 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.294.508 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.295.757 I llama_init_from_model: n_seq_max     = 1
0.00.295.759 I llama_init_from_model: n_ctx         = 128
0.00.295.760 I llama_init_from_model: n_ctx_per_seq = 128
0.00.295.760 I llama_init_from_model: n_batch       = 128
0.00.295.760 I llama_init_from_model: n_ubatch      = 128
0.00.295.761 I llama_init_from_model: flash_attn    = 0
0.00.295.762 I llama_init_from_model: freq_base     = 10000.0
0.00.295.763 I llama_init_from_model: freq_scale    = 1
0.00.295.763 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.295.764 I ggml_metal_init: allocating
0.00.295.817 I ggml_metal_init: found device: Apple M4
0.00.295.829 I ggml_metal_init: picking default device: Apple M4
0.00.297.128 I ggml_metal_init: using embedded metal library
0.00.303.132 I ggml_metal_init: GPU name:   Apple M4
0.00.303.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.303.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.303.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.303.138 I ggml_metal_init: simdgroup reduction   = true
0.00.303.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.303.139 I ggml_metal_init: has residency sets    = true
0.00.303.139 I ggml_metal_init: has bfloat            = true
0.00.303.139 I ggml_metal_init: use bfloat            = true
0.00.303.140 I ggml_metal_init: hasUnifiedMemory      = true
0.00.303.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.319.636 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.322.948 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.322.951 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.322.993 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.326.181 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.326.183 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.326.183 I llama_init_from_model: graph nodes  = 967
0.00.326.184 I llama_init_from_model: graph splits = 2
0.00.326.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.326.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.524 I 
0.00.356.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.625 I perplexity: tokenizing the input ..
0.00.362.163 I perplexity: tokenization took 5.536 ms
0.00.362.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.501.432 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.502.764 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.502.777 I llama_perf_context_print:        load time =     346.28 ms
0.00.502.778 I llama_perf_context_print: prompt eval time =     139.03 ms /   128 tokens (    1.09 ms per token,   920.68 tokens per second)
0.00.502.778 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.502.779 I llama_perf_context_print:       total time =     146.26 ms /   129 tokens
0.00.503.161 I ggml_metal_free: deallocating

real	0m0.518s
user	0m0.075s
sys	0m0.097s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4639 (3ec9fd4b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150804c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150905a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150905e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150906300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150906770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150906be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150907050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1509074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150907930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150907da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150908210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1509088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1509093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150909b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15090a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15090aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15090b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15090b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15090c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15090c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15090cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15090d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15090dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15090e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15090ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15090efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15090f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15090f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15090fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150910280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150910840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150910d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1509111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150911480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1509118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150911d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1509121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150912640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150912ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150912f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150913390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150913800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150913c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1509140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150914550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1509149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150914e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1509152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150915a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150915ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150916310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150916780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150916bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150917060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1509174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150917bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150918090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150918350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1509187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150918e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150919290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150919550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150919a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150919f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15091a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15091a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15091ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15091b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15091b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15091bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15091c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15091c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15091cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15091d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15091d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15091dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15091e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15091e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15091edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15091f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15091f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15091fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150920480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150920a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150920fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150921590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150921b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1509220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1509226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150922c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150923200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1509237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150923d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150924310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1509248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150924e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150925420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150915560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150925b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150925ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150926460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150926a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150926fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150927570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150927b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1509280d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150928680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150928c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1509291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150929790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150929d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15092a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15092a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15092ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15092b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15092b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15092bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15092c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15092c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15092cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15092d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15092d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15092db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15092e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15092e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15092ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15092ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15092f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15092f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15092fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150930350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150930850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150930d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150931250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150931750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150931c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150932150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150932650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150932b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150933050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150933550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150933a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150933f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150934450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150934950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150934e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150935350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150935850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150935d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150936250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150936750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150936c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150937150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150937650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150937b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150938050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150938550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150938a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150938f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150939450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150939950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150939e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15093a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15093a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15093ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15093b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15093b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15093bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15093c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15093c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15093cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15093d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15093d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15093da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15093df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15093e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15093e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15093ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15093f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15093f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15093fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150940250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150940750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150940c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150941150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150941650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150941b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150942050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150a05350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150a057c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150a08750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150a08a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150a08e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150a092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150a09760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150a09bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150a0a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150a0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150a0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150a0ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150a0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150a0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150a0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150a0c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150a0c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150a0ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150a0ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150a0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150a0d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150a0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150a0e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150a0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150a0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150a0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150a0fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150a10260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150a10820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150a10de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150a113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150a11960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150a11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150a124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150a12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150a13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150a13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150a13be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150a141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150a14760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150a14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150a152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150a158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150a15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150a169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150a16fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150a17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150a17b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150a180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150a186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150a18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150a19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150a197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150a19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150a1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150a1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150a1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150a1b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150a1ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150a1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150a1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150a1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150a1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150a1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150a1dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150a1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150a1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150a1ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150a1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150a1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150a1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150a20520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150a20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150a210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150a21660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150a21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150a221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150a227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150a23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150a23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150a23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150a24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150a24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150a24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150a25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150a25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150a25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150a25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150a26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150a26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150a26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150a27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150a27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150a28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150a28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150a290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150a297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150a29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150a2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150a2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150a2ab50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.739.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.121 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150a14460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150a11c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150a21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150a1c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150a1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150a127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150a14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x150a1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150a20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150a13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150a1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150a194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150a14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150a1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150a0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150a21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150a155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150a13320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150a16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150a19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150a224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150a1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150a1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150a2bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150a2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150a2c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150a2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150a2cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150a2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150a2dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150a2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150a121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150a2e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150a2ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150a2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150a2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150a2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150a30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150a30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150a309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150a30e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150a312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150a31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150a31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150a32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150a32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150a32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150a33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150a33890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150a33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150a344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150a34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150a350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150a358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150a35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150a36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150a364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150a36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150a372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150a37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150a37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150a380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150a38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150a389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150a38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150a39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150a397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150a39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150a3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150a3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150a3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150a3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150a3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150a3b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150a3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150a3c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150a3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150a3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150a3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150a3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150a3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150a3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150a3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150a3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150a3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150a3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150a3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x150a403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150a40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150a40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150a413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150a41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150a41e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150a423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150a42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150a42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150a433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150a43900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150a43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150a443a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150a448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150a44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150a45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150a458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150a45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150a46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150a468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150a46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150a47370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150a478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150a47e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150a48360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150a48800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150a48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150a49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150a495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150a49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150a49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150a4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150a4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150a4ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150a4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150a4b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150a4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150a4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150a4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150a4c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150a4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150a4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150a4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150a4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150a4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150a4e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150a4e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150a4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150a4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150a4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150a4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150a50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150a504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150a50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150a50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150a512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150a51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150a51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150a520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150a52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150a529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150a52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150a53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150a537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150a53c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150a54100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150a545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150a54a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150a54ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150a55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150a55820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150a55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150a56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150a56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150a56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150a56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150a573e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150a57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150a57d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150a581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150a58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150a58b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150a58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150a59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150a598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150a59d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150a5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150a5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150a5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150a5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150a5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150a5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150a5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150a5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150a5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150a5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150a5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150a5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150a5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150a5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150a5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150a5e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150a5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150a5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150a5f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150a5fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150a60000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150a60550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150a60aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150a60d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150a61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150a61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150a61f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150a62780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150a62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150a62ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150a634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150a63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150a642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150a64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150a64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150a650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150a65880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150a65dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150a66320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150a66870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150a66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150a67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150a67860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150a67db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150a68300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150a68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150a68da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150a692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150a69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150a69d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150a6a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150a6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150a6ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150a6b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150a6b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150a6bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150a6c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150a6c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150a6cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150a6d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150a6d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150a6dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150a6e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150a6e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150a6ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150a6f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150a6f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150a6fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150a70280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150a707d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150a70d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150a71270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150a717c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150a71d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150a72260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150a727b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150a72d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150a73250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150a737a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150a73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150a74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150a74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150a74ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150a75230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150a75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150a75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150a76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150a76770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150a76cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150a77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150a77760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150a77cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150a78200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150a786a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150a78b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150a78fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150a79480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150a79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150a79dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150a7a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150a7a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150a7aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150a7b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150a7b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150a7b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150a7be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150a7c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150a7c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150a7ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150a7d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150a7daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150a7e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150a7e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150a7ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150a7f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150a7f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150a7fcb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15091d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150922f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15091d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150925130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150922960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15092a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150929a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1509294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150924b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15091f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150927830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1509245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15091f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1509223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150920cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150927280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150928ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15091ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150920740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150923a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150921850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150928390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1509212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150918a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1509256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15090f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150942310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1509425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150942b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150942de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1509430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150943360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150926cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150943620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1509438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150943ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150943e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150944120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1509443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1509446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150944960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150944c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150944ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1509451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150945460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150945720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1509459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150945ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150945f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150946220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1509464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1509467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150946a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150946d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150946fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1509472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150947560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150947820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150947ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150947da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150948060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150948320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1509485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1509488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150948b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150948e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1509490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1509493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150949660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150949920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150949be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150949ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15094a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15094a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15094a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15094a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15094ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15094af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15094b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15094b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15094b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15094ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15094bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15094bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15094c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15094c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15094c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15094caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15094cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15094d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15094d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15094d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15094d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15094db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15094dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15094e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15094e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15094e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15094e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15094eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15094ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15094f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15094f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15094f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15094f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15094fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15094fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1509501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150950460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150950720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1509509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150950ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150950f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150951220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1509514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1509517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150951a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150951d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150951fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1509522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150952560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150952820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150952ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150952da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150953060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150953320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1509535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1509538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150953b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150953e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1509540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1509543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150954660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150954920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150954be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150954ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150955160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150955420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1509556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1509559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150955da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150956060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150956560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150956a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150956f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150957460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150957960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150957e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150958360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150958860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150958d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150959260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150959760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150959c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15095a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15095a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15095ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15095b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15095b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15095ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15095bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15095c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15095c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15095ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15095d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15095d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15095dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15095e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15095e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15095ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15095f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15095f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15095fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150960060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150960560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150960a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150960f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150961460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150961960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150961e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150962360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150962860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150962d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150963260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150963760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150963c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150964160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150964660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150964b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150965060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150965560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150965a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150965f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150966460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150966a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150966fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150967570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150967b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150968130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150968740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150968d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150969540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1509699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150969ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15096a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15096a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15096b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15096b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15096b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15096be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15096c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15096cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15096d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15096d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15096db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15096e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15096e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15096eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15096f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15096f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15096fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1509700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150970600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150970b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1509710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1509715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150971b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150972090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1509725e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150972b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150973080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1509735d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150973b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150974070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1509745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150974b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150975060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1509755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150975b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150976050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1509765a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150976af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150977040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150977590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150977ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150978030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150978580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150978ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150979020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150979570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150979ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15097a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15097a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15097aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15097b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15097b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15097baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15097bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15097c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15097ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15097cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15097d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15097da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15097dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15097e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15097ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15097efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15097f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15097f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15097fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150980240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1509806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150980b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150981020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1509814c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150981960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150981e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1509822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150982740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150982be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150983080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150983520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150983a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150984190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1509848b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150984fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1509856f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1509859b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1509861a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150986460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150986a70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.789s
user	0m0.277s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4639 (3ec9fd4b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13370d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13370e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13370e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13370eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13370f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13370f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13370fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133710220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1337107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133710cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1337111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1337116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1337121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1337129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1337131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1337138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133713ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133714710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133715600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133715d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133716440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133716b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133717b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133717de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1337183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1337195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133719860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133719d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133719fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13371a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13371ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13371b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13371b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13371b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13371be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13371c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13371c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13371cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13371d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13371d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13371d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13371dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13371e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13371e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13371f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13371f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13371fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133720a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133721040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133721650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133721e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1337222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133722780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133723050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133723840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133723b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133723fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1337248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133724d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133725220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1337256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133725b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133726000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1337264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133726940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133726de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133727280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1337277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133728270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1337287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133728d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133729260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1337297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133729d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13372a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13372a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13372acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13372b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13372b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13372bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13372c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13372c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13372ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13372d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13372d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13372dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13372e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13372e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13372ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13372f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13371eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13372f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13372fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133730370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1337308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133730e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133731360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1337318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133732350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1337328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133732df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133733340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133733890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133733de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133734330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1337347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133734c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133735110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1337355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133735a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133736390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133737610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133737ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133737f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1337383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133738890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1337391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133739670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133739b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133739fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13373a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13373a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13373ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13373b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13373b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13373bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13373c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13373c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13373c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13373cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13373d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13373d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13373dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13373e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13373e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13373e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13373ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13373f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13373f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13373fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1337400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133740a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133741350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1337417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133741c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133742130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1337433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133743850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133743cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133744190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133744630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133744f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133745410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1337458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133745d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1337461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133746b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133746fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133747470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133747910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133748250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1337486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133749030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1337494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133749970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133749e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13374a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13374a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13374abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13374b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13374b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13374ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13374bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13374c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13374ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13374cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13374d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13374d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13374df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13374e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13374ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13374eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13374f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13374fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1337502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133750760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133750c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1337510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133751850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133751da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1337522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133752840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133752d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1337532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133753830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133753d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1337542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133754820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133754d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1337552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133755810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133755d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1337562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133756800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133756d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1337572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1337577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133757d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133758290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1337587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133758d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133759280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1337597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133759d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13375a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13375a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13375ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13375b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13375b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13375bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13375c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13375c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13375ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13375d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13375d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13375dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13375e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13375e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13375ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13375f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13375f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13375fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133760210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133760760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133760cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133761200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133761750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133761ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1337621f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133762740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133762c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1337631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133763730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133763c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1337641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133764b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133764fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133765450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1337658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133765d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133766230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1337666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133766b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133767010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1337674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133767950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133768290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133768730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133768c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1337693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133769ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13376a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13376a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13376abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13376b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13376b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13376bc80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133608740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133608bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133609020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133609900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133609d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13360a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13360a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13360aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13360b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13360b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13360bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13360c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13360ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13360d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13360dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13360e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13360eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13360f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13360fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133610180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1336108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133610fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1336116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1336120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133612380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1336127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133612c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1336130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1336135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133613f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133614680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133614af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133615550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133615a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133615f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133616450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133616e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133617350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133617cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1336185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133618a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133618e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1336192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133619760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133619bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13361a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13361a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13361ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13361b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13361b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13361b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13361c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13361c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13361cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13361cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13361d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13361d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13361dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13361e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13361e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13361eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13361f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13361f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13361f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13361fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133620350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1336208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133620df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133621340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133621890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133621de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133622330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133622dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133623320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133623870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133623dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133624310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133624860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133624db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133625300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133625850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133625da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1336262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133626840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133626d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1336272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133627830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133627d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1336282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133628820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133628d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1336292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133629810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133629d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13362a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13362a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13362ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13362b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13362b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13362bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13362c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13362c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13362cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13362d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13362d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13362dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13362e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13362e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13362e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13362ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13362f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13362f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13362fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1336300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133630560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133630a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133630ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133631340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1336317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133631c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133632120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1336325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133632a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133632f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1336333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133633ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133634180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133634620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133634ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133634f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133635400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1336358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133635d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1336361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133636680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133636b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133636fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133637460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133637900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133637da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133638240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1336386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133638b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133639020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1336394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133639960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133639e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13363a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13363a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13363abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13363b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13363b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13363b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13363be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13363c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13363c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13363cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13363d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13363d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13363da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13363dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13363e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13363e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13363eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13363f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13363f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13363fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13363ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1336403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133640860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133640d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1336411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133641ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133641f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133642420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1336428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133642d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133643200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1336436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133643b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133643fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133644480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1336449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133644f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1336459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133645c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1336468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1336476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133647e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133648410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133648a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133649210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1336496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133649b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13364a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13364acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13364b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13364b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13364bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13364c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13364c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13364ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13364d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13364d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13364dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13364e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13364e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13364ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13364f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13364f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13364fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1336501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133650740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1336511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133651730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133651c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1336521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133652720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1336531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133653710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133653c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1336541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133654700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1336551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1336556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133655c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133656190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1336566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133656c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133657180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1336576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133657c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133658170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1336586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133658c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133659160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1336596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133659c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13365a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13365a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13365abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13365b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13365b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13365bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13365c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13365c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13365cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13365d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13365d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13365da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13365df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13365e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13365e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13365ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13365f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13365f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13365fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13365ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133660400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1336608a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133660d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1336611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133661680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133661bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1336622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133662a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133663130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133663850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133663b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1336645c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133664bd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13376b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13374d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13374cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13374dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133720cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1337206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133722d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13374f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1337180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13371eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13371f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13371fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13371df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1337200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1337170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133723310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13372f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13376ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13371a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13371a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13374fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13374e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1337186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133718970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133718c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13376c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13376c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13376c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13376c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13376cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13376cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13376d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13376d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13376d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13376d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13376dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13376df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13376e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13376e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13376e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13376ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13376ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13376efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13376f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13376f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13376f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13376faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13376fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133770020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1337702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1337705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133770860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133770b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133770de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1337710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133771360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133771620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1337718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133771ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133771e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133772120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1337723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1337726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133772960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133772c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133772ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1337731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133773460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133773720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1337739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133773ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133773f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133774220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1337744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1337747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133774a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133774d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133774fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1337752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133775560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133775820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133775ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133775da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133776060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133776320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1337765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1337768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133776b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133776e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1337770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1337773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133777660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133777920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133777be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133777ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133778160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133778420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1337786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1337789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133778c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133778f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1337791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1337794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133779760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133779a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133779ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133779fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13377a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13377a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13377a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13377aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13377ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13377b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13377b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13377b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13377b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13377bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13377bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13377c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13377c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13377c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13377c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13377cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13377ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13377d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13377d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13377d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13377d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13377dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13377dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13377e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13377e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13377e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13377e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13377eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13377ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13377f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13377f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13377f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13377fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13377fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13377ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1337802a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133780560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133780820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133780ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133780da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133781060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133781320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1337815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1337818a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133781b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133781e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1337820e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1337823a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133782660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133782920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133782be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133782ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133783160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133783420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1337836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1337839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133783c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133783f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1337841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1337844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133784760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133784a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133784ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133784fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133785260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133785520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1337857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133785aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133785d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133786020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1337862e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1337865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133786860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133786b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133786de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1337870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133787360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133787620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1337878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133787ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133787e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133788120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1337883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1337886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133788960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133788c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133788ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1337891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133789460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133789720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1337899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133789ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133789f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13378a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13378a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x135004280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1350046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135004b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135004fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135005440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1350059d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135005e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1350062b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135006e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1350070c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135007380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1350077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135007c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1350080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135008540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1350089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135008e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135009290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135009700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135009b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135009fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13500a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13500a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13500ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13500b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13500b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13500ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13500bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13500c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13500c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13500cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13500d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13500d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13500d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13500de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13500e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13500e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13500eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13500efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13500f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13500f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13500fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135010180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1350105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x135010a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x135010ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x135011340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1350117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x135011c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135012090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135012500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135012970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135012de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135013250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1350136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135013b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135013fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135014410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135014880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135014cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135015160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1350155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135015a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135015eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135016320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135016790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135016c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135017070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1350174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135017950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135017dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135018230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1350186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135018b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135018f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1350193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135019860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135019cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13501a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13501a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13501aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13501b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13501bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13501c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13501c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13501ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13501d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13501d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13501dd30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.968s
user	0m0.235s
sys	0m0.191s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.78 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.21 sec*proc (2 tests)

Total Test time (real) =   2.23 sec
        2.25 real         0.51 user         0.26 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
