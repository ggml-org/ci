Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.129s
user	0m1.036s
sys	0m1.505s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Linking CXX executable ../bin/test-arg-parser
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Built target test-arg-parser
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Built target test-quantize-fns
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched-bench
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-embedding
[ 70%] Built target llama-batched
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-infill
[ 78%] Built target llama-imatrix
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-cli
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Built target llama-save-load-state
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-run
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-tts
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-export-lora
[ 96%] Built target llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.214s
user	0m6.508s
sys	0m9.962s

main: quantize time =  4465.13 ms
main:    total time =  4465.13 ms

main: quantize time =  2884.51 ms
main:    total time =  2884.51 ms

main: quantize time =  4505.57 ms
main:    total time =  4505.57 ms

main: quantize time =  3668.80 ms
main:    total time =  3668.80 ms

main: quantize time =  2001.83 ms
main:    total time =  2001.83 ms

main: quantize time =  5283.72 ms
main:    total time =  5283.72 ms

main: quantize time =  6102.57 ms
main:    total time =  6102.57 ms

main: quantize time =  7100.11 ms
main:    total time =  7100.11 ms

main: quantize time =  6353.30 ms
main:    total time =  6353.30 ms

main: quantize time =  4353.34 ms
main:    total time =  4353.34 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.130 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.289 I main: llama backend init
0.00.000.295 I main: load the model and apply lora adapter, if any
0.00.024.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.034 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.046 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.049 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.050 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.053 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.054 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.060 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.060 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.365 I llama_model_loader: - type  f32:  194 tensors
0.00.046.366 I llama_model_loader: - type  f16:   98 tensors
0.00.046.367 I print_info: file format = GGUF V3 (latest)
0.00.046.367 I print_info: file type   = all F32 (guessed)
0.00.046.368 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.054.591 I load: special tokens cache size = 25
0.00.060.823 I load: token to piece cache size = 0.2984 MB
0.00.060.827 I print_info: arch             = gptneox
0.00.060.827 I print_info: vocab_only       = 0
0.00.060.828 I print_info: n_ctx_train      = 2048
0.00.060.828 I print_info: n_embd           = 2048
0.00.060.828 I print_info: n_layer          = 24
0.00.060.833 I print_info: n_head           = 16
0.00.060.834 I print_info: n_head_kv        = 16
0.00.060.834 I print_info: n_rot            = 32
0.00.060.834 I print_info: n_swa            = 0
0.00.060.834 I print_info: n_embd_head_k    = 128
0.00.060.835 I print_info: n_embd_head_v    = 128
0.00.060.835 I print_info: n_gqa            = 1
0.00.060.836 I print_info: n_embd_k_gqa     = 2048
0.00.060.837 I print_info: n_embd_v_gqa     = 2048
0.00.060.837 I print_info: f_norm_eps       = 1.0e-05
0.00.060.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.839 I print_info: f_logit_scale    = 0.0e+00
0.00.060.840 I print_info: n_ff             = 8192
0.00.060.840 I print_info: n_expert         = 0
0.00.060.840 I print_info: n_expert_used    = 0
0.00.060.840 I print_info: causal attn      = 1
0.00.060.840 I print_info: pooling type     = 0
0.00.060.840 I print_info: rope type        = 2
0.00.060.841 I print_info: rope scaling     = linear
0.00.060.843 I print_info: freq_base_train  = 10000.0
0.00.060.843 I print_info: freq_scale_train = 1
0.00.060.843 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.843 I print_info: rope_finetuned   = unknown
0.00.060.844 I print_info: ssm_d_conv       = 0
0.00.060.844 I print_info: ssm_d_inner      = 0
0.00.060.845 I print_info: ssm_d_state      = 0
0.00.060.845 I print_info: ssm_dt_rank      = 0
0.00.060.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.845 I print_info: model type       = 1.4B
0.00.060.846 I print_info: model params     = 1.41 B
0.00.060.846 I print_info: general.name     = 1.4B
0.00.060.846 I print_info: vocab type       = BPE
0.00.060.847 I print_info: n_vocab          = 50304
0.00.060.847 I print_info: n_merges         = 50009
0.00.060.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.847 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.847 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.848 I print_info: LF token         = 187 'Ċ'
0.00.060.848 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.848 I print_info: max token length = 1024
0.00.060.848 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.100.157 I load_tensors: offloading 24 repeating layers to GPU
0.00.100.160 I load_tensors: offloading output layer to GPU
0.00.100.160 I load_tensors: offloaded 25/25 layers to GPU
0.00.100.185 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.186 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.100.503 I llama_init_from_model: n_seq_max     = 1
0.00.100.504 I llama_init_from_model: n_ctx         = 2048
0.00.100.504 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.100.505 I llama_init_from_model: n_batch       = 2048
0.00.100.505 I llama_init_from_model: n_ubatch      = 512
0.00.100.505 I llama_init_from_model: flash_attn    = 0
0.00.100.506 I llama_init_from_model: freq_base     = 10000.0
0.00.100.506 I llama_init_from_model: freq_scale    = 1
0.00.100.507 I ggml_metal_init: allocating
0.00.100.524 I ggml_metal_init: found device: Apple M4
0.00.100.529 I ggml_metal_init: picking default device: Apple M4
0.00.101.176 I ggml_metal_init: using embedded metal library
0.00.148.560 I ggml_metal_init: GPU name:   Apple M4
0.00.148.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.148.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.148.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.148.566 I ggml_metal_init: simdgroup reduction   = true
0.00.148.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.148.566 I ggml_metal_init: has residency sets    = true
0.00.148.567 I ggml_metal_init: has bfloat            = true
0.00.148.567 I ggml_metal_init: use bfloat            = true
0.00.148.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.148.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.205.689 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.235.272 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.235.278 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.235.320 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.239.370 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.239.372 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.239.372 I llama_init_from_model: graph nodes  = 967
0.00.239.372 I llama_init_from_model: graph splits = 2
0.00.239.376 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.239.504 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.239.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.305.224 I main: llama threadpool init, n_threads = 4
0.00.305.271 I 
0.00.305.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.305.301 I 
0.00.305.343 I sampler seed: 1234
0.00.305.348 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.305.372 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.305.374 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.305.374 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.140.395 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.02.140.396 I llama_perf_context_print:        load time =     279.67 ms
0.02.140.396 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.41 tokens per second)
0.02.140.397 I llama_perf_context_print:        eval time =    1788.50 ms /    63 runs   (   28.39 ms per token,    35.23 tokens per second)
0.02.140.398 I llama_perf_context_print:       total time =    1836.09 ms /    70 tokens
0.02.140.626 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.113s
sys	0m0.127s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.716 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.718 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.719 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.389 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.390 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.390 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.391 I llama_model_loader: - type  f32:  194 tensors
0.00.033.391 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.392 I print_info: file format = GGUF V3 (latest)
0.00.033.392 I print_info: file type   = Q8_0
0.00.033.394 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.209 I load: special tokens cache size = 25
0.00.048.704 I load: token to piece cache size = 0.2984 MB
0.00.048.710 I print_info: arch             = gptneox
0.00.048.710 I print_info: vocab_only       = 0
0.00.048.710 I print_info: n_ctx_train      = 2048
0.00.048.713 I print_info: n_embd           = 2048
0.00.048.713 I print_info: n_layer          = 24
0.00.048.726 I print_info: n_head           = 16
0.00.048.729 I print_info: n_head_kv        = 16
0.00.048.729 I print_info: n_rot            = 32
0.00.048.729 I print_info: n_swa            = 0
0.00.048.729 I print_info: n_embd_head_k    = 128
0.00.048.730 I print_info: n_embd_head_v    = 128
0.00.048.730 I print_info: n_gqa            = 1
0.00.048.731 I print_info: n_embd_k_gqa     = 2048
0.00.048.732 I print_info: n_embd_v_gqa     = 2048
0.00.048.733 I print_info: f_norm_eps       = 1.0e-05
0.00.048.733 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.734 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.734 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.734 I print_info: f_logit_scale    = 0.0e+00
0.00.048.735 I print_info: n_ff             = 8192
0.00.048.735 I print_info: n_expert         = 0
0.00.048.735 I print_info: n_expert_used    = 0
0.00.048.735 I print_info: causal attn      = 1
0.00.048.735 I print_info: pooling type     = 0
0.00.048.735 I print_info: rope type        = 2
0.00.048.735 I print_info: rope scaling     = linear
0.00.048.736 I print_info: freq_base_train  = 10000.0
0.00.048.736 I print_info: freq_scale_train = 1
0.00.048.738 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.738 I print_info: rope_finetuned   = unknown
0.00.048.738 I print_info: ssm_d_conv       = 0
0.00.048.738 I print_info: ssm_d_inner      = 0
0.00.048.738 I print_info: ssm_d_state      = 0
0.00.048.738 I print_info: ssm_dt_rank      = 0
0.00.048.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.739 I print_info: model type       = 1.4B
0.00.048.740 I print_info: model params     = 1.41 B
0.00.048.740 I print_info: general.name     = 1.4B
0.00.048.740 I print_info: vocab type       = BPE
0.00.048.741 I print_info: n_vocab          = 50304
0.00.048.741 I print_info: n_merges         = 50009
0.00.048.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.741 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.742 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.745 I print_info: LF token         = 187 'Ċ'
0.00.048.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.745 I print_info: max token length = 1024
0.00.048.746 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.191.571 I load_tensors: offloading 24 repeating layers to GPU
0.01.191.577 I load_tensors: offloading output layer to GPU
0.01.191.578 I load_tensors: offloaded 25/25 layers to GPU
0.01.191.604 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.191.607 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.192.414 I llama_init_from_model: n_seq_max     = 1
0.01.192.416 I llama_init_from_model: n_ctx         = 2048
0.01.192.416 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.192.417 I llama_init_from_model: n_batch       = 2048
0.01.192.417 I llama_init_from_model: n_ubatch      = 512
0.01.192.417 I llama_init_from_model: flash_attn    = 0
0.01.192.418 I llama_init_from_model: freq_base     = 10000.0
0.01.192.419 I llama_init_from_model: freq_scale    = 1
0.01.192.420 I ggml_metal_init: allocating
0.01.192.428 I ggml_metal_init: found device: Apple M4
0.01.192.435 I ggml_metal_init: picking default device: Apple M4
0.01.193.738 I ggml_metal_init: using embedded metal library
0.01.199.220 I ggml_metal_init: GPU name:   Apple M4
0.01.199.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.199.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.199.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.199.225 I ggml_metal_init: simdgroup reduction   = true
0.01.199.225 I ggml_metal_init: simdgroup matrix mul. = true
0.01.199.226 I ggml_metal_init: has residency sets    = true
0.01.199.226 I ggml_metal_init: has bfloat            = true
0.01.199.226 I ggml_metal_init: use bfloat            = true
0.01.199.227 I ggml_metal_init: hasUnifiedMemory      = true
0.01.199.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.215.481 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.273.062 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.273.067 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.273.103 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.278.057 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.278.059 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.278.059 I llama_init_from_model: graph nodes  = 967
0.01.278.060 I llama_init_from_model: graph splits = 2
0.01.278.063 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.278.187 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.278.187 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.333.247 I main: llama threadpool init, n_threads = 4
0.01.333.301 I 
0.01.333.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.333.333 I 
0.01.333.503 I sampler seed: 1234
0.01.333.508 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.333.533 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.333.534 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.333.534 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.427.101 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.02.427.102 I llama_perf_context_print:        load time =    1322.70 ms
0.02.427.103 I llama_perf_context_print: prompt eval time =      49.21 ms /     7 tokens (    7.03 ms per token,   142.25 tokens per second)
0.02.427.107 I llama_perf_context_print:        eval time =    1041.57 ms /    63 runs   (   16.53 ms per token,    60.49 tokens per second)
0.02.427.107 I llama_perf_context_print:       total time =    1094.59 ms /    70 tokens
0.02.427.361 I ggml_metal_free: deallocating

real	0m2.446s
user	0m0.109s
sys	0m0.288s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.102 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.015.904 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.607 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.612 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.613 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.613 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.616 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.616 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.617 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.935 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.658 I llama_model_loader: - type  f32:  194 tensors
0.00.042.658 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.658 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.659 I print_info: file format = GGUF V3 (latest)
0.00.042.660 I print_info: file type   = Q4_0
0.00.042.660 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.052.581 I load: special tokens cache size = 25
0.00.060.572 I load: token to piece cache size = 0.2984 MB
0.00.060.576 I print_info: arch             = gptneox
0.00.060.576 I print_info: vocab_only       = 0
0.00.060.576 I print_info: n_ctx_train      = 2048
0.00.060.577 I print_info: n_embd           = 2048
0.00.060.577 I print_info: n_layer          = 24
0.00.060.581 I print_info: n_head           = 16
0.00.060.582 I print_info: n_head_kv        = 16
0.00.060.582 I print_info: n_rot            = 32
0.00.060.582 I print_info: n_swa            = 0
0.00.060.584 I print_info: n_embd_head_k    = 128
0.00.060.584 I print_info: n_embd_head_v    = 128
0.00.060.585 I print_info: n_gqa            = 1
0.00.060.586 I print_info: n_embd_k_gqa     = 2048
0.00.060.586 I print_info: n_embd_v_gqa     = 2048
0.00.060.587 I print_info: f_norm_eps       = 1.0e-05
0.00.060.587 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.588 I print_info: f_logit_scale    = 0.0e+00
0.00.060.589 I print_info: n_ff             = 8192
0.00.060.589 I print_info: n_expert         = 0
0.00.060.589 I print_info: n_expert_used    = 0
0.00.060.589 I print_info: causal attn      = 1
0.00.060.589 I print_info: pooling type     = 0
0.00.060.589 I print_info: rope type        = 2
0.00.060.590 I print_info: rope scaling     = linear
0.00.060.590 I print_info: freq_base_train  = 10000.0
0.00.060.591 I print_info: freq_scale_train = 1
0.00.060.591 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.591 I print_info: rope_finetuned   = unknown
0.00.060.591 I print_info: ssm_d_conv       = 0
0.00.060.592 I print_info: ssm_d_inner      = 0
0.00.060.594 I print_info: ssm_d_state      = 0
0.00.060.594 I print_info: ssm_dt_rank      = 0
0.00.060.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.594 I print_info: model type       = 1.4B
0.00.060.595 I print_info: model params     = 1.41 B
0.00.060.595 I print_info: general.name     = 1.4B
0.00.060.596 I print_info: vocab type       = BPE
0.00.060.597 I print_info: n_vocab          = 50304
0.00.060.597 I print_info: n_merges         = 50009
0.00.060.597 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.598 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.598 I print_info: LF token         = 187 'Ċ'
0.00.060.598 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.598 I print_info: max token length = 1024
0.00.060.599 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.936 I load_tensors: offloading output layer to GPU
0.00.649.937 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.972 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.649.973 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.651.624 I llama_init_from_model: n_seq_max     = 1
0.00.651.627 I llama_init_from_model: n_ctx         = 2048
0.00.651.627 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.651.628 I llama_init_from_model: n_batch       = 2048
0.00.651.629 I llama_init_from_model: n_ubatch      = 512
0.00.651.629 I llama_init_from_model: flash_attn    = 0
0.00.651.632 I llama_init_from_model: freq_base     = 10000.0
0.00.651.633 I llama_init_from_model: freq_scale    = 1
0.00.651.635 I ggml_metal_init: allocating
0.00.651.710 I ggml_metal_init: found device: Apple M4
0.00.651.723 I ggml_metal_init: picking default device: Apple M4
0.00.653.639 I ggml_metal_init: using embedded metal library
0.00.659.160 I ggml_metal_init: GPU name:   Apple M4
0.00.659.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.168 I ggml_metal_init: simdgroup reduction   = true
0.00.659.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.168 I ggml_metal_init: has residency sets    = true
0.00.659.168 I ggml_metal_init: has bfloat            = true
0.00.659.169 I ggml_metal_init: use bfloat            = true
0.00.659.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.802 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.317 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.327 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.372 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.415 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.416 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.417 I llama_init_from_model: graph nodes  = 967
0.00.742.417 I llama_init_from_model: graph splits = 2
0.00.742.426 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.274 I main: llama threadpool init, n_threads = 4
0.00.797.316 I 
0.00.797.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.340 I 
0.00.797.498 I sampler seed: 1234
0.00.797.503 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.514 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.514 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.472.896 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.472.897 I llama_perf_context_print:        load time =     780.64 ms
0.01.472.898 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.49 tokens per second)
0.01.472.898 I llama_perf_context_print:        eval time =     628.98 ms /    63 runs   (    9.98 ms per token,   100.16 tokens per second)
0.01.472.899 I llama_perf_context_print:       total time =     676.35 ms /    70 tokens
0.01.473.120 I ggml_metal_free: deallocating

real	0m1.497s
user	0m0.117s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.698 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.700 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.702 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.705 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.708 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.130 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.131 I llama_model_loader: - type  f32:  194 tensors
0.00.025.131 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.132 I print_info: file format = GGUF V3 (latest)
0.00.025.133 I print_info: file type   = Q4_1
0.00.025.134 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.996 I load: special tokens cache size = 25
0.00.039.031 I load: token to piece cache size = 0.2984 MB
0.00.039.034 I print_info: arch             = gptneox
0.00.039.035 I print_info: vocab_only       = 0
0.00.039.035 I print_info: n_ctx_train      = 2048
0.00.039.035 I print_info: n_embd           = 2048
0.00.039.035 I print_info: n_layer          = 24
0.00.039.038 I print_info: n_head           = 16
0.00.039.039 I print_info: n_head_kv        = 16
0.00.039.039 I print_info: n_rot            = 32
0.00.039.039 I print_info: n_swa            = 0
0.00.039.039 I print_info: n_embd_head_k    = 128
0.00.039.040 I print_info: n_embd_head_v    = 128
0.00.039.040 I print_info: n_gqa            = 1
0.00.039.041 I print_info: n_embd_k_gqa     = 2048
0.00.039.042 I print_info: n_embd_v_gqa     = 2048
0.00.039.042 I print_info: f_norm_eps       = 1.0e-05
0.00.039.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.044 I print_info: f_logit_scale    = 0.0e+00
0.00.039.044 I print_info: n_ff             = 8192
0.00.039.044 I print_info: n_expert         = 0
0.00.039.045 I print_info: n_expert_used    = 0
0.00.039.045 I print_info: causal attn      = 1
0.00.039.045 I print_info: pooling type     = 0
0.00.039.046 I print_info: rope type        = 2
0.00.039.048 I print_info: rope scaling     = linear
0.00.039.049 I print_info: freq_base_train  = 10000.0
0.00.039.049 I print_info: freq_scale_train = 1
0.00.039.049 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.049 I print_info: rope_finetuned   = unknown
0.00.039.050 I print_info: ssm_d_conv       = 0
0.00.039.050 I print_info: ssm_d_inner      = 0
0.00.039.050 I print_info: ssm_d_state      = 0
0.00.039.050 I print_info: ssm_dt_rank      = 0
0.00.039.050 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.050 I print_info: model type       = 1.4B
0.00.039.051 I print_info: model params     = 1.41 B
0.00.039.051 I print_info: general.name     = 1.4B
0.00.039.051 I print_info: vocab type       = BPE
0.00.039.051 I print_info: n_vocab          = 50304
0.00.039.052 I print_info: n_merges         = 50009
0.00.039.052 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.053 I print_info: LF token         = 187 'Ċ'
0.00.039.057 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.058 I print_info: max token length = 1024
0.00.039.058 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.659.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.659.508 I load_tensors: offloading output layer to GPU
0.00.659.508 I load_tensors: offloaded 25/25 layers to GPU
0.00.659.543 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.659.548 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.661.194 I llama_init_from_model: n_seq_max     = 1
0.00.661.197 I llama_init_from_model: n_ctx         = 2048
0.00.661.197 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.661.198 I llama_init_from_model: n_batch       = 2048
0.00.661.199 I llama_init_from_model: n_ubatch      = 512
0.00.661.199 I llama_init_from_model: flash_attn    = 0
0.00.661.201 I llama_init_from_model: freq_base     = 10000.0
0.00.661.202 I llama_init_from_model: freq_scale    = 1
0.00.661.210 I ggml_metal_init: allocating
0.00.661.273 I ggml_metal_init: found device: Apple M4
0.00.661.286 I ggml_metal_init: picking default device: Apple M4
0.00.663.181 I ggml_metal_init: using embedded metal library
0.00.668.732 I ggml_metal_init: GPU name:   Apple M4
0.00.668.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.739 I ggml_metal_init: simdgroup reduction   = true
0.00.668.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.740 I ggml_metal_init: has residency sets    = true
0.00.668.740 I ggml_metal_init: has bfloat            = true
0.00.668.740 I ggml_metal_init: use bfloat            = true
0.00.668.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.417 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.896 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.904 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.752.421 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.752.424 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.752.425 I llama_init_from_model: graph nodes  = 967
0.00.752.425 I llama_init_from_model: graph splits = 2
0.00.752.431 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.752.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.972 I main: llama threadpool init, n_threads = 4
0.00.807.018 I 
0.00.807.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.044 I 
0.00.807.214 I sampler seed: 1234
0.00.807.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.240 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.240 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.533.228 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.533.229 I llama_perf_context_print:        load time =     797.44 ms
0.01.533.230 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.33 tokens per second)
0.01.533.230 I llama_perf_context_print:        eval time =     674.41 ms /    63 runs   (   10.70 ms per token,    93.42 tokens per second)
0.01.533.231 I llama_perf_context_print:       total time =     726.98 ms /    70 tokens
0.01.533.466 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.110s
sys	0m0.234s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.172 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.766 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.778 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.779 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.779 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.780 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.780 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.781 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.328 I llama_model_loader: - type  f32:  194 tensors
0.00.027.329 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.329 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.330 I print_info: file format = GGUF V3 (latest)
0.00.027.330 I print_info: file type   = Q5_0
0.00.027.331 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.268 I load: special tokens cache size = 25
0.00.041.225 I load: token to piece cache size = 0.2984 MB
0.00.041.228 I print_info: arch             = gptneox
0.00.041.228 I print_info: vocab_only       = 0
0.00.041.228 I print_info: n_ctx_train      = 2048
0.00.041.229 I print_info: n_embd           = 2048
0.00.041.229 I print_info: n_layer          = 24
0.00.041.231 I print_info: n_head           = 16
0.00.041.232 I print_info: n_head_kv        = 16
0.00.041.232 I print_info: n_rot            = 32
0.00.041.233 I print_info: n_swa            = 0
0.00.041.233 I print_info: n_embd_head_k    = 128
0.00.041.233 I print_info: n_embd_head_v    = 128
0.00.041.235 I print_info: n_gqa            = 1
0.00.041.236 I print_info: n_embd_k_gqa     = 2048
0.00.041.237 I print_info: n_embd_v_gqa     = 2048
0.00.041.242 I print_info: f_norm_eps       = 1.0e-05
0.00.041.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.243 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.245 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.245 I print_info: f_logit_scale    = 0.0e+00
0.00.041.246 I print_info: n_ff             = 8192
0.00.041.246 I print_info: n_expert         = 0
0.00.041.246 I print_info: n_expert_used    = 0
0.00.041.246 I print_info: causal attn      = 1
0.00.041.246 I print_info: pooling type     = 0
0.00.041.247 I print_info: rope type        = 2
0.00.041.247 I print_info: rope scaling     = linear
0.00.041.247 I print_info: freq_base_train  = 10000.0
0.00.041.248 I print_info: freq_scale_train = 1
0.00.041.248 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.248 I print_info: rope_finetuned   = unknown
0.00.041.248 I print_info: ssm_d_conv       = 0
0.00.041.248 I print_info: ssm_d_inner      = 0
0.00.041.249 I print_info: ssm_d_state      = 0
0.00.041.249 I print_info: ssm_dt_rank      = 0
0.00.041.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.249 I print_info: model type       = 1.4B
0.00.041.250 I print_info: model params     = 1.41 B
0.00.041.250 I print_info: general.name     = 1.4B
0.00.041.256 I print_info: vocab type       = BPE
0.00.041.257 I print_info: n_vocab          = 50304
0.00.041.257 I print_info: n_merges         = 50009
0.00.041.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.258 I print_info: LF token         = 187 'Ċ'
0.00.041.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.259 I print_info: max token length = 1024
0.00.041.259 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.713.093 I load_tensors: offloading 24 repeating layers to GPU
0.00.713.108 I load_tensors: offloading output layer to GPU
0.00.713.109 I load_tensors: offloaded 25/25 layers to GPU
0.00.713.142 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.713.144 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.714.546 I llama_init_from_model: n_seq_max     = 1
0.00.714.549 I llama_init_from_model: n_ctx         = 2048
0.00.714.549 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.714.550 I llama_init_from_model: n_batch       = 2048
0.00.714.550 I llama_init_from_model: n_ubatch      = 512
0.00.714.550 I llama_init_from_model: flash_attn    = 0
0.00.714.552 I llama_init_from_model: freq_base     = 10000.0
0.00.714.553 I llama_init_from_model: freq_scale    = 1
0.00.714.556 I ggml_metal_init: allocating
0.00.714.630 I ggml_metal_init: found device: Apple M4
0.00.714.643 I ggml_metal_init: picking default device: Apple M4
0.00.716.492 I ggml_metal_init: using embedded metal library
0.00.722.916 I ggml_metal_init: GPU name:   Apple M4
0.00.722.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.722.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.722.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.722.922 I ggml_metal_init: simdgroup reduction   = true
0.00.722.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.722.922 I ggml_metal_init: has residency sets    = true
0.00.722.922 I ggml_metal_init: has bfloat            = true
0.00.722.923 I ggml_metal_init: use bfloat            = true
0.00.722.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.722.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.241 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.124 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.131 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.167 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.800.580 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.800.582 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.800.582 I llama_init_from_model: graph nodes  = 967
0.00.800.582 I llama_init_from_model: graph splits = 2
0.00.800.588 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.800.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.800.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.849.916 I main: llama threadpool init, n_threads = 4
0.00.849.959 I 
0.00.849.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.849.982 I 
0.00.850.098 I sampler seed: 1234
0.00.850.103 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.850.113 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.850.113 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.850.113 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.640.626 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.640.626 I llama_perf_context_print:        load time =     838.04 ms
0.01.640.627 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.53 tokens per second)
0.01.640.628 I llama_perf_context_print:        eval time =     744.78 ms /    63 runs   (   11.82 ms per token,    84.59 tokens per second)
0.01.640.630 I llama_perf_context_print:       total time =     791.41 ms /    70 tokens
0.01.640.877 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.108s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.725 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.734 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.736 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.737 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.737 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.740 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.424 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.425 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.425 I print_info: file format = GGUF V3 (latest)
0.00.025.426 I print_info: file type   = Q5_1
0.00.025.427 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.331 I load: special tokens cache size = 25
0.00.039.093 I load: token to piece cache size = 0.2984 MB
0.00.039.096 I print_info: arch             = gptneox
0.00.039.096 I print_info: vocab_only       = 0
0.00.039.096 I print_info: n_ctx_train      = 2048
0.00.039.096 I print_info: n_embd           = 2048
0.00.039.096 I print_info: n_layer          = 24
0.00.039.099 I print_info: n_head           = 16
0.00.039.100 I print_info: n_head_kv        = 16
0.00.039.100 I print_info: n_rot            = 32
0.00.039.100 I print_info: n_swa            = 0
0.00.039.101 I print_info: n_embd_head_k    = 128
0.00.039.102 I print_info: n_embd_head_v    = 128
0.00.039.103 I print_info: n_gqa            = 1
0.00.039.104 I print_info: n_embd_k_gqa     = 2048
0.00.039.104 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.110 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.111 I print_info: f_logit_scale    = 0.0e+00
0.00.039.111 I print_info: n_ff             = 8192
0.00.039.112 I print_info: n_expert         = 0
0.00.039.112 I print_info: n_expert_used    = 0
0.00.039.112 I print_info: causal attn      = 1
0.00.039.112 I print_info: pooling type     = 0
0.00.039.114 I print_info: rope type        = 2
0.00.039.116 I print_info: rope scaling     = linear
0.00.039.116 I print_info: freq_base_train  = 10000.0
0.00.039.116 I print_info: freq_scale_train = 1
0.00.039.117 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.117 I print_info: rope_finetuned   = unknown
0.00.039.117 I print_info: ssm_d_conv       = 0
0.00.039.117 I print_info: ssm_d_inner      = 0
0.00.039.117 I print_info: ssm_d_state      = 0
0.00.039.117 I print_info: ssm_dt_rank      = 0
0.00.039.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.118 I print_info: model type       = 1.4B
0.00.039.123 I print_info: model params     = 1.41 B
0.00.039.125 I print_info: general.name     = 1.4B
0.00.039.125 I print_info: vocab type       = BPE
0.00.039.126 I print_info: n_vocab          = 50304
0.00.039.126 I print_info: n_merges         = 50009
0.00.039.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: LF token         = 187 'Ċ'
0.00.039.127 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: max token length = 1024
0.00.039.128 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.421 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.436 I load_tensors: offloading output layer to GPU
0.00.647.436 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.472 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.647.473 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.649.077 I llama_init_from_model: n_seq_max     = 1
0.00.649.080 I llama_init_from_model: n_ctx         = 2048
0.00.649.081 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.649.081 I llama_init_from_model: n_batch       = 2048
0.00.649.082 I llama_init_from_model: n_ubatch      = 512
0.00.649.083 I llama_init_from_model: flash_attn    = 0
0.00.649.084 I llama_init_from_model: freq_base     = 10000.0
0.00.649.084 I llama_init_from_model: freq_scale    = 1
0.00.649.085 I ggml_metal_init: allocating
0.00.649.097 I ggml_metal_init: found device: Apple M4
0.00.649.104 I ggml_metal_init: picking default device: Apple M4
0.00.650.602 I ggml_metal_init: using embedded metal library
0.00.656.999 I ggml_metal_init: GPU name:   Apple M4
0.00.657.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.004 I ggml_metal_init: simdgroup reduction   = true
0.00.657.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.005 I ggml_metal_init: has residency sets    = true
0.00.657.005 I ggml_metal_init: has bfloat            = true
0.00.657.005 I ggml_metal_init: use bfloat            = true
0.00.657.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.261 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.730.636 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.730.642 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.730.684 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.735.129 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.735.132 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.735.132 I llama_init_from_model: graph nodes  = 967
0.00.735.132 I llama_init_from_model: graph splits = 2
0.00.735.139 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.735.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.735.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.721 I main: llama threadpool init, n_threads = 4
0.00.793.770 I 
0.00.793.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.794 I 
0.00.793.938 I sampler seed: 1234
0.00.793.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.954 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.955 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.650.207 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49442.90 tokens per second)
0.01.650.208 I llama_perf_context_print:        load time =     784.12 ms
0.01.650.209 I llama_perf_context_print: prompt eval time =      51.99 ms /     7 tokens (    7.43 ms per token,   134.65 tokens per second)
0.01.650.209 I llama_perf_context_print:        eval time =     801.37 ms /    63 runs   (   12.72 ms per token,    78.62 tokens per second)
0.01.650.209 I llama_perf_context_print:       total time =     857.21 ms /    70 tokens
0.01.650.502 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.013.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.021.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.642 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.211 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.211 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.212 I llama_model_loader: - type  f32:  194 tensors
0.00.030.212 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.212 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.213 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.213 I print_info: file format = GGUF V3 (latest)
0.00.030.214 I print_info: file type   = Q2_K - Medium
0.00.030.215 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.038.442 I load: special tokens cache size = 25
0.00.044.616 I load: token to piece cache size = 0.2984 MB
0.00.044.619 I print_info: arch             = gptneox
0.00.044.620 I print_info: vocab_only       = 0
0.00.044.620 I print_info: n_ctx_train      = 2048
0.00.044.620 I print_info: n_embd           = 2048
0.00.044.620 I print_info: n_layer          = 24
0.00.044.624 I print_info: n_head           = 16
0.00.044.625 I print_info: n_head_kv        = 16
0.00.044.625 I print_info: n_rot            = 32
0.00.044.625 I print_info: n_swa            = 0
0.00.044.626 I print_info: n_embd_head_k    = 128
0.00.044.626 I print_info: n_embd_head_v    = 128
0.00.044.629 I print_info: n_gqa            = 1
0.00.044.629 I print_info: n_embd_k_gqa     = 2048
0.00.044.630 I print_info: n_embd_v_gqa     = 2048
0.00.044.630 I print_info: f_norm_eps       = 1.0e-05
0.00.044.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.632 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.632 I print_info: f_logit_scale    = 0.0e+00
0.00.044.632 I print_info: n_ff             = 8192
0.00.044.632 I print_info: n_expert         = 0
0.00.044.633 I print_info: n_expert_used    = 0
0.00.044.633 I print_info: causal attn      = 1
0.00.044.633 I print_info: pooling type     = 0
0.00.044.633 I print_info: rope type        = 2
0.00.044.633 I print_info: rope scaling     = linear
0.00.044.634 I print_info: freq_base_train  = 10000.0
0.00.044.634 I print_info: freq_scale_train = 1
0.00.044.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.634 I print_info: rope_finetuned   = unknown
0.00.044.634 I print_info: ssm_d_conv       = 0
0.00.044.635 I print_info: ssm_d_inner      = 0
0.00.044.635 I print_info: ssm_d_state      = 0
0.00.044.635 I print_info: ssm_dt_rank      = 0
0.00.044.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.635 I print_info: model type       = 1.4B
0.00.044.636 I print_info: model params     = 1.41 B
0.00.044.636 I print_info: general.name     = 1.4B
0.00.044.636 I print_info: vocab type       = BPE
0.00.044.636 I print_info: n_vocab          = 50304
0.00.044.637 I print_info: n_merges         = 50009
0.00.044.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.639 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.639 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.640 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.640 I print_info: LF token         = 187 'Ċ'
0.00.044.640 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.640 I print_info: max token length = 1024
0.00.044.641 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.484 I load_tensors: offloading output layer to GPU
0.00.342.484 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.502 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.503 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.348 I llama_init_from_model: n_seq_max     = 1
0.00.343.352 I llama_init_from_model: n_ctx         = 2048
0.00.343.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.343.353 I llama_init_from_model: n_batch       = 2048
0.00.343.353 I llama_init_from_model: n_ubatch      = 512
0.00.343.353 I llama_init_from_model: flash_attn    = 0
0.00.343.355 I llama_init_from_model: freq_base     = 10000.0
0.00.343.355 I llama_init_from_model: freq_scale    = 1
0.00.343.356 I ggml_metal_init: allocating
0.00.343.393 I ggml_metal_init: found device: Apple M4
0.00.343.404 I ggml_metal_init: picking default device: Apple M4
0.00.344.476 I ggml_metal_init: using embedded metal library
0.00.348.716 I ggml_metal_init: GPU name:   Apple M4
0.00.348.722 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.723 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.724 I ggml_metal_init: simdgroup reduction   = true
0.00.348.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.724 I ggml_metal_init: has residency sets    = true
0.00.348.724 I ggml_metal_init: has bfloat            = true
0.00.348.725 I ggml_metal_init: use bfloat            = true
0.00.348.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.469 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.396.833 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.396.840 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.396.873 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.401.294 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.401.295 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.401.296 I llama_init_from_model: graph nodes  = 967
0.00.401.296 I llama_init_from_model: graph splits = 2
0.00.401.300 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.401.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.401.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.459.116 I main: llama threadpool init, n_threads = 4
0.00.459.162 I 
0.00.459.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.459.188 I 
0.00.459.372 I sampler seed: 1234
0.00.459.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.459.418 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.459.422 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.459.422 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.571 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.120.571 I llama_perf_context_print:        load time =     444.38 ms
0.01.120.574 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.62 tokens per second)
0.01.120.575 I llama_perf_context_print:        eval time =     623.07 ms /    63 runs   (    9.89 ms per token,   101.11 tokens per second)
0.01.120.575 I llama_perf_context_print:       total time =     662.24 ms /    70 tokens
0.01.120.827 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.108s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.076 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.077 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.077 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.078 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.080 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.081 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.084 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.084 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.914 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.917 I llama_model_loader: - type  f32:  194 tensors
0.00.025.917 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.917 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.918 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.918 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.919 I print_info: file format = GGUF V3 (latest)
0.00.025.923 I print_info: file type   = Q3_K - Medium
0.00.025.924 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.253 I load: special tokens cache size = 25
0.00.040.273 I load: token to piece cache size = 0.2984 MB
0.00.040.276 I print_info: arch             = gptneox
0.00.040.276 I print_info: vocab_only       = 0
0.00.040.276 I print_info: n_ctx_train      = 2048
0.00.040.276 I print_info: n_embd           = 2048
0.00.040.277 I print_info: n_layer          = 24
0.00.040.279 I print_info: n_head           = 16
0.00.040.280 I print_info: n_head_kv        = 16
0.00.040.280 I print_info: n_rot            = 32
0.00.040.280 I print_info: n_swa            = 0
0.00.040.282 I print_info: n_embd_head_k    = 128
0.00.040.282 I print_info: n_embd_head_v    = 128
0.00.040.283 I print_info: n_gqa            = 1
0.00.040.284 I print_info: n_embd_k_gqa     = 2048
0.00.040.284 I print_info: n_embd_v_gqa     = 2048
0.00.040.285 I print_info: f_norm_eps       = 1.0e-05
0.00.040.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.286 I print_info: f_logit_scale    = 0.0e+00
0.00.040.287 I print_info: n_ff             = 8192
0.00.040.287 I print_info: n_expert         = 0
0.00.040.287 I print_info: n_expert_used    = 0
0.00.040.287 I print_info: causal attn      = 1
0.00.040.287 I print_info: pooling type     = 0
0.00.040.287 I print_info: rope type        = 2
0.00.040.288 I print_info: rope scaling     = linear
0.00.040.288 I print_info: freq_base_train  = 10000.0
0.00.040.288 I print_info: freq_scale_train = 1
0.00.040.289 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.289 I print_info: rope_finetuned   = unknown
0.00.040.289 I print_info: ssm_d_conv       = 0
0.00.040.292 I print_info: ssm_d_inner      = 0
0.00.040.292 I print_info: ssm_d_state      = 0
0.00.040.292 I print_info: ssm_dt_rank      = 0
0.00.040.292 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.293 I print_info: model type       = 1.4B
0.00.040.293 I print_info: model params     = 1.41 B
0.00.040.293 I print_info: general.name     = 1.4B
0.00.040.294 I print_info: vocab type       = BPE
0.00.040.294 I print_info: n_vocab          = 50304
0.00.040.294 I print_info: n_merges         = 50009
0.00.040.294 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.294 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: LF token         = 187 'Ċ'
0.00.040.295 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.295 I print_info: max token length = 1024
0.00.040.296 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.460.577 I load_tensors: offloading 24 repeating layers to GPU
0.00.460.590 I load_tensors: offloading output layer to GPU
0.00.460.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.460.624 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.460.626 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.462.170 I llama_init_from_model: n_seq_max     = 1
0.00.462.175 I llama_init_from_model: n_ctx         = 2048
0.00.462.175 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.462.176 I llama_init_from_model: n_batch       = 2048
0.00.462.176 I llama_init_from_model: n_ubatch      = 512
0.00.462.176 I llama_init_from_model: flash_attn    = 0
0.00.462.179 I llama_init_from_model: freq_base     = 10000.0
0.00.462.179 I llama_init_from_model: freq_scale    = 1
0.00.462.181 I ggml_metal_init: allocating
0.00.462.267 I ggml_metal_init: found device: Apple M4
0.00.462.282 I ggml_metal_init: picking default device: Apple M4
0.00.464.120 I ggml_metal_init: using embedded metal library
0.00.469.689 I ggml_metal_init: GPU name:   Apple M4
0.00.469.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.469.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.469.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.469.701 I ggml_metal_init: simdgroup reduction   = true
0.00.469.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.469.701 I ggml_metal_init: has residency sets    = true
0.00.469.702 I ggml_metal_init: has bfloat            = true
0.00.469.702 I ggml_metal_init: use bfloat            = true
0.00.469.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.469.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.489.904 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.535 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.546.545 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.546.592 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.551.014 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.551.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.551.016 I llama_init_from_model: graph nodes  = 967
0.00.551.017 I llama_init_from_model: graph splits = 2
0.00.551.022 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.551.150 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.551.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.568 I main: llama threadpool init, n_threads = 4
0.00.608.614 I 
0.00.608.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.637 I 
0.00.608.816 I sampler seed: 1234
0.00.608.821 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.608.841 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.608.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.608.842 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.351.033 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.351.034 I llama_perf_context_print:        load time =     598.76 ms
0.01.351.035 I llama_perf_context_print: prompt eval time =      45.91 ms /     7 tokens (    6.56 ms per token,   152.46 tokens per second)
0.01.351.035 I llama_perf_context_print:        eval time =     693.45 ms /    63 runs   (   11.01 ms per token,    90.85 tokens per second)
0.01.351.036 I llama_perf_context_print:       total time =     743.23 ms /    70 tokens
0.01.351.272 I ggml_metal_free: deallocating

real	0m1.371s
user	0m0.112s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.227 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.806 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.807 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.807 I llama_model_loader: - type  f32:  194 tensors
0.00.024.808 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.808 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.808 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.809 I print_info: file format = GGUF V3 (latest)
0.00.024.809 I print_info: file type   = Q4_K - Medium
0.00.024.810 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.154 I load: special tokens cache size = 25
0.00.039.132 I load: token to piece cache size = 0.2984 MB
0.00.039.135 I print_info: arch             = gptneox
0.00.039.135 I print_info: vocab_only       = 0
0.00.039.135 I print_info: n_ctx_train      = 2048
0.00.039.135 I print_info: n_embd           = 2048
0.00.039.136 I print_info: n_layer          = 24
0.00.039.138 I print_info: n_head           = 16
0.00.039.139 I print_info: n_head_kv        = 16
0.00.039.139 I print_info: n_rot            = 32
0.00.039.139 I print_info: n_swa            = 0
0.00.039.142 I print_info: n_embd_head_k    = 128
0.00.039.142 I print_info: n_embd_head_v    = 128
0.00.039.143 I print_info: n_gqa            = 1
0.00.039.143 I print_info: n_embd_k_gqa     = 2048
0.00.039.144 I print_info: n_embd_v_gqa     = 2048
0.00.039.144 I print_info: f_norm_eps       = 1.0e-05
0.00.039.145 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.145 I print_info: f_logit_scale    = 0.0e+00
0.00.039.146 I print_info: n_ff             = 8192
0.00.039.146 I print_info: n_expert         = 0
0.00.039.146 I print_info: n_expert_used    = 0
0.00.039.146 I print_info: causal attn      = 1
0.00.039.146 I print_info: pooling type     = 0
0.00.039.147 I print_info: rope type        = 2
0.00.039.147 I print_info: rope scaling     = linear
0.00.039.152 I print_info: freq_base_train  = 10000.0
0.00.039.155 I print_info: freq_scale_train = 1
0.00.039.155 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.156 I print_info: rope_finetuned   = unknown
0.00.039.156 I print_info: ssm_d_conv       = 0
0.00.039.156 I print_info: ssm_d_inner      = 0
0.00.039.156 I print_info: ssm_d_state      = 0
0.00.039.157 I print_info: ssm_dt_rank      = 0
0.00.039.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.158 I print_info: model type       = 1.4B
0.00.039.158 I print_info: model params     = 1.41 B
0.00.039.158 I print_info: general.name     = 1.4B
0.00.039.159 I print_info: vocab type       = BPE
0.00.039.159 I print_info: n_vocab          = 50304
0.00.039.159 I print_info: n_merges         = 50009
0.00.039.159 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: LF token         = 187 'Ċ'
0.00.039.163 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.163 I print_info: max token length = 1024
0.00.039.164 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.535.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.535.600 I load_tensors: offloading output layer to GPU
0.00.535.601 I load_tensors: offloaded 25/25 layers to GPU
0.00.535.634 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.535.635 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.537.206 I llama_init_from_model: n_seq_max     = 1
0.00.537.208 I llama_init_from_model: n_ctx         = 2048
0.00.537.209 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.537.209 I llama_init_from_model: n_batch       = 2048
0.00.537.210 I llama_init_from_model: n_ubatch      = 512
0.00.537.210 I llama_init_from_model: flash_attn    = 0
0.00.537.212 I llama_init_from_model: freq_base     = 10000.0
0.00.537.213 I llama_init_from_model: freq_scale    = 1
0.00.537.218 I ggml_metal_init: allocating
0.00.537.297 I ggml_metal_init: found device: Apple M4
0.00.537.310 I ggml_metal_init: picking default device: Apple M4
0.00.539.247 I ggml_metal_init: using embedded metal library
0.00.545.811 I ggml_metal_init: GPU name:   Apple M4
0.00.545.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.545.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.545.816 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.545.817 I ggml_metal_init: simdgroup reduction   = true
0.00.545.817 I ggml_metal_init: simdgroup matrix mul. = true
0.00.545.817 I ggml_metal_init: has residency sets    = true
0.00.545.818 I ggml_metal_init: has bfloat            = true
0.00.545.818 I ggml_metal_init: use bfloat            = true
0.00.545.819 I ggml_metal_init: hasUnifiedMemory      = true
0.00.545.820 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.564.029 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.621.190 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.621.229 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.427 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.625.429 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.625.430 I llama_init_from_model: graph nodes  = 967
0.00.625.430 I llama_init_from_model: graph splits = 2
0.00.625.435 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.625.549 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.625.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.825 I main: llama threadpool init, n_threads = 4
0.00.681.869 I 
0.00.681.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.894 I 
0.00.682.073 I sampler seed: 1234
0.00.682.078 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.682.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.682.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.682.089 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.441.447 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.441.448 I llama_perf_context_print:        load time =     671.85 ms
0.01.441.449 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.20 tokens per second)
0.01.441.451 I llama_perf_context_print:        eval time =     709.39 ms /    63 runs   (   11.26 ms per token,    88.81 tokens per second)
0.01.441.453 I llama_perf_context_print:       total time =     760.37 ms /    70 tokens
0.01.441.702 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.111s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.480 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.068 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.069 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.069 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.532 I llama_model_loader: - type  f32:  194 tensors
0.00.026.532 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.533 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.533 I print_info: file format = GGUF V3 (latest)
0.00.026.534 I print_info: file type   = Q5_K - Medium
0.00.026.534 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.520 I load: special tokens cache size = 25
0.00.040.562 I load: token to piece cache size = 0.2984 MB
0.00.040.565 I print_info: arch             = gptneox
0.00.040.565 I print_info: vocab_only       = 0
0.00.040.566 I print_info: n_ctx_train      = 2048
0.00.040.566 I print_info: n_embd           = 2048
0.00.040.566 I print_info: n_layer          = 24
0.00.040.568 I print_info: n_head           = 16
0.00.040.569 I print_info: n_head_kv        = 16
0.00.040.569 I print_info: n_rot            = 32
0.00.040.569 I print_info: n_swa            = 0
0.00.040.571 I print_info: n_embd_head_k    = 128
0.00.040.571 I print_info: n_embd_head_v    = 128
0.00.040.572 I print_info: n_gqa            = 1
0.00.040.573 I print_info: n_embd_k_gqa     = 2048
0.00.040.574 I print_info: n_embd_v_gqa     = 2048
0.00.040.574 I print_info: f_norm_eps       = 1.0e-05
0.00.040.575 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.575 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.575 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.575 I print_info: f_logit_scale    = 0.0e+00
0.00.040.576 I print_info: n_ff             = 8192
0.00.040.576 I print_info: n_expert         = 0
0.00.040.576 I print_info: n_expert_used    = 0
0.00.040.578 I print_info: causal attn      = 1
0.00.040.578 I print_info: pooling type     = 0
0.00.040.578 I print_info: rope type        = 2
0.00.040.578 I print_info: rope scaling     = linear
0.00.040.579 I print_info: freq_base_train  = 10000.0
0.00.040.579 I print_info: freq_scale_train = 1
0.00.040.579 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.580 I print_info: rope_finetuned   = unknown
0.00.040.580 I print_info: ssm_d_conv       = 0
0.00.040.580 I print_info: ssm_d_inner      = 0
0.00.040.580 I print_info: ssm_d_state      = 0
0.00.040.580 I print_info: ssm_dt_rank      = 0
0.00.040.580 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.581 I print_info: model type       = 1.4B
0.00.040.581 I print_info: model params     = 1.41 B
0.00.040.581 I print_info: general.name     = 1.4B
0.00.040.582 I print_info: vocab type       = BPE
0.00.040.582 I print_info: n_vocab          = 50304
0.00.040.582 I print_info: n_merges         = 50009
0.00.040.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: LF token         = 187 'Ċ'
0.00.040.584 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.584 I print_info: max token length = 1024
0.00.040.586 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.889 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.893 I load_tensors: offloading output layer to GPU
0.00.628.893 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.918 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.628.921 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.630.297 I llama_init_from_model: n_seq_max     = 1
0.00.630.299 I llama_init_from_model: n_ctx         = 2048
0.00.630.300 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.630.300 I llama_init_from_model: n_batch       = 2048
0.00.630.301 I llama_init_from_model: n_ubatch      = 512
0.00.630.301 I llama_init_from_model: flash_attn    = 0
0.00.630.302 I llama_init_from_model: freq_base     = 10000.0
0.00.630.303 I llama_init_from_model: freq_scale    = 1
0.00.630.304 I ggml_metal_init: allocating
0.00.630.329 I ggml_metal_init: found device: Apple M4
0.00.630.338 I ggml_metal_init: picking default device: Apple M4
0.00.631.915 I ggml_metal_init: using embedded metal library
0.00.638.029 I ggml_metal_init: GPU name:   Apple M4
0.00.638.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.034 I ggml_metal_init: simdgroup reduction   = true
0.00.638.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.034 I ggml_metal_init: has residency sets    = true
0.00.638.035 I ggml_metal_init: has bfloat            = true
0.00.638.035 I ggml_metal_init: use bfloat            = true
0.00.638.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.067 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.006 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.016 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.052 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.131 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.714.133 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.714.133 I llama_init_from_model: graph nodes  = 967
0.00.714.133 I llama_init_from_model: graph splits = 2
0.00.714.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.714.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.258 I main: llama threadpool init, n_threads = 4
0.00.775.297 I 
0.00.775.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.321 I 
0.00.775.497 I sampler seed: 1234
0.00.775.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.548 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.551 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.551 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.618.215 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.618.216 I llama_perf_context_print:        load time =     763.05 ms
0.01.618.216 I llama_perf_context_print: prompt eval time =      52.91 ms /     7 tokens (    7.56 ms per token,   132.29 tokens per second)
0.01.618.217 I llama_perf_context_print:        eval time =     786.81 ms /    63 runs   (   12.49 ms per token,    80.07 tokens per second)
0.01.618.217 I llama_perf_context_print:       total time =     843.68 ms /    70 tokens
0.01.618.444 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.109s
sys	0m0.235s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.404 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.404 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.012 I llama_model_loader: - type  f32:  194 tensors
0.00.024.013 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.013 I print_info: file format = GGUF V3 (latest)
0.00.024.014 I print_info: file type   = Q6_K
0.00.024.014 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.940 I load: special tokens cache size = 25
0.00.038.034 I load: token to piece cache size = 0.2984 MB
0.00.038.037 I print_info: arch             = gptneox
0.00.038.037 I print_info: vocab_only       = 0
0.00.038.038 I print_info: n_ctx_train      = 2048
0.00.038.038 I print_info: n_embd           = 2048
0.00.038.038 I print_info: n_layer          = 24
0.00.038.041 I print_info: n_head           = 16
0.00.038.041 I print_info: n_head_kv        = 16
0.00.038.041 I print_info: n_rot            = 32
0.00.038.042 I print_info: n_swa            = 0
0.00.038.043 I print_info: n_embd_head_k    = 128
0.00.038.043 I print_info: n_embd_head_v    = 128
0.00.038.044 I print_info: n_gqa            = 1
0.00.038.045 I print_info: n_embd_k_gqa     = 2048
0.00.038.045 I print_info: n_embd_v_gqa     = 2048
0.00.038.046 I print_info: f_norm_eps       = 1.0e-05
0.00.038.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.048 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.048 I print_info: f_logit_scale    = 0.0e+00
0.00.038.049 I print_info: n_ff             = 8192
0.00.038.049 I print_info: n_expert         = 0
0.00.038.049 I print_info: n_expert_used    = 0
0.00.038.049 I print_info: causal attn      = 1
0.00.038.050 I print_info: pooling type     = 0
0.00.038.050 I print_info: rope type        = 2
0.00.038.050 I print_info: rope scaling     = linear
0.00.038.051 I print_info: freq_base_train  = 10000.0
0.00.038.051 I print_info: freq_scale_train = 1
0.00.038.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.051 I print_info: rope_finetuned   = unknown
0.00.038.051 I print_info: ssm_d_conv       = 0
0.00.038.051 I print_info: ssm_d_inner      = 0
0.00.038.052 I print_info: ssm_d_state      = 0
0.00.038.053 I print_info: ssm_dt_rank      = 0
0.00.038.053 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.053 I print_info: model type       = 1.4B
0.00.038.054 I print_info: model params     = 1.41 B
0.00.038.054 I print_info: general.name     = 1.4B
0.00.038.054 I print_info: vocab type       = BPE
0.00.038.055 I print_info: n_vocab          = 50304
0.00.038.055 I print_info: n_merges         = 50009
0.00.038.055 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.055 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.055 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.056 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.056 I print_info: LF token         = 187 'Ċ'
0.00.038.056 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.056 I print_info: max token length = 1024
0.00.038.057 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.272 I load_tensors: offloading output layer to GPU
0.00.653.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.653.311 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.653.334 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.655.029 I llama_init_from_model: n_seq_max     = 1
0.00.655.039 I llama_init_from_model: n_ctx         = 2048
0.00.655.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.040 I llama_init_from_model: n_batch       = 2048
0.00.655.040 I llama_init_from_model: n_ubatch      = 512
0.00.655.040 I llama_init_from_model: flash_attn    = 0
0.00.655.042 I llama_init_from_model: freq_base     = 10000.0
0.00.655.042 I llama_init_from_model: freq_scale    = 1
0.00.655.047 I ggml_metal_init: allocating
0.00.655.110 I ggml_metal_init: found device: Apple M4
0.00.655.124 I ggml_metal_init: picking default device: Apple M4
0.00.656.748 I ggml_metal_init: using embedded metal library
0.00.663.277 I ggml_metal_init: GPU name:   Apple M4
0.00.663.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.282 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.282 I ggml_metal_init: simdgroup reduction   = true
0.00.663.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.283 I ggml_metal_init: has residency sets    = true
0.00.663.283 I ggml_metal_init: has bfloat            = true
0.00.663.283 I ggml_metal_init: use bfloat            = true
0.00.663.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.194 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.192 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.198 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.231 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.063 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.065 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.065 I llama_init_from_model: graph nodes  = 967
0.00.748.066 I llama_init_from_model: graph splits = 2
0.00.748.071 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.789 I main: llama threadpool init, n_threads = 4
0.00.818.838 I 
0.00.818.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.862 I 
0.00.819.035 I sampler seed: 1234
0.00.819.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.060 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.060 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.060 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.693.526 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.693.527 I llama_perf_context_print:        load time =     809.28 ms
0.01.693.528 I llama_perf_context_print: prompt eval time =      57.58 ms /     7 tokens (    8.22 ms per token,   121.58 tokens per second)
0.01.693.529 I llama_perf_context_print:        eval time =     813.97 ms /    63 runs   (   12.92 ms per token,    77.40 tokens per second)
0.01.693.530 I llama_perf_context_print:       total time =     875.46 ms /    70 tokens
0.01.693.798 I ggml_metal_free: deallocating

real	0m1.709s
user	0m0.109s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.551 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.200 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.218 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.219 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.219 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.600 I llama_model_loader: - type  f32:  194 tensors
0.00.054.601 I llama_model_loader: - type  f16:   98 tensors
0.00.054.602 I print_info: file format = GGUF V3 (latest)
0.00.054.603 I print_info: file type   = all F32 (guessed)
0.00.054.604 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.066.739 I load: special tokens cache size = 25
0.00.074.786 I load: token to piece cache size = 0.2984 MB
0.00.074.790 I print_info: arch             = gptneox
0.00.074.790 I print_info: vocab_only       = 0
0.00.074.790 I print_info: n_ctx_train      = 2048
0.00.074.790 I print_info: n_embd           = 2048
0.00.074.790 I print_info: n_layer          = 24
0.00.074.793 I print_info: n_head           = 16
0.00.074.794 I print_info: n_head_kv        = 16
0.00.074.794 I print_info: n_rot            = 32
0.00.074.795 I print_info: n_swa            = 0
0.00.074.795 I print_info: n_embd_head_k    = 128
0.00.074.795 I print_info: n_embd_head_v    = 128
0.00.074.796 I print_info: n_gqa            = 1
0.00.074.797 I print_info: n_embd_k_gqa     = 2048
0.00.074.797 I print_info: n_embd_v_gqa     = 2048
0.00.074.799 I print_info: f_norm_eps       = 1.0e-05
0.00.074.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.074.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.074.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.074.800 I print_info: f_logit_scale    = 0.0e+00
0.00.074.802 I print_info: n_ff             = 8192
0.00.074.803 I print_info: n_expert         = 0
0.00.074.803 I print_info: n_expert_used    = 0
0.00.074.803 I print_info: causal attn      = 1
0.00.074.803 I print_info: pooling type     = 0
0.00.074.803 I print_info: rope type        = 2
0.00.074.803 I print_info: rope scaling     = linear
0.00.074.804 I print_info: freq_base_train  = 10000.0
0.00.074.805 I print_info: freq_scale_train = 1
0.00.074.805 I print_info: n_ctx_orig_yarn  = 2048
0.00.074.806 I print_info: rope_finetuned   = unknown
0.00.074.806 I print_info: ssm_d_conv       = 0
0.00.074.806 I print_info: ssm_d_inner      = 0
0.00.074.806 I print_info: ssm_d_state      = 0
0.00.074.806 I print_info: ssm_dt_rank      = 0
0.00.074.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.074.807 I print_info: model type       = 1.4B
0.00.074.807 I print_info: model params     = 1.41 B
0.00.074.807 I print_info: general.name     = 1.4B
0.00.074.811 I print_info: vocab type       = BPE
0.00.074.812 I print_info: n_vocab          = 50304
0.00.074.812 I print_info: n_merges         = 50009
0.00.074.812 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.074.812 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.074.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.074.813 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.074.813 I print_info: LF token         = 187 'Ċ'
0.00.074.814 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.074.814 I print_info: max token length = 1024
0.00.074.816 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.245.749 I load_tensors: offloading 24 repeating layers to GPU
0.01.245.755 I load_tensors: offloading output layer to GPU
0.01.245.755 I load_tensors: offloaded 25/25 layers to GPU
0.01.245.782 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.245.783 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.246.797 I llama_init_from_model: n_seq_max     = 1
0.01.246.798 I llama_init_from_model: n_ctx         = 128
0.01.246.798 I llama_init_from_model: n_ctx_per_seq = 128
0.01.246.798 I llama_init_from_model: n_batch       = 128
0.01.246.799 I llama_init_from_model: n_ubatch      = 128
0.01.246.799 I llama_init_from_model: flash_attn    = 0
0.01.246.800 I llama_init_from_model: freq_base     = 10000.0
0.01.246.800 I llama_init_from_model: freq_scale    = 1
0.01.246.800 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.246.801 I ggml_metal_init: allocating
0.01.246.897 I ggml_metal_init: found device: Apple M4
0.01.246.909 I ggml_metal_init: picking default device: Apple M4
0.01.248.121 I ggml_metal_init: using embedded metal library
0.01.252.019 I ggml_metal_init: GPU name:   Apple M4
0.01.252.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.252.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.252.024 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.252.024 I ggml_metal_init: simdgroup reduction   = true
0.01.252.024 I ggml_metal_init: simdgroup matrix mul. = true
0.01.252.024 I ggml_metal_init: has residency sets    = true
0.01.252.024 I ggml_metal_init: has bfloat            = true
0.01.252.024 I ggml_metal_init: use bfloat            = true
0.01.252.025 I ggml_metal_init: hasUnifiedMemory      = true
0.01.252.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.262.713 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.264.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.264.451 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.264.475 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.266.139 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.266.140 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.266.140 I llama_init_from_model: graph nodes  = 967
0.01.266.141 I llama_init_from_model: graph splits = 2
0.01.266.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.266.142 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.301.592 I 
0.01.301.631 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.301.635 I perplexity: tokenizing the input ..
0.01.306.736 I perplexity: tokenization took 5.099 ms
0.01.306.740 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.425.201 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.426.543 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.426.578 I llama_perf_context_print:        load time =    1277.74 ms
0.01.426.579 I llama_perf_context_print: prompt eval time =     118.15 ms /   128 tokens (    0.92 ms per token,  1083.36 tokens per second)
0.01.426.580 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.426.580 I llama_perf_context_print:       total time =     124.99 ms /   129 tokens
0.01.426.959 I ggml_metal_free: deallocating

real	0m1.618s
user	0m0.097s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.510 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.511 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.511 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.264 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.265 I llama_model_loader: - type  f32:  194 tensors
0.00.025.265 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.266 I print_info: file format = GGUF V3 (latest)
0.00.025.267 I print_info: file type   = Q8_0
0.00.025.268 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.377 I load: special tokens cache size = 25
0.00.039.424 I load: token to piece cache size = 0.2984 MB
0.00.039.428 I print_info: arch             = gptneox
0.00.039.428 I print_info: vocab_only       = 0
0.00.039.428 I print_info: n_ctx_train      = 2048
0.00.039.429 I print_info: n_embd           = 2048
0.00.039.429 I print_info: n_layer          = 24
0.00.039.433 I print_info: n_head           = 16
0.00.039.434 I print_info: n_head_kv        = 16
0.00.039.434 I print_info: n_rot            = 32
0.00.039.434 I print_info: n_swa            = 0
0.00.039.434 I print_info: n_embd_head_k    = 128
0.00.039.434 I print_info: n_embd_head_v    = 128
0.00.039.435 I print_info: n_gqa            = 1
0.00.039.436 I print_info: n_embd_k_gqa     = 2048
0.00.039.439 I print_info: n_embd_v_gqa     = 2048
0.00.039.440 I print_info: f_norm_eps       = 1.0e-05
0.00.039.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.440 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.440 I print_info: f_logit_scale    = 0.0e+00
0.00.039.441 I print_info: n_ff             = 8192
0.00.039.441 I print_info: n_expert         = 0
0.00.039.441 I print_info: n_expert_used    = 0
0.00.039.442 I print_info: causal attn      = 1
0.00.039.442 I print_info: pooling type     = 0
0.00.039.442 I print_info: rope type        = 2
0.00.039.442 I print_info: rope scaling     = linear
0.00.039.443 I print_info: freq_base_train  = 10000.0
0.00.039.443 I print_info: freq_scale_train = 1
0.00.039.444 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.444 I print_info: rope_finetuned   = unknown
0.00.039.444 I print_info: ssm_d_conv       = 0
0.00.039.444 I print_info: ssm_d_inner      = 0
0.00.039.444 I print_info: ssm_d_state      = 0
0.00.039.444 I print_info: ssm_dt_rank      = 0
0.00.039.445 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.445 I print_info: model type       = 1.4B
0.00.039.445 I print_info: model params     = 1.41 B
0.00.039.445 I print_info: general.name     = 1.4B
0.00.039.446 I print_info: vocab type       = BPE
0.00.039.446 I print_info: n_vocab          = 50304
0.00.039.446 I print_info: n_merges         = 50009
0.00.039.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: LF token         = 187 'Ċ'
0.00.039.447 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: max token length = 1024
0.00.039.448 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.900.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.900.540 I load_tensors: offloading output layer to GPU
0.00.900.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.900.580 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.900.582 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.902.119 I llama_init_from_model: n_seq_max     = 1
0.00.902.121 I llama_init_from_model: n_ctx         = 128
0.00.902.122 I llama_init_from_model: n_ctx_per_seq = 128
0.00.902.122 I llama_init_from_model: n_batch       = 128
0.00.902.122 I llama_init_from_model: n_ubatch      = 128
0.00.902.123 I llama_init_from_model: flash_attn    = 0
0.00.902.124 I llama_init_from_model: freq_base     = 10000.0
0.00.902.124 I llama_init_from_model: freq_scale    = 1
0.00.902.125 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.902.126 I ggml_metal_init: allocating
0.00.902.220 I ggml_metal_init: found device: Apple M4
0.00.902.232 I ggml_metal_init: picking default device: Apple M4
0.00.903.742 I ggml_metal_init: using embedded metal library
0.00.909.111 I ggml_metal_init: GPU name:   Apple M4
0.00.909.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.909.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.909.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.909.116 I ggml_metal_init: simdgroup reduction   = true
0.00.909.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.909.117 I ggml_metal_init: has residency sets    = true
0.00.909.117 I ggml_metal_init: has bfloat            = true
0.00.909.117 I ggml_metal_init: use bfloat            = true
0.00.909.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.909.123 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.924.540 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.927.968 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.927.972 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.928.017 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.931.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.931.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.931.090 I llama_init_from_model: graph nodes  = 967
0.00.931.090 I llama_init_from_model: graph splits = 2
0.00.931.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.931.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.956.190 I 
0.00.956.268 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.956.277 I perplexity: tokenizing the input ..
0.00.963.469 I perplexity: tokenization took 7.189 ms
0.00.963.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.088.932 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.090.248 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.090.271 I llama_perf_context_print:        load time =     946.99 ms
0.01.090.272 I llama_perf_context_print: prompt eval time =     124.53 ms /   128 tokens (    0.97 ms per token,  1027.84 tokens per second)
0.01.090.273 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.273 I llama_perf_context_print:       total time =     134.09 ms /   129 tokens
0.01.090.623 I ggml_metal_free: deallocating

real	0m1.105s
user	0m0.077s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.586 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.591 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.369 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.196 I llama_model_loader: - type  f32:  194 tensors
0.00.026.197 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.197 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.198 I print_info: file format = GGUF V3 (latest)
0.00.026.198 I print_info: file type   = Q4_0
0.00.026.200 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.978 I load: special tokens cache size = 25
0.00.041.125 I load: token to piece cache size = 0.2984 MB
0.00.041.129 I print_info: arch             = gptneox
0.00.041.130 I print_info: vocab_only       = 0
0.00.041.130 I print_info: n_ctx_train      = 2048
0.00.041.130 I print_info: n_embd           = 2048
0.00.041.130 I print_info: n_layer          = 24
0.00.041.135 I print_info: n_head           = 16
0.00.041.135 I print_info: n_head_kv        = 16
0.00.041.136 I print_info: n_rot            = 32
0.00.041.136 I print_info: n_swa            = 0
0.00.041.136 I print_info: n_embd_head_k    = 128
0.00.041.136 I print_info: n_embd_head_v    = 128
0.00.041.137 I print_info: n_gqa            = 1
0.00.041.138 I print_info: n_embd_k_gqa     = 2048
0.00.041.138 I print_info: n_embd_v_gqa     = 2048
0.00.041.139 I print_info: f_norm_eps       = 1.0e-05
0.00.041.139 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.139 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.139 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.140 I print_info: f_logit_scale    = 0.0e+00
0.00.041.140 I print_info: n_ff             = 8192
0.00.041.140 I print_info: n_expert         = 0
0.00.041.140 I print_info: n_expert_used    = 0
0.00.041.141 I print_info: causal attn      = 1
0.00.041.141 I print_info: pooling type     = 0
0.00.041.141 I print_info: rope type        = 2
0.00.041.141 I print_info: rope scaling     = linear
0.00.041.141 I print_info: freq_base_train  = 10000.0
0.00.041.141 I print_info: freq_scale_train = 1
0.00.041.142 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.142 I print_info: rope_finetuned   = unknown
0.00.041.142 I print_info: ssm_d_conv       = 0
0.00.041.142 I print_info: ssm_d_inner      = 0
0.00.041.142 I print_info: ssm_d_state      = 0
0.00.041.142 I print_info: ssm_dt_rank      = 0
0.00.041.142 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.143 I print_info: model type       = 1.4B
0.00.041.143 I print_info: model params     = 1.41 B
0.00.041.143 I print_info: general.name     = 1.4B
0.00.041.144 I print_info: vocab type       = BPE
0.00.041.144 I print_info: n_vocab          = 50304
0.00.041.144 I print_info: n_merges         = 50009
0.00.041.144 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.144 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.145 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.145 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.148 I print_info: LF token         = 187 'Ċ'
0.00.041.148 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.148 I print_info: max token length = 1024
0.00.041.149 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.605.004 I load_tensors: offloading 24 repeating layers to GPU
0.00.605.019 I load_tensors: offloading output layer to GPU
0.00.605.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.605.055 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.605.057 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.606.737 I llama_init_from_model: n_seq_max     = 1
0.00.606.740 I llama_init_from_model: n_ctx         = 128
0.00.606.741 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.741 I llama_init_from_model: n_batch       = 128
0.00.606.742 I llama_init_from_model: n_ubatch      = 128
0.00.606.742 I llama_init_from_model: flash_attn    = 0
0.00.606.744 I llama_init_from_model: freq_base     = 10000.0
0.00.606.744 I llama_init_from_model: freq_scale    = 1
0.00.606.745 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.759 I ggml_metal_init: allocating
0.00.606.851 I ggml_metal_init: found device: Apple M4
0.00.606.894 I ggml_metal_init: picking default device: Apple M4
0.00.608.781 I ggml_metal_init: using embedded metal library
0.00.615.941 I ggml_metal_init: GPU name:   Apple M4
0.00.615.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.952 I ggml_metal_init: simdgroup reduction   = true
0.00.615.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.952 I ggml_metal_init: has residency sets    = true
0.00.615.953 I ggml_metal_init: has bfloat            = true
0.00.615.953 I ggml_metal_init: use bfloat            = true
0.00.615.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.172 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.796 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.802 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.642.205 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.642.207 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.642.207 I llama_init_from_model: graph nodes  = 967
0.00.642.207 I llama_init_from_model: graph splits = 2
0.00.642.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.642.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.085 I 
0.00.670.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.152 I perplexity: tokenizing the input ..
0.00.676.527 I perplexity: tokenization took 6.373 ms
0.00.676.532 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.928 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.810.478 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.810.496 I llama_perf_context_print:        load time =     660.05 ms
0.00.810.497 I llama_perf_context_print: prompt eval time =     132.01 ms /   128 tokens (    1.03 ms per token,   969.63 tokens per second)
0.00.810.503 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.503 I llama_perf_context_print:       total time =     140.41 ms /   129 tokens
0.00.810.850 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.080s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.779 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.782 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.783 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.785 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.785 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.646 I llama_model_loader: - type  f32:  194 tensors
0.00.025.646 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.647 I print_info: file format = GGUF V3 (latest)
0.00.025.648 I print_info: file type   = Q4_1
0.00.025.649 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.224 I load: special tokens cache size = 25
0.00.040.263 I load: token to piece cache size = 0.2984 MB
0.00.040.267 I print_info: arch             = gptneox
0.00.040.267 I print_info: vocab_only       = 0
0.00.040.268 I print_info: n_ctx_train      = 2048
0.00.040.268 I print_info: n_embd           = 2048
0.00.040.268 I print_info: n_layer          = 24
0.00.040.273 I print_info: n_head           = 16
0.00.040.273 I print_info: n_head_kv        = 16
0.00.040.276 I print_info: n_rot            = 32
0.00.040.277 I print_info: n_swa            = 0
0.00.040.277 I print_info: n_embd_head_k    = 128
0.00.040.277 I print_info: n_embd_head_v    = 128
0.00.040.278 I print_info: n_gqa            = 1
0.00.040.278 I print_info: n_embd_k_gqa     = 2048
0.00.040.279 I print_info: n_embd_v_gqa     = 2048
0.00.040.279 I print_info: f_norm_eps       = 1.0e-05
0.00.040.279 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.280 I print_info: f_logit_scale    = 0.0e+00
0.00.040.280 I print_info: n_ff             = 8192
0.00.040.281 I print_info: n_expert         = 0
0.00.040.281 I print_info: n_expert_used    = 0
0.00.040.281 I print_info: causal attn      = 1
0.00.040.281 I print_info: pooling type     = 0
0.00.040.285 I print_info: rope type        = 2
0.00.040.285 I print_info: rope scaling     = linear
0.00.040.285 I print_info: freq_base_train  = 10000.0
0.00.040.286 I print_info: freq_scale_train = 1
0.00.040.286 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.286 I print_info: rope_finetuned   = unknown
0.00.040.286 I print_info: ssm_d_conv       = 0
0.00.040.286 I print_info: ssm_d_inner      = 0
0.00.040.286 I print_info: ssm_d_state      = 0
0.00.040.286 I print_info: ssm_dt_rank      = 0
0.00.040.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.287 I print_info: model type       = 1.4B
0.00.040.287 I print_info: model params     = 1.41 B
0.00.040.287 I print_info: general.name     = 1.4B
0.00.040.288 I print_info: vocab type       = BPE
0.00.040.288 I print_info: n_vocab          = 50304
0.00.040.288 I print_info: n_merges         = 50009
0.00.040.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: LF token         = 187 'Ċ'
0.00.040.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.289 I print_info: max token length = 1024
0.00.040.290 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.376 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.389 I load_tensors: offloading output layer to GPU
0.00.646.389 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.424 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.646.425 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.647.808 I llama_init_from_model: n_seq_max     = 1
0.00.647.811 I llama_init_from_model: n_ctx         = 128
0.00.647.812 I llama_init_from_model: n_ctx_per_seq = 128
0.00.647.812 I llama_init_from_model: n_batch       = 128
0.00.647.812 I llama_init_from_model: n_ubatch      = 128
0.00.647.813 I llama_init_from_model: flash_attn    = 0
0.00.647.814 I llama_init_from_model: freq_base     = 10000.0
0.00.647.815 I llama_init_from_model: freq_scale    = 1
0.00.647.815 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.647.818 I ggml_metal_init: allocating
0.00.647.901 I ggml_metal_init: found device: Apple M4
0.00.647.917 I ggml_metal_init: picking default device: Apple M4
0.00.649.698 I ggml_metal_init: using embedded metal library
0.00.656.179 I ggml_metal_init: GPU name:   Apple M4
0.00.656.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.194 I ggml_metal_init: simdgroup reduction   = true
0.00.656.195 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.195 I ggml_metal_init: has residency sets    = true
0.00.656.195 I ggml_metal_init: has bfloat            = true
0.00.656.196 I ggml_metal_init: use bfloat            = true
0.00.656.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.827 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.539 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.681.541 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.681.542 I llama_init_from_model: graph nodes  = 967
0.00.681.542 I llama_init_from_model: graph splits = 2
0.00.681.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.681.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.651 I 
0.00.712.748 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.773 I perplexity: tokenizing the input ..
0.00.719.613 I perplexity: tokenization took 6.838 ms
0.00.719.618 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.466 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.850.815 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.850.838 I llama_perf_context_print:        load time =     703.38 ms
0.00.850.839 I llama_perf_context_print: prompt eval time =     129.44 ms /   128 tokens (    1.01 ms per token,   988.84 tokens per second)
0.00.850.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.841 I llama_perf_context_print:       total time =     138.19 ms /   129 tokens
0.00.851.266 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.080s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.370 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.377 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.379 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.380 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.381 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.382 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.382 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.063 I llama_model_loader: - type  f32:  194 tensors
0.00.025.063 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.064 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.064 I print_info: file format = GGUF V3 (latest)
0.00.025.065 I print_info: file type   = Q5_0
0.00.025.066 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.080 I load: special tokens cache size = 25
0.00.039.117 I load: token to piece cache size = 0.2984 MB
0.00.039.122 I print_info: arch             = gptneox
0.00.039.122 I print_info: vocab_only       = 0
0.00.039.122 I print_info: n_ctx_train      = 2048
0.00.039.123 I print_info: n_embd           = 2048
0.00.039.123 I print_info: n_layer          = 24
0.00.039.127 I print_info: n_head           = 16
0.00.039.128 I print_info: n_head_kv        = 16
0.00.039.130 I print_info: n_rot            = 32
0.00.039.130 I print_info: n_swa            = 0
0.00.039.130 I print_info: n_embd_head_k    = 128
0.00.039.132 I print_info: n_embd_head_v    = 128
0.00.039.132 I print_info: n_gqa            = 1
0.00.039.133 I print_info: n_embd_k_gqa     = 2048
0.00.039.135 I print_info: n_embd_v_gqa     = 2048
0.00.039.135 I print_info: f_norm_eps       = 1.0e-05
0.00.039.135 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.136 I print_info: f_logit_scale    = 0.0e+00
0.00.039.136 I print_info: n_ff             = 8192
0.00.039.137 I print_info: n_expert         = 0
0.00.039.137 I print_info: n_expert_used    = 0
0.00.039.137 I print_info: causal attn      = 1
0.00.039.137 I print_info: pooling type     = 0
0.00.039.137 I print_info: rope type        = 2
0.00.039.137 I print_info: rope scaling     = linear
0.00.039.138 I print_info: freq_base_train  = 10000.0
0.00.039.138 I print_info: freq_scale_train = 1
0.00.039.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.138 I print_info: rope_finetuned   = unknown
0.00.039.138 I print_info: ssm_d_conv       = 0
0.00.039.139 I print_info: ssm_d_inner      = 0
0.00.039.139 I print_info: ssm_d_state      = 0
0.00.039.139 I print_info: ssm_dt_rank      = 0
0.00.039.139 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.139 I print_info: model type       = 1.4B
0.00.039.140 I print_info: model params     = 1.41 B
0.00.039.140 I print_info: general.name     = 1.4B
0.00.039.140 I print_info: vocab type       = BPE
0.00.039.140 I print_info: n_vocab          = 50304
0.00.039.141 I print_info: n_merges         = 50009
0.00.039.141 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.141 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.141 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.141 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.141 I print_info: LF token         = 187 'Ċ'
0.00.039.142 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: max token length = 1024
0.00.039.142 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.709.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.709.225 I load_tensors: offloading output layer to GPU
0.00.709.226 I load_tensors: offloaded 25/25 layers to GPU
0.00.709.260 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.709.273 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.710.958 I llama_init_from_model: n_seq_max     = 1
0.00.710.962 I llama_init_from_model: n_ctx         = 128
0.00.710.962 I llama_init_from_model: n_ctx_per_seq = 128
0.00.710.962 I llama_init_from_model: n_batch       = 128
0.00.710.963 I llama_init_from_model: n_ubatch      = 128
0.00.710.963 I llama_init_from_model: flash_attn    = 0
0.00.710.966 I llama_init_from_model: freq_base     = 10000.0
0.00.710.966 I llama_init_from_model: freq_scale    = 1
0.00.710.967 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.710.970 I ggml_metal_init: allocating
0.00.711.030 I ggml_metal_init: found device: Apple M4
0.00.711.043 I ggml_metal_init: picking default device: Apple M4
0.00.712.534 I ggml_metal_init: using embedded metal library
0.00.718.922 I ggml_metal_init: GPU name:   Apple M4
0.00.718.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.718.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.718.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.718.928 I ggml_metal_init: simdgroup reduction   = true
0.00.718.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.718.929 I ggml_metal_init: has residency sets    = true
0.00.718.929 I ggml_metal_init: has bfloat            = true
0.00.718.929 I ggml_metal_init: use bfloat            = true
0.00.718.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.718.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.979 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.739.460 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.739.463 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.739.505 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.716 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.742.718 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.742.718 I llama_init_from_model: graph nodes  = 967
0.00.742.718 I llama_init_from_model: graph splits = 2
0.00.742.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.742.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.926 I 
0.00.772.012 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.021 I perplexity: tokenizing the input ..
0.00.779.385 I perplexity: tokenization took 7.36 ms
0.00.779.402 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.924.721 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.926.060 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.926.082 I llama_perf_context_print:        load time =     762.96 ms
0.00.926.083 I llama_perf_context_print: prompt eval time =     144.38 ms /   128 tokens (    1.13 ms per token,   886.55 tokens per second)
0.00.926.084 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.926.085 I llama_perf_context_print:       total time =     154.16 ms /   129 tokens
0.00.926.468 I ggml_metal_free: deallocating

real	0m0.941s
user	0m0.079s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.095 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.096 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.097 I llama_model_loader: - type  f32:  194 tensors
0.00.026.098 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.099 I print_info: file format = GGUF V3 (latest)
0.00.026.099 I print_info: file type   = Q5_1
0.00.026.101 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.206 I load: special tokens cache size = 25
0.00.040.334 I load: token to piece cache size = 0.2984 MB
0.00.040.338 I print_info: arch             = gptneox
0.00.040.338 I print_info: vocab_only       = 0
0.00.040.339 I print_info: n_ctx_train      = 2048
0.00.040.339 I print_info: n_embd           = 2048
0.00.040.339 I print_info: n_layer          = 24
0.00.040.343 I print_info: n_head           = 16
0.00.040.344 I print_info: n_head_kv        = 16
0.00.040.344 I print_info: n_rot            = 32
0.00.040.344 I print_info: n_swa            = 0
0.00.040.344 I print_info: n_embd_head_k    = 128
0.00.040.345 I print_info: n_embd_head_v    = 128
0.00.040.345 I print_info: n_gqa            = 1
0.00.040.346 I print_info: n_embd_k_gqa     = 2048
0.00.040.347 I print_info: n_embd_v_gqa     = 2048
0.00.040.347 I print_info: f_norm_eps       = 1.0e-05
0.00.040.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.348 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.348 I print_info: f_logit_scale    = 0.0e+00
0.00.040.349 I print_info: n_ff             = 8192
0.00.040.349 I print_info: n_expert         = 0
0.00.040.349 I print_info: n_expert_used    = 0
0.00.040.349 I print_info: causal attn      = 1
0.00.040.349 I print_info: pooling type     = 0
0.00.040.349 I print_info: rope type        = 2
0.00.040.350 I print_info: rope scaling     = linear
0.00.040.350 I print_info: freq_base_train  = 10000.0
0.00.040.350 I print_info: freq_scale_train = 1
0.00.040.350 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.351 I print_info: rope_finetuned   = unknown
0.00.040.351 I print_info: ssm_d_conv       = 0
0.00.040.351 I print_info: ssm_d_inner      = 0
0.00.040.351 I print_info: ssm_d_state      = 0
0.00.040.351 I print_info: ssm_dt_rank      = 0
0.00.040.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.352 I print_info: model type       = 1.4B
0.00.040.352 I print_info: model params     = 1.41 B
0.00.040.352 I print_info: general.name     = 1.4B
0.00.040.353 I print_info: vocab type       = BPE
0.00.040.353 I print_info: n_vocab          = 50304
0.00.040.353 I print_info: n_merges         = 50009
0.00.040.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.354 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.354 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.354 I print_info: LF token         = 187 'Ċ'
0.00.040.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.354 I print_info: max token length = 1024
0.00.040.358 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.970 I load_tensors: offloading output layer to GPU
0.00.647.970 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.001 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.648.003 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.649.338 I llama_init_from_model: n_seq_max     = 1
0.00.649.346 I llama_init_from_model: n_ctx         = 128
0.00.649.346 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.347 I llama_init_from_model: n_batch       = 128
0.00.649.347 I llama_init_from_model: n_ubatch      = 128
0.00.649.347 I llama_init_from_model: flash_attn    = 0
0.00.649.348 I llama_init_from_model: freq_base     = 10000.0
0.00.649.349 I llama_init_from_model: freq_scale    = 1
0.00.649.349 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.352 I ggml_metal_init: allocating
0.00.649.417 I ggml_metal_init: found device: Apple M4
0.00.649.433 I ggml_metal_init: picking default device: Apple M4
0.00.651.512 I ggml_metal_init: using embedded metal library
0.00.658.220 I ggml_metal_init: GPU name:   Apple M4
0.00.658.224 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.226 I ggml_metal_init: simdgroup reduction   = true
0.00.658.226 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.226 I ggml_metal_init: has residency sets    = true
0.00.658.226 I ggml_metal_init: has bfloat            = true
0.00.658.227 I ggml_metal_init: use bfloat            = true
0.00.658.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.570 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.679.240 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.679.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.814 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.682.816 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.682.817 I llama_init_from_model: graph nodes  = 967
0.00.682.817 I llama_init_from_model: graph splits = 2
0.00.682.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.781 I 
0.00.712.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.869 I perplexity: tokenizing the input ..
0.00.719.687 I perplexity: tokenization took 6.816 ms
0.00.719.693 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.282 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.856.890 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.856.910 I llama_perf_context_print:        load time =     702.89 ms
0.00.856.911 I llama_perf_context_print: prompt eval time =     134.54 ms /   128 tokens (    1.05 ms per token,   951.41 tokens per second)
0.00.856.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.912 I llama_perf_context_print:       total time =     144.13 ms /   129 tokens
0.00.857.273 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.078s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.487 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.487 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.487 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.489 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.489 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.490 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.490 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.491 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.492 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.492 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.172 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.173 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.174 I print_info: file format = GGUF V3 (latest)
0.00.025.179 I print_info: file type   = Q2_K - Medium
0.00.025.180 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.540 I load: special tokens cache size = 25
0.00.039.588 I load: token to piece cache size = 0.2984 MB
0.00.039.592 I print_info: arch             = gptneox
0.00.039.592 I print_info: vocab_only       = 0
0.00.039.592 I print_info: n_ctx_train      = 2048
0.00.039.592 I print_info: n_embd           = 2048
0.00.039.593 I print_info: n_layer          = 24
0.00.039.597 I print_info: n_head           = 16
0.00.039.598 I print_info: n_head_kv        = 16
0.00.039.598 I print_info: n_rot            = 32
0.00.039.598 I print_info: n_swa            = 0
0.00.039.599 I print_info: n_embd_head_k    = 128
0.00.039.599 I print_info: n_embd_head_v    = 128
0.00.039.602 I print_info: n_gqa            = 1
0.00.039.603 I print_info: n_embd_k_gqa     = 2048
0.00.039.603 I print_info: n_embd_v_gqa     = 2048
0.00.039.604 I print_info: f_norm_eps       = 1.0e-05
0.00.039.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.604 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.605 I print_info: f_logit_scale    = 0.0e+00
0.00.039.605 I print_info: n_ff             = 8192
0.00.039.605 I print_info: n_expert         = 0
0.00.039.606 I print_info: n_expert_used    = 0
0.00.039.606 I print_info: causal attn      = 1
0.00.039.606 I print_info: pooling type     = 0
0.00.039.606 I print_info: rope type        = 2
0.00.039.607 I print_info: rope scaling     = linear
0.00.039.608 I print_info: freq_base_train  = 10000.0
0.00.039.608 I print_info: freq_scale_train = 1
0.00.039.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.608 I print_info: rope_finetuned   = unknown
0.00.039.609 I print_info: ssm_d_conv       = 0
0.00.039.609 I print_info: ssm_d_inner      = 0
0.00.039.609 I print_info: ssm_d_state      = 0
0.00.039.609 I print_info: ssm_dt_rank      = 0
0.00.039.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.609 I print_info: model type       = 1.4B
0.00.039.610 I print_info: model params     = 1.41 B
0.00.039.610 I print_info: general.name     = 1.4B
0.00.039.610 I print_info: vocab type       = BPE
0.00.039.612 I print_info: n_vocab          = 50304
0.00.039.612 I print_info: n_merges         = 50009
0.00.039.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: LF token         = 187 'Ċ'
0.00.039.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: max token length = 1024
0.00.039.614 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.355.324 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.333 I load_tensors: offloading output layer to GPU
0.00.355.333 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.365 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.366 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.356.848 I llama_init_from_model: n_seq_max     = 1
0.00.356.850 I llama_init_from_model: n_ctx         = 128
0.00.356.851 I llama_init_from_model: n_ctx_per_seq = 128
0.00.356.851 I llama_init_from_model: n_batch       = 128
0.00.356.851 I llama_init_from_model: n_ubatch      = 128
0.00.356.852 I llama_init_from_model: flash_attn    = 0
0.00.356.853 I llama_init_from_model: freq_base     = 10000.0
0.00.356.853 I llama_init_from_model: freq_scale    = 1
0.00.356.854 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.356.856 I ggml_metal_init: allocating
0.00.356.898 I ggml_metal_init: found device: Apple M4
0.00.356.909 I ggml_metal_init: picking default device: Apple M4
0.00.358.764 I ggml_metal_init: using embedded metal library
0.00.363.808 I ggml_metal_init: GPU name:   Apple M4
0.00.363.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.815 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.815 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.816 I ggml_metal_init: simdgroup reduction   = true
0.00.363.816 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.816 I ggml_metal_init: has residency sets    = true
0.00.363.816 I ggml_metal_init: has bfloat            = true
0.00.363.817 I ggml_metal_init: use bfloat            = true
0.00.363.818 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.379.750 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.381.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.381.717 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.381.745 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.383.576 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.383.578 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.383.578 I llama_init_from_model: graph nodes  = 967
0.00.383.579 I llama_init_from_model: graph splits = 2
0.00.383.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.383.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.408.187 I 
0.00.408.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.408.230 I perplexity: tokenizing the input ..
0.00.412.173 I perplexity: tokenization took 3.942 ms
0.00.412.177 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.543.109 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.473 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.498 I llama_perf_context_print:        load time =     398.84 ms
0.00.544.499 I llama_perf_context_print: prompt eval time =     130.71 ms /   128 tokens (    1.02 ms per token,   979.30 tokens per second)
0.00.544.500 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.500 I llama_perf_context_print:       total time =     136.31 ms /   129 tokens
0.00.544.858 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.071s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.588 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.937 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.947 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.954 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.778 I llama_model_loader: - type  f32:  194 tensors
0.00.024.779 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.779 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.779 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.780 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.780 I print_info: file format = GGUF V3 (latest)
0.00.024.781 I print_info: file type   = Q3_K - Medium
0.00.024.782 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.236 I load: special tokens cache size = 25
0.00.039.135 I load: token to piece cache size = 0.2984 MB
0.00.039.140 I print_info: arch             = gptneox
0.00.039.140 I print_info: vocab_only       = 0
0.00.039.140 I print_info: n_ctx_train      = 2048
0.00.039.141 I print_info: n_embd           = 2048
0.00.039.141 I print_info: n_layer          = 24
0.00.039.145 I print_info: n_head           = 16
0.00.039.145 I print_info: n_head_kv        = 16
0.00.039.146 I print_info: n_rot            = 32
0.00.039.146 I print_info: n_swa            = 0
0.00.039.146 I print_info: n_embd_head_k    = 128
0.00.039.146 I print_info: n_embd_head_v    = 128
0.00.039.149 I print_info: n_gqa            = 1
0.00.039.151 I print_info: n_embd_k_gqa     = 2048
0.00.039.151 I print_info: n_embd_v_gqa     = 2048
0.00.039.152 I print_info: f_norm_eps       = 1.0e-05
0.00.039.152 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.152 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.153 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.153 I print_info: f_logit_scale    = 0.0e+00
0.00.039.153 I print_info: n_ff             = 8192
0.00.039.154 I print_info: n_expert         = 0
0.00.039.154 I print_info: n_expert_used    = 0
0.00.039.154 I print_info: causal attn      = 1
0.00.039.154 I print_info: pooling type     = 0
0.00.039.155 I print_info: rope type        = 2
0.00.039.156 I print_info: rope scaling     = linear
0.00.039.156 I print_info: freq_base_train  = 10000.0
0.00.039.156 I print_info: freq_scale_train = 1
0.00.039.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.158 I print_info: rope_finetuned   = unknown
0.00.039.158 I print_info: ssm_d_conv       = 0
0.00.039.158 I print_info: ssm_d_inner      = 0
0.00.039.158 I print_info: ssm_d_state      = 0
0.00.039.158 I print_info: ssm_dt_rank      = 0
0.00.039.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.159 I print_info: model type       = 1.4B
0.00.039.159 I print_info: model params     = 1.41 B
0.00.039.159 I print_info: general.name     = 1.4B
0.00.039.159 I print_info: vocab type       = BPE
0.00.039.160 I print_info: n_vocab          = 50304
0.00.039.160 I print_info: n_merges         = 50009
0.00.039.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: LF token         = 187 'Ċ'
0.00.039.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.161 I print_info: max token length = 1024
0.00.039.173 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.426.825 I load_tensors: offloading 24 repeating layers to GPU
0.00.426.843 I load_tensors: offloading output layer to GPU
0.00.426.844 I load_tensors: offloaded 25/25 layers to GPU
0.00.426.879 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.426.881 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.428.649 I llama_init_from_model: n_seq_max     = 1
0.00.428.652 I llama_init_from_model: n_ctx         = 128
0.00.428.653 I llama_init_from_model: n_ctx_per_seq = 128
0.00.428.653 I llama_init_from_model: n_batch       = 128
0.00.428.654 I llama_init_from_model: n_ubatch      = 128
0.00.428.654 I llama_init_from_model: flash_attn    = 0
0.00.428.656 I llama_init_from_model: freq_base     = 10000.0
0.00.428.656 I llama_init_from_model: freq_scale    = 1
0.00.428.657 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.428.659 I ggml_metal_init: allocating
0.00.428.772 I ggml_metal_init: found device: Apple M4
0.00.428.785 I ggml_metal_init: picking default device: Apple M4
0.00.430.637 I ggml_metal_init: using embedded metal library
0.00.436.017 I ggml_metal_init: GPU name:   Apple M4
0.00.436.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.436.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.436.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.436.033 I ggml_metal_init: simdgroup reduction   = true
0.00.436.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.436.034 I ggml_metal_init: has residency sets    = true
0.00.436.034 I ggml_metal_init: has bfloat            = true
0.00.436.035 I ggml_metal_init: use bfloat            = true
0.00.436.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.436.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.456.750 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.475 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.460.484 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.460.543 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.463.963 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.463.964 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.463.965 I llama_init_from_model: graph nodes  = 967
0.00.463.965 I llama_init_from_model: graph splits = 2
0.00.463.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.463.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.492.950 I 
0.00.493.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.032 I perplexity: tokenizing the input ..
0.00.498.611 I perplexity: tokenization took 5.578 ms
0.00.498.618 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.631.790 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.633.123 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.633.147 I llama_perf_context_print:        load time =     484.35 ms
0.00.633.148 I llama_perf_context_print: prompt eval time =     132.95 ms /   128 tokens (    1.04 ms per token,   962.80 tokens per second)
0.00.633.149 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.633.149 I llama_perf_context_print:       total time =     140.20 ms /   129 tokens
0.00.633.518 I ggml_metal_free: deallocating

real	0m0.647s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.661 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.674 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.675 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.678 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.301 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.303 I llama_model_loader: - type  f32:  194 tensors
0.00.025.304 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.304 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.304 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.305 I print_info: file format = GGUF V3 (latest)
0.00.025.307 I print_info: file type   = Q4_K - Medium
0.00.025.308 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.037 I load: special tokens cache size = 25
0.00.040.039 I load: token to piece cache size = 0.2984 MB
0.00.040.044 I print_info: arch             = gptneox
0.00.040.044 I print_info: vocab_only       = 0
0.00.040.044 I print_info: n_ctx_train      = 2048
0.00.040.045 I print_info: n_embd           = 2048
0.00.040.045 I print_info: n_layer          = 24
0.00.040.049 I print_info: n_head           = 16
0.00.040.050 I print_info: n_head_kv        = 16
0.00.040.050 I print_info: n_rot            = 32
0.00.040.050 I print_info: n_swa            = 0
0.00.040.050 I print_info: n_embd_head_k    = 128
0.00.040.050 I print_info: n_embd_head_v    = 128
0.00.040.051 I print_info: n_gqa            = 1
0.00.040.052 I print_info: n_embd_k_gqa     = 2048
0.00.040.053 I print_info: n_embd_v_gqa     = 2048
0.00.040.054 I print_info: f_norm_eps       = 1.0e-05
0.00.040.054 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.054 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.054 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.054 I print_info: f_logit_scale    = 0.0e+00
0.00.040.055 I print_info: n_ff             = 8192
0.00.040.055 I print_info: n_expert         = 0
0.00.040.055 I print_info: n_expert_used    = 0
0.00.040.055 I print_info: causal attn      = 1
0.00.040.057 I print_info: pooling type     = 0
0.00.040.057 I print_info: rope type        = 2
0.00.040.057 I print_info: rope scaling     = linear
0.00.040.057 I print_info: freq_base_train  = 10000.0
0.00.040.058 I print_info: freq_scale_train = 1
0.00.040.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.058 I print_info: rope_finetuned   = unknown
0.00.040.058 I print_info: ssm_d_conv       = 0
0.00.040.058 I print_info: ssm_d_inner      = 0
0.00.040.058 I print_info: ssm_d_state      = 0
0.00.040.058 I print_info: ssm_dt_rank      = 0
0.00.040.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.059 I print_info: model type       = 1.4B
0.00.040.059 I print_info: model params     = 1.41 B
0.00.040.059 I print_info: general.name     = 1.4B
0.00.040.060 I print_info: vocab type       = BPE
0.00.040.060 I print_info: n_vocab          = 50304
0.00.040.060 I print_info: n_merges         = 50009
0.00.040.060 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.061 I print_info: LF token         = 187 'Ċ'
0.00.040.062 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.062 I print_info: max token length = 1024
0.00.040.062 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.554.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.554.257 I load_tensors: offloading output layer to GPU
0.00.554.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.554.291 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.554.293 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.555.880 I llama_init_from_model: n_seq_max     = 1
0.00.555.882 I llama_init_from_model: n_ctx         = 128
0.00.555.883 I llama_init_from_model: n_ctx_per_seq = 128
0.00.555.883 I llama_init_from_model: n_batch       = 128
0.00.555.884 I llama_init_from_model: n_ubatch      = 128
0.00.555.884 I llama_init_from_model: flash_attn    = 0
0.00.555.886 I llama_init_from_model: freq_base     = 10000.0
0.00.555.887 I llama_init_from_model: freq_scale    = 1
0.00.555.888 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.555.890 I ggml_metal_init: allocating
0.00.555.968 I ggml_metal_init: found device: Apple M4
0.00.555.982 I ggml_metal_init: picking default device: Apple M4
0.00.557.819 I ggml_metal_init: using embedded metal library
0.00.564.416 I ggml_metal_init: GPU name:   Apple M4
0.00.564.424 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.564.424 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.564.425 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.564.426 I ggml_metal_init: simdgroup reduction   = true
0.00.564.426 I ggml_metal_init: simdgroup matrix mul. = true
0.00.564.427 I ggml_metal_init: has residency sets    = true
0.00.564.427 I ggml_metal_init: has bfloat            = true
0.00.564.427 I ggml_metal_init: use bfloat            = true
0.00.564.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.564.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.583.565 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.587.133 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.587.137 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.587.193 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.590.453 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.590.455 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.590.456 I llama_init_from_model: graph nodes  = 967
0.00.590.456 I llama_init_from_model: graph splits = 2
0.00.590.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.590.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.057 I 
0.00.623.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.155 I perplexity: tokenizing the input ..
0.00.629.329 I perplexity: tokenization took 6.17 ms
0.00.629.337 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.284 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.763.616 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.763.640 I llama_perf_context_print:        load time =     613.18 ms
0.00.763.641 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.29 tokens per second)
0.00.763.641 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.646 I llama_perf_context_print:       total time =     140.59 ms /   129 tokens
0.00.764.018 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.079s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.118 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.009 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.148 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.148 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.893 I llama_model_loader: - type  f32:  194 tensors
0.00.024.893 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.893 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.894 I print_info: file format = GGUF V3 (latest)
0.00.024.895 I print_info: file type   = Q5_K - Medium
0.00.024.896 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.446 I load: special tokens cache size = 25
0.00.039.265 I load: token to piece cache size = 0.2984 MB
0.00.039.270 I print_info: arch             = gptneox
0.00.039.270 I print_info: vocab_only       = 0
0.00.039.270 I print_info: n_ctx_train      = 2048
0.00.039.270 I print_info: n_embd           = 2048
0.00.039.270 I print_info: n_layer          = 24
0.00.039.275 I print_info: n_head           = 16
0.00.039.275 I print_info: n_head_kv        = 16
0.00.039.276 I print_info: n_rot            = 32
0.00.039.276 I print_info: n_swa            = 0
0.00.039.279 I print_info: n_embd_head_k    = 128
0.00.039.279 I print_info: n_embd_head_v    = 128
0.00.039.280 I print_info: n_gqa            = 1
0.00.039.280 I print_info: n_embd_k_gqa     = 2048
0.00.039.281 I print_info: n_embd_v_gqa     = 2048
0.00.039.281 I print_info: f_norm_eps       = 1.0e-05
0.00.039.282 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.282 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.282 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.282 I print_info: f_logit_scale    = 0.0e+00
0.00.039.283 I print_info: n_ff             = 8192
0.00.039.283 I print_info: n_expert         = 0
0.00.039.283 I print_info: n_expert_used    = 0
0.00.039.283 I print_info: causal attn      = 1
0.00.039.283 I print_info: pooling type     = 0
0.00.039.283 I print_info: rope type        = 2
0.00.039.284 I print_info: rope scaling     = linear
0.00.039.284 I print_info: freq_base_train  = 10000.0
0.00.039.284 I print_info: freq_scale_train = 1
0.00.039.284 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.285 I print_info: rope_finetuned   = unknown
0.00.039.285 I print_info: ssm_d_conv       = 0
0.00.039.285 I print_info: ssm_d_inner      = 0
0.00.039.285 I print_info: ssm_d_state      = 0
0.00.039.285 I print_info: ssm_dt_rank      = 0
0.00.039.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.285 I print_info: model type       = 1.4B
0.00.039.286 I print_info: model params     = 1.41 B
0.00.039.286 I print_info: general.name     = 1.4B
0.00.039.286 I print_info: vocab type       = BPE
0.00.039.286 I print_info: n_vocab          = 50304
0.00.039.287 I print_info: n_merges         = 50009
0.00.039.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: LF token         = 187 'Ċ'
0.00.039.290 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: max token length = 1024
0.00.039.290 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.175 I load_tensors: offloading output layer to GPU
0.00.591.176 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.211 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.213 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.709 I llama_init_from_model: n_seq_max     = 1
0.00.592.714 I llama_init_from_model: n_ctx         = 128
0.00.592.714 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.714 I llama_init_from_model: n_batch       = 128
0.00.592.715 I llama_init_from_model: n_ubatch      = 128
0.00.592.716 I llama_init_from_model: flash_attn    = 0
0.00.592.718 I llama_init_from_model: freq_base     = 10000.0
0.00.592.718 I llama_init_from_model: freq_scale    = 1
0.00.592.719 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.731 I ggml_metal_init: allocating
0.00.592.812 I ggml_metal_init: found device: Apple M4
0.00.592.826 I ggml_metal_init: picking default device: Apple M4
0.00.594.439 I ggml_metal_init: using embedded metal library
0.00.600.958 I ggml_metal_init: GPU name:   Apple M4
0.00.600.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.968 I ggml_metal_init: simdgroup reduction   = true
0.00.600.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.968 I ggml_metal_init: has residency sets    = true
0.00.600.968 I ggml_metal_init: has bfloat            = true
0.00.600.969 I ggml_metal_init: use bfloat            = true
0.00.600.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.077 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.684 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.688 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.731 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.074 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.075 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.076 I llama_init_from_model: graph nodes  = 967
0.00.625.076 I llama_init_from_model: graph splits = 2
0.00.625.078 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.093 I 
0.00.656.178 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.186 I perplexity: tokenizing the input ..
0.00.663.568 I perplexity: tokenization took 7.379 ms
0.00.663.575 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.830 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.188 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.213 I llama_perf_context_print:        load time =     647.08 ms
0.00.803.214 I llama_perf_context_print: prompt eval time =     137.38 ms /   128 tokens (    1.07 ms per token,   931.74 tokens per second)
0.00.803.214 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.215 I llama_perf_context_print:       total time =     147.12 ms /   129 tokens
0.00.803.584 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.080s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.755 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.756 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.757 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.759 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.761 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.486 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.488 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.488 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.489 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.489 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.490 I llama_model_loader: - type  f32:  194 tensors
0.00.024.490 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.491 I print_info: file format = GGUF V3 (latest)
0.00.024.491 I print_info: file type   = Q6_K
0.00.024.492 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.669 I load: special tokens cache size = 25
0.00.038.678 I load: token to piece cache size = 0.2984 MB
0.00.038.682 I print_info: arch             = gptneox
0.00.038.682 I print_info: vocab_only       = 0
0.00.038.683 I print_info: n_ctx_train      = 2048
0.00.038.683 I print_info: n_embd           = 2048
0.00.038.683 I print_info: n_layer          = 24
0.00.038.687 I print_info: n_head           = 16
0.00.038.688 I print_info: n_head_kv        = 16
0.00.038.688 I print_info: n_rot            = 32
0.00.038.688 I print_info: n_swa            = 0
0.00.038.689 I print_info: n_embd_head_k    = 128
0.00.038.689 I print_info: n_embd_head_v    = 128
0.00.038.689 I print_info: n_gqa            = 1
0.00.038.690 I print_info: n_embd_k_gqa     = 2048
0.00.038.691 I print_info: n_embd_v_gqa     = 2048
0.00.038.691 I print_info: f_norm_eps       = 1.0e-05
0.00.038.692 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.692 I print_info: f_logit_scale    = 0.0e+00
0.00.038.693 I print_info: n_ff             = 8192
0.00.038.693 I print_info: n_expert         = 0
0.00.038.693 I print_info: n_expert_used    = 0
0.00.038.693 I print_info: causal attn      = 1
0.00.038.694 I print_info: pooling type     = 0
0.00.038.694 I print_info: rope type        = 2
0.00.038.694 I print_info: rope scaling     = linear
0.00.038.694 I print_info: freq_base_train  = 10000.0
0.00.038.694 I print_info: freq_scale_train = 1
0.00.038.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.695 I print_info: rope_finetuned   = unknown
0.00.038.695 I print_info: ssm_d_conv       = 0
0.00.038.695 I print_info: ssm_d_inner      = 0
0.00.038.695 I print_info: ssm_d_state      = 0
0.00.038.695 I print_info: ssm_dt_rank      = 0
0.00.038.695 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.696 I print_info: model type       = 1.4B
0.00.038.696 I print_info: model params     = 1.41 B
0.00.038.696 I print_info: general.name     = 1.4B
0.00.038.699 I print_info: vocab type       = BPE
0.00.038.699 I print_info: n_vocab          = 50304
0.00.038.699 I print_info: n_merges         = 50009
0.00.038.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.701 I print_info: LF token         = 187 'Ċ'
0.00.038.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.702 I print_info: max token length = 1024
0.00.038.704 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.718 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.727 I load_tensors: offloading output layer to GPU
0.00.444.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.758 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.444.761 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.446.202 I llama_init_from_model: n_seq_max     = 1
0.00.446.204 I llama_init_from_model: n_ctx         = 128
0.00.446.204 I llama_init_from_model: n_ctx_per_seq = 128
0.00.446.205 I llama_init_from_model: n_batch       = 128
0.00.446.205 I llama_init_from_model: n_ubatch      = 128
0.00.446.205 I llama_init_from_model: flash_attn    = 0
0.00.446.206 I llama_init_from_model: freq_base     = 10000.0
0.00.446.206 I llama_init_from_model: freq_scale    = 1
0.00.446.207 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.446.209 I ggml_metal_init: allocating
0.00.446.263 I ggml_metal_init: found device: Apple M4
0.00.446.274 I ggml_metal_init: picking default device: Apple M4
0.00.447.644 I ggml_metal_init: using embedded metal library
0.00.453.751 I ggml_metal_init: GPU name:   Apple M4
0.00.453.755 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.755 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.757 I ggml_metal_init: simdgroup reduction   = true
0.00.453.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.757 I ggml_metal_init: has residency sets    = true
0.00.453.757 I ggml_metal_init: has bfloat            = true
0.00.453.757 I ggml_metal_init: use bfloat            = true
0.00.453.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.469.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.473.228 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.473.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.476.381 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.476.383 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.476.383 I llama_init_from_model: graph nodes  = 967
0.00.476.383 I llama_init_from_model: graph splits = 2
0.00.476.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.476.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.402 I 
0.00.507.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.490 I perplexity: tokenizing the input ..
0.00.514.684 I perplexity: tokenization took 7.189 ms
0.00.514.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.774 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.647.114 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.647.152 I llama_perf_context_print:        load time =     498.43 ms
0.00.647.155 I llama_perf_context_print: prompt eval time =     130.73 ms /   128 tokens (    1.02 ms per token,   979.14 tokens per second)
0.00.647.157 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.157 I llama_perf_context_print:       total time =     139.75 ms /   129 tokens
0.00.647.563 I ggml_metal_free: deallocating

real	0m0.662s
user	0m0.077s
sys	0m0.121s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.351 I build: 4771 (3e9a2860) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.367 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.120 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.132 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.137 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.344 I llama_model_loader: - type  f32:  194 tensors
0.00.055.345 I llama_model_loader: - type  f16:   98 tensors
0.00.055.346 I print_info: file format = GGUF V3 (latest)
0.00.055.346 I print_info: file type   = all F32 (guessed)
0.00.055.348 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.856 I load: special tokens cache size = 25
0.00.075.370 I load: token to piece cache size = 0.2984 MB
0.00.075.375 I print_info: arch             = gptneox
0.00.075.375 I print_info: vocab_only       = 0
0.00.075.376 I print_info: n_ctx_train      = 2048
0.00.075.378 I print_info: n_embd           = 2048
0.00.075.378 I print_info: n_layer          = 24
0.00.075.381 I print_info: n_head           = 16
0.00.075.382 I print_info: n_head_kv        = 16
0.00.075.383 I print_info: n_rot            = 32
0.00.075.383 I print_info: n_swa            = 0
0.00.075.383 I print_info: n_embd_head_k    = 128
0.00.075.384 I print_info: n_embd_head_v    = 128
0.00.075.384 I print_info: n_gqa            = 1
0.00.075.385 I print_info: n_embd_k_gqa     = 2048
0.00.075.386 I print_info: n_embd_v_gqa     = 2048
0.00.075.386 I print_info: f_norm_eps       = 1.0e-05
0.00.075.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.387 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.387 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.387 I print_info: f_logit_scale    = 0.0e+00
0.00.075.388 I print_info: n_ff             = 8192
0.00.075.388 I print_info: n_expert         = 0
0.00.075.388 I print_info: n_expert_used    = 0
0.00.075.388 I print_info: causal attn      = 1
0.00.075.388 I print_info: pooling type     = 0
0.00.075.388 I print_info: rope type        = 2
0.00.075.389 I print_info: rope scaling     = linear
0.00.075.389 I print_info: freq_base_train  = 10000.0
0.00.075.389 I print_info: freq_scale_train = 1
0.00.075.389 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.390 I print_info: rope_finetuned   = unknown
0.00.075.390 I print_info: ssm_d_conv       = 0
0.00.075.390 I print_info: ssm_d_inner      = 0
0.00.075.390 I print_info: ssm_d_state      = 0
0.00.075.391 I print_info: ssm_dt_rank      = 0
0.00.075.391 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.391 I print_info: model type       = 1.4B
0.00.075.392 I print_info: model params     = 1.41 B
0.00.075.392 I print_info: general.name     = 1.4B
0.00.075.392 I print_info: vocab type       = BPE
0.00.075.393 I print_info: n_vocab          = 50304
0.00.075.393 I print_info: n_merges         = 50009
0.00.075.393 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.393 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.394 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.394 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.394 I print_info: LF token         = 187 'Ċ'
0.00.075.394 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.395 I print_info: max token length = 1024
0.00.075.395 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.327.942 I load_tensors: offloading 24 repeating layers to GPU
0.01.327.949 I load_tensors: offloading output layer to GPU
0.01.327.950 I load_tensors: offloaded 25/25 layers to GPU
0.01.327.976 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.327.978 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.328.683 I llama_init_from_model: n_seq_max     = 1
0.01.328.684 I llama_init_from_model: n_ctx         = 128
0.01.328.684 I llama_init_from_model: n_ctx_per_seq = 128
0.01.328.684 I llama_init_from_model: n_batch       = 128
0.01.328.684 I llama_init_from_model: n_ubatch      = 128
0.01.328.685 I llama_init_from_model: flash_attn    = 0
0.01.328.685 I llama_init_from_model: freq_base     = 10000.0
0.01.328.685 I llama_init_from_model: freq_scale    = 1
0.01.328.686 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.328.687 I ggml_metal_init: allocating
0.01.328.714 I ggml_metal_init: found device: Apple M4
0.01.328.720 I ggml_metal_init: picking default device: Apple M4
0.01.329.826 I ggml_metal_init: using embedded metal library
0.01.333.678 I ggml_metal_init: GPU name:   Apple M4
0.01.333.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.333.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.333.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.333.682 I ggml_metal_init: simdgroup reduction   = true
0.01.333.682 I ggml_metal_init: simdgroup matrix mul. = true
0.01.333.682 I ggml_metal_init: has residency sets    = true
0.01.333.682 I ggml_metal_init: has bfloat            = true
0.01.333.682 I ggml_metal_init: use bfloat            = true
0.01.333.683 I ggml_metal_init: hasUnifiedMemory      = true
0.01.333.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.344.354 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.346.050 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.346.053 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.346.081 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.347.749 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.347.751 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.347.751 I llama_init_from_model: graph nodes  = 967
0.01.347.751 I llama_init_from_model: graph splits = 2
0.01.347.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.347.753 I 
0.01.347.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.347.794 I compute_imatrix: tokenizing the input ..
0.01.351.824 I compute_imatrix: tokenization took 4.03 ms
0.01.351.826 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.617.003 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.619.411 I llama_perf_context_print:        load time =    1592.63 ms
0.01.619.412 I llama_perf_context_print: prompt eval time =     263.44 ms /   128 tokens (    2.06 ms per token,   485.87 tokens per second)
0.01.619.412 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.619.413 I llama_perf_context_print:       total time =    1595.04 ms /   129 tokens
0.01.619.896 I ggml_metal_free: deallocating

real	0m1.806s
user	0m0.124s
sys	0m0.255s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4771 (3e9a2860)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1226047c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122608ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122609080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122609630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122609be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12260a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12260a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12260acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12260b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12260b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12260bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12260c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12260ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12260d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12260dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12260e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12260eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12260f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12260f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1226100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1226107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122610f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122611630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122611ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1226125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1226128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122612ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122613b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122614330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1226147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122614a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122615320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122615b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122616460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122616900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122617240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1226176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122618020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1226184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1226193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122619cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12261a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12261a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12261aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12261b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12261bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12261c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12261c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12261cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12261d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12261d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12261db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12261e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12261e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12261ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12261ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12261f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12261f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12261fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122620190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122620630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122620ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122620f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122621410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1226218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122621d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1226222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1226227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122622d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122623290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1226237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122624280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1226247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1226257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122625d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122626260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1226267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122626d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1226277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122627cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122628240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122628790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122628ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122629230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122629780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122629cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1226199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12262a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12262a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12262ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12262b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12262b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12262be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12262c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12262c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12262ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12262d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12262d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12262de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12262e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12262e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12262ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12262f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12262f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12262fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122630080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1226309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122630e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122631300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1226317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122631c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1226320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122632580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x122632a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122632ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122633360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122633800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122633ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1226345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122634a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122634f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1226353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122635860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122635d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1226361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122636640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122636ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122636f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122637420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1226378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122637d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122638200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1226386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122638b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122638fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122639480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122639920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122639dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12263a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12263a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12263aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12263b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12263b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12263b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12263be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12263c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12263c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12263cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12263d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12263d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12263d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12263de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12263e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12263e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12263ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12263f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12263f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12263fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12263fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122640380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122640820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122640cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122641160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122641600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122641aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122641f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1226423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122642880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122642d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1226431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122643660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122643b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122643fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122644440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1226448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122644d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122645220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1226456c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122645b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122646000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122646550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122646aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122647540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122647800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122647e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122648420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122649220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1226496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122649980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122649f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12264a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12264ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12264b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12264b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12264bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12264c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12264c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12264cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12264d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12264d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12264ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12264e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12264e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12264eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12264f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12264f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12264fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1226502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122650d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1226512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122651820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122651d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1226522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122652810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122652d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1226532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122653800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122653d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1226542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1226547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122654d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122655290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1226557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122655d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122656280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1226567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122656d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122657270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1226577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122657d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122658260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1226587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122658d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122659250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1226597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122659cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12265a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12265a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12265ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12265b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12265b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12265bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12265c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12265c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12265ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12265d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12265d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12265dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12265e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12265e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12265eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12265f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12265f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12265fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12265ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1226603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122660860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122660d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1226611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122661640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122661ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122661f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122662420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1226628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122662d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122663200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1226636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x122663b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x122663fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x122664480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x122664920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x122664dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x122665260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x122665700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x122665ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x122666040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122666590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x122666cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1226673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x122667af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122668210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1226684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x122668cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122668f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122669590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.723.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.134 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ac04bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ac05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ac054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ac05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ac05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ac06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ac06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ac06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ac06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ac073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ac07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ac07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ac08a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ac091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ac09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ac0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ac0a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ac0af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ac0b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ac0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ac0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ac0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ac0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ac0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ac0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ac0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ac0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ac0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ac0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ac0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ac0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ac0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ac10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ac104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ac10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ac10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ac11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ac116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ac11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ac11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ac12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ac12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ac12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ac13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ac135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ac13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ac13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ac14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ac14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ac14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ac15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ac154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ac15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ac15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ac16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ac16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ac16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ac17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ac17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ac179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ac17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ac182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ac18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ac18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ac19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ac19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ac198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ac19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ac1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ac1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ac1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ac1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ac1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ac1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ac1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ac1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ac1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ac1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ac1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ac1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ac1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ac1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ac1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ac1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ac1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ac1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ac1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ac1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ac1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ac1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ac20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ac207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ac20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ac210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ac21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ac219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ac21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ac22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ac226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ac22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ac22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ac23440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ac238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ac23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ac24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ac24600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ac24a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ac24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ac25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ac257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ac25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ac260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ac26510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ac26980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ac26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ac27260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ac276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ac27b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ac27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ac28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ac28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ac28d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ac29170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ac295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ac29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ac29ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ac2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ac2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ac2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ac2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ac2b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ac2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ac2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ac2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ac2c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ac2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ac2cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ac2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ac2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ac2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ac2e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ac2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ac2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ac2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ac2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ac2f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ac2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ac30060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ac304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ac30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ac30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ac31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ac31690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ac31b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ac31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ac323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ac32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ac32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ac33130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ac335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ac33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ac33e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ac342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ac34760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ac34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ac35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ac35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ac35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ac361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ac36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ac36ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ac36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ac373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ac37820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ac37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ac38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ac38570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ac389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ac38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ac392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ac39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ac39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ac3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ac3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ac3a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ac3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ac3b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ac3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ac3bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ac3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ac3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ac3c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ac3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ac3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ac3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ac3d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ac3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ac3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ac3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ac3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ac3eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ac3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ac3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ac3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ac40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ac407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ac40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ac41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ac415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ac41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ac42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ac428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ac42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ac43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ac43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ac43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ac445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ac44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ac45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ac456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ac45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ac46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ac46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ac46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ac473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ac47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ac47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ac484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ac48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ac49070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ac49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ac49bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ac4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ac4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ac4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ac4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ac4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ac4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ac4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ac4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ac4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ac4d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ac4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ac4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ac4e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ac4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ac4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ac4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ac4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ac50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ac50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ac50ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ac514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ac51a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ac52030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ac525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ac52bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ac53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ac53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ac53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ac542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ac54870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ac54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ac553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ac559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ac55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ac56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ac56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ac56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ac574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ac579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ac57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ac583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ac588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ac58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ac592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ac597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ac59cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ac5a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ac5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ac5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ac5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11ac5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11ac5baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11ac5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11ac5c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11ac5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11ac5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11ac5d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11ac5d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11ac5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11ac5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ac5e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ac5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ac5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ac60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ac60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ac60a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ac61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ac614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ac61ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1226480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122669240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122647ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1226486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12261b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12261b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12261d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12264a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122612b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122619660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122619050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12261bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12261aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122611b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12262a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122668790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122614d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122615010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12264a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122648cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122613180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122613440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122613700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1226699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122669cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122669f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12266a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12266a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12266a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12266aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12266ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12266aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12266b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12266b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12266b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12266baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12266bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12266c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12266c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12266c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12266c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12266cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12266ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12266d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12266d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12266d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12266d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12266dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12266deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12266e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12266e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12266e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12266e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12266ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12266ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12266f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12266f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12266f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12266fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12266fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12266ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122670270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122670530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1226707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122670ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122670d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122671030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1226712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1226715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122671870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122671b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122671df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1226720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122672370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122672630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1226728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122672bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122672e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122673130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1226733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1226736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122673970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122673c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122673ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1226741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122674470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x122674730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1226749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122674cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122674f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122675230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1226754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1226757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122675a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122675d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122675ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1226762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122676570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122676830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122676af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122676db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122677070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122677330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1226775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1226778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122677b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122677e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1226780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1226783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122678670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122678930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122678bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122678eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122679170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122679430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1226796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1226799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122679c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122679f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12267a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12267a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12267a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12267aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12267acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12267afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12267b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12267b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12267b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12267bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12267bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12267c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12267c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12267c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12267c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12267cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12267cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12267d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12267d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12267d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12267d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12267dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12267de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12267e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12267e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12267e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12267e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12267ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12267eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12267f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12267f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12267f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12267f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12267fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12267ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122680230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1226804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1226807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122680a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122680d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122680ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1226812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122681570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122681830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122681af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122681db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122682070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122682330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1226825f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1226828b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122682b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122682e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1226830f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1226833b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122683670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122683930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122683bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122683eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122684170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122684430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1226846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1226849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122684c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122684f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1226851f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1226854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122685770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122685a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122685cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122685fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122686270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122686530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1226867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122686ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122686d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122687030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1226872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1226875b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122687870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122687b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122687df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1226880b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122688370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122688630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1226888f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122688bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122688e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122689130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1226893f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1226899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122689c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122689f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12268a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12268a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12268a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12268aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12268ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12268afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12268b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12268ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12268bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12268c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12268ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12268cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12268d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12268da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12268df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12268e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12268ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12268ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12268f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12268fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12268ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1226904c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122690a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122690f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1226914b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x122691a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122691f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1226924a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1226929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122692f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122693490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1226939e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122693f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122694480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1226949d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122694f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122695470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1226959c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122695f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122696460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1226969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122696f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122697450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1226979a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122697ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122698440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122698990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122698ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122699430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122699980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122699ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12269a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12269a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12269aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12269b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12269b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12269b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12269bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12269bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12269c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12269c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12269cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12269d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12269d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12269da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12269def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12269e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12269e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12269ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12269f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12269f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12269f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12269fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1226a0270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1226a06e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1226a0b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1226a0fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1226a1430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1226a18a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1226a1d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1226a2770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1226a2e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1226a35b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1226a3cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1226a3f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1226a4400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1226a4a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1226a5010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.806s
user	0m0.282s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4771 (3e9a2860)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15770f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15770f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15770fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157710270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157710820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157710dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157711380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157711930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1577123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1577128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157712de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157713900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1577140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1577148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157714fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157715700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157715e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157716d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157717430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157717b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157718b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1577194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157719b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15771a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15771acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15771af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15771b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15771b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15771bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15771c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15771c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15771cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15771d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15771d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15771d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15771de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15771e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15771e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15771ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15771f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15771f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15771f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15771ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157720900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157721520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157721b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157722140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157723550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1577239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157723e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157724150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157724760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157725210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1577256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157725b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157725ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157726490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157726dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157727270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157727710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157727bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157728050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1577284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157728ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157729430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157729980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157729ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15772a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15772a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15772aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15772b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15772b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15772beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15772c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15772c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15772cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15772d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15772d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15772de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15772e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15772e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15772ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15772f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15772f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15772fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1577303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157730910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1577205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157730d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157731530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157731a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157732520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157732a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157732fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157733510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157733fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157734500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157734a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157734fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1577354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157735a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157736380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157736820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157736cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157737160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157737600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157737aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157737f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1577383e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157738880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157738d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1577391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157739660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157739b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157739fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15773a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15773a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15773ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15773b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15773b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15773bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15773c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15773c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15773c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15773cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15773d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15773d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15773dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15773e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15773e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15773e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15773ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15773f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15773f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15773fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1577400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157740560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157740a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157740ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157741340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1577417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157741c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157742120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1577425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157742a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157742f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1577433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157743840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157743ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157744180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157744620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157744ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157744f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157745400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1577458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157745d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1577461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157746680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157746b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157746fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157747460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157747900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157747da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157748240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1577486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157748b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157749020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1577494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157749960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157749e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15774a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15774a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15774abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15774b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15774b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15774b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15774be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15774c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15774c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15774cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15774d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15774d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15774dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15774e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15774e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15774ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15774f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15774f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15774fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157750300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1577505c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157750bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1577511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1577519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157751e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157752310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1577527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157752f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1577534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157753a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157753f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1577544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1577549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157754f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157755490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1577559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157755f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157756480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1577569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157756f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157757470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1577579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157757f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157758460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1577589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157758f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157759450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1577599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157759ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15775a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15775a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15775aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15775b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15775b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15775bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15775c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15775c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15775cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15775d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15775d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15775deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15775e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15775e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15775eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15775f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15775f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15775fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1577603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157760930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157760e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1577613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157761920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157761e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1577623c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157762910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157762e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1577633b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157763900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157763e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1577643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1577648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157764e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157765390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1577658e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157765d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157766220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1577666c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157766b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157767000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1577674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157767940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157767de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157768280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157768720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157768bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157769060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157769500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1577699a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157769e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15776a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15776a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15776ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15776b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15776b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15776ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15776bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15776c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15776c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15776cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15776d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15776d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15776e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15776e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15776ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15776f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15776f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15776fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1577701d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.101.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157605bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157606020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157606490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157606900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157606d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1576071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157607650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157607ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1576083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157608e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15760a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15760a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15760b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15760b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15760beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15760c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15760cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15760d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15760dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15760e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15760ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15760f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15760f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15760f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15760fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15760ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157610410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157610db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157611220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1576114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1576126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157612f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1576133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157613860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1576145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157614a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1576164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1576180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1576189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1576192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157619b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15761a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15761a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15761a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15761ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15761b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15761b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15761baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15761bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15761c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15761c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15761cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15761d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15761d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15761d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15761de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15761e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15761e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15761eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15761efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15761f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15761f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15761fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1576201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1576217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1576220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1576236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1576248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1576255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1576267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1576286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15762a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15762a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15762aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15762aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15762b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15762b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15762bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15762c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15762c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15762c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15762cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15762d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15762d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15762db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15762df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15762e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15762e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15762ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15762f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15762f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15762fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15762fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1576314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157631da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157632210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1576333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1576352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1576367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157636aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157636f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157637380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1576377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157637c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1576380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157638540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1576389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157638e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157639700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157639b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157639fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15763a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15763a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15763ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15763b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15763b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15763ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15763bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15763c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15763c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15763cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15763d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15763d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15763d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15763de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15763e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15763e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15763eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15763efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15763f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15763f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15763fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157640180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1576405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157640a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157640ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157641340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1576417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157641c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157642090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157642500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1576430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157643370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157643aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157643f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1576447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157644c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1576450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1576459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157645e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157646fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1576478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157647d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1576481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157648610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157648ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157649360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1576497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157649c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15764a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15764a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15764a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15764ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15764b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15764b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15764bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15764bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15764c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15764c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15764cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15764d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15764d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15764da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15764ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15764e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15764e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15764ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15764f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15764f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15764f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15764fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157650250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1576506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157650fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157651410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157651cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1576525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157652a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157653c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1576544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157654950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157654dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157655230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1576556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157655b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157655f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1576563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x157656cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x157657140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1576575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x157657a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x157657e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x157658300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x157658770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x157658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x157659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1576594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15765a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15765ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15765b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15765b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15765bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15765bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15765c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15765c7a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15776fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157750880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15774e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15774f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157722400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157721df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157724410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157750e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1577197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1577202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157720bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1577211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15771f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1577217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1577187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157731040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15776f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15771b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15771bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1577514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15774f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15771a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15771a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157770630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1577708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157770bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157770e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157771130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1577713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1577716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157771970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157771c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157771ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1577721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157772470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157772730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1577729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157772cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157772f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157773230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1577734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1577737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157773a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157773d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157773ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1577742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157774570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157774830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157774af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157774db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157775070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157775330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1577755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1577758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157775b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157775e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1577760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1577763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157776670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157776930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157776bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157776eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157777170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157777430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1577776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1577779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157777c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157777f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1577781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1577784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157778770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157778a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157778cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157778fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157779270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157779530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1577797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157779ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157779d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15777a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15777a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15777a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15777a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15777ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15777adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15777b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15777b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15777b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15777b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15777bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15777be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15777c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15777c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15777c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15777c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15777cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15777cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15777d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15777d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15777d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15777d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15777dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15777df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15777e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15777e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15777e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15777ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15777ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15777eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15777f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15777f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15777f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15777faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15777fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157780070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157780330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1577805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1577808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157780b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157780e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1577810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1577813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157781670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157781930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157781bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157781eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157782170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157782430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1577826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1577829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157782c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157782f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1577831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1577834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157783770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157783a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157783cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157783fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157784270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157784530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1577847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157784ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157784d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157785030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1577852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1577855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157785870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157785b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157785df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1577860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157786370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157786630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1577868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157786bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157786e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157787130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1577873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1577876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157787970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157787c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157787ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1577881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157788470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157788730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1577889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157788cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157788f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157789230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1577894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1577897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157789a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157789d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157789ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15778a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15778a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15778a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15778aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15778adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15778b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15778b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15778b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15778b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15778bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15778be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15778c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15778c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15778c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15778c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15778cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15778ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15778d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15778d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15778d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15778d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15778dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15778df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15778e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15778e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1477044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1477056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147707430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1477076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1477079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147707e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147708290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147708700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147708fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147709450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1477098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147709d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14770a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14770a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14770aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14770aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14770b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14770b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14770bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14770c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14770c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14770c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14770ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14770d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14770d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14770db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14770dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14770e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14770e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14770ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14770f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14770f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14770fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14770fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147710340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1477107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147710c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147711090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147711500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147711970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147711de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147712250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1477126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147712b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147712fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147713410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147713880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147713cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147714160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1477145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147714a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147714eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147715320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147715790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147715c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147716070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1477164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147716950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147716dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147717230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1477176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147717b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1477183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147718860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147718cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147719140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1477195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147719a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147719e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14771a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14771a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14771abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14771b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14771b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14771b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14771bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14771c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14771c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14771caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14771cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14771d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14771d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14771dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14771e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14771ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14771f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14771fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14771ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147720240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1477206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147720b20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.954s
user	0m0.232s
sys	0m0.192s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
