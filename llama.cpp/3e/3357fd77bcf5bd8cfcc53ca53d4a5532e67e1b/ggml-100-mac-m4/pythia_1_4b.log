Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.521s
user	0m0.905s
sys	0m1.184s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Built target test-arg-parser
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Built target test-gguf
[ 60%] Built target test-chat-template
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-quantize-fns
[ 69%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target test-rope
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-batched
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-bench
[ 79%] Built target llama-imatrix
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-infill
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Built target llama-lookup-create
[ 79%] Built target llama-lookup-stats
[ 79%] Built target llama-lookup
[ 79%] Built target llama-lookup-merge
[ 79%] Built target llama-cli
[ 79%] Generating loading.html.hpp
[ 80%] Built target llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-passkey
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-tts
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.974s
user	0m6.486s
sys	0m10.737s

main: quantize time =  2992.30 ms
main:    total time =  2992.30 ms

main: quantize time =  2509.10 ms
main:    total time =  2509.10 ms

main: quantize time =  1569.27 ms
main:    total time =  1569.27 ms

main: quantize time =  2376.70 ms
main:    total time =  2376.70 ms

main: quantize time =  1769.71 ms
main:    total time =  1769.71 ms

main: quantize time =  4652.45 ms
main:    total time =  4652.45 ms

main: quantize time =  5321.44 ms
main:    total time =  5321.44 ms

main: quantize time =  6413.03 ms
main:    total time =  6413.03 ms

main: quantize time =  5601.89 ms
main:    total time =  5601.89 ms

main: quantize time =  4366.98 ms
main:    total time =  4366.98 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.304 I main: llama backend init
0.00.000.310 I main: load the model and apply lora adapter, if any
0.00.077.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.102.254 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.102.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.102.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.102.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.102.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.102.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.102.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.102.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.102.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.102.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.102.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.102.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.102.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.102.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.102.283 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.102.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.102.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.111.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.113.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.122.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.122.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.122.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.122.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.122.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.122.270 I llama_model_loader: - type  f32:  194 tensors
0.00.122.270 I llama_model_loader: - type  f16:   98 tensors
0.00.122.272 I print_info: file format = GGUF V3 (latest)
0.00.122.276 I print_info: file type   = all F32 (guessed)
0.00.122.277 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.153.613 I load: special tokens cache size = 25
0.00.161.015 I load: token to piece cache size = 0.2984 MB
0.00.161.018 I print_info: arch             = gptneox
0.00.161.018 I print_info: vocab_only       = 0
0.00.161.019 I print_info: n_ctx_train      = 2048
0.00.161.019 I print_info: n_embd           = 2048
0.00.161.019 I print_info: n_layer          = 24
0.00.161.022 I print_info: n_head           = 16
0.00.161.022 I print_info: n_head_kv        = 16
0.00.161.022 I print_info: n_rot            = 32
0.00.161.023 I print_info: n_swa            = 0
0.00.161.023 I print_info: n_embd_head_k    = 128
0.00.161.023 I print_info: n_embd_head_v    = 128
0.00.161.024 I print_info: n_gqa            = 1
0.00.161.025 I print_info: n_embd_k_gqa     = 2048
0.00.161.025 I print_info: n_embd_v_gqa     = 2048
0.00.161.026 I print_info: f_norm_eps       = 1.0e-05
0.00.161.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.161.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.161.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.161.027 I print_info: f_logit_scale    = 0.0e+00
0.00.161.028 I print_info: n_ff             = 8192
0.00.161.028 I print_info: n_expert         = 0
0.00.161.028 I print_info: n_expert_used    = 0
0.00.161.028 I print_info: causal attn      = 1
0.00.161.029 I print_info: pooling type     = 0
0.00.161.029 I print_info: rope type        = 2
0.00.161.029 I print_info: rope scaling     = linear
0.00.161.029 I print_info: freq_base_train  = 10000.0
0.00.161.030 I print_info: freq_scale_train = 1
0.00.161.030 I print_info: n_ctx_orig_yarn  = 2048
0.00.161.030 I print_info: rope_finetuned   = unknown
0.00.161.030 I print_info: ssm_d_conv       = 0
0.00.161.030 I print_info: ssm_d_inner      = 0
0.00.161.030 I print_info: ssm_d_state      = 0
0.00.161.031 I print_info: ssm_dt_rank      = 0
0.00.161.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.161.031 I print_info: model type       = 1.4B
0.00.161.031 I print_info: model params     = 1.41 B
0.00.161.031 I print_info: general.name     = 1.4B
0.00.161.032 I print_info: vocab type       = BPE
0.00.161.032 I print_info: n_vocab          = 50304
0.00.161.032 I print_info: n_merges         = 50009
0.00.161.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.161.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.161.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.161.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.161.033 I print_info: LF token         = 128 'Ä'
0.00.161.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.161.034 I print_info: max token length = 1024
0.00.163.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.163.152 I load_tensors: offloading output layer to GPU
0.00.163.152 I load_tensors: offloaded 25/25 layers to GPU
0.00.163.171 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.163.172 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.163.490 I llama_init_from_model: n_seq_max     = 1
0.00.163.491 I llama_init_from_model: n_ctx         = 2048
0.00.163.491 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.163.491 I llama_init_from_model: n_batch       = 2048
0.00.163.491 I llama_init_from_model: n_ubatch      = 512
0.00.163.492 I llama_init_from_model: flash_attn    = 0
0.00.163.492 I llama_init_from_model: freq_base     = 10000.0
0.00.163.492 I llama_init_from_model: freq_scale    = 1
0.00.163.493 I ggml_metal_init: allocating
0.00.163.496 I ggml_metal_init: found device: Apple M4
0.00.163.498 I ggml_metal_init: picking default device: Apple M4
0.00.164.232 I ggml_metal_init: using embedded metal library
0.00.174.259 I ggml_metal_init: GPU name:   Apple M4
0.00.174.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.174.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.174.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.174.262 I ggml_metal_init: simdgroup reduction   = true
0.00.174.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.174.262 I ggml_metal_init: has bfloat            = true
0.00.174.263 I ggml_metal_init: use bfloat            = true
0.00.174.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.174.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.199.029 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.220.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.220.405 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.220.425 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.221.457 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.221.459 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.221.460 I llama_init_from_model: graph nodes  = 967
0.00.221.460 I llama_init_from_model: graph splits = 2
0.00.221.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.221.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.221.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.303.080 I main: llama threadpool init, n_threads = 4
0.00.303.128 I 
0.00.303.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.303.161 I 
0.00.303.342 I sampler seed: 1234
0.00.303.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.303.371 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.303.372 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.303.373 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.104.658 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.02.104.659 I llama_perf_context_print:        load time =     223.99 ms
0.02.104.659 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.49 tokens per second)
0.02.104.660 I llama_perf_context_print:        eval time =    1754.78 ms /    63 runs   (   27.85 ms per token,    35.90 tokens per second)
0.02.104.660 I llama_perf_context_print:       total time =    1802.69 ms /    70 tokens
0.02.104.884 I ggml_metal_free: deallocating

real	0m2.411s
user	0m0.162s
sys	0m0.113s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.461 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.542 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.544 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.562 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.566 I llama_model_loader: - type  f32:  194 tensors
0.00.035.566 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.567 I print_info: file format = GGUF V3 (latest)
0.00.035.567 I print_info: file type   = Q8_0
0.00.035.569 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.396 I load: special tokens cache size = 25
0.00.061.618 I load: token to piece cache size = 0.2984 MB
0.00.061.623 I print_info: arch             = gptneox
0.00.061.623 I print_info: vocab_only       = 0
0.00.061.623 I print_info: n_ctx_train      = 2048
0.00.061.623 I print_info: n_embd           = 2048
0.00.061.624 I print_info: n_layer          = 24
0.00.061.633 I print_info: n_head           = 16
0.00.061.634 I print_info: n_head_kv        = 16
0.00.061.634 I print_info: n_rot            = 32
0.00.061.634 I print_info: n_swa            = 0
0.00.061.634 I print_info: n_embd_head_k    = 128
0.00.061.634 I print_info: n_embd_head_v    = 128
0.00.061.636 I print_info: n_gqa            = 1
0.00.061.637 I print_info: n_embd_k_gqa     = 2048
0.00.061.638 I print_info: n_embd_v_gqa     = 2048
0.00.061.638 I print_info: f_norm_eps       = 1.0e-05
0.00.061.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.640 I print_info: f_logit_scale    = 0.0e+00
0.00.061.640 I print_info: n_ff             = 8192
0.00.061.641 I print_info: n_expert         = 0
0.00.061.641 I print_info: n_expert_used    = 0
0.00.061.641 I print_info: causal attn      = 1
0.00.061.641 I print_info: pooling type     = 0
0.00.061.641 I print_info: rope type        = 2
0.00.061.642 I print_info: rope scaling     = linear
0.00.061.642 I print_info: freq_base_train  = 10000.0
0.00.061.642 I print_info: freq_scale_train = 1
0.00.061.643 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.652 I print_info: rope_finetuned   = unknown
0.00.061.654 I print_info: ssm_d_conv       = 0
0.00.061.655 I print_info: ssm_d_inner      = 0
0.00.061.655 I print_info: ssm_d_state      = 0
0.00.061.656 I print_info: ssm_dt_rank      = 0
0.00.061.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.657 I print_info: model type       = 1.4B
0.00.061.657 I print_info: model params     = 1.41 B
0.00.061.658 I print_info: general.name     = 1.4B
0.00.061.658 I print_info: vocab type       = BPE
0.00.061.658 I print_info: n_vocab          = 50304
0.00.061.660 I print_info: n_merges         = 50009
0.00.061.660 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.660 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.661 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.662 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.662 I print_info: LF token         = 128 'Ä'
0.00.061.663 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.663 I print_info: max token length = 1024
0.00.063.625 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.626 I load_tensors: offloading output layer to GPU
0.00.063.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.637 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.639 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.011 I llama_init_from_model: n_seq_max     = 1
0.00.064.011 I llama_init_from_model: n_ctx         = 2048
0.00.064.011 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.012 I llama_init_from_model: n_batch       = 2048
0.00.064.012 I llama_init_from_model: n_ubatch      = 512
0.00.064.012 I llama_init_from_model: flash_attn    = 0
0.00.064.012 I llama_init_from_model: freq_base     = 10000.0
0.00.064.012 I llama_init_from_model: freq_scale    = 1
0.00.064.013 I ggml_metal_init: allocating
0.00.064.016 I ggml_metal_init: found device: Apple M4
0.00.064.017 I ggml_metal_init: picking default device: Apple M4
0.00.064.734 I ggml_metal_init: using embedded metal library
0.00.067.252 I ggml_metal_init: GPU name:   Apple M4
0.00.067.254 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.256 I ggml_metal_init: simdgroup reduction   = true
0.00.067.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.256 I ggml_metal_init: has bfloat            = true
0.00.067.256 I ggml_metal_init: use bfloat            = true
0.00.067.256 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.257 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.795 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.675 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.701 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.799 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.801 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.801 I llama_init_from_model: graph nodes  = 967
0.00.104.801 I llama_init_from_model: graph splits = 2
0.00.104.821 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.305.683 I main: llama threadpool init, n_threads = 4
0.01.305.718 I 
0.01.305.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.305.740 I 
0.01.305.912 I sampler seed: 1234
0.01.305.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.305.935 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.305.936 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.305.936 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.390.805 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.02.390.806 I llama_perf_context_print:        load time =    1295.27 ms
0.02.390.807 I llama_perf_context_print: prompt eval time =      40.22 ms /     7 tokens (    5.75 ms per token,   174.05 tokens per second)
0.02.390.808 I llama_perf_context_print:        eval time =    1041.74 ms /    63 runs   (   16.54 ms per token,    60.48 tokens per second)
0.02.390.808 I llama_perf_context_print:       total time =    1086.07 ms /    70 tokens
0.02.391.110 I ggml_metal_free: deallocating

real	0m2.408s
user	0m0.114s
sys	0m0.272s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.023.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.717 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.266 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.050.266 I llama_model_loader: - type  f32:  194 tensors
0.00.050.267 I llama_model_loader: - type q4_0:   97 tensors
0.00.050.267 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.268 I print_info: file format = GGUF V3 (latest)
0.00.050.268 I print_info: file type   = Q4_0
0.00.050.269 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.078.394 I load: special tokens cache size = 25
0.00.089.618 I load: token to piece cache size = 0.2984 MB
0.00.089.623 I print_info: arch             = gptneox
0.00.089.623 I print_info: vocab_only       = 0
0.00.089.623 I print_info: n_ctx_train      = 2048
0.00.089.624 I print_info: n_embd           = 2048
0.00.089.624 I print_info: n_layer          = 24
0.00.089.628 I print_info: n_head           = 16
0.00.089.630 I print_info: n_head_kv        = 16
0.00.089.630 I print_info: n_rot            = 32
0.00.089.630 I print_info: n_swa            = 0
0.00.089.633 I print_info: n_embd_head_k    = 128
0.00.089.633 I print_info: n_embd_head_v    = 128
0.00.089.634 I print_info: n_gqa            = 1
0.00.089.635 I print_info: n_embd_k_gqa     = 2048
0.00.089.637 I print_info: n_embd_v_gqa     = 2048
0.00.089.644 I print_info: f_norm_eps       = 1.0e-05
0.00.089.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.645 I print_info: f_logit_scale    = 0.0e+00
0.00.089.646 I print_info: n_ff             = 8192
0.00.089.646 I print_info: n_expert         = 0
0.00.089.648 I print_info: n_expert_used    = 0
0.00.089.649 I print_info: causal attn      = 1
0.00.089.649 I print_info: pooling type     = 0
0.00.089.649 I print_info: rope type        = 2
0.00.089.649 I print_info: rope scaling     = linear
0.00.089.650 I print_info: freq_base_train  = 10000.0
0.00.089.650 I print_info: freq_scale_train = 1
0.00.089.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.651 I print_info: rope_finetuned   = unknown
0.00.089.651 I print_info: ssm_d_conv       = 0
0.00.089.651 I print_info: ssm_d_inner      = 0
0.00.089.651 I print_info: ssm_d_state      = 0
0.00.089.655 I print_info: ssm_dt_rank      = 0
0.00.089.655 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.656 I print_info: model type       = 1.4B
0.00.089.656 I print_info: model params     = 1.41 B
0.00.089.656 I print_info: general.name     = 1.4B
0.00.089.657 I print_info: vocab type       = BPE
0.00.089.658 I print_info: n_vocab          = 50304
0.00.089.658 I print_info: n_merges         = 50009
0.00.089.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.663 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.663 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.663 I print_info: LF token         = 128 'Ä'
0.00.089.664 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.664 I print_info: max token length = 1024
0.00.092.694 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.694 I load_tensors: offloading output layer to GPU
0.00.092.695 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.707 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.709 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.093.170 I llama_init_from_model: n_seq_max     = 1
0.00.093.171 I llama_init_from_model: n_ctx         = 2048
0.00.093.172 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.093.172 I llama_init_from_model: n_batch       = 2048
0.00.093.172 I llama_init_from_model: n_ubatch      = 512
0.00.093.173 I llama_init_from_model: flash_attn    = 0
0.00.093.173 I llama_init_from_model: freq_base     = 10000.0
0.00.093.174 I llama_init_from_model: freq_scale    = 1
0.00.093.174 I ggml_metal_init: allocating
0.00.093.179 I ggml_metal_init: found device: Apple M4
0.00.093.182 I ggml_metal_init: picking default device: Apple M4
0.00.094.173 I ggml_metal_init: using embedded metal library
0.00.098.396 I ggml_metal_init: GPU name:   Apple M4
0.00.098.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.400 I ggml_metal_init: simdgroup reduction   = true
0.00.098.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.400 I ggml_metal_init: has bfloat            = true
0.00.098.400 I ggml_metal_init: use bfloat            = true
0.00.098.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.934 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.135.853 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.862 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.887 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.136.932 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.136.934 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.136.935 I llama_init_from_model: graph nodes  = 967
0.00.136.935 I llama_init_from_model: graph splits = 2
0.00.136.955 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.137.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.722 I main: llama threadpool init, n_threads = 4
0.00.788.794 I 
0.00.788.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.837 I 
0.00.789.109 I sampler seed: 1234
0.00.789.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.145 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.145 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.145 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.480.222 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.480.222 I llama_perf_context_print:        load time =     764.10 ms
0.01.480.223 I llama_perf_context_print: prompt eval time =      50.41 ms /     7 tokens (    7.20 ms per token,   138.87 tokens per second)
0.01.480.224 I llama_perf_context_print:        eval time =     637.62 ms /    63 runs   (   10.12 ms per token,    98.80 tokens per second)
0.01.480.224 I llama_perf_context_print:       total time =     693.02 ms /    70 tokens
0.01.480.454 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.135s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.131 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.133 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.133 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.895 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.896 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.896 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.897 I llama_model_loader: - type  f32:  194 tensors
0.00.026.897 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.898 I print_info: file format = GGUF V3 (latest)
0.00.026.899 I print_info: file type   = Q4_1
0.00.026.899 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.099 I load: special tokens cache size = 25
0.00.050.813 I load: token to piece cache size = 0.2984 MB
0.00.050.817 I print_info: arch             = gptneox
0.00.050.817 I print_info: vocab_only       = 0
0.00.050.817 I print_info: n_ctx_train      = 2048
0.00.050.817 I print_info: n_embd           = 2048
0.00.050.818 I print_info: n_layer          = 24
0.00.050.820 I print_info: n_head           = 16
0.00.050.821 I print_info: n_head_kv        = 16
0.00.050.821 I print_info: n_rot            = 32
0.00.050.821 I print_info: n_swa            = 0
0.00.050.822 I print_info: n_embd_head_k    = 128
0.00.050.822 I print_info: n_embd_head_v    = 128
0.00.050.822 I print_info: n_gqa            = 1
0.00.050.823 I print_info: n_embd_k_gqa     = 2048
0.00.050.824 I print_info: n_embd_v_gqa     = 2048
0.00.050.825 I print_info: f_norm_eps       = 1.0e-05
0.00.050.825 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.825 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.825 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.825 I print_info: f_logit_scale    = 0.0e+00
0.00.050.826 I print_info: n_ff             = 8192
0.00.050.826 I print_info: n_expert         = 0
0.00.050.826 I print_info: n_expert_used    = 0
0.00.050.827 I print_info: causal attn      = 1
0.00.050.827 I print_info: pooling type     = 0
0.00.050.827 I print_info: rope type        = 2
0.00.050.827 I print_info: rope scaling     = linear
0.00.050.827 I print_info: freq_base_train  = 10000.0
0.00.050.828 I print_info: freq_scale_train = 1
0.00.050.828 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.828 I print_info: rope_finetuned   = unknown
0.00.050.828 I print_info: ssm_d_conv       = 0
0.00.050.828 I print_info: ssm_d_inner      = 0
0.00.050.829 I print_info: ssm_d_state      = 0
0.00.050.829 I print_info: ssm_dt_rank      = 0
0.00.050.829 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.829 I print_info: model type       = 1.4B
0.00.050.830 I print_info: model params     = 1.41 B
0.00.050.830 I print_info: general.name     = 1.4B
0.00.050.830 I print_info: vocab type       = BPE
0.00.050.830 I print_info: n_vocab          = 50304
0.00.050.832 I print_info: n_merges         = 50009
0.00.050.832 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.833 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.833 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.833 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.833 I print_info: LF token         = 128 'Ä'
0.00.050.833 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.834 I print_info: max token length = 1024
0.00.052.572 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.572 I load_tensors: offloading output layer to GPU
0.00.052.573 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.583 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.584 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.890 I llama_init_from_model: n_seq_max     = 1
0.00.052.891 I llama_init_from_model: n_ctx         = 2048
0.00.052.892 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.892 I llama_init_from_model: n_batch       = 2048
0.00.052.892 I llama_init_from_model: n_ubatch      = 512
0.00.052.892 I llama_init_from_model: flash_attn    = 0
0.00.052.893 I llama_init_from_model: freq_base     = 10000.0
0.00.052.893 I llama_init_from_model: freq_scale    = 1
0.00.052.893 I ggml_metal_init: allocating
0.00.052.895 I ggml_metal_init: found device: Apple M4
0.00.052.897 I ggml_metal_init: picking default device: Apple M4
0.00.053.482 I ggml_metal_init: using embedded metal library
0.00.055.859 I ggml_metal_init: GPU name:   Apple M4
0.00.055.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.861 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.861 I ggml_metal_init: simdgroup reduction   = true
0.00.055.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.862 I ggml_metal_init: has bfloat            = true
0.00.055.862 I ggml_metal_init: use bfloat            = true
0.00.055.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.081 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.559 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.566 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.585 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.502 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.503 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.503 I llama_init_from_model: graph nodes  = 967
0.00.085.504 I llama_init_from_model: graph splits = 2
0.00.085.520 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.631 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.941.617 I main: llama threadpool init, n_threads = 4
0.00.941.655 I 
0.00.941.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.941.674 I 
0.00.941.880 I sampler seed: 1234
0.00.941.886 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.941.901 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.941.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.941.902 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.667.531 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65558.63 tokens per second)
0.01.667.532 I llama_perf_context_print:        load time =     931.87 ms
0.01.667.533 I llama_perf_context_print: prompt eval time =      39.96 ms /     7 tokens (    5.71 ms per token,   175.18 tokens per second)
0.01.667.533 I llama_perf_context_print:        eval time =     682.81 ms /    63 runs   (   10.84 ms per token,    92.27 tokens per second)
0.01.667.534 I llama_perf_context_print:       total time =     726.85 ms /    70 tokens
0.01.667.765 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.107s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.019.401 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.251 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.251 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.252 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.253 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.253 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.255 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.716 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.288 I llama_model_loader: - type  f32:  194 tensors
0.00.048.289 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.289 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.289 I print_info: file format = GGUF V3 (latest)
0.00.048.290 I print_info: file type   = Q5_0
0.00.048.291 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.081.575 I load: special tokens cache size = 25
0.00.092.127 I load: token to piece cache size = 0.2984 MB
0.00.092.131 I print_info: arch             = gptneox
0.00.092.131 I print_info: vocab_only       = 0
0.00.092.131 I print_info: n_ctx_train      = 2048
0.00.092.132 I print_info: n_embd           = 2048
0.00.092.132 I print_info: n_layer          = 24
0.00.092.135 I print_info: n_head           = 16
0.00.092.136 I print_info: n_head_kv        = 16
0.00.092.136 I print_info: n_rot            = 32
0.00.092.136 I print_info: n_swa            = 0
0.00.092.142 I print_info: n_embd_head_k    = 128
0.00.092.142 I print_info: n_embd_head_v    = 128
0.00.092.143 I print_info: n_gqa            = 1
0.00.092.144 I print_info: n_embd_k_gqa     = 2048
0.00.092.147 I print_info: n_embd_v_gqa     = 2048
0.00.092.148 I print_info: f_norm_eps       = 1.0e-05
0.00.092.148 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.148 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.149 I print_info: f_logit_scale    = 0.0e+00
0.00.092.149 I print_info: n_ff             = 8192
0.00.092.150 I print_info: n_expert         = 0
0.00.092.150 I print_info: n_expert_used    = 0
0.00.092.150 I print_info: causal attn      = 1
0.00.092.150 I print_info: pooling type     = 0
0.00.092.150 I print_info: rope type        = 2
0.00.092.151 I print_info: rope scaling     = linear
0.00.092.151 I print_info: freq_base_train  = 10000.0
0.00.092.151 I print_info: freq_scale_train = 1
0.00.092.152 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.152 I print_info: rope_finetuned   = unknown
0.00.092.152 I print_info: ssm_d_conv       = 0
0.00.092.152 I print_info: ssm_d_inner      = 0
0.00.092.152 I print_info: ssm_d_state      = 0
0.00.092.153 I print_info: ssm_dt_rank      = 0
0.00.092.153 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.153 I print_info: model type       = 1.4B
0.00.092.154 I print_info: model params     = 1.41 B
0.00.092.154 I print_info: general.name     = 1.4B
0.00.092.154 I print_info: vocab type       = BPE
0.00.092.155 I print_info: n_vocab          = 50304
0.00.092.155 I print_info: n_merges         = 50009
0.00.092.155 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.156 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.156 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.156 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.156 I print_info: LF token         = 128 'Ä'
0.00.092.157 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.157 I print_info: max token length = 1024
0.00.094.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.592 I load_tensors: offloading output layer to GPU
0.00.094.593 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.604 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.094.606 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.094.998 I llama_init_from_model: n_seq_max     = 1
0.00.094.999 I llama_init_from_model: n_ctx         = 2048
0.00.095.000 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.095.000 I llama_init_from_model: n_batch       = 2048
0.00.095.000 I llama_init_from_model: n_ubatch      = 512
0.00.095.000 I llama_init_from_model: flash_attn    = 0
0.00.095.001 I llama_init_from_model: freq_base     = 10000.0
0.00.095.001 I llama_init_from_model: freq_scale    = 1
0.00.095.002 I ggml_metal_init: allocating
0.00.095.006 I ggml_metal_init: found device: Apple M4
0.00.095.008 I ggml_metal_init: picking default device: Apple M4
0.00.095.849 I ggml_metal_init: using embedded metal library
0.00.099.630 I ggml_metal_init: GPU name:   Apple M4
0.00.099.635 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.636 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.636 I ggml_metal_init: simdgroup reduction   = true
0.00.099.636 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.637 I ggml_metal_init: has bfloat            = true
0.00.099.637 I ggml_metal_init: use bfloat            = true
0.00.099.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.638 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.133.556 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.563 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.134.674 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.134.676 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.134.676 I llama_init_from_model: graph nodes  = 967
0.00.134.676 I llama_init_from_model: graph splits = 2
0.00.134.693 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.134.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.582 I main: llama threadpool init, n_threads = 4
0.00.861.650 I 
0.00.861.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.861.682 I 
0.00.861.954 I sampler seed: 1234
0.00.861.961 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.978 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.980 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.980 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.660.146 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.660.147 I llama_perf_context_print:        load time =     840.61 ms
0.01.660.147 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.42 tokens per second)
0.01.660.148 I llama_perf_context_print:        eval time =     740.67 ms /    63 runs   (   11.76 ms per token,    85.06 tokens per second)
0.01.660.148 I llama_perf_context_print:       total time =     800.14 ms /    70 tokens
0.01.660.389 I ggml_metal_free: deallocating

real	0m1.687s
user	0m0.148s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.224 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.227 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.227 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.228 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.228 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.230 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.065 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.809 I llama_model_loader: - type  f32:  194 tensors
0.00.024.809 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.809 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.810 I print_info: file format = GGUF V3 (latest)
0.00.024.810 I print_info: file type   = Q5_1
0.00.024.811 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.196 I load: special tokens cache size = 25
0.00.049.132 I load: token to piece cache size = 0.2984 MB
0.00.049.135 I print_info: arch             = gptneox
0.00.049.135 I print_info: vocab_only       = 0
0.00.049.135 I print_info: n_ctx_train      = 2048
0.00.049.135 I print_info: n_embd           = 2048
0.00.049.135 I print_info: n_layer          = 24
0.00.049.138 I print_info: n_head           = 16
0.00.049.138 I print_info: n_head_kv        = 16
0.00.049.138 I print_info: n_rot            = 32
0.00.049.138 I print_info: n_swa            = 0
0.00.049.139 I print_info: n_embd_head_k    = 128
0.00.049.140 I print_info: n_embd_head_v    = 128
0.00.049.140 I print_info: n_gqa            = 1
0.00.049.141 I print_info: n_embd_k_gqa     = 2048
0.00.049.141 I print_info: n_embd_v_gqa     = 2048
0.00.049.142 I print_info: f_norm_eps       = 1.0e-05
0.00.049.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.143 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.143 I print_info: f_logit_scale    = 0.0e+00
0.00.049.144 I print_info: n_ff             = 8192
0.00.049.144 I print_info: n_expert         = 0
0.00.049.144 I print_info: n_expert_used    = 0
0.00.049.146 I print_info: causal attn      = 1
0.00.049.146 I print_info: pooling type     = 0
0.00.049.147 I print_info: rope type        = 2
0.00.049.147 I print_info: rope scaling     = linear
0.00.049.147 I print_info: freq_base_train  = 10000.0
0.00.049.148 I print_info: freq_scale_train = 1
0.00.049.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.148 I print_info: rope_finetuned   = unknown
0.00.049.148 I print_info: ssm_d_conv       = 0
0.00.049.148 I print_info: ssm_d_inner      = 0
0.00.049.148 I print_info: ssm_d_state      = 0
0.00.049.149 I print_info: ssm_dt_rank      = 0
0.00.049.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.149 I print_info: model type       = 1.4B
0.00.049.149 I print_info: model params     = 1.41 B
0.00.049.151 I print_info: general.name     = 1.4B
0.00.049.152 I print_info: vocab type       = BPE
0.00.049.152 I print_info: n_vocab          = 50304
0.00.049.152 I print_info: n_merges         = 50009
0.00.049.152 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.153 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.153 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.153 I print_info: LF token         = 128 'Ä'
0.00.049.153 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.153 I print_info: max token length = 1024
0.00.050.869 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.869 I load_tensors: offloading output layer to GPU
0.00.050.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.880 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.881 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.147 I llama_init_from_model: n_seq_max     = 1
0.00.051.148 I llama_init_from_model: n_ctx         = 2048
0.00.051.148 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.148 I llama_init_from_model: n_batch       = 2048
0.00.051.148 I llama_init_from_model: n_ubatch      = 512
0.00.051.149 I llama_init_from_model: flash_attn    = 0
0.00.051.149 I llama_init_from_model: freq_base     = 10000.0
0.00.051.149 I llama_init_from_model: freq_scale    = 1
0.00.051.149 I ggml_metal_init: allocating
0.00.051.152 I ggml_metal_init: found device: Apple M4
0.00.051.154 I ggml_metal_init: picking default device: Apple M4
0.00.051.734 I ggml_metal_init: using embedded metal library
0.00.054.048 I ggml_metal_init: GPU name:   Apple M4
0.00.054.049 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.050 I ggml_metal_init: simdgroup reduction   = true
0.00.054.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.051 I ggml_metal_init: has bfloat            = true
0.00.054.051 I ggml_metal_init: use bfloat            = true
0.00.054.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.588 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.120 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.125 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.144 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.089 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.082.090 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.082.090 I llama_init_from_model: graph nodes  = 967
0.00.082.091 I llama_init_from_model: graph splits = 2
0.00.082.107 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.082.227 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.082.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.984 I main: llama threadpool init, n_threads = 4
0.00.775.021 I 
0.00.775.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.043 I 
0.00.775.203 I sampler seed: 1234
0.00.775.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.219 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.219 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.609.299 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.609.300 I llama_perf_context_print:        load time =     765.27 ms
0.01.609.300 I llama_perf_context_print: prompt eval time =      42.62 ms /     7 tokens (    6.09 ms per token,   164.25 tokens per second)
0.01.609.301 I llama_perf_context_print:        eval time =     788.48 ms /    63 runs   (   12.52 ms per token,    79.90 tokens per second)
0.01.609.302 I llama_perf_context_print:       total time =     835.23 ms /    70 tokens
0.01.609.536 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.107s
sys	0m0.194s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.647 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.963 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.978 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.979 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.860 I llama_model_loader: - type  f32:  194 tensors
0.00.024.860 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.860 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.860 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.861 I print_info: file format = GGUF V3 (latest)
0.00.024.861 I print_info: file type   = Q2_K - Medium
0.00.024.862 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.064 I load: special tokens cache size = 25
0.00.049.060 I load: token to piece cache size = 0.2984 MB
0.00.049.063 I print_info: arch             = gptneox
0.00.049.063 I print_info: vocab_only       = 0
0.00.049.063 I print_info: n_ctx_train      = 2048
0.00.049.063 I print_info: n_embd           = 2048
0.00.049.063 I print_info: n_layer          = 24
0.00.049.066 I print_info: n_head           = 16
0.00.049.066 I print_info: n_head_kv        = 16
0.00.049.067 I print_info: n_rot            = 32
0.00.049.067 I print_info: n_swa            = 0
0.00.049.067 I print_info: n_embd_head_k    = 128
0.00.049.067 I print_info: n_embd_head_v    = 128
0.00.049.068 I print_info: n_gqa            = 1
0.00.049.069 I print_info: n_embd_k_gqa     = 2048
0.00.049.069 I print_info: n_embd_v_gqa     = 2048
0.00.049.070 I print_info: f_norm_eps       = 1.0e-05
0.00.049.070 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.070 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.071 I print_info: f_logit_scale    = 0.0e+00
0.00.049.071 I print_info: n_ff             = 8192
0.00.049.071 I print_info: n_expert         = 0
0.00.049.072 I print_info: n_expert_used    = 0
0.00.049.072 I print_info: causal attn      = 1
0.00.049.072 I print_info: pooling type     = 0
0.00.049.072 I print_info: rope type        = 2
0.00.049.074 I print_info: rope scaling     = linear
0.00.049.075 I print_info: freq_base_train  = 10000.0
0.00.049.075 I print_info: freq_scale_train = 1
0.00.049.075 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.075 I print_info: rope_finetuned   = unknown
0.00.049.076 I print_info: ssm_d_conv       = 0
0.00.049.076 I print_info: ssm_d_inner      = 0
0.00.049.076 I print_info: ssm_d_state      = 0
0.00.049.076 I print_info: ssm_dt_rank      = 0
0.00.049.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.076 I print_info: model type       = 1.4B
0.00.049.078 I print_info: model params     = 1.41 B
0.00.049.078 I print_info: general.name     = 1.4B
0.00.049.079 I print_info: vocab type       = BPE
0.00.049.079 I print_info: n_vocab          = 50304
0.00.049.079 I print_info: n_merges         = 50009
0.00.049.080 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.080 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.080 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.080 I print_info: LF token         = 128 'Ä'
0.00.049.081 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.081 I print_info: max token length = 1024
0.00.050.799 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.800 I load_tensors: offloading output layer to GPU
0.00.050.800 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.810 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.811 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.077 I llama_init_from_model: n_seq_max     = 1
0.00.051.078 I llama_init_from_model: n_ctx         = 2048
0.00.051.078 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.078 I llama_init_from_model: n_batch       = 2048
0.00.051.078 I llama_init_from_model: n_ubatch      = 512
0.00.051.078 I llama_init_from_model: flash_attn    = 0
0.00.051.079 I llama_init_from_model: freq_base     = 10000.0
0.00.051.079 I llama_init_from_model: freq_scale    = 1
0.00.051.079 I ggml_metal_init: allocating
0.00.051.082 I ggml_metal_init: found device: Apple M4
0.00.051.084 I ggml_metal_init: picking default device: Apple M4
0.00.051.670 I ggml_metal_init: using embedded metal library
0.00.054.052 I ggml_metal_init: GPU name:   Apple M4
0.00.054.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.054 I ggml_metal_init: simdgroup reduction   = true
0.00.054.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.055 I ggml_metal_init: has bfloat            = true
0.00.054.055 I ggml_metal_init: use bfloat            = true
0.00.054.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.056 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.355 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.333 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.356 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.082.335 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.082.336 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.082.337 I llama_init_from_model: graph nodes  = 967
0.00.082.337 I llama_init_from_model: graph splits = 2
0.00.082.354 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.082.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.082.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.383 I main: llama threadpool init, n_threads = 4
0.00.479.424 I 
0.00.479.443 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.443 I 
0.00.479.597 I sampler seed: 1234
0.00.479.602 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.479.612 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.479.613 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.479.613 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.166.637 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66792.10 tokens per second)
0.01.166.639 I llama_perf_context_print:        load time =     468.81 ms
0.01.166.639 I llama_perf_context_print: prompt eval time =      36.10 ms /     7 tokens (    5.16 ms per token,   193.92 tokens per second)
0.01.166.640 I llama_perf_context_print:        eval time =     648.14 ms /    63 runs   (   10.29 ms per token,    97.20 tokens per second)
0.01.166.641 I llama_perf_context_print:       total time =     688.17 ms /    70 tokens
0.01.166.835 I ggml_metal_free: deallocating

real	0m1.183s
user	0m0.107s
sys	0m0.113s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.768 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.769 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.769 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.924 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.968 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.969 I llama_model_loader: - type  f32:  194 tensors
0.00.025.969 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.969 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.969 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.970 I print_info: file format = GGUF V3 (latest)
0.00.025.970 I print_info: file type   = Q3_K - Medium
0.00.025.971 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.038 I load: special tokens cache size = 25
0.00.051.200 I load: token to piece cache size = 0.2984 MB
0.00.051.203 I print_info: arch             = gptneox
0.00.051.203 I print_info: vocab_only       = 0
0.00.051.203 I print_info: n_ctx_train      = 2048
0.00.051.204 I print_info: n_embd           = 2048
0.00.051.204 I print_info: n_layer          = 24
0.00.051.206 I print_info: n_head           = 16
0.00.051.207 I print_info: n_head_kv        = 16
0.00.051.207 I print_info: n_rot            = 32
0.00.051.207 I print_info: n_swa            = 0
0.00.051.207 I print_info: n_embd_head_k    = 128
0.00.051.207 I print_info: n_embd_head_v    = 128
0.00.051.208 I print_info: n_gqa            = 1
0.00.051.209 I print_info: n_embd_k_gqa     = 2048
0.00.051.211 I print_info: n_embd_v_gqa     = 2048
0.00.051.212 I print_info: f_norm_eps       = 1.0e-05
0.00.051.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.213 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.213 I print_info: f_logit_scale    = 0.0e+00
0.00.051.213 I print_info: n_ff             = 8192
0.00.051.214 I print_info: n_expert         = 0
0.00.051.214 I print_info: n_expert_used    = 0
0.00.051.214 I print_info: causal attn      = 1
0.00.051.214 I print_info: pooling type     = 0
0.00.051.214 I print_info: rope type        = 2
0.00.051.214 I print_info: rope scaling     = linear
0.00.051.215 I print_info: freq_base_train  = 10000.0
0.00.051.215 I print_info: freq_scale_train = 1
0.00.051.215 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.215 I print_info: rope_finetuned   = unknown
0.00.051.216 I print_info: ssm_d_conv       = 0
0.00.051.216 I print_info: ssm_d_inner      = 0
0.00.051.216 I print_info: ssm_d_state      = 0
0.00.051.216 I print_info: ssm_dt_rank      = 0
0.00.051.216 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.216 I print_info: model type       = 1.4B
0.00.051.217 I print_info: model params     = 1.41 B
0.00.051.217 I print_info: general.name     = 1.4B
0.00.051.218 I print_info: vocab type       = BPE
0.00.051.218 I print_info: n_vocab          = 50304
0.00.051.218 I print_info: n_merges         = 50009
0.00.051.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.220 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.220 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.221 I print_info: LF token         = 128 'Ä'
0.00.051.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.221 I print_info: max token length = 1024
0.00.053.003 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.004 I load_tensors: offloading output layer to GPU
0.00.053.004 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.014 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.015 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.288 I llama_init_from_model: n_seq_max     = 1
0.00.053.289 I llama_init_from_model: n_ctx         = 2048
0.00.053.289 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.289 I llama_init_from_model: n_batch       = 2048
0.00.053.289 I llama_init_from_model: n_ubatch      = 512
0.00.053.289 I llama_init_from_model: flash_attn    = 0
0.00.053.290 I llama_init_from_model: freq_base     = 10000.0
0.00.053.290 I llama_init_from_model: freq_scale    = 1
0.00.053.291 I ggml_metal_init: allocating
0.00.053.294 I ggml_metal_init: found device: Apple M4
0.00.053.296 I ggml_metal_init: picking default device: Apple M4
0.00.053.890 I ggml_metal_init: using embedded metal library
0.00.056.213 I ggml_metal_init: GPU name:   Apple M4
0.00.056.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.215 I ggml_metal_init: simdgroup reduction   = true
0.00.056.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.216 I ggml_metal_init: has bfloat            = true
0.00.056.216 I ggml_metal_init: use bfloat            = true
0.00.056.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.790 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.175 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.180 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.202 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.325 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.327 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.327 I llama_init_from_model: graph nodes  = 967
0.00.087.327 I llama_init_from_model: graph splits = 2
0.00.087.344 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.574.304 I main: llama threadpool init, n_threads = 4
0.00.574.347 I 
0.00.574.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.368 I 
0.00.574.582 I sampler seed: 1234
0.00.574.588 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.574.625 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.574.641 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.574.641 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.313.482 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.313.483 I llama_perf_context_print:        load time =     564.52 ms
0.01.313.484 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.68 tokens per second)
0.01.313.485 I llama_perf_context_print:        eval time =     691.50 ms /    63 runs   (   10.98 ms per token,    91.11 tokens per second)
0.01.313.485 I llama_perf_context_print:       total time =     740.13 ms /    70 tokens
0.01.313.711 I ggml_metal_free: deallocating

real	0m1.329s
user	0m0.110s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.869 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.872 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.876 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.878 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.770 I llama_model_loader: - type  f32:  194 tensors
0.00.025.770 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.770 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.770 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.771 I print_info: file format = GGUF V3 (latest)
0.00.025.771 I print_info: file type   = Q4_K - Medium
0.00.025.772 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.161 I load: special tokens cache size = 25
0.00.050.165 I load: token to piece cache size = 0.2984 MB
0.00.050.167 I print_info: arch             = gptneox
0.00.050.168 I print_info: vocab_only       = 0
0.00.050.168 I print_info: n_ctx_train      = 2048
0.00.050.168 I print_info: n_embd           = 2048
0.00.050.168 I print_info: n_layer          = 24
0.00.050.171 I print_info: n_head           = 16
0.00.050.171 I print_info: n_head_kv        = 16
0.00.050.172 I print_info: n_rot            = 32
0.00.050.172 I print_info: n_swa            = 0
0.00.050.172 I print_info: n_embd_head_k    = 128
0.00.050.172 I print_info: n_embd_head_v    = 128
0.00.050.173 I print_info: n_gqa            = 1
0.00.050.174 I print_info: n_embd_k_gqa     = 2048
0.00.050.174 I print_info: n_embd_v_gqa     = 2048
0.00.050.177 I print_info: f_norm_eps       = 1.0e-05
0.00.050.178 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.178 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.178 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.178 I print_info: f_logit_scale    = 0.0e+00
0.00.050.179 I print_info: n_ff             = 8192
0.00.050.179 I print_info: n_expert         = 0
0.00.050.179 I print_info: n_expert_used    = 0
0.00.050.179 I print_info: causal attn      = 1
0.00.050.180 I print_info: pooling type     = 0
0.00.050.180 I print_info: rope type        = 2
0.00.050.181 I print_info: rope scaling     = linear
0.00.050.181 I print_info: freq_base_train  = 10000.0
0.00.050.181 I print_info: freq_scale_train = 1
0.00.050.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.182 I print_info: rope_finetuned   = unknown
0.00.050.182 I print_info: ssm_d_conv       = 0
0.00.050.182 I print_info: ssm_d_inner      = 0
0.00.050.182 I print_info: ssm_d_state      = 0
0.00.050.182 I print_info: ssm_dt_rank      = 0
0.00.050.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.182 I print_info: model type       = 1.4B
0.00.050.183 I print_info: model params     = 1.41 B
0.00.050.183 I print_info: general.name     = 1.4B
0.00.050.184 I print_info: vocab type       = BPE
0.00.050.185 I print_info: n_vocab          = 50304
0.00.050.189 I print_info: n_merges         = 50009
0.00.050.190 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.190 I print_info: LF token         = 128 'Ä'
0.00.050.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.191 I print_info: max token length = 1024
0.00.051.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.951 I load_tensors: offloading output layer to GPU
0.00.051.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.962 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.963 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.234 I llama_init_from_model: n_seq_max     = 1
0.00.052.234 I llama_init_from_model: n_ctx         = 2048
0.00.052.234 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.235 I llama_init_from_model: n_batch       = 2048
0.00.052.235 I llama_init_from_model: n_ubatch      = 512
0.00.052.235 I llama_init_from_model: flash_attn    = 0
0.00.052.235 I llama_init_from_model: freq_base     = 10000.0
0.00.052.236 I llama_init_from_model: freq_scale    = 1
0.00.052.236 I ggml_metal_init: allocating
0.00.052.238 I ggml_metal_init: found device: Apple M4
0.00.052.240 I ggml_metal_init: picking default device: Apple M4
0.00.052.836 I ggml_metal_init: using embedded metal library
0.00.055.213 I ggml_metal_init: GPU name:   Apple M4
0.00.055.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.215 I ggml_metal_init: simdgroup reduction   = true
0.00.055.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.215 I ggml_metal_init: has bfloat            = true
0.00.055.215 I ggml_metal_init: use bfloat            = true
0.00.055.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.541 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.547 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.516 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.517 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.517 I llama_init_from_model: graph nodes  = 967
0.00.083.518 I llama_init_from_model: graph splits = 2
0.00.083.533 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.668 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.548 I main: llama threadpool init, n_threads = 4
0.00.653.586 I 
0.00.653.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.622 I 
0.00.653.774 I sampler seed: 1234
0.00.653.779 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.798 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.798 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.798 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.408.751 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.408.752 I llama_perf_context_print:        load time =     643.29 ms
0.01.408.753 I llama_perf_context_print: prompt eval time =      48.51 ms /     7 tokens (    6.93 ms per token,   144.31 tokens per second)
0.01.408.753 I llama_perf_context_print:        eval time =     703.45 ms /    63 runs   (   11.17 ms per token,    89.56 tokens per second)
0.01.408.755 I llama_perf_context_print:       total time =     756.13 ms /    70 tokens
0.01.408.986 I ggml_metal_free: deallocating

real	0m1.424s
user	0m0.107s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.475 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.408 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.409 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.200 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.202 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.202 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.203 I llama_model_loader: - type  f32:  194 tensors
0.00.026.203 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.203 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.204 I print_info: file format = GGUF V3 (latest)
0.00.026.204 I print_info: file type   = Q5_K - Medium
0.00.026.205 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.340 I load: special tokens cache size = 25
0.00.051.405 I load: token to piece cache size = 0.2984 MB
0.00.051.408 I print_info: arch             = gptneox
0.00.051.408 I print_info: vocab_only       = 0
0.00.051.408 I print_info: n_ctx_train      = 2048
0.00.051.408 I print_info: n_embd           = 2048
0.00.051.408 I print_info: n_layer          = 24
0.00.051.411 I print_info: n_head           = 16
0.00.051.412 I print_info: n_head_kv        = 16
0.00.051.412 I print_info: n_rot            = 32
0.00.051.412 I print_info: n_swa            = 0
0.00.051.413 I print_info: n_embd_head_k    = 128
0.00.051.413 I print_info: n_embd_head_v    = 128
0.00.051.415 I print_info: n_gqa            = 1
0.00.051.416 I print_info: n_embd_k_gqa     = 2048
0.00.051.417 I print_info: n_embd_v_gqa     = 2048
0.00.051.417 I print_info: f_norm_eps       = 1.0e-05
0.00.051.418 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.425 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.427 I print_info: f_logit_scale    = 0.0e+00
0.00.051.434 I print_info: n_ff             = 8192
0.00.051.434 I print_info: n_expert         = 0
0.00.051.434 I print_info: n_expert_used    = 0
0.00.051.434 I print_info: causal attn      = 1
0.00.051.434 I print_info: pooling type     = 0
0.00.051.435 I print_info: rope type        = 2
0.00.051.436 I print_info: rope scaling     = linear
0.00.051.436 I print_info: freq_base_train  = 10000.0
0.00.051.436 I print_info: freq_scale_train = 1
0.00.051.437 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.437 I print_info: rope_finetuned   = unknown
0.00.051.438 I print_info: ssm_d_conv       = 0
0.00.051.438 I print_info: ssm_d_inner      = 0
0.00.051.438 I print_info: ssm_d_state      = 0
0.00.051.438 I print_info: ssm_dt_rank      = 0
0.00.051.438 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.438 I print_info: model type       = 1.4B
0.00.051.439 I print_info: model params     = 1.41 B
0.00.051.439 I print_info: general.name     = 1.4B
0.00.051.439 I print_info: vocab type       = BPE
0.00.051.440 I print_info: n_vocab          = 50304
0.00.051.440 I print_info: n_merges         = 50009
0.00.051.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.440 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.441 I print_info: LF token         = 128 'Ä'
0.00.051.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.441 I print_info: max token length = 1024
0.00.053.171 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.171 I load_tensors: offloading output layer to GPU
0.00.053.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.181 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.182 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.445 I llama_init_from_model: n_seq_max     = 1
0.00.053.446 I llama_init_from_model: n_ctx         = 2048
0.00.053.446 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.446 I llama_init_from_model: n_batch       = 2048
0.00.053.446 I llama_init_from_model: n_ubatch      = 512
0.00.053.446 I llama_init_from_model: flash_attn    = 0
0.00.053.447 I llama_init_from_model: freq_base     = 10000.0
0.00.053.447 I llama_init_from_model: freq_scale    = 1
0.00.053.447 I ggml_metal_init: allocating
0.00.053.450 I ggml_metal_init: found device: Apple M4
0.00.053.452 I ggml_metal_init: picking default device: Apple M4
0.00.054.047 I ggml_metal_init: using embedded metal library
0.00.056.390 I ggml_metal_init: GPU name:   Apple M4
0.00.056.391 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.392 I ggml_metal_init: simdgroup reduction   = true
0.00.056.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.393 I ggml_metal_init: has bfloat            = true
0.00.056.393 I ggml_metal_init: use bfloat            = true
0.00.056.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.829 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.945 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.046 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.048 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.048 I llama_init_from_model: graph nodes  = 967
0.00.086.049 I llama_init_from_model: graph splits = 2
0.00.086.064 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.917 I main: llama threadpool init, n_threads = 4
0.00.744.957 I 
0.00.744.977 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.981 I 
0.00.745.132 I sampler seed: 1234
0.00.745.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.148 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.151 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.590.916 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60787.67 tokens per second)
0.01.590.917 I llama_perf_context_print:        load time =     734.49 ms
0.01.590.918 I llama_perf_context_print: prompt eval time =      52.11 ms /     7 tokens (    7.44 ms per token,   134.33 tokens per second)
0.01.590.918 I llama_perf_context_print:        eval time =     790.61 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.590.919 I llama_perf_context_print:       total time =     846.95 ms /    70 tokens
0.01.591.166 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.023 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.440 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.304 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.305 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.306 I llama_model_loader: - type  f32:  194 tensors
0.00.025.306 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.306 I print_info: file format = GGUF V3 (latest)
0.00.025.307 I print_info: file type   = Q6_K
0.00.025.308 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.665 I load: special tokens cache size = 25
0.00.049.603 I load: token to piece cache size = 0.2984 MB
0.00.049.606 I print_info: arch             = gptneox
0.00.049.606 I print_info: vocab_only       = 0
0.00.049.606 I print_info: n_ctx_train      = 2048
0.00.049.607 I print_info: n_embd           = 2048
0.00.049.607 I print_info: n_layer          = 24
0.00.049.609 I print_info: n_head           = 16
0.00.049.610 I print_info: n_head_kv        = 16
0.00.049.610 I print_info: n_rot            = 32
0.00.049.610 I print_info: n_swa            = 0
0.00.049.610 I print_info: n_embd_head_k    = 128
0.00.049.611 I print_info: n_embd_head_v    = 128
0.00.049.611 I print_info: n_gqa            = 1
0.00.049.612 I print_info: n_embd_k_gqa     = 2048
0.00.049.613 I print_info: n_embd_v_gqa     = 2048
0.00.049.613 I print_info: f_norm_eps       = 1.0e-05
0.00.049.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.614 I print_info: f_logit_scale    = 0.0e+00
0.00.049.615 I print_info: n_ff             = 8192
0.00.049.615 I print_info: n_expert         = 0
0.00.049.615 I print_info: n_expert_used    = 0
0.00.049.615 I print_info: causal attn      = 1
0.00.049.615 I print_info: pooling type     = 0
0.00.049.616 I print_info: rope type        = 2
0.00.049.616 I print_info: rope scaling     = linear
0.00.049.616 I print_info: freq_base_train  = 10000.0
0.00.049.616 I print_info: freq_scale_train = 1
0.00.049.617 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.617 I print_info: rope_finetuned   = unknown
0.00.049.617 I print_info: ssm_d_conv       = 0
0.00.049.617 I print_info: ssm_d_inner      = 0
0.00.049.617 I print_info: ssm_d_state      = 0
0.00.049.618 I print_info: ssm_dt_rank      = 0
0.00.049.618 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.618 I print_info: model type       = 1.4B
0.00.049.618 I print_info: model params     = 1.41 B
0.00.049.618 I print_info: general.name     = 1.4B
0.00.049.619 I print_info: vocab type       = BPE
0.00.049.619 I print_info: n_vocab          = 50304
0.00.049.620 I print_info: n_merges         = 50009
0.00.049.620 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.620 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.620 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.620 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.621 I print_info: LF token         = 128 'Ä'
0.00.049.621 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.621 I print_info: max token length = 1024
0.00.051.331 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.332 I load_tensors: offloading output layer to GPU
0.00.051.332 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.341 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.343 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.608 I llama_init_from_model: n_seq_max     = 1
0.00.051.608 I llama_init_from_model: n_ctx         = 2048
0.00.051.609 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.609 I llama_init_from_model: n_batch       = 2048
0.00.051.609 I llama_init_from_model: n_ubatch      = 512
0.00.051.609 I llama_init_from_model: flash_attn    = 0
0.00.051.610 I llama_init_from_model: freq_base     = 10000.0
0.00.051.610 I llama_init_from_model: freq_scale    = 1
0.00.051.610 I ggml_metal_init: allocating
0.00.051.612 I ggml_metal_init: found device: Apple M4
0.00.051.614 I ggml_metal_init: picking default device: Apple M4
0.00.052.204 I ggml_metal_init: using embedded metal library
0.00.054.540 I ggml_metal_init: GPU name:   Apple M4
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.543 I ggml_metal_init: simdgroup reduction   = true
0.00.054.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.543 I ggml_metal_init: has bfloat            = true
0.00.054.543 I ggml_metal_init: use bfloat            = true
0.00.054.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.261 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.694 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.702 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.703 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.704 I llama_init_from_model: graph nodes  = 967
0.00.083.704 I llama_init_from_model: graph splits = 2
0.00.083.720 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.849 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.543 I main: llama threadpool init, n_threads = 4
0.00.828.580 I 
0.00.828.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.601 I 
0.00.828.784 I sampler seed: 1234
0.00.828.789 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.833 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.835 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.695.917 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.695.917 I llama_perf_context_print:        load time =     818.61 ms
0.01.695.918 I llama_perf_context_print: prompt eval time =      54.87 ms /     7 tokens (    7.84 ms per token,   127.57 tokens per second)
0.01.695.919 I llama_perf_context_print:        eval time =     809.17 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.695.919 I llama_perf_context_print:       total time =     868.28 ms /    70 tokens
0.01.696.139 I ggml_metal_free: deallocating

real	0m1.712s
user	0m0.108s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.788 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.968 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.978 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.979 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.979 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.980 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.144 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.761 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.762 I llama_model_loader: - type  f32:  194 tensors
0.00.050.763 I llama_model_loader: - type  f16:   98 tensors
0.00.050.763 I print_info: file format = GGUF V3 (latest)
0.00.050.764 I print_info: file type   = all F32 (guessed)
0.00.050.765 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.075.441 I load: special tokens cache size = 25
0.00.082.169 I load: token to piece cache size = 0.2984 MB
0.00.082.172 I print_info: arch             = gptneox
0.00.082.172 I print_info: vocab_only       = 0
0.00.082.172 I print_info: n_ctx_train      = 2048
0.00.082.172 I print_info: n_embd           = 2048
0.00.082.173 I print_info: n_layer          = 24
0.00.082.175 I print_info: n_head           = 16
0.00.082.176 I print_info: n_head_kv        = 16
0.00.082.176 I print_info: n_rot            = 32
0.00.082.176 I print_info: n_swa            = 0
0.00.082.176 I print_info: n_embd_head_k    = 128
0.00.082.176 I print_info: n_embd_head_v    = 128
0.00.082.177 I print_info: n_gqa            = 1
0.00.082.178 I print_info: n_embd_k_gqa     = 2048
0.00.082.179 I print_info: n_embd_v_gqa     = 2048
0.00.082.179 I print_info: f_norm_eps       = 1.0e-05
0.00.082.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.180 I print_info: f_logit_scale    = 0.0e+00
0.00.082.180 I print_info: n_ff             = 8192
0.00.082.181 I print_info: n_expert         = 0
0.00.082.181 I print_info: n_expert_used    = 0
0.00.082.181 I print_info: causal attn      = 1
0.00.082.181 I print_info: pooling type     = 0
0.00.082.181 I print_info: rope type        = 2
0.00.082.181 I print_info: rope scaling     = linear
0.00.082.182 I print_info: freq_base_train  = 10000.0
0.00.082.182 I print_info: freq_scale_train = 1
0.00.082.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.182 I print_info: rope_finetuned   = unknown
0.00.082.183 I print_info: ssm_d_conv       = 0
0.00.082.183 I print_info: ssm_d_inner      = 0
0.00.082.183 I print_info: ssm_d_state      = 0
0.00.082.183 I print_info: ssm_dt_rank      = 0
0.00.082.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.183 I print_info: model type       = 1.4B
0.00.082.184 I print_info: model params     = 1.41 B
0.00.082.184 I print_info: general.name     = 1.4B
0.00.082.184 I print_info: vocab type       = BPE
0.00.082.184 I print_info: n_vocab          = 50304
0.00.082.185 I print_info: n_merges         = 50009
0.00.082.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.185 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.185 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.186 I print_info: LF token         = 128 'Ä'
0.00.082.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.186 I print_info: max token length = 1024
0.00.084.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.084.058 I load_tensors: offloading output layer to GPU
0.00.084.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.084.069 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.084.070 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.084.345 I llama_init_from_model: n_seq_max     = 1
0.00.084.346 I llama_init_from_model: n_ctx         = 128
0.00.084.346 I llama_init_from_model: n_ctx_per_seq = 128
0.00.084.346 I llama_init_from_model: n_batch       = 128
0.00.084.346 I llama_init_from_model: n_ubatch      = 128
0.00.084.347 I llama_init_from_model: flash_attn    = 0
0.00.084.347 I llama_init_from_model: freq_base     = 10000.0
0.00.084.347 I llama_init_from_model: freq_scale    = 1
0.00.084.348 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.348 I ggml_metal_init: allocating
0.00.084.350 I ggml_metal_init: found device: Apple M4
0.00.084.352 I ggml_metal_init: picking default device: Apple M4
0.00.084.919 I ggml_metal_init: using embedded metal library
0.00.087.427 I ggml_metal_init: GPU name:   Apple M4
0.00.087.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.430 I ggml_metal_init: simdgroup reduction   = true
0.00.087.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.430 I ggml_metal_init: has bfloat            = true
0.00.087.430 I ggml_metal_init: use bfloat            = true
0.00.087.431 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.381 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.747 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.097.751 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.097.767 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.098.611 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.098.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.098.613 I llama_init_from_model: graph nodes  = 967
0.00.098.613 I llama_init_from_model: graph splits = 2
0.00.098.614 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.098.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.580.027 I 
0.01.580.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.580.093 I perplexity: tokenizing the input ..
0.01.591.071 I perplexity: tokenization took 10.975 ms
0.01.591.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.712.619 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.714.256 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.714.277 I llama_perf_context_print:        load time =    1561.68 ms
0.01.714.279 I llama_perf_context_print: prompt eval time =     121.01 ms /   128 tokens (    0.95 ms per token,  1057.75 tokens per second)
0.01.714.281 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.714.281 I llama_perf_context_print:       total time =     134.25 ms /   129 tokens
0.01.714.928 I ggml_metal_free: deallocating

real	0m1.918s
user	0m0.121s
sys	0m0.338s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.906 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.702 I llama_model_loader: - type  f32:  194 tensors
0.00.035.703 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.704 I print_info: file format = GGUF V3 (latest)
0.00.035.705 I print_info: file type   = Q8_0
0.00.035.706 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.137 I load: special tokens cache size = 25
0.00.066.754 I load: token to piece cache size = 0.2984 MB
0.00.066.761 I print_info: arch             = gptneox
0.00.066.761 I print_info: vocab_only       = 0
0.00.066.761 I print_info: n_ctx_train      = 2048
0.00.066.761 I print_info: n_embd           = 2048
0.00.066.762 I print_info: n_layer          = 24
0.00.066.765 I print_info: n_head           = 16
0.00.066.766 I print_info: n_head_kv        = 16
0.00.066.766 I print_info: n_rot            = 32
0.00.066.767 I print_info: n_swa            = 0
0.00.066.767 I print_info: n_embd_head_k    = 128
0.00.066.767 I print_info: n_embd_head_v    = 128
0.00.066.769 I print_info: n_gqa            = 1
0.00.066.770 I print_info: n_embd_k_gqa     = 2048
0.00.066.771 I print_info: n_embd_v_gqa     = 2048
0.00.066.771 I print_info: f_norm_eps       = 1.0e-05
0.00.066.774 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.775 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.775 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.775 I print_info: f_logit_scale    = 0.0e+00
0.00.066.776 I print_info: n_ff             = 8192
0.00.066.776 I print_info: n_expert         = 0
0.00.066.776 I print_info: n_expert_used    = 0
0.00.066.776 I print_info: causal attn      = 1
0.00.066.776 I print_info: pooling type     = 0
0.00.066.776 I print_info: rope type        = 2
0.00.066.777 I print_info: rope scaling     = linear
0.00.066.777 I print_info: freq_base_train  = 10000.0
0.00.066.777 I print_info: freq_scale_train = 1
0.00.066.777 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.781 I print_info: rope_finetuned   = unknown
0.00.066.781 I print_info: ssm_d_conv       = 0
0.00.066.782 I print_info: ssm_d_inner      = 0
0.00.066.782 I print_info: ssm_d_state      = 0
0.00.066.782 I print_info: ssm_dt_rank      = 0
0.00.066.782 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.782 I print_info: model type       = 1.4B
0.00.066.782 I print_info: model params     = 1.41 B
0.00.066.783 I print_info: general.name     = 1.4B
0.00.066.783 I print_info: vocab type       = BPE
0.00.066.784 I print_info: n_vocab          = 50304
0.00.066.784 I print_info: n_merges         = 50009
0.00.066.784 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.784 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.784 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.784 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.785 I print_info: LF token         = 128 'Ä'
0.00.066.785 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.785 I print_info: max token length = 1024
0.00.068.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.777 I load_tensors: offloading output layer to GPU
0.00.068.778 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.788 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.789 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.102 I llama_init_from_model: n_seq_max     = 1
0.00.069.103 I llama_init_from_model: n_ctx         = 128
0.00.069.103 I llama_init_from_model: n_ctx_per_seq = 128
0.00.069.103 I llama_init_from_model: n_batch       = 128
0.00.069.104 I llama_init_from_model: n_ubatch      = 128
0.00.069.104 I llama_init_from_model: flash_attn    = 0
0.00.069.104 I llama_init_from_model: freq_base     = 10000.0
0.00.069.104 I llama_init_from_model: freq_scale    = 1
0.00.069.105 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.105 I ggml_metal_init: allocating
0.00.069.108 I ggml_metal_init: found device: Apple M4
0.00.069.110 I ggml_metal_init: picking default device: Apple M4
0.00.069.779 I ggml_metal_init: using embedded metal library
0.00.072.471 I ggml_metal_init: GPU name:   Apple M4
0.00.072.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.474 I ggml_metal_init: simdgroup reduction   = true
0.00.072.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.474 I ggml_metal_init: has bfloat            = true
0.00.072.475 I ggml_metal_init: use bfloat            = true
0.00.072.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.846 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.368 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.387 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.388 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.388 I llama_init_from_model: graph nodes  = 967
0.00.084.388 I llama_init_from_model: graph splits = 2
0.00.084.389 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.084.313 I 
0.01.084.341 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.084.375 I perplexity: tokenizing the input ..
0.01.091.964 I perplexity: tokenization took 7.587 ms
0.01.091.968 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.216.193 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.217.276 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.217.294 I llama_perf_context_print:        load time =    1073.41 ms
0.01.217.296 I llama_perf_context_print: prompt eval time =     124.01 ms /   128 tokens (    0.97 ms per token,  1032.22 tokens per second)
0.01.217.297 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.217.298 I llama_perf_context_print:       total time =     132.98 ms /   129 tokens
0.01.217.799 I ggml_metal_free: deallocating

real	0m1.236s
user	0m0.096s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.497 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.430 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.430 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.431 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.844 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.844 I llama_model_loader: - type  f32:  194 tensors
0.00.024.845 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.845 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.845 I print_info: file format = GGUF V3 (latest)
0.00.024.846 I print_info: file type   = Q4_0
0.00.024.846 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.989 I load: special tokens cache size = 25
0.00.048.717 I load: token to piece cache size = 0.2984 MB
0.00.048.720 I print_info: arch             = gptneox
0.00.048.720 I print_info: vocab_only       = 0
0.00.048.721 I print_info: n_ctx_train      = 2048
0.00.048.721 I print_info: n_embd           = 2048
0.00.048.721 I print_info: n_layer          = 24
0.00.048.724 I print_info: n_head           = 16
0.00.048.724 I print_info: n_head_kv        = 16
0.00.048.724 I print_info: n_rot            = 32
0.00.048.725 I print_info: n_swa            = 0
0.00.048.725 I print_info: n_embd_head_k    = 128
0.00.048.725 I print_info: n_embd_head_v    = 128
0.00.048.726 I print_info: n_gqa            = 1
0.00.048.726 I print_info: n_embd_k_gqa     = 2048
0.00.048.727 I print_info: n_embd_v_gqa     = 2048
0.00.048.727 I print_info: f_norm_eps       = 1.0e-05
0.00.048.728 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.728 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.728 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.728 I print_info: f_logit_scale    = 0.0e+00
0.00.048.729 I print_info: n_ff             = 8192
0.00.048.730 I print_info: n_expert         = 0
0.00.048.730 I print_info: n_expert_used    = 0
0.00.048.731 I print_info: causal attn      = 1
0.00.048.731 I print_info: pooling type     = 0
0.00.048.731 I print_info: rope type        = 2
0.00.048.731 I print_info: rope scaling     = linear
0.00.048.732 I print_info: freq_base_train  = 10000.0
0.00.048.732 I print_info: freq_scale_train = 1
0.00.048.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.732 I print_info: rope_finetuned   = unknown
0.00.048.732 I print_info: ssm_d_conv       = 0
0.00.048.733 I print_info: ssm_d_inner      = 0
0.00.048.733 I print_info: ssm_d_state      = 0
0.00.048.734 I print_info: ssm_dt_rank      = 0
0.00.048.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.734 I print_info: model type       = 1.4B
0.00.048.735 I print_info: model params     = 1.41 B
0.00.048.735 I print_info: general.name     = 1.4B
0.00.048.735 I print_info: vocab type       = BPE
0.00.048.738 I print_info: n_vocab          = 50304
0.00.048.738 I print_info: n_merges         = 50009
0.00.048.738 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.738 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.738 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.739 I print_info: LF token         = 128 'Ä'
0.00.048.739 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.739 I print_info: max token length = 1024
0.00.050.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.338 I load_tensors: offloading output layer to GPU
0.00.050.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.348 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.050.349 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.611 I llama_init_from_model: n_seq_max     = 1
0.00.050.612 I llama_init_from_model: n_ctx         = 128
0.00.050.612 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.612 I llama_init_from_model: n_batch       = 128
0.00.050.613 I llama_init_from_model: n_ubatch      = 128
0.00.050.613 I llama_init_from_model: flash_attn    = 0
0.00.050.613 I llama_init_from_model: freq_base     = 10000.0
0.00.050.613 I llama_init_from_model: freq_scale    = 1
0.00.050.614 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.614 I ggml_metal_init: allocating
0.00.050.616 I ggml_metal_init: found device: Apple M4
0.00.050.618 I ggml_metal_init: picking default device: Apple M4
0.00.051.183 I ggml_metal_init: using embedded metal library
0.00.053.541 I ggml_metal_init: GPU name:   Apple M4
0.00.053.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.543 I ggml_metal_init: simdgroup reduction   = true
0.00.053.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.543 I ggml_metal_init: has bfloat            = true
0.00.053.544 I ggml_metal_init: use bfloat            = true
0.00.053.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.787 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.128 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.132 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.147 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.008 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.009 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.010 I llama_init_from_model: graph nodes  = 967
0.00.064.010 I llama_init_from_model: graph splits = 2
0.00.064.011 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.139 I 
0.00.695.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.186 I perplexity: tokenizing the input ..
0.00.702.552 I perplexity: tokenization took 7.364 ms
0.00.702.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.544 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.826.618 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.826.632 I llama_perf_context_print:        load time =     685.64 ms
0.00.826.633 I llama_perf_context_print: prompt eval time =     122.77 ms /   128 tokens (    0.96 ms per token,  1042.63 tokens per second)
0.00.826.633 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.634 I llama_perf_context_print:       total time =     131.49 ms /   129 tokens
0.00.827.073 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.075s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.309 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.311 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.311 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.311 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.153 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.154 I llama_model_loader: - type  f32:  194 tensors
0.00.024.154 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.155 I print_info: file format = GGUF V3 (latest)
0.00.024.155 I print_info: file type   = Q4_1
0.00.024.156 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.204 I load: special tokens cache size = 25
0.00.048.279 I load: token to piece cache size = 0.2984 MB
0.00.048.282 I print_info: arch             = gptneox
0.00.048.282 I print_info: vocab_only       = 0
0.00.048.282 I print_info: n_ctx_train      = 2048
0.00.048.283 I print_info: n_embd           = 2048
0.00.048.283 I print_info: n_layer          = 24
0.00.048.286 I print_info: n_head           = 16
0.00.048.286 I print_info: n_head_kv        = 16
0.00.048.287 I print_info: n_rot            = 32
0.00.048.287 I print_info: n_swa            = 0
0.00.048.287 I print_info: n_embd_head_k    = 128
0.00.048.287 I print_info: n_embd_head_v    = 128
0.00.048.288 I print_info: n_gqa            = 1
0.00.048.289 I print_info: n_embd_k_gqa     = 2048
0.00.048.289 I print_info: n_embd_v_gqa     = 2048
0.00.048.290 I print_info: f_norm_eps       = 1.0e-05
0.00.048.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.291 I print_info: f_logit_scale    = 0.0e+00
0.00.048.291 I print_info: n_ff             = 8192
0.00.048.292 I print_info: n_expert         = 0
0.00.048.292 I print_info: n_expert_used    = 0
0.00.048.292 I print_info: causal attn      = 1
0.00.048.292 I print_info: pooling type     = 0
0.00.048.292 I print_info: rope type        = 2
0.00.048.292 I print_info: rope scaling     = linear
0.00.048.293 I print_info: freq_base_train  = 10000.0
0.00.048.293 I print_info: freq_scale_train = 1
0.00.048.293 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.293 I print_info: rope_finetuned   = unknown
0.00.048.294 I print_info: ssm_d_conv       = 0
0.00.048.294 I print_info: ssm_d_inner      = 0
0.00.048.294 I print_info: ssm_d_state      = 0
0.00.048.294 I print_info: ssm_dt_rank      = 0
0.00.048.294 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.294 I print_info: model type       = 1.4B
0.00.048.295 I print_info: model params     = 1.41 B
0.00.048.295 I print_info: general.name     = 1.4B
0.00.048.295 I print_info: vocab type       = BPE
0.00.048.296 I print_info: n_vocab          = 50304
0.00.048.296 I print_info: n_merges         = 50009
0.00.048.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.296 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.297 I print_info: LF token         = 128 'Ä'
0.00.048.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.297 I print_info: max token length = 1024
0.00.051.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.455 I load_tensors: offloading output layer to GPU
0.00.051.456 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.466 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.467 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.816 I llama_init_from_model: n_seq_max     = 1
0.00.051.817 I llama_init_from_model: n_ctx         = 128
0.00.051.817 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.817 I llama_init_from_model: n_batch       = 128
0.00.051.817 I llama_init_from_model: n_ubatch      = 128
0.00.051.817 I llama_init_from_model: flash_attn    = 0
0.00.051.818 I llama_init_from_model: freq_base     = 10000.0
0.00.051.818 I llama_init_from_model: freq_scale    = 1
0.00.051.818 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.819 I ggml_metal_init: allocating
0.00.051.821 I ggml_metal_init: found device: Apple M4
0.00.051.823 I ggml_metal_init: picking default device: Apple M4
0.00.052.369 I ggml_metal_init: using embedded metal library
0.00.054.705 I ggml_metal_init: GPU name:   Apple M4
0.00.054.706 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.707 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.707 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.707 I ggml_metal_init: simdgroup reduction   = true
0.00.054.707 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.707 I ggml_metal_init: has bfloat            = true
0.00.054.707 I ggml_metal_init: use bfloat            = true
0.00.054.708 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.799 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.168 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.062 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.064 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.064 I llama_init_from_model: graph nodes  = 967
0.00.065.064 I llama_init_from_model: graph splits = 2
0.00.065.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.664 I 
0.00.716.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.710 I perplexity: tokenizing the input ..
0.00.724.452 I perplexity: tokenization took 7.741 ms
0.00.724.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.748 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.848.862 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.848.879 I llama_perf_context_print:        load time =     707.67 ms
0.00.848.880 I llama_perf_context_print: prompt eval time =     123.07 ms /   128 tokens (    0.96 ms per token,  1040.06 tokens per second)
0.00.848.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.881 I llama_perf_context_print:       total time =     132.22 ms /   129 tokens
0.00.849.336 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.076s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.555 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.049 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.050 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.050 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.050 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.051 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.052 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.052 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.052 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.053 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.846 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.725 I llama_model_loader: - type  f32:  194 tensors
0.00.026.725 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.725 I print_info: file format = GGUF V3 (latest)
0.00.026.726 I print_info: file type   = Q5_0
0.00.026.727 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.876 I load: special tokens cache size = 25
0.00.050.854 I load: token to piece cache size = 0.2984 MB
0.00.050.859 I print_info: arch             = gptneox
0.00.050.860 I print_info: vocab_only       = 0
0.00.050.860 I print_info: n_ctx_train      = 2048
0.00.050.860 I print_info: n_embd           = 2048
0.00.050.860 I print_info: n_layer          = 24
0.00.050.863 I print_info: n_head           = 16
0.00.050.864 I print_info: n_head_kv        = 16
0.00.050.864 I print_info: n_rot            = 32
0.00.050.864 I print_info: n_swa            = 0
0.00.050.864 I print_info: n_embd_head_k    = 128
0.00.050.864 I print_info: n_embd_head_v    = 128
0.00.050.865 I print_info: n_gqa            = 1
0.00.050.866 I print_info: n_embd_k_gqa     = 2048
0.00.050.866 I print_info: n_embd_v_gqa     = 2048
0.00.050.867 I print_info: f_norm_eps       = 1.0e-05
0.00.050.867 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.868 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.868 I print_info: f_logit_scale    = 0.0e+00
0.00.050.869 I print_info: n_ff             = 8192
0.00.050.869 I print_info: n_expert         = 0
0.00.050.869 I print_info: n_expert_used    = 0
0.00.050.869 I print_info: causal attn      = 1
0.00.050.869 I print_info: pooling type     = 0
0.00.050.870 I print_info: rope type        = 2
0.00.050.870 I print_info: rope scaling     = linear
0.00.050.870 I print_info: freq_base_train  = 10000.0
0.00.050.870 I print_info: freq_scale_train = 1
0.00.050.871 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.871 I print_info: rope_finetuned   = unknown
0.00.050.871 I print_info: ssm_d_conv       = 0
0.00.050.871 I print_info: ssm_d_inner      = 0
0.00.050.871 I print_info: ssm_d_state      = 0
0.00.050.871 I print_info: ssm_dt_rank      = 0
0.00.050.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.874 I print_info: model type       = 1.4B
0.00.050.875 I print_info: model params     = 1.41 B
0.00.050.875 I print_info: general.name     = 1.4B
0.00.050.875 I print_info: vocab type       = BPE
0.00.050.876 I print_info: n_vocab          = 50304
0.00.050.876 I print_info: n_merges         = 50009
0.00.050.876 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.876 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.877 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.877 I print_info: LF token         = 128 'Ä'
0.00.050.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.877 I print_info: max token length = 1024
0.00.052.626 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.626 I load_tensors: offloading output layer to GPU
0.00.052.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.636 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.638 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.912 I llama_init_from_model: n_seq_max     = 1
0.00.052.913 I llama_init_from_model: n_ctx         = 128
0.00.052.913 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.913 I llama_init_from_model: n_batch       = 128
0.00.052.913 I llama_init_from_model: n_ubatch      = 128
0.00.052.914 I llama_init_from_model: flash_attn    = 0
0.00.052.914 I llama_init_from_model: freq_base     = 10000.0
0.00.052.914 I llama_init_from_model: freq_scale    = 1
0.00.052.914 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.915 I ggml_metal_init: allocating
0.00.052.917 I ggml_metal_init: found device: Apple M4
0.00.052.919 I ggml_metal_init: picking default device: Apple M4
0.00.053.472 I ggml_metal_init: using embedded metal library
0.00.055.851 I ggml_metal_init: GPU name:   Apple M4
0.00.055.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.854 I ggml_metal_init: simdgroup reduction   = true
0.00.055.854 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.854 I ggml_metal_init: has bfloat            = true
0.00.055.854 I ggml_metal_init: use bfloat            = true
0.00.055.855 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.855 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.097 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.379 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.381 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.395 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.288 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.289 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.289 I llama_init_from_model: graph nodes  = 967
0.00.066.290 I llama_init_from_model: graph splits = 2
0.00.066.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.291 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.460 I 
0.00.746.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.506 I perplexity: tokenizing the input ..
0.00.754.270 I perplexity: tokenization took 7.763 ms
0.00.754.274 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.692 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.890.798 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.890.815 I llama_perf_context_print:        load time =     734.90 ms
0.00.890.816 I llama_perf_context_print: prompt eval time =     135.20 ms /   128 tokens (    1.06 ms per token,   946.77 tokens per second)
0.00.890.817 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.817 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.891.310 I ggml_metal_free: deallocating

real	0m0.906s
user	0m0.076s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.431 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.399 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.406 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.407 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.182 I llama_model_loader: - type  f32:  194 tensors
0.00.025.182 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.182 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.183 I print_info: file format = GGUF V3 (latest)
0.00.025.183 I print_info: file type   = Q5_1
0.00.025.184 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.000 I load: special tokens cache size = 25
0.00.050.149 I load: token to piece cache size = 0.2984 MB
0.00.050.151 I print_info: arch             = gptneox
0.00.050.151 I print_info: vocab_only       = 0
0.00.050.152 I print_info: n_ctx_train      = 2048
0.00.050.152 I print_info: n_embd           = 2048
0.00.050.152 I print_info: n_layer          = 24
0.00.050.154 I print_info: n_head           = 16
0.00.050.155 I print_info: n_head_kv        = 16
0.00.050.155 I print_info: n_rot            = 32
0.00.050.155 I print_info: n_swa            = 0
0.00.050.155 I print_info: n_embd_head_k    = 128
0.00.050.156 I print_info: n_embd_head_v    = 128
0.00.050.156 I print_info: n_gqa            = 1
0.00.050.157 I print_info: n_embd_k_gqa     = 2048
0.00.050.158 I print_info: n_embd_v_gqa     = 2048
0.00.050.158 I print_info: f_norm_eps       = 1.0e-05
0.00.050.159 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.159 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.159 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.159 I print_info: f_logit_scale    = 0.0e+00
0.00.050.160 I print_info: n_ff             = 8192
0.00.050.160 I print_info: n_expert         = 0
0.00.050.160 I print_info: n_expert_used    = 0
0.00.050.160 I print_info: causal attn      = 1
0.00.050.161 I print_info: pooling type     = 0
0.00.050.162 I print_info: rope type        = 2
0.00.050.162 I print_info: rope scaling     = linear
0.00.050.162 I print_info: freq_base_train  = 10000.0
0.00.050.163 I print_info: freq_scale_train = 1
0.00.050.163 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.163 I print_info: rope_finetuned   = unknown
0.00.050.163 I print_info: ssm_d_conv       = 0
0.00.050.165 I print_info: ssm_d_inner      = 0
0.00.050.166 I print_info: ssm_d_state      = 0
0.00.050.166 I print_info: ssm_dt_rank      = 0
0.00.050.166 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.166 I print_info: model type       = 1.4B
0.00.050.166 I print_info: model params     = 1.41 B
0.00.050.167 I print_info: general.name     = 1.4B
0.00.050.167 I print_info: vocab type       = BPE
0.00.050.167 I print_info: n_vocab          = 50304
0.00.050.167 I print_info: n_merges         = 50009
0.00.050.168 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: LF token         = 128 'Ä'
0.00.050.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.170 I print_info: max token length = 1024
0.00.051.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.881 I load_tensors: offloading output layer to GPU
0.00.051.881 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.891 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.892 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.191 I llama_init_from_model: n_seq_max     = 1
0.00.052.192 I llama_init_from_model: n_ctx         = 128
0.00.052.192 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.192 I llama_init_from_model: n_batch       = 128
0.00.052.192 I llama_init_from_model: n_ubatch      = 128
0.00.052.192 I llama_init_from_model: flash_attn    = 0
0.00.052.192 I llama_init_from_model: freq_base     = 10000.0
0.00.052.193 I llama_init_from_model: freq_scale    = 1
0.00.052.193 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.193 I ggml_metal_init: allocating
0.00.052.196 I ggml_metal_init: found device: Apple M4
0.00.052.198 I ggml_metal_init: picking default device: Apple M4
0.00.052.788 I ggml_metal_init: using embedded metal library
0.00.055.123 I ggml_metal_init: GPU name:   Apple M4
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.126 I ggml_metal_init: simdgroup reduction   = true
0.00.055.126 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.126 I ggml_metal_init: has bfloat            = true
0.00.055.126 I ggml_metal_init: use bfloat            = true
0.00.055.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.215 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.448 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.450 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.465 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.327 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.328 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.328 I llama_init_from_model: graph nodes  = 967
0.00.066.328 I llama_init_from_model: graph splits = 2
0.00.066.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.828 I 
0.00.703.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.909 I perplexity: tokenizing the input ..
0.00.711.312 I perplexity: tokenization took 7.401 ms
0.00.711.319 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.678 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.847.765 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.847.789 I llama_perf_context_print:        load time =     694.39 ms
0.00.847.790 I llama_perf_context_print: prompt eval time =     135.14 ms /   128 tokens (    1.06 ms per token,   947.17 tokens per second)
0.00.847.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.791 I llama_perf_context_print:       total time =     143.96 ms /   129 tokens
0.00.848.166 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.077s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.302 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.850 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.855 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.861 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.595 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.595 I llama_model_loader: - type  f32:  194 tensors
0.00.025.595 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.596 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.596 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.596 I print_info: file format = GGUF V3 (latest)
0.00.025.597 I print_info: file type   = Q2_K - Medium
0.00.025.597 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.688 I load: special tokens cache size = 25
0.00.049.634 I load: token to piece cache size = 0.2984 MB
0.00.049.637 I print_info: arch             = gptneox
0.00.049.637 I print_info: vocab_only       = 0
0.00.049.638 I print_info: n_ctx_train      = 2048
0.00.049.638 I print_info: n_embd           = 2048
0.00.049.638 I print_info: n_layer          = 24
0.00.049.641 I print_info: n_head           = 16
0.00.049.641 I print_info: n_head_kv        = 16
0.00.049.642 I print_info: n_rot            = 32
0.00.049.642 I print_info: n_swa            = 0
0.00.049.642 I print_info: n_embd_head_k    = 128
0.00.049.642 I print_info: n_embd_head_v    = 128
0.00.049.643 I print_info: n_gqa            = 1
0.00.049.643 I print_info: n_embd_k_gqa     = 2048
0.00.049.644 I print_info: n_embd_v_gqa     = 2048
0.00.049.645 I print_info: f_norm_eps       = 1.0e-05
0.00.049.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.646 I print_info: f_logit_scale    = 0.0e+00
0.00.049.647 I print_info: n_ff             = 8192
0.00.049.647 I print_info: n_expert         = 0
0.00.049.648 I print_info: n_expert_used    = 0
0.00.049.648 I print_info: causal attn      = 1
0.00.049.648 I print_info: pooling type     = 0
0.00.049.648 I print_info: rope type        = 2
0.00.049.654 I print_info: rope scaling     = linear
0.00.049.655 I print_info: freq_base_train  = 10000.0
0.00.049.655 I print_info: freq_scale_train = 1
0.00.049.655 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.656 I print_info: rope_finetuned   = unknown
0.00.049.656 I print_info: ssm_d_conv       = 0
0.00.049.656 I print_info: ssm_d_inner      = 0
0.00.049.656 I print_info: ssm_d_state      = 0
0.00.049.656 I print_info: ssm_dt_rank      = 0
0.00.049.657 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.657 I print_info: model type       = 1.4B
0.00.049.657 I print_info: model params     = 1.41 B
0.00.049.657 I print_info: general.name     = 1.4B
0.00.049.658 I print_info: vocab type       = BPE
0.00.049.658 I print_info: n_vocab          = 50304
0.00.049.658 I print_info: n_merges         = 50009
0.00.049.658 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.659 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.659 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.659 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.659 I print_info: LF token         = 128 'Ä'
0.00.049.659 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.660 I print_info: max token length = 1024
0.00.051.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.363 I load_tensors: offloading output layer to GPU
0.00.051.363 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.373 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.374 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.668 I llama_init_from_model: n_seq_max     = 1
0.00.051.668 I llama_init_from_model: n_ctx         = 128
0.00.051.669 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.669 I llama_init_from_model: n_batch       = 128
0.00.051.669 I llama_init_from_model: n_ubatch      = 128
0.00.051.669 I llama_init_from_model: flash_attn    = 0
0.00.051.669 I llama_init_from_model: freq_base     = 10000.0
0.00.051.670 I llama_init_from_model: freq_scale    = 1
0.00.051.670 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.670 I ggml_metal_init: allocating
0.00.051.673 I ggml_metal_init: found device: Apple M4
0.00.051.675 I ggml_metal_init: picking default device: Apple M4
0.00.052.252 I ggml_metal_init: using embedded metal library
0.00.054.569 I ggml_metal_init: GPU name:   Apple M4
0.00.054.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.571 I ggml_metal_init: simdgroup reduction   = true
0.00.054.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.571 I ggml_metal_init: has bfloat            = true
0.00.054.571 I ggml_metal_init: use bfloat            = true
0.00.054.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.205 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.207 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.065 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.066 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.066 I llama_init_from_model: graph nodes  = 967
0.00.065.066 I llama_init_from_model: graph splits = 2
0.00.065.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.068 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.411.410 I 
0.00.411.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.411.458 I perplexity: tokenizing the input ..
0.00.418.638 I perplexity: tokenization took 7.179 ms
0.00.418.642 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.579 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.677 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.691 I llama_perf_context_print:        load time =     401.10 ms
0.00.552.692 I llama_perf_context_print: prompt eval time =     132.72 ms /   128 tokens (    1.04 ms per token,   964.45 tokens per second)
0.00.552.693 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.693 I llama_perf_context_print:       total time =     141.28 ms /   129 tokens
0.00.553.170 I ggml_metal_free: deallocating

real	0m0.568s
user	0m0.076s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.398 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.053 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.054 I llama_model_loader: - type  f32:  194 tensors
0.00.024.054 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.054 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.055 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.055 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.056 I print_info: file format = GGUF V3 (latest)
0.00.024.056 I print_info: file type   = Q3_K - Medium
0.00.024.058 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.224 I load: special tokens cache size = 25
0.00.048.219 I load: token to piece cache size = 0.2984 MB
0.00.048.224 I print_info: arch             = gptneox
0.00.048.225 I print_info: vocab_only       = 0
0.00.048.225 I print_info: n_ctx_train      = 2048
0.00.048.225 I print_info: n_embd           = 2048
0.00.048.225 I print_info: n_layer          = 24
0.00.048.228 I print_info: n_head           = 16
0.00.048.229 I print_info: n_head_kv        = 16
0.00.048.230 I print_info: n_rot            = 32
0.00.048.230 I print_info: n_swa            = 0
0.00.048.230 I print_info: n_embd_head_k    = 128
0.00.048.230 I print_info: n_embd_head_v    = 128
0.00.048.231 I print_info: n_gqa            = 1
0.00.048.232 I print_info: n_embd_k_gqa     = 2048
0.00.048.232 I print_info: n_embd_v_gqa     = 2048
0.00.048.233 I print_info: f_norm_eps       = 1.0e-05
0.00.048.233 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.233 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.234 I print_info: f_logit_scale    = 0.0e+00
0.00.048.235 I print_info: n_ff             = 8192
0.00.048.235 I print_info: n_expert         = 0
0.00.048.235 I print_info: n_expert_used    = 0
0.00.048.237 I print_info: causal attn      = 1
0.00.048.237 I print_info: pooling type     = 0
0.00.048.237 I print_info: rope type        = 2
0.00.048.238 I print_info: rope scaling     = linear
0.00.048.238 I print_info: freq_base_train  = 10000.0
0.00.048.239 I print_info: freq_scale_train = 1
0.00.048.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.239 I print_info: rope_finetuned   = unknown
0.00.048.239 I print_info: ssm_d_conv       = 0
0.00.048.239 I print_info: ssm_d_inner      = 0
0.00.048.240 I print_info: ssm_d_state      = 0
0.00.048.240 I print_info: ssm_dt_rank      = 0
0.00.048.240 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.240 I print_info: model type       = 1.4B
0.00.048.241 I print_info: model params     = 1.41 B
0.00.048.241 I print_info: general.name     = 1.4B
0.00.048.241 I print_info: vocab type       = BPE
0.00.048.241 I print_info: n_vocab          = 50304
0.00.048.242 I print_info: n_merges         = 50009
0.00.048.242 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.242 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.242 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.242 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.243 I print_info: LF token         = 128 'Ä'
0.00.048.243 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.243 I print_info: max token length = 1024
0.00.049.934 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.934 I load_tensors: offloading output layer to GPU
0.00.049.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.944 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.049.946 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.256 I llama_init_from_model: n_seq_max     = 1
0.00.050.256 I llama_init_from_model: n_ctx         = 128
0.00.050.256 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.257 I llama_init_from_model: n_batch       = 128
0.00.050.257 I llama_init_from_model: n_ubatch      = 128
0.00.050.257 I llama_init_from_model: flash_attn    = 0
0.00.050.257 I llama_init_from_model: freq_base     = 10000.0
0.00.050.258 I llama_init_from_model: freq_scale    = 1
0.00.050.258 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.258 I ggml_metal_init: allocating
0.00.050.261 I ggml_metal_init: found device: Apple M4
0.00.050.263 I ggml_metal_init: picking default device: Apple M4
0.00.050.836 I ggml_metal_init: using embedded metal library
0.00.053.169 I ggml_metal_init: GPU name:   Apple M4
0.00.053.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.171 I ggml_metal_init: simdgroup reduction   = true
0.00.053.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.172 I ggml_metal_init: has bfloat            = true
0.00.053.172 I ggml_metal_init: use bfloat            = true
0.00.053.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.414 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.704 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.717 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.610 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.612 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.612 I llama_init_from_model: graph nodes  = 967
0.00.063.612 I llama_init_from_model: graph splits = 2
0.00.063.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.463 I 
0.00.508.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.526 I perplexity: tokenizing the input ..
0.00.515.940 I perplexity: tokenization took 7.412 ms
0.00.515.943 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.496 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.592 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.611 I llama_perf_context_print:        load time =     499.69 ms
0.00.649.612 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.26 tokens per second)
0.00.649.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.614 I llama_perf_context_print:       total time =     141.15 ms /   129 tokens
0.00.650.098 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.076s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.279 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.162 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.163 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.163 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.164 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.860 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.838 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.521 I llama_model_loader: - type  f32:  194 tensors
0.00.024.522 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.522 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.522 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.522 I print_info: file format = GGUF V3 (latest)
0.00.024.523 I print_info: file type   = Q4_K - Medium
0.00.024.523 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.624 I load: special tokens cache size = 25
0.00.048.499 I load: token to piece cache size = 0.2984 MB
0.00.048.503 I print_info: arch             = gptneox
0.00.048.503 I print_info: vocab_only       = 0
0.00.048.503 I print_info: n_ctx_train      = 2048
0.00.048.503 I print_info: n_embd           = 2048
0.00.048.504 I print_info: n_layer          = 24
0.00.048.506 I print_info: n_head           = 16
0.00.048.507 I print_info: n_head_kv        = 16
0.00.048.509 I print_info: n_rot            = 32
0.00.048.509 I print_info: n_swa            = 0
0.00.048.509 I print_info: n_embd_head_k    = 128
0.00.048.509 I print_info: n_embd_head_v    = 128
0.00.048.510 I print_info: n_gqa            = 1
0.00.048.511 I print_info: n_embd_k_gqa     = 2048
0.00.048.512 I print_info: n_embd_v_gqa     = 2048
0.00.048.512 I print_info: f_norm_eps       = 1.0e-05
0.00.048.513 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.513 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.513 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.513 I print_info: f_logit_scale    = 0.0e+00
0.00.048.514 I print_info: n_ff             = 8192
0.00.048.514 I print_info: n_expert         = 0
0.00.048.514 I print_info: n_expert_used    = 0
0.00.048.519 I print_info: causal attn      = 1
0.00.048.519 I print_info: pooling type     = 0
0.00.048.519 I print_info: rope type        = 2
0.00.048.520 I print_info: rope scaling     = linear
0.00.048.520 I print_info: freq_base_train  = 10000.0
0.00.048.520 I print_info: freq_scale_train = 1
0.00.048.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.521 I print_info: rope_finetuned   = unknown
0.00.048.521 I print_info: ssm_d_conv       = 0
0.00.048.521 I print_info: ssm_d_inner      = 0
0.00.048.521 I print_info: ssm_d_state      = 0
0.00.048.521 I print_info: ssm_dt_rank      = 0
0.00.048.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.522 I print_info: model type       = 1.4B
0.00.048.522 I print_info: model params     = 1.41 B
0.00.048.522 I print_info: general.name     = 1.4B
0.00.048.523 I print_info: vocab type       = BPE
0.00.048.523 I print_info: n_vocab          = 50304
0.00.048.523 I print_info: n_merges         = 50009
0.00.048.524 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: LF token         = 128 'Ä'
0.00.048.525 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.525 I print_info: max token length = 1024
0.00.050.184 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.184 I load_tensors: offloading output layer to GPU
0.00.050.185 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.195 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.196 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.473 I llama_init_from_model: n_seq_max     = 1
0.00.050.474 I llama_init_from_model: n_ctx         = 128
0.00.050.474 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.474 I llama_init_from_model: n_batch       = 128
0.00.050.474 I llama_init_from_model: n_ubatch      = 128
0.00.050.474 I llama_init_from_model: flash_attn    = 0
0.00.050.475 I llama_init_from_model: freq_base     = 10000.0
0.00.050.475 I llama_init_from_model: freq_scale    = 1
0.00.050.475 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.476 I ggml_metal_init: allocating
0.00.050.479 I ggml_metal_init: found device: Apple M4
0.00.050.481 I ggml_metal_init: picking default device: Apple M4
0.00.051.054 I ggml_metal_init: using embedded metal library
0.00.053.409 I ggml_metal_init: GPU name:   Apple M4
0.00.053.411 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.412 I ggml_metal_init: simdgroup reduction   = true
0.00.053.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.412 I ggml_metal_init: has bfloat            = true
0.00.053.412 I ggml_metal_init: use bfloat            = true
0.00.053.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.814 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.817 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.830 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.701 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.702 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.703 I llama_init_from_model: graph nodes  = 967
0.00.063.703 I llama_init_from_model: graph splits = 2
0.00.063.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.343 I 
0.00.607.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.385 I perplexity: tokenizing the input ..
0.00.615.012 I perplexity: tokenization took 7.626 ms
0.00.615.018 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.770 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.750.881 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.750.897 I llama_perf_context_print:        load time =     598.06 ms
0.00.750.898 I llama_perf_context_print: prompt eval time =     134.53 ms /   128 tokens (    1.05 ms per token,   951.44 tokens per second)
0.00.750.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.902 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.751.324 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.075s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.396 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.994 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.996 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.996 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.996 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.998 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.004 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.412 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.412 I llama_model_loader: - type  f32:  194 tensors
0.00.025.412 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.413 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.413 I print_info: file format = GGUF V3 (latest)
0.00.025.414 I print_info: file type   = Q5_K - Medium
0.00.025.414 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.098 I load: special tokens cache size = 25
0.00.050.239 I load: token to piece cache size = 0.2984 MB
0.00.050.244 I print_info: arch             = gptneox
0.00.050.244 I print_info: vocab_only       = 0
0.00.050.245 I print_info: n_ctx_train      = 2048
0.00.050.245 I print_info: n_embd           = 2048
0.00.050.245 I print_info: n_layer          = 24
0.00.050.247 I print_info: n_head           = 16
0.00.050.248 I print_info: n_head_kv        = 16
0.00.050.248 I print_info: n_rot            = 32
0.00.050.248 I print_info: n_swa            = 0
0.00.050.249 I print_info: n_embd_head_k    = 128
0.00.050.249 I print_info: n_embd_head_v    = 128
0.00.050.250 I print_info: n_gqa            = 1
0.00.050.250 I print_info: n_embd_k_gqa     = 2048
0.00.050.251 I print_info: n_embd_v_gqa     = 2048
0.00.050.252 I print_info: f_norm_eps       = 1.0e-05
0.00.050.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.252 I print_info: f_logit_scale    = 0.0e+00
0.00.050.253 I print_info: n_ff             = 8192
0.00.050.253 I print_info: n_expert         = 0
0.00.050.253 I print_info: n_expert_used    = 0
0.00.050.254 I print_info: causal attn      = 1
0.00.050.254 I print_info: pooling type     = 0
0.00.050.254 I print_info: rope type        = 2
0.00.050.254 I print_info: rope scaling     = linear
0.00.050.255 I print_info: freq_base_train  = 10000.0
0.00.050.255 I print_info: freq_scale_train = 1
0.00.050.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.255 I print_info: rope_finetuned   = unknown
0.00.050.255 I print_info: ssm_d_conv       = 0
0.00.050.256 I print_info: ssm_d_inner      = 0
0.00.050.256 I print_info: ssm_d_state      = 0
0.00.050.256 I print_info: ssm_dt_rank      = 0
0.00.050.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.256 I print_info: model type       = 1.4B
0.00.050.257 I print_info: model params     = 1.41 B
0.00.050.257 I print_info: general.name     = 1.4B
0.00.050.257 I print_info: vocab type       = BPE
0.00.050.258 I print_info: n_vocab          = 50304
0.00.050.258 I print_info: n_merges         = 50009
0.00.050.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: LF token         = 128 'Ä'
0.00.050.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: max token length = 1024
0.00.051.996 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.996 I load_tensors: offloading output layer to GPU
0.00.051.997 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.007 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.008 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.287 I llama_init_from_model: n_seq_max     = 1
0.00.052.288 I llama_init_from_model: n_ctx         = 128
0.00.052.288 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.288 I llama_init_from_model: n_batch       = 128
0.00.052.289 I llama_init_from_model: n_ubatch      = 128
0.00.052.289 I llama_init_from_model: flash_attn    = 0
0.00.052.289 I llama_init_from_model: freq_base     = 10000.0
0.00.052.289 I llama_init_from_model: freq_scale    = 1
0.00.052.290 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.290 I ggml_metal_init: allocating
0.00.052.292 I ggml_metal_init: found device: Apple M4
0.00.052.294 I ggml_metal_init: picking default device: Apple M4
0.00.052.857 I ggml_metal_init: using embedded metal library
0.00.055.186 I ggml_metal_init: GPU name:   Apple M4
0.00.055.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.189 I ggml_metal_init: simdgroup reduction   = true
0.00.055.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.189 I ggml_metal_init: has bfloat            = true
0.00.055.189 I ggml_metal_init: use bfloat            = true
0.00.055.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.834 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.113 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.115 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.973 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.974 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.974 I llama_init_from_model: graph nodes  = 967
0.00.065.974 I llama_init_from_model: graph splits = 2
0.00.065.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.168 I 
0.00.683.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.213 I perplexity: tokenizing the input ..
0.00.691.019 I perplexity: tokenization took 7.805 ms
0.00.691.029 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.011 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.833.113 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.833.127 I llama_perf_context_print:        load time =     672.77 ms
0.00.833.128 I llama_perf_context_print: prompt eval time =     140.76 ms /   128 tokens (    1.10 ms per token,   909.34 tokens per second)
0.00.833.129 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.129 I llama_perf_context_print:       total time =     149.96 ms /   129 tokens
0.00.833.472 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.077s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.110 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.810 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.810 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.701 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.702 I llama_model_loader: - type  f32:  194 tensors
0.00.024.702 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.703 I print_info: file format = GGUF V3 (latest)
0.00.024.703 I print_info: file type   = Q6_K
0.00.024.704 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.448 I load: special tokens cache size = 25
0.00.048.288 I load: token to piece cache size = 0.2984 MB
0.00.048.294 I print_info: arch             = gptneox
0.00.048.294 I print_info: vocab_only       = 0
0.00.048.294 I print_info: n_ctx_train      = 2048
0.00.048.295 I print_info: n_embd           = 2048
0.00.048.295 I print_info: n_layer          = 24
0.00.048.297 I print_info: n_head           = 16
0.00.048.298 I print_info: n_head_kv        = 16
0.00.048.298 I print_info: n_rot            = 32
0.00.048.299 I print_info: n_swa            = 0
0.00.048.299 I print_info: n_embd_head_k    = 128
0.00.048.299 I print_info: n_embd_head_v    = 128
0.00.048.300 I print_info: n_gqa            = 1
0.00.048.300 I print_info: n_embd_k_gqa     = 2048
0.00.048.301 I print_info: n_embd_v_gqa     = 2048
0.00.048.305 I print_info: f_norm_eps       = 1.0e-05
0.00.048.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.305 I print_info: f_logit_scale    = 0.0e+00
0.00.048.306 I print_info: n_ff             = 8192
0.00.048.306 I print_info: n_expert         = 0
0.00.048.306 I print_info: n_expert_used    = 0
0.00.048.307 I print_info: causal attn      = 1
0.00.048.307 I print_info: pooling type     = 0
0.00.048.307 I print_info: rope type        = 2
0.00.048.307 I print_info: rope scaling     = linear
0.00.048.307 I print_info: freq_base_train  = 10000.0
0.00.048.308 I print_info: freq_scale_train = 1
0.00.048.310 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.310 I print_info: rope_finetuned   = unknown
0.00.048.310 I print_info: ssm_d_conv       = 0
0.00.048.310 I print_info: ssm_d_inner      = 0
0.00.048.310 I print_info: ssm_d_state      = 0
0.00.048.310 I print_info: ssm_dt_rank      = 0
0.00.048.311 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.311 I print_info: model type       = 1.4B
0.00.048.311 I print_info: model params     = 1.41 B
0.00.048.311 I print_info: general.name     = 1.4B
0.00.048.312 I print_info: vocab type       = BPE
0.00.048.312 I print_info: n_vocab          = 50304
0.00.048.312 I print_info: n_merges         = 50009
0.00.048.313 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.316 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.317 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.317 I print_info: LF token         = 128 'Ä'
0.00.048.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.318 I print_info: max token length = 1024
0.00.050.022 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.022 I load_tensors: offloading output layer to GPU
0.00.050.022 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.027 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.028 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.304 I llama_init_from_model: n_seq_max     = 1
0.00.050.305 I llama_init_from_model: n_ctx         = 128
0.00.050.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.305 I llama_init_from_model: n_batch       = 128
0.00.050.305 I llama_init_from_model: n_ubatch      = 128
0.00.050.305 I llama_init_from_model: flash_attn    = 0
0.00.050.306 I llama_init_from_model: freq_base     = 10000.0
0.00.050.306 I llama_init_from_model: freq_scale    = 1
0.00.050.306 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.306 I ggml_metal_init: allocating
0.00.050.309 I ggml_metal_init: found device: Apple M4
0.00.050.310 I ggml_metal_init: picking default device: Apple M4
0.00.050.870 I ggml_metal_init: using embedded metal library
0.00.053.162 I ggml_metal_init: GPU name:   Apple M4
0.00.053.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.164 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.164 I ggml_metal_init: simdgroup reduction   = true
0.00.053.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.164 I ggml_metal_init: has bfloat            = true
0.00.053.164 I ggml_metal_init: use bfloat            = true
0.00.053.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.506 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.730 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.575 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.576 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.576 I llama_init_from_model: graph nodes  = 967
0.00.063.577 I llama_init_from_model: graph splits = 2
0.00.063.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.923 I 
0.00.653.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.972 I perplexity: tokenizing the input ..
0.00.661.188 I perplexity: tokenization took 7.214 ms
0.00.661.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.464 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.802.692 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.802.711 I llama_perf_context_print:        load time =     644.81 ms
0.00.802.712 I llama_perf_context_print: prompt eval time =     140.03 ms /   128 tokens (    1.09 ms per token,   914.12 tokens per second)
0.00.802.713 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.714 I llama_perf_context_print:       total time =     148.79 ms /   129 tokens
0.00.803.177 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.075s
sys	0m0.147s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4525 (3e3357fd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.451 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.253 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.267 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.268 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.272 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.272 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.273 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.277 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.113 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.229 I llama_model_loader: - type  f32:  194 tensors
0.00.058.229 I llama_model_loader: - type  f16:   98 tensors
0.00.058.230 I print_info: file format = GGUF V3 (latest)
0.00.058.231 I print_info: file type   = all F32 (guessed)
0.00.058.236 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.606 I load: special tokens cache size = 25
0.00.088.774 I load: token to piece cache size = 0.2984 MB
0.00.088.779 I print_info: arch             = gptneox
0.00.088.780 I print_info: vocab_only       = 0
0.00.088.784 I print_info: n_ctx_train      = 2048
0.00.088.784 I print_info: n_embd           = 2048
0.00.088.784 I print_info: n_layer          = 24
0.00.088.787 I print_info: n_head           = 16
0.00.088.787 I print_info: n_head_kv        = 16
0.00.088.788 I print_info: n_rot            = 32
0.00.088.788 I print_info: n_swa            = 0
0.00.088.789 I print_info: n_embd_head_k    = 128
0.00.088.789 I print_info: n_embd_head_v    = 128
0.00.088.790 I print_info: n_gqa            = 1
0.00.088.791 I print_info: n_embd_k_gqa     = 2048
0.00.088.791 I print_info: n_embd_v_gqa     = 2048
0.00.088.792 I print_info: f_norm_eps       = 1.0e-05
0.00.088.792 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.792 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.792 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.793 I print_info: f_logit_scale    = 0.0e+00
0.00.088.794 I print_info: n_ff             = 8192
0.00.088.794 I print_info: n_expert         = 0
0.00.088.795 I print_info: n_expert_used    = 0
0.00.088.795 I print_info: causal attn      = 1
0.00.088.795 I print_info: pooling type     = 0
0.00.088.795 I print_info: rope type        = 2
0.00.088.795 I print_info: rope scaling     = linear
0.00.088.795 I print_info: freq_base_train  = 10000.0
0.00.088.796 I print_info: freq_scale_train = 1
0.00.088.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.797 I print_info: rope_finetuned   = unknown
0.00.088.798 I print_info: ssm_d_conv       = 0
0.00.088.798 I print_info: ssm_d_inner      = 0
0.00.088.799 I print_info: ssm_d_state      = 0
0.00.088.799 I print_info: ssm_dt_rank      = 0
0.00.088.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.799 I print_info: model type       = 1.4B
0.00.088.799 I print_info: model params     = 1.41 B
0.00.088.800 I print_info: general.name     = 1.4B
0.00.088.803 I print_info: vocab type       = BPE
0.00.088.803 I print_info: n_vocab          = 50304
0.00.088.803 I print_info: n_merges         = 50009
0.00.088.803 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.804 I print_info: LF token         = 128 'Ä'
0.00.088.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.806 I print_info: max token length = 1024
0.00.090.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.701 I load_tensors: offloading output layer to GPU
0.00.090.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.707 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.707 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.950 I llama_init_from_model: n_seq_max     = 1
0.00.090.951 I llama_init_from_model: n_ctx         = 128
0.00.090.951 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.951 I llama_init_from_model: n_batch       = 128
0.00.090.951 I llama_init_from_model: n_ubatch      = 128
0.00.090.951 I llama_init_from_model: flash_attn    = 0
0.00.090.952 I llama_init_from_model: freq_base     = 10000.0
0.00.090.952 I llama_init_from_model: freq_scale    = 1
0.00.090.952 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.953 I ggml_metal_init: allocating
0.00.090.955 I ggml_metal_init: found device: Apple M4
0.00.090.957 I ggml_metal_init: picking default device: Apple M4
0.00.091.571 I ggml_metal_init: using embedded metal library
0.00.094.099 I ggml_metal_init: GPU name:   Apple M4
0.00.094.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.102 I ggml_metal_init: simdgroup reduction   = true
0.00.094.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.102 I ggml_metal_init: has bfloat            = true
0.00.094.102 I ggml_metal_init: use bfloat            = true
0.00.094.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.509 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.511 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.525 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.382 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.383 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.383 I llama_init_from_model: graph nodes  = 967
0.00.105.384 I llama_init_from_model: graph splits = 2
0.00.105.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.385 I 
0.00.105.422 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.423 I compute_imatrix: tokenizing the input ..
0.00.112.260 I compute_imatrix: tokenization took 6.836 ms
0.00.112.262 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.651.110 I compute_imatrix: 1.54 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.653.767 I llama_perf_context_print:        load time =    1628.66 ms
0.01.653.768 I llama_perf_context_print: prompt eval time =    1538.24 ms /   128 tokens (   12.02 ms per token,    83.21 tokens per second)
0.01.653.769 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.653.769 I llama_perf_context_print:       total time =    1631.31 ms /   129 tokens
0.01.654.425 I ggml_metal_free: deallocating

real	0m1.841s
user	0m0.167s
sys	0m0.240s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4525 (3e3357fd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149c0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x149c0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149c0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149c0bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149c0c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149c0c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149c0cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149c0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149c0d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149c0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149c0e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149c0e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149c0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x149c0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149c102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149c10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149c11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149c11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149c11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149c12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149c12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149c13580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149c13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149c14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149c14c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149c14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149c15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149c161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149c166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149c169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149c16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149c17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149c17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149c17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149c18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149c18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149c18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149c18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149c19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149c198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149c19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149c1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149c1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149c1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149c1adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149c1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x149c1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149c1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149c1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149c1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149c1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x149c1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149c1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x149c1e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149c1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149c1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149c1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149c1fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149c20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149c20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149c20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149c210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149c21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149c21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x149c21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149c22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149c22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149c22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149c23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149c235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149c23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149c23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149c243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149c24910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149c24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149c253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149c25900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149c25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149c263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149c268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149c26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149c27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149c278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149c27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149c28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149c288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149c28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149c29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149c298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149c29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149c2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149c2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149c2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149c2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149c2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149c2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149c2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149c1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149c2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149c2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x149c2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149c2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149c2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149c2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149c2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149c2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149c2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149c2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149c2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149c30480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149c309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149c30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149c31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149c31910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149c31db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149c32250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149c326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149c32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149c33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149c334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149c33970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149c33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149c342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149c34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x149c34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149c35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149c35530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149c359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149c35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149c36310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149c367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149c36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149c370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149c37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149c37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149c37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149c38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149c38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149c38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149c39150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x149c395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149c39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149c39f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149c3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149c3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149c3ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149c3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149c3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x149c3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149c3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149c3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149c3c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149c3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149c3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149c3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x149c3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149c3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149c3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149c3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149c3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149c3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149c3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149c3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149c40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149c404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149c40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149c40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149c412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149c41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149c41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149c420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149c42550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149c429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149c42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149c43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149c437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149c43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149c44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149c445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149c44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149c44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149c45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149c45830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149c45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149c46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149c46610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149c46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149c46f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149c473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149c47890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149c47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149c481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149c48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149c48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149c49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149c49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149c49bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149c4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149c4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149c4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149c4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149c4bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149c4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149c4c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149c4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149c4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149c4d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149c4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149c4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149c4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149c4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149c4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149c4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149c4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149c50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149c50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149c50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149c51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149c51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149c51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149c52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149c52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149c52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149c533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149c53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149c53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149c543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149c54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149c54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149c553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149c55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149c55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149c563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149c56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149c56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149c573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x149c57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149c57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149c583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x149c588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149c58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149c59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149c598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149c59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149c5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149c5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149c5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149c5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149c5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149c5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149c5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149c5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149c5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x149c5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149c5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149c5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149c5e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149c5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149c5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149c5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149c5f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149c5fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149c60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149c60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149c60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149c61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149c617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149c61c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149c620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149c62590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149c62a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149c62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149c63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149c63810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149c63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149c64150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149c645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149c64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149c64f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149c653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149c65870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149c65dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149c664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x149c66c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149c67320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149c67a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149c67d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149c684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149c687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149c68dc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.146.753 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.146.757 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105004dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105005240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1050056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105005b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105006400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105006870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105006ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105007150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1050075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105007a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105008120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105008c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1050093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105009c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10500a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10500aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10500b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10500b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10500bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10500c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10500cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10500d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10500dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10500e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10500e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10500e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10500ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10500f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10500f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10500fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10500ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105010430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1050106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105010b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105010fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105011440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1050118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105011d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105012190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105012a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105012ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105013350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1050137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105013c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1050140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105014510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105014980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105014df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105015260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1050156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105015b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105015fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105016420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105016890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105017300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105017770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105017be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105018050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1050184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105018930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105018da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105019210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105019680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105019af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105019f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10501a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10501a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10501acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10501b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10501b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10501ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10501be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10501c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10501c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10501cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10501d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10501d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10501d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10501dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10501e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10501e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10501ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10501ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10501f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10501f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10501fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105020100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105020570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1050209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105020e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1050212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105021730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105022010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105022480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1050228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105022d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1050231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105023640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105023ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105023f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105024390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105024800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105024c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1050250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105025550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1050259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105025e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1050262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105026710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105026b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105026ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105027460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1050278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105027d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1050281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105028620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105028a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105028f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105029370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1050297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105029c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10502a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10502a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10502a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10502ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10502b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10502b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10502bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10502bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10502c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10502c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10502cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10502d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10502d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10502da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10502dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10502e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10502e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10502ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10502f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10502f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10502f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10502fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105030260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1050306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105030b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105030fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105031420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105031890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105031d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105032170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1050325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105032a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105032ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105033330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1050337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105033c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105034080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1050344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105034960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105034dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105035240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105035e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105036130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1050363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105036860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105036cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105037140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1050375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105037a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105037e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105038300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105038770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105038be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105039050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1050394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105039930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105039da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10503a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10503a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10503aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10503af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10503b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10503b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10503bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10503c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10503c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10503ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10503ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10503d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10503d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10503dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10503e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10503e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10503e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10503ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10503f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10503f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10503fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1050400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105040540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1050409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105040e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105041290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1050417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105041cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105042830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105042af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1050430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105043670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105043c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1050441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1050447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105045330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1050458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105045eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105046470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105046a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105046ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1050475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105047b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105048130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1050486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105048cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105049270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105049830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105049df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10504a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10504a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10504af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10504b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10504bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10504c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10504c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10504cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10504d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10504d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10504dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10504e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10504e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10504ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10504f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10504f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10504ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105050570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105050b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1050510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1050516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105051c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105052230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1050527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105052db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105053370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105053930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105053ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1050544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105054a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105055030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1050555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105056170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105056730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105056cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1050571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1050576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105057bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1050580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1050585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105058af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105058ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1050594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1050599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105059ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10505a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10505a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10505adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10505b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10505b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10505c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10505c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10505d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10505d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10505da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10505e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10505e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10505eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10505bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10504c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10504b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1050483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105045bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1050552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105052ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105050830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10504e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105046730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105043ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10504a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10504f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10504c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1050541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105047e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1050513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10504ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10504ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105047870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1050558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105044a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105043370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1050455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105055e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10504b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105053630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105049530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10504bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10504fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1050472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105050270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105051970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105054770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105051f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10504da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1050569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105045030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105056430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1050444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105054d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10504eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105050df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105053bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1050524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10504a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105041f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105004880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10505dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10500bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10505ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10505f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10505f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10505f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10505fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10505fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10505ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105060280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105060540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105060800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105060ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105060d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105061040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105061300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1050615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105061880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105061b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105061e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1050620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105062380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105062640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105062900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105062bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105062e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105063140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105063400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1050636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105063980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105063c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105063f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1050641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105064480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105064740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105064a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105064cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105064f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105065240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105065500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1050657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105065a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105065d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105066000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1050662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105066580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105066840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105066b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105066dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105067080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105067340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105067600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1050678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105067b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105067e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105068100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1050683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105068680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105068940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105068c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105068ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105069180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105069440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105069700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1050699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105069c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105069f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10506a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10506a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10506a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10506aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10506ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10506afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10506b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10506b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10506b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10506bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10506bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10506c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10506c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10506c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10506c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10506cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10506ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10506d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10506d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10506d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10506d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10506dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10506de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10506e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10506e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10506e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10506e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10506ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10506ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10506f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10506f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10506f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10506fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10506fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10506ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105070240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105070500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1050707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105070a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105070d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105071000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1050712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105071580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105071840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105071b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105071dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105072080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105072340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105072600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1050728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105072b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105072e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105073100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1050733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105073680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105073940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105073c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105073ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105074180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105074440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105074700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1050749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105074c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105074f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105075200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1050754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105075780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105075a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105075d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105075fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105076280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105076540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105076800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105076ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105076d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105077040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105077300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1050775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105077880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105077b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105077e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1050780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105078380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105078640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105078900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105078bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105078e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105079140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105079400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1050796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105079980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105079c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105079f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10507a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10507a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10507aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10507ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10507afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10507b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10507b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10507b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10507bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10507c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10507c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10507cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10507d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10507d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10507dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10507e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10507e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10507ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10507f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10507f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10507fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105080270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1050807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105080d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105081260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1050817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105081d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105082250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1050827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105082cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105083240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105083790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105083ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105084230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105084780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105084cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105085220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105085770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105085cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105086210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105086760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105086cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105087200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105087750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105087ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1050881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105088740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105088c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1050891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105089730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105089c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10508a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10508a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10508ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10508b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10508b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10508bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10508bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10508c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10508c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10508c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10508cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10508d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10508d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10508dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10508df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10508e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10508e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10508ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10508f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10508f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10508f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10508fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1050902c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105090fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1050916d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105091df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1050920b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105092520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105092b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105093130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.849s
user	0m0.294s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4525 (3e3357fd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14260e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14260ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14260f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14260f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14260fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1426102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142610870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142610e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1426113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1426118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142611dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1426122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142612df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1426135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142613db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1426144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142614bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142615310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142615a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142616200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142616920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142617040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142618720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1426189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142618ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142619c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14261a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14261a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14261a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14261abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14261b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14261b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14261bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14261c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14261c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14261ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14261ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14261d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14261d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14261dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14261e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14261e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14261e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14261eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14261f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14261fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142620400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142621020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142621630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142622250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142623c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142624ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142625040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1426254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1426262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142626760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142626c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1426270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1426279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1426283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1426293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14262a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14262a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14262ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14262b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14262b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14262be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14262c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14262c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14262ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14262d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14262d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14262de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14262e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14262e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14262ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14262f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14262f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14262fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14261fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142630270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1426314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1426324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142632a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142632f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1426334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1426339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142633f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1426349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1426353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142635870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142635d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1426361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142636650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1426378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142637d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1426386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x142638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142639490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14263a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14263a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14263abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14263b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14263b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14263b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14263be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14263c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14263c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14263cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14263d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14263d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14263d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14263de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14263e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14263e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14263ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14263f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14263f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14263fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14263fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142640cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142641170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142641610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142641f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1426423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142642d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1426431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142643b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142643fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142644450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1426448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142644d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142645230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1426456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142645b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1426464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142646950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142647290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142647730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142647bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142648510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1426489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142648e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1426492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142649790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142649c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14264a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14264a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14264aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14264aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14264b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14264b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14264bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14264c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14264c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14264cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14264d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14264d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14264d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14264df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14264e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14264eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14264f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14264f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14264fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1426500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1426506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142650ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142651360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142651800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1426529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1426583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1426593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14265a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14265a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14265ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14265b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14265b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14265be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14265c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14265c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14265ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14265d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14265d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14265de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14265e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14265e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14265ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14265f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14265f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14265fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1426608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1426618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142662350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1426628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142663340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142663890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142663de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142664330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142664880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142664dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142665270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142665710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142665bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142666050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1426664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142666e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1426672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142667770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142667c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1426680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142668550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1426689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142668e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142669330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142669880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142669fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14266a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14266ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14266b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14266b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14266bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14266c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14266c880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142706a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142706ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x142707340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1427077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142707c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142708090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142708500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142708970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142708de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142709250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1427096c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142709d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14270a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14270b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14270b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14270bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14270c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14270cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14270d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14270dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14270e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14270eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14270f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14270f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142710010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1427102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142710590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142710a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142710e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1427112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142711c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1427120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1427123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142712c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142713570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1427139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1427142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1427158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1427161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142716640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142716ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142716f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142717390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142717c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1427180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142718550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142718ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1427198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14271a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14271a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14271aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14271aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14271b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14271b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14271bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14271c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14271c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14271c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14271cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14271d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14271d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14271db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14271dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14271e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14271e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14271ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14271f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14271f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14271fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14271feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1427214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1427226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1427233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1427245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142724e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142725770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142725be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142726050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1427264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142726da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142727210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142727680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142727af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142727f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1427283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142728840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142728cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142729590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142729a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142729e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14272a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14272a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14272abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14272b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14272b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14272b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14272bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14272c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14272c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14272cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14272cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14272d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14272d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14272dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14272e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14272e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14272e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14272ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14272f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14272f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14272fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142730010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142730480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1427308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142730d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1427311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142731640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142731ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142731f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142732390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142732800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1427330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1427339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142733e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1427342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142734710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142734b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142734ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142735460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1427358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142735d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1427361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142736a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142736f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142737df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1427380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142738520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142738990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1427396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14273a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14273a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14273ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14273b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14273b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14273ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14273bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14273c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14273c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14273cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14273d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14273d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14273d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14273dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14273e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14273e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14273eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14273efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14273f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14273f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14273fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142740160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1427405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x142740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x142740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x142741320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x142741880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x142741d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x142742200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142742ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142742f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142743470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142743980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1427444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1427447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142745330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1427458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142746470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142746a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142746ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1427475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142747b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1427486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142749270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142749830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142749df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14274a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14274a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14274af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14274b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14274bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14274c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14274c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14274cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14274d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14274d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14274dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14274e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14274e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14274ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14274f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14274f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14274ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142750570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142750b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1427510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1427516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142751c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142752230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1427527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142753930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142753ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1427544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142754a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142755030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1427555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142755bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142756170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142756730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142756cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1427572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142757870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142757e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1427583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1427589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142758eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1427593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1427598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142759db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14275a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14275a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14275acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14275b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14275b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14275bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14275c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14275c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14275cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14275cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14275d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14275dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14275e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14275ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14275f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14275f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14275fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142760190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1427607a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14275d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14274e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14274d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14274a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142747870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142756fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1427524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142750270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1427483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142745bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14274ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14274bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1427513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14274dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142755e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1427489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x142750df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14274b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142744a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14274f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14274a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14274fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1427455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1427472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142757b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14274ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1427552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14274b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14274da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142751970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14274c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142748f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142753630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142747e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142756430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142753bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14274f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1427586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142746cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1427580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142746170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1427569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142750830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142752ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1427558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1427541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14274c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142743c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142709980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142706600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14275f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142760e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142761140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142761400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1427616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142761980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142761c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142761f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1427621c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142762480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142762740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142762a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142762cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142762f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142763240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142763500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1427637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142763a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142764000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1427642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142764580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142764840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142764b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142764dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142765080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142765340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142765600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1427658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142765b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142765e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142766100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1427663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142766680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142766940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142766c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x142766ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x142767180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142767440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142767700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1427679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142767c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142767f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142768200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1427684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142768780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142768a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142768d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142768fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x142769280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142769800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142769ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142769d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14276a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14276a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14276a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14276a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14276ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14276ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14276b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14276b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14276b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14276b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14276bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14276be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14276c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14276c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14276c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14276c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14276cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14276cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14276d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14276d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14276d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14276da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14276dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14276df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14276e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14276e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14276e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14276ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14276ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14276f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14276f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14276f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14276f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14276fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14276fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142770080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142770340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142770600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1427708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142770b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142770e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142771100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1427713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142771680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142771940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142771c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142771ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142772180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142772440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142772700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1427729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142772c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142772f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142773200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1427734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142773780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142773a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142773d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142773fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142774280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142774540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142774800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142774ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142774d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142775040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142775300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1427755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142775880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142775b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142775e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1427760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142776380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142776640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142776900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142776bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142776e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142777140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142777400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1427776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142777980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142777c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142777f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1427781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x142778480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142778740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142778a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x142778cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x142778f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x142779240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x142779500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1427797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x142779a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x142779d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14277a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14277a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14277a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14277a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14277ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14277adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14277b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14277b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14277b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14277b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14277bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14277c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14277c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14277c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14277c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14277cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14277cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14277d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14277d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14277dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14277e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14277e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14277ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14277f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14277f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14277fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1427801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1427806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142780c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142781190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1427816e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142781c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142782180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1427826d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142782c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142783170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1427836c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142783c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142784160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1427846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142784c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x142785150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1427856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142785bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142786140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x142786690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142786be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142787130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x142787680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142787bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142788120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x142788670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142788bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142789110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142789660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142789bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14278a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14278a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14278aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14278b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14278b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14278bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14278c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14278c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14278cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14278d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14278d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14278db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14278de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14278e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14278e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14278eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14278eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14278f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14278f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14278fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1427903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1427908f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142790df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1427912f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1427917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142791cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1427921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1427926f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142793100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142793820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142793f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142794660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142794920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142795110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1427953d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1427959e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.913s
user	0m0.245s
sys	0m0.132s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
