### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   13.15 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.84 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.85 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.65 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.90 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.28 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 248.34 sec*proc (29 tests)

Total Test time (real) = 248.36 sec

real	4m8.487s
user	8m30.706s
sys	0m7.153s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.36 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.87 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.36 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.21 sec*proc (29 tests)

Total Test time (real) =  54.22 sec

real	0m54.231s
user	1m16.001s
sys	0m6.630s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.155 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.422 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.013 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.024 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.025 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.026 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.027 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.027 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.029 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.029 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.030 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.031 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.031 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.035 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.035 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.036 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.037 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.037 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.038 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.039 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.651 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.652 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.653 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.653 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.654 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.654 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.655 I llama_model_loader: - type  f32:  124 tensors
0.00.029.655 I llama_model_loader: - type  f16:   73 tensors
0.00.029.656 I print_info: file format = GGUF V3 (latest)
0.00.029.657 I print_info: file type   = F16
0.00.029.658 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.033.815 I load: special tokens cache size = 5
0.00.035.982 I load: token to piece cache size = 0.2032 MB
0.00.035.999 I print_info: arch             = bert
0.00.036.001 I print_info: vocab_only       = 0
0.00.036.002 I print_info: n_ctx_train      = 512
0.00.036.002 I print_info: n_embd           = 384
0.00.036.002 I print_info: n_layer          = 12
0.00.036.006 I print_info: n_head           = 12
0.00.036.013 I print_info: n_head_kv        = 12
0.00.036.013 I print_info: n_rot            = 32
0.00.036.013 I print_info: n_swa            = 0
0.00.036.014 I print_info: n_embd_head_k    = 32
0.00.036.014 I print_info: n_embd_head_v    = 32
0.00.036.015 I print_info: n_gqa            = 1
0.00.036.016 I print_info: n_embd_k_gqa     = 384
0.00.036.017 I print_info: n_embd_v_gqa     = 384
0.00.036.017 I print_info: f_norm_eps       = 1.0e-12
0.00.036.018 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.019 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.019 I print_info: f_logit_scale    = 0.0e+00
0.00.036.020 I print_info: n_ff             = 1536
0.00.036.024 I print_info: n_expert         = 0
0.00.036.024 I print_info: n_expert_used    = 0
0.00.036.025 I print_info: causal attn      = 0
0.00.036.025 I print_info: pooling type     = 2
0.00.036.025 I print_info: rope type        = 2
0.00.036.025 I print_info: rope scaling     = linear
0.00.036.028 I print_info: freq_base_train  = 10000.0
0.00.036.029 I print_info: freq_scale_train = 1
0.00.036.029 I print_info: n_ctx_orig_yarn  = 512
0.00.036.029 I print_info: rope_finetuned   = unknown
0.00.036.029 I print_info: ssm_d_conv       = 0
0.00.036.029 I print_info: ssm_d_inner      = 0
0.00.036.030 I print_info: ssm_d_state      = 0
0.00.036.030 I print_info: ssm_dt_rank      = 0
0.00.036.030 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.030 I print_info: model type       = 33M
0.00.036.031 I print_info: model params     = 33.21 M
0.00.036.031 I print_info: general.name     = Bge Small
0.00.036.035 I print_info: vocab type       = WPM
0.00.036.036 I print_info: n_vocab          = 30522
0.00.036.036 I print_info: n_merges         = 0
0.00.036.036 I print_info: BOS token        = 101 '[CLS]'
0.00.036.037 I print_info: UNK token        = 100 '[UNK]'
0.00.036.037 I print_info: SEP token        = 102 '[SEP]'
0.00.036.039 I print_info: PAD token        = 0 '[PAD]'
0.00.036.039 I print_info: MASK token       = 103 '[MASK]'
0.00.036.040 I print_info: LF token         = 0 '[PAD]'
0.00.036.040 I print_info: max token length = 21
0.00.039.332 I load_tensors: offloading 12 repeating layers to GPU
0.00.039.334 I load_tensors: offloading output layer to GPU
0.00.039.334 I load_tensors: offloaded 13/13 layers to GPU
0.00.039.359 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.360 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.039.597 I llama_context: n_seq_max     = 1
0.00.039.599 I llama_context: n_ctx         = 512
0.00.039.599 I llama_context: n_ctx_per_seq = 512
0.00.039.599 I llama_context: n_batch       = 2048
0.00.039.600 I llama_context: n_ubatch      = 2048
0.00.039.600 I llama_context: flash_attn    = 0
0.00.039.600 I llama_context: freq_base     = 10000.0
0.00.039.601 I llama_context: freq_scale    = 1
0.00.039.601 I ggml_metal_init: allocating
0.00.039.613 I ggml_metal_init: found device: Apple M4
0.00.039.620 I ggml_metal_init: picking default device: Apple M4
0.00.040.304 I ggml_metal_init: using embedded metal library
0.00.044.391 I ggml_metal_init: GPU name:   Apple M4
0.00.044.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.395 I ggml_metal_init: simdgroup reduction   = true
0.00.044.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.395 I ggml_metal_init: has residency sets    = true
0.00.044.396 I ggml_metal_init: has bfloat            = true
0.00.044.396 I ggml_metal_init: use bfloat            = true
0.00.044.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.239 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.056.939 I init:      Metal KV buffer size =     9.00 MiB
0.00.056.941 I llama_context: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.943 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.058.282 I llama_context:      Metal compute buffer size =    16.00 MiB
0.00.058.283 I llama_context:        CPU compute buffer size =     2.51 MiB
0.00.058.283 I llama_context: graph nodes  = 429
0.00.058.284 I llama_context: graph splits = 2
0.00.058.285 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.058.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.800 I 
0.00.063.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.064.531 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.466 I llama_perf_context_print:        load time =      45.36 ms
0.00.068.467 I llama_perf_context_print: prompt eval time =       3.80 ms /     9 tokens (    0.42 ms per token,  2370.29 tokens per second)
0.00.068.468 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.469 I llama_perf_context_print:       total time =       4.66 ms /    10 tokens
0.00.068.675 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.049s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.251 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.956 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.962 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.962 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.963 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.963 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.964 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.964 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.964 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.965 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.965 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.967 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.969 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.969 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.969 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.970 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.970 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.417 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.079 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.081 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.081 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.081 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.082 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.082 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.082 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.083 I llama_model_loader: - type  f32:  124 tensors
0.00.015.083 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.084 I print_info: file format = GGUF V3 (latest)
0.00.015.084 I print_info: file type   = Q8_0
0.00.015.085 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.643 I load: special tokens cache size = 5
0.00.018.921 I load: token to piece cache size = 0.2032 MB
0.00.018.932 I print_info: arch             = bert
0.00.018.933 I print_info: vocab_only       = 0
0.00.018.933 I print_info: n_ctx_train      = 512
0.00.018.933 I print_info: n_embd           = 384
0.00.018.934 I print_info: n_layer          = 12
0.00.018.937 I print_info: n_head           = 12
0.00.018.937 I print_info: n_head_kv        = 12
0.00.018.937 I print_info: n_rot            = 32
0.00.018.937 I print_info: n_swa            = 0
0.00.018.937 I print_info: n_embd_head_k    = 32
0.00.018.938 I print_info: n_embd_head_v    = 32
0.00.018.939 I print_info: n_gqa            = 1
0.00.018.939 I print_info: n_embd_k_gqa     = 384
0.00.018.940 I print_info: n_embd_v_gqa     = 384
0.00.018.940 I print_info: f_norm_eps       = 1.0e-12
0.00.018.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.941 I print_info: f_logit_scale    = 0.0e+00
0.00.018.942 I print_info: n_ff             = 1536
0.00.018.942 I print_info: n_expert         = 0
0.00.018.942 I print_info: n_expert_used    = 0
0.00.018.942 I print_info: causal attn      = 0
0.00.018.942 I print_info: pooling type     = 2
0.00.018.942 I print_info: rope type        = 2
0.00.018.944 I print_info: rope scaling     = linear
0.00.018.944 I print_info: freq_base_train  = 10000.0
0.00.018.945 I print_info: freq_scale_train = 1
0.00.018.945 I print_info: n_ctx_orig_yarn  = 512
0.00.018.945 I print_info: rope_finetuned   = unknown
0.00.018.945 I print_info: ssm_d_conv       = 0
0.00.018.945 I print_info: ssm_d_inner      = 0
0.00.018.945 I print_info: ssm_d_state      = 0
0.00.018.945 I print_info: ssm_dt_rank      = 0
0.00.018.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.946 I print_info: model type       = 33M
0.00.018.946 I print_info: model params     = 33.21 M
0.00.018.946 I print_info: general.name     = Bge Small
0.00.018.947 I print_info: vocab type       = WPM
0.00.018.947 I print_info: n_vocab          = 30522
0.00.018.947 I print_info: n_merges         = 0
0.00.018.947 I print_info: BOS token        = 101 '[CLS]'
0.00.018.947 I print_info: UNK token        = 100 '[UNK]'
0.00.018.948 I print_info: SEP token        = 102 '[SEP]'
0.00.018.948 I print_info: PAD token        = 0 '[PAD]'
0.00.018.948 I print_info: MASK token       = 103 '[MASK]'
0.00.018.950 I print_info: LF token         = 0 '[PAD]'
0.00.018.950 I print_info: max token length = 21
0.00.020.775 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.776 I load_tensors: offloading output layer to GPU
0.00.020.776 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.784 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.784 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.000 I llama_context: n_seq_max     = 1
0.00.021.002 I llama_context: n_ctx         = 512
0.00.021.002 I llama_context: n_ctx_per_seq = 512
0.00.021.002 I llama_context: n_batch       = 2048
0.00.021.002 I llama_context: n_ubatch      = 2048
0.00.021.002 I llama_context: flash_attn    = 0
0.00.021.003 I llama_context: freq_base     = 10000.0
0.00.021.003 I llama_context: freq_scale    = 1
0.00.021.003 I ggml_metal_init: allocating
0.00.021.007 I ggml_metal_init: found device: Apple M4
0.00.021.011 I ggml_metal_init: picking default device: Apple M4
0.00.021.533 I ggml_metal_init: using embedded metal library
0.00.024.113 I ggml_metal_init: GPU name:   Apple M4
0.00.024.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.116 I ggml_metal_init: simdgroup reduction   = true
0.00.024.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.116 I ggml_metal_init: has residency sets    = true
0.00.024.117 I ggml_metal_init: has bfloat            = true
0.00.024.117 I ggml_metal_init: use bfloat            = true
0.00.024.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.120 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.445 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.074 I init:      Metal KV buffer size =     9.00 MiB
0.00.035.076 I llama_context: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.078 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.036.094 I llama_context:      Metal compute buffer size =    16.00 MiB
0.00.036.095 I llama_context:        CPU compute buffer size =     2.51 MiB
0.00.036.095 I llama_context: graph nodes  = 429
0.00.036.095 I llama_context: graph splits = 2
0.00.036.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.709 I 
0.00.039.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.299 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.608 I llama_perf_context_print:        load time =      30.45 ms
0.00.043.610 I llama_perf_context_print: prompt eval time =       3.19 ms /     9 tokens (    0.35 ms per token,  2825.75 tokens per second)
0.00.043.611 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.612 I llama_perf_context_print:       total time =       3.90 ms /    10 tokens
0.00.043.849 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.159 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.259 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.262 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.264 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.021.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.265 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.021.271 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.021.271 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.021.272 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.021.275 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.021.275 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.021.276 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.021.276 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.021.278 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.278 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.279 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.021.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.280 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.025.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.026.564 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.029.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.029.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.029.380 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.029.380 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.029.380 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.029.380 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.029.381 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.029.381 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.029.381 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.029.382 I llama_model_loader: - type  f32:   40 tensors
0.00.029.383 I llama_model_loader: - type  f16:   30 tensors
0.00.029.384 I print_info: file format = GGUF V3 (latest)
0.00.029.385 I print_info: file type   = F16
0.00.029.386 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.032.184 W load: empty token at index 5
0.00.035.744 W load: model vocab missing newline token, using special_pad_id instead
0.00.036.816 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.036.849 I load: special tokens cache size = 5
0.00.302.702 I load: token to piece cache size = 1.5060 MB
0.00.302.709 I print_info: arch             = jina-bert-v2
0.00.302.709 I print_info: vocab_only       = 0
0.00.302.709 I print_info: n_ctx_train      = 8192
0.00.302.710 I print_info: n_embd           = 384
0.00.302.710 I print_info: n_layer          = 4
0.00.302.715 I print_info: n_head           = 12
0.00.302.715 I print_info: n_head_kv        = 12
0.00.302.715 I print_info: n_rot            = 32
0.00.302.715 I print_info: n_swa            = 0
0.00.302.716 I print_info: n_embd_head_k    = 32
0.00.302.716 I print_info: n_embd_head_v    = 32
0.00.302.716 I print_info: n_gqa            = 1
0.00.302.718 I print_info: n_embd_k_gqa     = 384
0.00.302.719 I print_info: n_embd_v_gqa     = 384
0.00.302.724 I print_info: f_norm_eps       = 1.0e-12
0.00.302.726 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.302.726 I print_info: f_clamp_kqv      = 0.0e+00
0.00.302.726 I print_info: f_max_alibi_bias = 8.0e+00
0.00.302.726 I print_info: f_logit_scale    = 0.0e+00
0.00.302.727 I print_info: n_ff             = 1536
0.00.302.727 I print_info: n_expert         = 0
0.00.302.727 I print_info: n_expert_used    = 0
0.00.302.727 I print_info: causal attn      = 0
0.00.302.728 I print_info: pooling type     = -1
0.00.302.728 I print_info: rope type        = -1
0.00.302.728 I print_info: rope scaling     = linear
0.00.302.730 I print_info: freq_base_train  = 10000.0
0.00.302.730 I print_info: freq_scale_train = 1
0.00.302.730 I print_info: n_ctx_orig_yarn  = 8192
0.00.302.730 I print_info: rope_finetuned   = unknown
0.00.302.730 I print_info: ssm_d_conv       = 0
0.00.302.733 I print_info: ssm_d_inner      = 0
0.00.302.734 I print_info: ssm_d_state      = 0
0.00.302.734 I print_info: ssm_dt_rank      = 0
0.00.302.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.302.734 I print_info: model type       = 33M
0.00.302.735 I print_info: model params     = 32.90 M
0.00.302.735 I print_info: general.name     = Jina Bert Implementation
0.00.302.736 I print_info: vocab type       = BPE
0.00.302.737 I print_info: n_vocab          = 61056
0.00.302.737 I print_info: n_merges         = 39382
0.00.302.738 I print_info: BOS token        = 0 '<s>'
0.00.302.738 I print_info: EOS token        = 2 '</s>'
0.00.302.738 I print_info: UNK token        = 3 '<unk>'
0.00.302.738 I print_info: SEP token        = 2 '</s>'
0.00.302.739 I print_info: PAD token        = 1 '<pad>'
0.00.302.739 I print_info: MASK token       = 4 '<mask>'
0.00.302.739 I print_info: EOG token        = 2 '</s>'
0.00.302.739 I print_info: max token length = 45
0.00.304.025 I load_tensors: offloading 4 repeating layers to GPU
0.00.304.026 I load_tensors: offloading output layer to GPU
0.00.304.027 I load_tensors: offloaded 5/5 layers to GPU
0.00.304.045 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.304.046 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.304.199 I llama_context: n_seq_max     = 1
0.00.304.200 I llama_context: n_ctx         = 8192
0.00.304.200 I llama_context: n_ctx_per_seq = 8192
0.00.304.200 I llama_context: n_batch       = 2048
0.00.304.200 I llama_context: n_ubatch      = 2048
0.00.304.200 I llama_context: flash_attn    = 0
0.00.304.201 I llama_context: freq_base     = 10000.0
0.00.304.201 I llama_context: freq_scale    = 1
0.00.304.201 I ggml_metal_init: allocating
0.00.304.205 I ggml_metal_init: found device: Apple M4
0.00.304.208 I ggml_metal_init: picking default device: Apple M4
0.00.304.723 I ggml_metal_init: using embedded metal library
0.00.307.231 I ggml_metal_init: GPU name:   Apple M4
0.00.307.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.307.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.307.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.307.233 I ggml_metal_init: simdgroup reduction   = true
0.00.307.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.307.234 I ggml_metal_init: has residency sets    = true
0.00.307.234 I ggml_metal_init: has bfloat            = true
0.00.307.234 I ggml_metal_init: use bfloat            = true
0.00.307.234 I ggml_metal_init: hasUnifiedMemory      = true
0.00.307.235 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.317.853 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.320.984 I init:      Metal KV buffer size =    48.00 MiB
0.00.320.988 I llama_context: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.320.992 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.328.084 I llama_context:      Metal compute buffer size =   220.01 MiB
0.00.328.086 I llama_context:        CPU compute buffer size =    22.02 MiB
0.00.328.086 I llama_context: graph nodes  = 154
0.00.328.087 I llama_context: graph splits = 2
0.00.328.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.328.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.335.315 I 
0.00.335.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.335.463 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.335.464 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.335.475 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.335.475 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.335.479 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.335.479 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.335.996 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.339.528 I llama_perf_context_print:        load time =     319.81 ms
0.00.339.529 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17593.64 tokens per second)
0.00.339.530 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.339.531 I llama_perf_context_print:       total time =       4.21 ms /    63 tokens
0.00.340.021 I ggml_metal_free: deallocating

real	0m1.054s
user	0m0.315s
sys	0m0.039s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.169 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.329 I main: llama backend init
0.00.000.336 I main: load the model and apply lora adapter, if any
0.00.054.281 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.068.232 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.279 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.087.696 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.698 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.698 I llama_model_loader: - type  f32:  194 tensors
0.00.087.699 I llama_model_loader: - type  f16:   98 tensors
0.00.087.702 I print_info: file format = GGUF V3 (latest)
0.00.087.704 I print_info: file type   = all F32 (guessed)
0.00.087.706 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.102.684 I load: special tokens cache size = 25
0.00.111.795 I load: token to piece cache size = 0.2984 MB
0.00.111.800 I print_info: arch             = gptneox
0.00.111.800 I print_info: vocab_only       = 0
0.00.111.800 I print_info: n_ctx_train      = 2048
0.00.111.800 I print_info: n_embd           = 2048
0.00.111.801 I print_info: n_layer          = 24
0.00.111.805 I print_info: n_head           = 16
0.00.111.806 I print_info: n_head_kv        = 16
0.00.111.806 I print_info: n_rot            = 32
0.00.111.806 I print_info: n_swa            = 0
0.00.111.806 I print_info: n_embd_head_k    = 128
0.00.111.806 I print_info: n_embd_head_v    = 128
0.00.111.807 I print_info: n_gqa            = 1
0.00.111.808 I print_info: n_embd_k_gqa     = 2048
0.00.111.809 I print_info: n_embd_v_gqa     = 2048
0.00.111.810 I print_info: f_norm_eps       = 1.0e-05
0.00.111.810 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.810 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.810 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.811 I print_info: f_logit_scale    = 0.0e+00
0.00.111.812 I print_info: n_ff             = 8192
0.00.111.814 I print_info: n_expert         = 0
0.00.111.814 I print_info: n_expert_used    = 0
0.00.111.814 I print_info: causal attn      = 1
0.00.111.815 I print_info: pooling type     = 0
0.00.111.815 I print_info: rope type        = 2
0.00.111.815 I print_info: rope scaling     = linear
0.00.111.815 I print_info: freq_base_train  = 10000.0
0.00.111.816 I print_info: freq_scale_train = 1
0.00.111.816 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.816 I print_info: rope_finetuned   = unknown
0.00.111.816 I print_info: ssm_d_conv       = 0
0.00.111.816 I print_info: ssm_d_inner      = 0
0.00.111.817 I print_info: ssm_d_state      = 0
0.00.111.817 I print_info: ssm_dt_rank      = 0
0.00.111.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.819 I print_info: model type       = 1.4B
0.00.111.819 I print_info: model params     = 1.41 B
0.00.111.820 I print_info: general.name     = 1.4B
0.00.111.820 I print_info: vocab type       = BPE
0.00.111.820 I print_info: n_vocab          = 50304
0.00.111.821 I print_info: n_merges         = 50009
0.00.111.821 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.821 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.822 I print_info: LF token         = 187 'Ċ'
0.00.111.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.827 I print_info: max token length = 1024
0.00.145.502 I load_tensors: offloading 24 repeating layers to GPU
0.00.145.506 I load_tensors: offloading output layer to GPU
0.00.145.506 I load_tensors: offloaded 25/25 layers to GPU
0.00.145.522 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.145.523 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.145.819 I llama_context: n_seq_max     = 1
0.00.145.820 I llama_context: n_ctx         = 2048
0.00.145.820 I llama_context: n_ctx_per_seq = 2048
0.00.145.821 I llama_context: n_batch       = 2048
0.00.145.821 I llama_context: n_ubatch      = 512
0.00.145.821 I llama_context: flash_attn    = 0
0.00.145.821 I llama_context: freq_base     = 10000.0
0.00.145.822 I llama_context: freq_scale    = 1
0.00.145.822 I ggml_metal_init: allocating
0.00.145.838 I ggml_metal_init: found device: Apple M4
0.00.145.843 I ggml_metal_init: picking default device: Apple M4
0.00.146.406 I ggml_metal_init: using embedded metal library
0.00.167.483 I ggml_metal_init: GPU name:   Apple M4
0.00.167.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.167.485 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.167.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.167.486 I ggml_metal_init: simdgroup reduction   = true
0.00.167.486 I ggml_metal_init: simdgroup matrix mul. = true
0.00.167.486 I ggml_metal_init: has residency sets    = true
0.00.167.486 I ggml_metal_init: has bfloat            = true
0.00.167.487 I ggml_metal_init: use bfloat            = true
0.00.167.487 I ggml_metal_init: hasUnifiedMemory      = true
0.00.167.488 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.320.337 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.349.512 I init:      Metal KV buffer size =   384.00 MiB
0.00.349.518 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.349.541 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.353.187 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.353.189 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.353.189 I llama_context: graph nodes  = 967
0.00.353.190 I llama_context: graph splits = 2
0.00.353.193 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.353.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.353.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.419.562 I main: llama threadpool init, n_threads = 4
0.00.419.604 I 
0.00.419.639 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.419.640 I 
0.00.419.692 I sampler seed: 1234
0.00.419.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.419.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.419.728 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.419.728 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.246.449 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.246.449 I llama_perf_context_print:        load time =     364.00 ms
0.02.246.450 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.58 tokens per second)
0.02.246.451 I llama_perf_context_print:        eval time =    1779.97 ms /    63 runs   (   28.25 ms per token,    35.39 tokens per second)
0.02.246.452 I llama_perf_context_print:       total time =    1828.14 ms /    70 tokens
0.02.249.458 I ggml_metal_free: deallocating

real	0m2.580s
user	0m0.138s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.844 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.136 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.154 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.154 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.159 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.166 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.147 I llama_model_loader: - type  f32:  194 tensors
0.00.057.148 I llama_model_loader: - type  f16:   98 tensors
0.00.057.148 I print_info: file format = GGUF V3 (latest)
0.00.057.149 I print_info: file type   = all F32 (guessed)
0.00.057.151 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.782 I load: special tokens cache size = 25
0.00.079.020 I load: token to piece cache size = 0.2984 MB
0.00.079.024 I print_info: arch             = gptneox
0.00.079.024 I print_info: vocab_only       = 0
0.00.079.024 I print_info: n_ctx_train      = 2048
0.00.079.024 I print_info: n_embd           = 2048
0.00.079.025 I print_info: n_layer          = 24
0.00.079.028 I print_info: n_head           = 16
0.00.079.029 I print_info: n_head_kv        = 16
0.00.079.029 I print_info: n_rot            = 32
0.00.079.029 I print_info: n_swa            = 0
0.00.079.029 I print_info: n_embd_head_k    = 128
0.00.079.029 I print_info: n_embd_head_v    = 128
0.00.079.030 I print_info: n_gqa            = 1
0.00.079.031 I print_info: n_embd_k_gqa     = 2048
0.00.079.034 I print_info: n_embd_v_gqa     = 2048
0.00.079.035 I print_info: f_norm_eps       = 1.0e-05
0.00.079.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.036 I print_info: f_logit_scale    = 0.0e+00
0.00.079.036 I print_info: n_ff             = 8192
0.00.079.037 I print_info: n_expert         = 0
0.00.079.037 I print_info: n_expert_used    = 0
0.00.079.037 I print_info: causal attn      = 1
0.00.079.037 I print_info: pooling type     = 0
0.00.079.037 I print_info: rope type        = 2
0.00.079.037 I print_info: rope scaling     = linear
0.00.079.038 I print_info: freq_base_train  = 10000.0
0.00.079.038 I print_info: freq_scale_train = 1
0.00.079.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.038 I print_info: rope_finetuned   = unknown
0.00.079.039 I print_info: ssm_d_conv       = 0
0.00.079.039 I print_info: ssm_d_inner      = 0
0.00.079.039 I print_info: ssm_d_state      = 0
0.00.079.039 I print_info: ssm_dt_rank      = 0
0.00.079.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.039 I print_info: model type       = 1.4B
0.00.079.040 I print_info: model params     = 1.41 B
0.00.079.040 I print_info: general.name     = 1.4B
0.00.079.040 I print_info: vocab type       = BPE
0.00.079.041 I print_info: n_vocab          = 50304
0.00.079.041 I print_info: n_merges         = 50009
0.00.079.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.042 I print_info: LF token         = 187 'Ċ'
0.00.079.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.042 I print_info: max token length = 1024
0.01.153.462 I load_tensors: offloading 24 repeating layers to GPU
0.01.153.468 I load_tensors: offloading output layer to GPU
0.01.153.469 I load_tensors: offloaded 25/25 layers to GPU
0.01.153.495 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.153.497 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.154.433 I llama_context: n_seq_max     = 1
0.01.154.434 I llama_context: n_ctx         = 128
0.01.154.434 I llama_context: n_ctx_per_seq = 128
0.01.154.434 I llama_context: n_batch       = 128
0.01.154.435 I llama_context: n_ubatch      = 128
0.01.154.435 I llama_context: flash_attn    = 0
0.01.154.436 I llama_context: freq_base     = 10000.0
0.01.154.436 I llama_context: freq_scale    = 1
0.01.154.436 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.154.437 I ggml_metal_init: allocating
0.01.154.499 I ggml_metal_init: found device: Apple M4
0.01.154.511 I ggml_metal_init: picking default device: Apple M4
0.01.155.566 I ggml_metal_init: using embedded metal library
0.01.159.660 I ggml_metal_init: GPU name:   Apple M4
0.01.159.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.159.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.159.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.159.663 I ggml_metal_init: simdgroup reduction   = true
0.01.159.663 I ggml_metal_init: simdgroup matrix mul. = true
0.01.159.664 I ggml_metal_init: has residency sets    = true
0.01.159.664 I ggml_metal_init: has bfloat            = true
0.01.159.664 I ggml_metal_init: use bfloat            = true
0.01.159.665 I ggml_metal_init: hasUnifiedMemory      = true
0.01.159.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.170.214 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.171.936 I init:      Metal KV buffer size =    24.00 MiB
0.01.171.939 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.171.952 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.173.665 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.173.667 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.173.667 I llama_context: graph nodes  = 967
0.01.173.667 I llama_context: graph splits = 2
0.01.173.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.173.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.209.296 I 
0.01.209.336 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.209.357 I perplexity: tokenizing the input ..
0.01.214.499 I perplexity: tokenization took 5.14 ms
0.01.214.504 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.346.162 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.347.503 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.347.534 I llama_perf_context_print:        load time =    1184.78 ms
0.01.347.536 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.16 tokens per second)
0.01.347.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.347.537 I llama_perf_context_print:       total time =     138.24 ms /   129 tokens
0.01.348.084 I ggml_metal_free: deallocating

real	0m1.533s
user	0m0.100s
sys	0m0.230s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.191 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.201 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.203 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.204 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.205 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.206 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.206 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.207 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.239 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.240 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.240 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.241 I llama_model_loader: - type  f32:  194 tensors
0.00.037.241 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.242 I print_info: file format = GGUF V3 (latest)
0.00.037.243 I print_info: file type   = Q8_0
0.00.037.244 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.685 I load: special tokens cache size = 25
0.00.054.142 I load: token to piece cache size = 0.2984 MB
0.00.054.146 I print_info: arch             = gptneox
0.00.054.146 I print_info: vocab_only       = 0
0.00.054.147 I print_info: n_ctx_train      = 2048
0.00.054.147 I print_info: n_embd           = 2048
0.00.054.147 I print_info: n_layer          = 24
0.00.054.152 I print_info: n_head           = 16
0.00.054.153 I print_info: n_head_kv        = 16
0.00.054.153 I print_info: n_rot            = 32
0.00.054.153 I print_info: n_swa            = 0
0.00.054.153 I print_info: n_embd_head_k    = 128
0.00.054.153 I print_info: n_embd_head_v    = 128
0.00.054.154 I print_info: n_gqa            = 1
0.00.054.155 I print_info: n_embd_k_gqa     = 2048
0.00.054.156 I print_info: n_embd_v_gqa     = 2048
0.00.054.156 I print_info: f_norm_eps       = 1.0e-05
0.00.054.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.157 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.158 I print_info: f_logit_scale    = 0.0e+00
0.00.054.160 I print_info: n_ff             = 8192
0.00.054.160 I print_info: n_expert         = 0
0.00.054.160 I print_info: n_expert_used    = 0
0.00.054.160 I print_info: causal attn      = 1
0.00.054.160 I print_info: pooling type     = 0
0.00.054.161 I print_info: rope type        = 2
0.00.054.163 I print_info: rope scaling     = linear
0.00.054.164 I print_info: freq_base_train  = 10000.0
0.00.054.164 I print_info: freq_scale_train = 1
0.00.054.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.165 I print_info: rope_finetuned   = unknown
0.00.054.165 I print_info: ssm_d_conv       = 0
0.00.054.165 I print_info: ssm_d_inner      = 0
0.00.054.165 I print_info: ssm_d_state      = 0
0.00.054.165 I print_info: ssm_dt_rank      = 0
0.00.054.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.166 I print_info: model type       = 1.4B
0.00.054.166 I print_info: model params     = 1.41 B
0.00.054.166 I print_info: general.name     = 1.4B
0.00.054.167 I print_info: vocab type       = BPE
0.00.054.167 I print_info: n_vocab          = 50304
0.00.054.167 I print_info: n_merges         = 50009
0.00.054.168 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.168 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.168 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.168 I print_info: LF token         = 187 'Ċ'
0.00.054.169 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.169 I print_info: max token length = 1024
0.01.159.860 I load_tensors: offloading 24 repeating layers to GPU
0.01.159.865 I load_tensors: offloading output layer to GPU
0.01.159.866 I load_tensors: offloaded 25/25 layers to GPU
0.01.159.890 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.159.890 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.161.030 I llama_context: n_seq_max     = 1
0.01.161.032 I llama_context: n_ctx         = 2048
0.01.161.032 I llama_context: n_ctx_per_seq = 2048
0.01.161.033 I llama_context: n_batch       = 2048
0.01.161.033 I llama_context: n_ubatch      = 512
0.01.161.033 I llama_context: flash_attn    = 0
0.01.161.034 I llama_context: freq_base     = 10000.0
0.01.161.035 I llama_context: freq_scale    = 1
0.01.161.036 I ggml_metal_init: allocating
0.01.161.043 I ggml_metal_init: found device: Apple M4
0.01.161.050 I ggml_metal_init: picking default device: Apple M4
0.01.162.308 I ggml_metal_init: using embedded metal library
0.01.167.784 I ggml_metal_init: GPU name:   Apple M4
0.01.167.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.167.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.167.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.167.790 I ggml_metal_init: simdgroup reduction   = true
0.01.167.790 I ggml_metal_init: simdgroup matrix mul. = true
0.01.167.791 I ggml_metal_init: has residency sets    = true
0.01.167.791 I ggml_metal_init: has bfloat            = true
0.01.167.791 I ggml_metal_init: use bfloat            = true
0.01.167.792 I ggml_metal_init: hasUnifiedMemory      = true
0.01.167.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.183.932 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.254.801 I init:      Metal KV buffer size =   384.00 MiB
0.01.254.813 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.254.844 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.259.208 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.259.210 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.259.211 I llama_context: graph nodes  = 967
0.01.259.211 I llama_context: graph splits = 2
0.01.259.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.259.352 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.259.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.317.951 I main: llama threadpool init, n_threads = 4
0.01.317.988 I 
0.01.318.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.318.013 I 
0.01.318.161 I sampler seed: 1234
0.01.318.165 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.318.202 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.318.205 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.318.206 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.403.472 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.02.403.473 I llama_perf_context_print:        load time =    1307.03 ms
0.02.403.474 I llama_perf_context_print: prompt eval time =      49.37 ms /     7 tokens (    7.05 ms per token,   141.79 tokens per second)
0.02.403.475 I llama_perf_context_print:        eval time =    1033.06 ms /    63 runs   (   16.40 ms per token,    60.98 tokens per second)
0.02.403.475 I llama_perf_context_print:       total time =    1086.51 ms /    70 tokens
0.02.407.144 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.113s
sys	0m0.287s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.277 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.449 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.456 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.457 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.458 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.459 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.460 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.523 I llama_model_loader: - type  f32:  194 tensors
0.00.025.523 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.524 I print_info: file format = GGUF V3 (latest)
0.00.025.525 I print_info: file type   = Q8_0
0.00.025.526 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.811 I load: special tokens cache size = 25
0.00.042.071 I load: token to piece cache size = 0.2984 MB
0.00.042.075 I print_info: arch             = gptneox
0.00.042.075 I print_info: vocab_only       = 0
0.00.042.075 I print_info: n_ctx_train      = 2048
0.00.042.075 I print_info: n_embd           = 2048
0.00.042.076 I print_info: n_layer          = 24
0.00.042.080 I print_info: n_head           = 16
0.00.042.081 I print_info: n_head_kv        = 16
0.00.042.081 I print_info: n_rot            = 32
0.00.042.085 I print_info: n_swa            = 0
0.00.042.085 I print_info: n_embd_head_k    = 128
0.00.042.085 I print_info: n_embd_head_v    = 128
0.00.042.086 I print_info: n_gqa            = 1
0.00.042.087 I print_info: n_embd_k_gqa     = 2048
0.00.042.088 I print_info: n_embd_v_gqa     = 2048
0.00.042.089 I print_info: f_norm_eps       = 1.0e-05
0.00.042.089 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.089 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.090 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.090 I print_info: f_logit_scale    = 0.0e+00
0.00.042.091 I print_info: n_ff             = 8192
0.00.042.091 I print_info: n_expert         = 0
0.00.042.091 I print_info: n_expert_used    = 0
0.00.042.091 I print_info: causal attn      = 1
0.00.042.091 I print_info: pooling type     = 0
0.00.042.091 I print_info: rope type        = 2
0.00.042.092 I print_info: rope scaling     = linear
0.00.042.092 I print_info: freq_base_train  = 10000.0
0.00.042.092 I print_info: freq_scale_train = 1
0.00.042.092 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.093 I print_info: rope_finetuned   = unknown
0.00.042.093 I print_info: ssm_d_conv       = 0
0.00.042.093 I print_info: ssm_d_inner      = 0
0.00.042.093 I print_info: ssm_d_state      = 0
0.00.042.093 I print_info: ssm_dt_rank      = 0
0.00.042.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.094 I print_info: model type       = 1.4B
0.00.042.094 I print_info: model params     = 1.41 B
0.00.042.094 I print_info: general.name     = 1.4B
0.00.042.095 I print_info: vocab type       = BPE
0.00.042.095 I print_info: n_vocab          = 50304
0.00.042.095 I print_info: n_merges         = 50009
0.00.042.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.096 I print_info: LF token         = 187 'Ċ'
0.00.042.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.097 I print_info: max token length = 1024
0.00.898.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.898.633 I load_tensors: offloading output layer to GPU
0.00.898.634 I load_tensors: offloaded 25/25 layers to GPU
0.00.898.659 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.898.661 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.899.868 I llama_context: n_seq_max     = 1
0.00.899.869 I llama_context: n_ctx         = 128
0.00.899.870 I llama_context: n_ctx_per_seq = 128
0.00.899.870 I llama_context: n_batch       = 128
0.00.899.870 I llama_context: n_ubatch      = 128
0.00.899.871 I llama_context: flash_attn    = 0
0.00.899.872 I llama_context: freq_base     = 10000.0
0.00.899.872 I llama_context: freq_scale    = 1
0.00.899.873 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.899.874 I ggml_metal_init: allocating
0.00.899.915 I ggml_metal_init: found device: Apple M4
0.00.899.927 I ggml_metal_init: picking default device: Apple M4
0.00.901.103 I ggml_metal_init: using embedded metal library
0.00.906.304 I ggml_metal_init: GPU name:   Apple M4
0.00.906.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.906.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.906.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.906.309 I ggml_metal_init: simdgroup reduction   = true
0.00.906.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.906.310 I ggml_metal_init: has residency sets    = true
0.00.906.310 I ggml_metal_init: has bfloat            = true
0.00.906.310 I ggml_metal_init: use bfloat            = true
0.00.906.311 I ggml_metal_init: hasUnifiedMemory      = true
0.00.906.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.921.347 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.924.722 I init:      Metal KV buffer size =    24.00 MiB
0.00.924.726 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.924.755 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.927.716 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.927.717 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.927.718 I llama_context: graph nodes  = 967
0.00.927.718 I llama_context: graph splits = 2
0.00.927.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.927.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.430 I 
0.00.957.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.531 I perplexity: tokenizing the input ..
0.00.964.501 I perplexity: tokenization took 6.969 ms
0.00.964.506 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.101.388 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.102.724 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.102.753 I llama_perf_context_print:        load time =     948.14 ms
0.01.102.753 I llama_perf_context_print: prompt eval time =     136.62 ms /   128 tokens (    1.07 ms per token,   936.93 tokens per second)
0.01.102.754 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.102.754 I llama_perf_context_print:       total time =     145.33 ms /   129 tokens
0.01.103.327 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.078s
sys	0m0.208s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.066 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.103 I main: llama backend init
0.00.000.105 I main: load the model and apply lora adapter, if any
0.00.011.883 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.369 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.370 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.372 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.372 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.375 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.029 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.030 I llama_model_loader: - type  f32:  194 tensors
0.00.028.030 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.031 I print_info: file format = GGUF V3 (latest)
0.00.028.032 I print_info: file type   = Q4_0
0.00.028.033 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.383 I load: special tokens cache size = 25
0.00.042.509 I load: token to piece cache size = 0.2984 MB
0.00.042.513 I print_info: arch             = gptneox
0.00.042.513 I print_info: vocab_only       = 0
0.00.042.513 I print_info: n_ctx_train      = 2048
0.00.042.513 I print_info: n_embd           = 2048
0.00.042.513 I print_info: n_layer          = 24
0.00.042.518 I print_info: n_head           = 16
0.00.042.519 I print_info: n_head_kv        = 16
0.00.042.519 I print_info: n_rot            = 32
0.00.042.520 I print_info: n_swa            = 0
0.00.042.523 I print_info: n_embd_head_k    = 128
0.00.042.523 I print_info: n_embd_head_v    = 128
0.00.042.524 I print_info: n_gqa            = 1
0.00.042.524 I print_info: n_embd_k_gqa     = 2048
0.00.042.525 I print_info: n_embd_v_gqa     = 2048
0.00.042.526 I print_info: f_norm_eps       = 1.0e-05
0.00.042.526 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.528 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.528 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.528 I print_info: f_logit_scale    = 0.0e+00
0.00.042.529 I print_info: n_ff             = 8192
0.00.042.529 I print_info: n_expert         = 0
0.00.042.529 I print_info: n_expert_used    = 0
0.00.042.529 I print_info: causal attn      = 1
0.00.042.530 I print_info: pooling type     = 0
0.00.042.530 I print_info: rope type        = 2
0.00.042.530 I print_info: rope scaling     = linear
0.00.042.530 I print_info: freq_base_train  = 10000.0
0.00.042.530 I print_info: freq_scale_train = 1
0.00.042.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.531 I print_info: rope_finetuned   = unknown
0.00.042.531 I print_info: ssm_d_conv       = 0
0.00.042.531 I print_info: ssm_d_inner      = 0
0.00.042.531 I print_info: ssm_d_state      = 0
0.00.042.531 I print_info: ssm_dt_rank      = 0
0.00.042.531 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.532 I print_info: model type       = 1.4B
0.00.042.537 I print_info: model params     = 1.41 B
0.00.042.538 I print_info: general.name     = 1.4B
0.00.042.538 I print_info: vocab type       = BPE
0.00.042.539 I print_info: n_vocab          = 50304
0.00.042.539 I print_info: n_merges         = 50009
0.00.042.539 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.539 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.541 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.541 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.541 I print_info: LF token         = 187 'Ċ'
0.00.042.541 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.541 I print_info: max token length = 1024
0.00.560.481 I load_tensors: offloading 24 repeating layers to GPU
0.00.560.496 I load_tensors: offloading output layer to GPU
0.00.560.497 I load_tensors: offloaded 25/25 layers to GPU
0.00.560.532 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.560.533 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.562.108 I llama_context: n_seq_max     = 1
0.00.562.114 I llama_context: n_ctx         = 2048
0.00.562.114 I llama_context: n_ctx_per_seq = 2048
0.00.562.115 I llama_context: n_batch       = 2048
0.00.562.115 I llama_context: n_ubatch      = 512
0.00.562.116 I llama_context: flash_attn    = 0
0.00.562.118 I llama_context: freq_base     = 10000.0
0.00.562.119 I llama_context: freq_scale    = 1
0.00.562.125 I ggml_metal_init: allocating
0.00.562.223 I ggml_metal_init: found device: Apple M4
0.00.562.237 I ggml_metal_init: picking default device: Apple M4
0.00.564.082 I ggml_metal_init: using embedded metal library
0.00.569.960 I ggml_metal_init: GPU name:   Apple M4
0.00.569.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.569.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.569.967 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.569.968 I ggml_metal_init: simdgroup reduction   = true
0.00.569.968 I ggml_metal_init: simdgroup matrix mul. = true
0.00.569.968 I ggml_metal_init: has residency sets    = true
0.00.569.969 I ggml_metal_init: has bfloat            = true
0.00.569.969 I ggml_metal_init: use bfloat            = true
0.00.569.970 I ggml_metal_init: hasUnifiedMemory      = true
0.00.569.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.589.666 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.645.581 I init:      Metal KV buffer size =   384.00 MiB
0.00.645.590 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.645.623 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.649.804 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.649.806 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.649.806 I llama_context: graph nodes  = 967
0.00.649.806 I llama_context: graph splits = 2
0.00.649.811 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.649.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.649.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.528 I main: llama threadpool init, n_threads = 4
0.00.707.578 I 
0.00.707.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.603 I 
0.00.707.778 I sampler seed: 1234
0.00.707.782 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.831 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.834 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.389.545 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.389.546 I llama_perf_context_print:        load time =     694.65 ms
0.01.389.546 I llama_perf_context_print: prompt eval time =      49.27 ms /     7 tokens (    7.04 ms per token,   142.07 tokens per second)
0.01.389.547 I llama_perf_context_print:        eval time =     629.56 ms /    63 runs   (    9.99 ms per token,   100.07 tokens per second)
0.01.389.547 I llama_perf_context_print:       total time =     683.00 ms /    70 tokens
0.01.393.265 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.054 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.084 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.091 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.106 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.958 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.778 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.778 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.779 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.779 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.780 I llama_model_loader: - type  f32:  194 tensors
0.00.025.780 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.780 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.781 I print_info: file format = GGUF V3 (latest)
0.00.025.782 I print_info: file type   = Q4_0
0.00.025.783 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.979 I load: special tokens cache size = 25
0.00.040.130 I load: token to piece cache size = 0.2984 MB
0.00.040.133 I print_info: arch             = gptneox
0.00.040.133 I print_info: vocab_only       = 0
0.00.040.133 I print_info: n_ctx_train      = 2048
0.00.040.134 I print_info: n_embd           = 2048
0.00.040.134 I print_info: n_layer          = 24
0.00.040.137 I print_info: n_head           = 16
0.00.040.140 I print_info: n_head_kv        = 16
0.00.040.140 I print_info: n_rot            = 32
0.00.040.140 I print_info: n_swa            = 0
0.00.040.140 I print_info: n_embd_head_k    = 128
0.00.040.140 I print_info: n_embd_head_v    = 128
0.00.040.141 I print_info: n_gqa            = 1
0.00.040.142 I print_info: n_embd_k_gqa     = 2048
0.00.040.143 I print_info: n_embd_v_gqa     = 2048
0.00.040.143 I print_info: f_norm_eps       = 1.0e-05
0.00.040.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.145 I print_info: f_logit_scale    = 0.0e+00
0.00.040.145 I print_info: n_ff             = 8192
0.00.040.146 I print_info: n_expert         = 0
0.00.040.146 I print_info: n_expert_used    = 0
0.00.040.146 I print_info: causal attn      = 1
0.00.040.146 I print_info: pooling type     = 0
0.00.040.146 I print_info: rope type        = 2
0.00.040.146 I print_info: rope scaling     = linear
0.00.040.147 I print_info: freq_base_train  = 10000.0
0.00.040.147 I print_info: freq_scale_train = 1
0.00.040.147 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.148 I print_info: rope_finetuned   = unknown
0.00.040.148 I print_info: ssm_d_conv       = 0
0.00.040.148 I print_info: ssm_d_inner      = 0
0.00.040.148 I print_info: ssm_d_state      = 0
0.00.040.148 I print_info: ssm_dt_rank      = 0
0.00.040.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.152 I print_info: model type       = 1.4B
0.00.040.153 I print_info: model params     = 1.41 B
0.00.040.153 I print_info: general.name     = 1.4B
0.00.040.153 I print_info: vocab type       = BPE
0.00.040.154 I print_info: n_vocab          = 50304
0.00.040.154 I print_info: n_merges         = 50009
0.00.040.154 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.155 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.155 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.156 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.156 I print_info: LF token         = 187 'Ċ'
0.00.040.156 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.156 I print_info: max token length = 1024
0.00.529.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.529.349 I load_tensors: offloading output layer to GPU
0.00.529.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.529.384 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.529.385 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.530.989 I llama_context: n_seq_max     = 1
0.00.530.994 I llama_context: n_ctx         = 128
0.00.530.994 I llama_context: n_ctx_per_seq = 128
0.00.530.995 I llama_context: n_batch       = 128
0.00.530.995 I llama_context: n_ubatch      = 128
0.00.530.996 I llama_context: flash_attn    = 0
0.00.530.998 I llama_context: freq_base     = 10000.0
0.00.530.998 I llama_context: freq_scale    = 1
0.00.530.999 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.531.001 I ggml_metal_init: allocating
0.00.531.072 I ggml_metal_init: found device: Apple M4
0.00.531.086 I ggml_metal_init: picking default device: Apple M4
0.00.532.832 I ggml_metal_init: using embedded metal library
0.00.539.506 I ggml_metal_init: GPU name:   Apple M4
0.00.539.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.513 I ggml_metal_init: simdgroup reduction   = true
0.00.539.514 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.514 I ggml_metal_init: has residency sets    = true
0.00.539.514 I ggml_metal_init: has bfloat            = true
0.00.539.515 I ggml_metal_init: use bfloat            = true
0.00.539.516 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.557.380 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.921 I init:      Metal KV buffer size =    24.00 MiB
0.00.560.925 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.965 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.564.205 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.564.207 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.564.208 I llama_context: graph nodes  = 967
0.00.564.208 I llama_context: graph splits = 2
0.00.564.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.564.211 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.172 I 
0.00.590.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.272 I perplexity: tokenizing the input ..
0.00.596.879 I perplexity: tokenization took 6.605 ms
0.00.596.886 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.862 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.731.198 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.731.220 I llama_perf_context_print:        load time =     580.10 ms
0.00.731.221 I llama_perf_context_print: prompt eval time =     132.59 ms /   128 tokens (    1.04 ms per token,   965.40 tokens per second)
0.00.731.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.222 I llama_perf_context_print:       total time =     141.05 ms /   129 tokens
0.00.731.780 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.078s
sys	0m0.114s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.195 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.923 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.942 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.942 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.943 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.860 I llama_model_loader: - type  f32:  194 tensors
0.00.025.860 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.861 I print_info: file format = GGUF V3 (latest)
0.00.025.862 I print_info: file type   = Q4_1
0.00.025.863 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.941 I load: special tokens cache size = 25
0.00.039.887 I load: token to piece cache size = 0.2984 MB
0.00.039.892 I print_info: arch             = gptneox
0.00.039.892 I print_info: vocab_only       = 0
0.00.039.892 I print_info: n_ctx_train      = 2048
0.00.039.892 I print_info: n_embd           = 2048
0.00.039.893 I print_info: n_layer          = 24
0.00.039.897 I print_info: n_head           = 16
0.00.039.897 I print_info: n_head_kv        = 16
0.00.039.897 I print_info: n_rot            = 32
0.00.039.898 I print_info: n_swa            = 0
0.00.039.898 I print_info: n_embd_head_k    = 128
0.00.039.898 I print_info: n_embd_head_v    = 128
0.00.039.899 I print_info: n_gqa            = 1
0.00.039.900 I print_info: n_embd_k_gqa     = 2048
0.00.039.900 I print_info: n_embd_v_gqa     = 2048
0.00.039.901 I print_info: f_norm_eps       = 1.0e-05
0.00.039.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.902 I print_info: f_logit_scale    = 0.0e+00
0.00.039.905 I print_info: n_ff             = 8192
0.00.039.905 I print_info: n_expert         = 0
0.00.039.905 I print_info: n_expert_used    = 0
0.00.039.907 I print_info: causal attn      = 1
0.00.039.907 I print_info: pooling type     = 0
0.00.039.908 I print_info: rope type        = 2
0.00.039.910 I print_info: rope scaling     = linear
0.00.039.910 I print_info: freq_base_train  = 10000.0
0.00.039.910 I print_info: freq_scale_train = 1
0.00.039.910 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.911 I print_info: rope_finetuned   = unknown
0.00.039.911 I print_info: ssm_d_conv       = 0
0.00.039.911 I print_info: ssm_d_inner      = 0
0.00.039.911 I print_info: ssm_d_state      = 0
0.00.039.911 I print_info: ssm_dt_rank      = 0
0.00.039.911 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.911 I print_info: model type       = 1.4B
0.00.039.912 I print_info: model params     = 1.41 B
0.00.039.912 I print_info: general.name     = 1.4B
0.00.039.912 I print_info: vocab type       = BPE
0.00.039.912 I print_info: n_vocab          = 50304
0.00.039.913 I print_info: n_merges         = 50009
0.00.039.913 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.917 I print_info: LF token         = 187 'Ċ'
0.00.039.917 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.918 I print_info: max token length = 1024
0.00.584.400 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.409 I load_tensors: offloading output layer to GPU
0.00.584.409 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.444 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.584.448 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.585.537 I llama_context: n_seq_max     = 1
0.00.585.545 I llama_context: n_ctx         = 2048
0.00.585.546 I llama_context: n_ctx_per_seq = 2048
0.00.585.546 I llama_context: n_batch       = 2048
0.00.585.547 I llama_context: n_ubatch      = 512
0.00.585.547 I llama_context: flash_attn    = 0
0.00.585.548 I llama_context: freq_base     = 10000.0
0.00.585.548 I llama_context: freq_scale    = 1
0.00.585.555 I ggml_metal_init: allocating
0.00.585.639 I ggml_metal_init: found device: Apple M4
0.00.585.656 I ggml_metal_init: picking default device: Apple M4
0.00.587.568 I ggml_metal_init: using embedded metal library
0.00.594.644 I ggml_metal_init: GPU name:   Apple M4
0.00.594.652 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.655 I ggml_metal_init: simdgroup reduction   = true
0.00.594.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.655 I ggml_metal_init: has residency sets    = true
0.00.594.656 I ggml_metal_init: has bfloat            = true
0.00.594.656 I ggml_metal_init: use bfloat            = true
0.00.594.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.857 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.868 I init:      Metal KV buffer size =   384.00 MiB
0.00.665.876 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.665.895 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.672.301 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.672.304 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.672.304 I llama_context: graph nodes  = 967
0.00.672.304 I llama_context: graph splits = 2
0.00.672.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.414 I main: llama threadpool init, n_threads = 4
0.00.730.461 I 
0.00.730.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.486 I 
0.00.730.659 I sampler seed: 1234
0.00.730.663 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.708 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.712 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.455.347 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.455.348 I llama_perf_context_print:        load time =     720.30 ms
0.01.455.348 I llama_perf_context_print: prompt eval time =      48.92 ms /     7 tokens (    6.99 ms per token,   143.09 tokens per second)
0.01.455.352 I llama_perf_context_print:        eval time =     672.94 ms /    63 runs   (   10.68 ms per token,    93.62 tokens per second)
0.01.455.354 I llama_perf_context_print:       total time =     725.85 ms /    70 tokens
0.01.459.124 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.110s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.781 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.791 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.793 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.735 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.735 I llama_model_loader: - type  f32:  194 tensors
0.00.024.736 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.736 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.737 I print_info: file format = GGUF V3 (latest)
0.00.024.737 I print_info: file type   = Q4_1
0.00.024.739 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.548 I load: special tokens cache size = 25
0.00.038.692 I load: token to piece cache size = 0.2984 MB
0.00.038.695 I print_info: arch             = gptneox
0.00.038.695 I print_info: vocab_only       = 0
0.00.038.695 I print_info: n_ctx_train      = 2048
0.00.038.695 I print_info: n_embd           = 2048
0.00.038.695 I print_info: n_layer          = 24
0.00.038.699 I print_info: n_head           = 16
0.00.038.700 I print_info: n_head_kv        = 16
0.00.038.700 I print_info: n_rot            = 32
0.00.038.700 I print_info: n_swa            = 0
0.00.038.700 I print_info: n_embd_head_k    = 128
0.00.038.701 I print_info: n_embd_head_v    = 128
0.00.038.701 I print_info: n_gqa            = 1
0.00.038.702 I print_info: n_embd_k_gqa     = 2048
0.00.038.703 I print_info: n_embd_v_gqa     = 2048
0.00.038.703 I print_info: f_norm_eps       = 1.0e-05
0.00.038.704 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.707 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.707 I print_info: f_logit_scale    = 0.0e+00
0.00.038.707 I print_info: n_ff             = 8192
0.00.038.708 I print_info: n_expert         = 0
0.00.038.708 I print_info: n_expert_used    = 0
0.00.038.708 I print_info: causal attn      = 1
0.00.038.708 I print_info: pooling type     = 0
0.00.038.708 I print_info: rope type        = 2
0.00.038.709 I print_info: rope scaling     = linear
0.00.038.709 I print_info: freq_base_train  = 10000.0
0.00.038.711 I print_info: freq_scale_train = 1
0.00.038.711 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.712 I print_info: rope_finetuned   = unknown
0.00.038.712 I print_info: ssm_d_conv       = 0
0.00.038.712 I print_info: ssm_d_inner      = 0
0.00.038.712 I print_info: ssm_d_state      = 0
0.00.038.712 I print_info: ssm_dt_rank      = 0
0.00.038.712 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.712 I print_info: model type       = 1.4B
0.00.038.713 I print_info: model params     = 1.41 B
0.00.038.713 I print_info: general.name     = 1.4B
0.00.038.714 I print_info: vocab type       = BPE
0.00.038.714 I print_info: n_vocab          = 50304
0.00.038.714 I print_info: n_merges         = 50009
0.00.038.715 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.715 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.719 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.719 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.719 I print_info: LF token         = 187 'Ċ'
0.00.038.719 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.720 I print_info: max token length = 1024
0.00.588.733 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.748 I load_tensors: offloading output layer to GPU
0.00.588.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.783 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.588.784 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.590.314 I llama_context: n_seq_max     = 1
0.00.590.318 I llama_context: n_ctx         = 128
0.00.590.319 I llama_context: n_ctx_per_seq = 128
0.00.590.320 I llama_context: n_batch       = 128
0.00.590.320 I llama_context: n_ubatch      = 128
0.00.590.321 I llama_context: flash_attn    = 0
0.00.590.323 I llama_context: freq_base     = 10000.0
0.00.590.323 I llama_context: freq_scale    = 1
0.00.590.324 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.326 I ggml_metal_init: allocating
0.00.590.400 I ggml_metal_init: found device: Apple M4
0.00.590.414 I ggml_metal_init: picking default device: Apple M4
0.00.592.106 I ggml_metal_init: using embedded metal library
0.00.598.555 I ggml_metal_init: GPU name:   Apple M4
0.00.598.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.563 I ggml_metal_init: simdgroup reduction   = true
0.00.598.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.564 I ggml_metal_init: has residency sets    = true
0.00.598.564 I ggml_metal_init: has bfloat            = true
0.00.598.565 I ggml_metal_init: use bfloat            = true
0.00.598.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.616.790 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.390 I init:      Metal KV buffer size =    24.00 MiB
0.00.620.394 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.424 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.623.513 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.623.515 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.623.515 I llama_context: graph nodes  = 967
0.00.623.515 I llama_context: graph splits = 2
0.00.623.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.584 I 
0.00.646.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.670 I perplexity: tokenizing the input ..
0.00.653.183 I perplexity: tokenization took 6.51 ms
0.00.653.189 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.733 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.115 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.138 I llama_perf_context_print:        load time =     637.79 ms
0.00.790.138 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.18 tokens per second)
0.00.790.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.140 I llama_perf_context_print:       total time =     143.56 ms /   129 tokens
0.00.790.702 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.079s
sys	0m0.117s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.064 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.658 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.664 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.665 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.480 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.481 I llama_model_loader: - type  f32:  194 tensors
0.00.027.481 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.481 I print_info: file format = GGUF V3 (latest)
0.00.027.482 I print_info: file type   = Q5_0
0.00.027.482 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.187 I load: special tokens cache size = 25
0.00.041.356 I load: token to piece cache size = 0.2984 MB
0.00.041.359 I print_info: arch             = gptneox
0.00.041.359 I print_info: vocab_only       = 0
0.00.041.359 I print_info: n_ctx_train      = 2048
0.00.041.359 I print_info: n_embd           = 2048
0.00.041.360 I print_info: n_layer          = 24
0.00.041.362 I print_info: n_head           = 16
0.00.041.363 I print_info: n_head_kv        = 16
0.00.041.363 I print_info: n_rot            = 32
0.00.041.363 I print_info: n_swa            = 0
0.00.041.364 I print_info: n_embd_head_k    = 128
0.00.041.364 I print_info: n_embd_head_v    = 128
0.00.041.365 I print_info: n_gqa            = 1
0.00.041.366 I print_info: n_embd_k_gqa     = 2048
0.00.041.366 I print_info: n_embd_v_gqa     = 2048
0.00.041.368 I print_info: f_norm_eps       = 1.0e-05
0.00.041.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.369 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.369 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.369 I print_info: f_logit_scale    = 0.0e+00
0.00.041.370 I print_info: n_ff             = 8192
0.00.041.370 I print_info: n_expert         = 0
0.00.041.370 I print_info: n_expert_used    = 0
0.00.041.370 I print_info: causal attn      = 1
0.00.041.371 I print_info: pooling type     = 0
0.00.041.372 I print_info: rope type        = 2
0.00.041.374 I print_info: rope scaling     = linear
0.00.041.374 I print_info: freq_base_train  = 10000.0
0.00.041.374 I print_info: freq_scale_train = 1
0.00.041.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.375 I print_info: rope_finetuned   = unknown
0.00.041.375 I print_info: ssm_d_conv       = 0
0.00.041.375 I print_info: ssm_d_inner      = 0
0.00.041.375 I print_info: ssm_d_state      = 0
0.00.041.375 I print_info: ssm_dt_rank      = 0
0.00.041.375 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.376 I print_info: model type       = 1.4B
0.00.041.376 I print_info: model params     = 1.41 B
0.00.041.376 I print_info: general.name     = 1.4B
0.00.041.377 I print_info: vocab type       = BPE
0.00.041.377 I print_info: n_vocab          = 50304
0.00.041.381 I print_info: n_merges         = 50009
0.00.041.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.381 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.381 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: LF token         = 187 'Ċ'
0.00.041.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.382 I print_info: max token length = 1024
0.00.676.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.633 I load_tensors: offloading output layer to GPU
0.00.676.634 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.667 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.676.673 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.678.306 I llama_context: n_seq_max     = 1
0.00.678.311 I llama_context: n_ctx         = 2048
0.00.678.312 I llama_context: n_ctx_per_seq = 2048
0.00.678.312 I llama_context: n_batch       = 2048
0.00.678.313 I llama_context: n_ubatch      = 512
0.00.678.313 I llama_context: flash_attn    = 0
0.00.678.315 I llama_context: freq_base     = 10000.0
0.00.678.315 I llama_context: freq_scale    = 1
0.00.678.322 I ggml_metal_init: allocating
0.00.678.403 I ggml_metal_init: found device: Apple M4
0.00.678.416 I ggml_metal_init: picking default device: Apple M4
0.00.680.185 I ggml_metal_init: using embedded metal library
0.00.686.917 I ggml_metal_init: GPU name:   Apple M4
0.00.686.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.924 I ggml_metal_init: simdgroup reduction   = true
0.00.686.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.925 I ggml_metal_init: has residency sets    = true
0.00.686.925 I ggml_metal_init: has bfloat            = true
0.00.686.925 I ggml_metal_init: use bfloat            = true
0.00.686.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.094 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.786 I init:      Metal KV buffer size =   384.00 MiB
0.00.761.794 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.761.815 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.766.197 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.766.199 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.766.199 I llama_context: graph nodes  = 967
0.00.766.199 I llama_context: graph splits = 2
0.00.766.204 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.766.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.766.338 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.415 I main: llama threadpool init, n_threads = 4
0.00.825.459 I 
0.00.825.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.483 I 
0.00.825.653 I sampler seed: 1234
0.00.825.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.698 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.698 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.623.844 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47747.14 tokens per second)
0.01.623.844 I llama_perf_context_print:        load time =     813.42 ms
0.01.623.845 I llama_perf_context_print: prompt eval time =      52.52 ms /     7 tokens (    7.50 ms per token,   133.27 tokens per second)
0.01.623.846 I llama_perf_context_print:        eval time =     743.04 ms /    63 runs   (   11.79 ms per token,    84.79 tokens per second)
0.01.623.846 I llama_perf_context_print:       total time =     799.36 ms /    70 tokens
0.01.627.090 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.110s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.700 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.598 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.599 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.599 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.602 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.602 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.556 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.557 I llama_model_loader: - type  f32:  194 tensors
0.00.025.557 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.558 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.558 I print_info: file format = GGUF V3 (latest)
0.00.025.559 I print_info: file type   = Q5_0
0.00.025.560 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.324 I load: special tokens cache size = 25
0.00.039.291 I load: token to piece cache size = 0.2984 MB
0.00.039.294 I print_info: arch             = gptneox
0.00.039.294 I print_info: vocab_only       = 0
0.00.039.294 I print_info: n_ctx_train      = 2048
0.00.039.294 I print_info: n_embd           = 2048
0.00.039.295 I print_info: n_layer          = 24
0.00.039.298 I print_info: n_head           = 16
0.00.039.299 I print_info: n_head_kv        = 16
0.00.039.299 I print_info: n_rot            = 32
0.00.039.299 I print_info: n_swa            = 0
0.00.039.299 I print_info: n_embd_head_k    = 128
0.00.039.302 I print_info: n_embd_head_v    = 128
0.00.039.303 I print_info: n_gqa            = 1
0.00.039.304 I print_info: n_embd_k_gqa     = 2048
0.00.039.304 I print_info: n_embd_v_gqa     = 2048
0.00.039.305 I print_info: f_norm_eps       = 1.0e-05
0.00.039.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.306 I print_info: f_logit_scale    = 0.0e+00
0.00.039.312 I print_info: n_ff             = 8192
0.00.039.313 I print_info: n_expert         = 0
0.00.039.313 I print_info: n_expert_used    = 0
0.00.039.314 I print_info: causal attn      = 1
0.00.039.314 I print_info: pooling type     = 0
0.00.039.314 I print_info: rope type        = 2
0.00.039.314 I print_info: rope scaling     = linear
0.00.039.315 I print_info: freq_base_train  = 10000.0
0.00.039.315 I print_info: freq_scale_train = 1
0.00.039.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.315 I print_info: rope_finetuned   = unknown
0.00.039.316 I print_info: ssm_d_conv       = 0
0.00.039.316 I print_info: ssm_d_inner      = 0
0.00.039.316 I print_info: ssm_d_state      = 0
0.00.039.316 I print_info: ssm_dt_rank      = 0
0.00.039.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.316 I print_info: model type       = 1.4B
0.00.039.317 I print_info: model params     = 1.41 B
0.00.039.317 I print_info: general.name     = 1.4B
0.00.039.318 I print_info: vocab type       = BPE
0.00.039.318 I print_info: n_vocab          = 50304
0.00.039.319 I print_info: n_merges         = 50009
0.00.039.319 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: LF token         = 187 'Ċ'
0.00.039.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: max token length = 1024
0.00.677.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.271 I load_tensors: offloading output layer to GPU
0.00.677.271 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.308 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.677.310 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.678.899 I llama_context: n_seq_max     = 1
0.00.678.904 I llama_context: n_ctx         = 128
0.00.678.904 I llama_context: n_ctx_per_seq = 128
0.00.678.905 I llama_context: n_batch       = 128
0.00.678.905 I llama_context: n_ubatch      = 128
0.00.678.906 I llama_context: flash_attn    = 0
0.00.678.908 I llama_context: freq_base     = 10000.0
0.00.678.908 I llama_context: freq_scale    = 1
0.00.678.909 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.678.911 I ggml_metal_init: allocating
0.00.678.988 I ggml_metal_init: found device: Apple M4
0.00.679.002 I ggml_metal_init: picking default device: Apple M4
0.00.680.753 I ggml_metal_init: using embedded metal library
0.00.687.295 I ggml_metal_init: GPU name:   Apple M4
0.00.687.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.687.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.687.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.687.302 I ggml_metal_init: simdgroup reduction   = true
0.00.687.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.687.303 I ggml_metal_init: has residency sets    = true
0.00.687.303 I ggml_metal_init: has bfloat            = true
0.00.687.303 I ggml_metal_init: use bfloat            = true
0.00.687.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.687.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.047 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.637 I init:      Metal KV buffer size =    24.00 MiB
0.00.708.641 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.708.665 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.711.940 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.711.941 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.711.942 I llama_context: graph nodes  = 967
0.00.711.942 I llama_context: graph splits = 2
0.00.711.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.711.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.744 I 
0.00.741.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.850 I perplexity: tokenizing the input ..
0.00.748.994 I perplexity: tokenization took 7.141 ms
0.00.749.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.748 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.886.083 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.886.110 I llama_perf_context_print:        load time =     732.03 ms
0.00.886.111 I llama_perf_context_print: prompt eval time =     134.90 ms /   128 tokens (    1.05 ms per token,   948.84 tokens per second)
0.00.886.111 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.112 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.886.677 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.079s
sys	0m0.144s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.100 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.101 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.101 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.104 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.104 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.105 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.105 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.107 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.107 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.186 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.187 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.187 I print_info: file format = GGUF V3 (latest)
0.00.025.188 I print_info: file type   = Q5_1
0.00.025.189 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.196 I load: special tokens cache size = 25
0.00.039.274 I load: token to piece cache size = 0.2984 MB
0.00.039.279 I print_info: arch             = gptneox
0.00.039.279 I print_info: vocab_only       = 0
0.00.039.279 I print_info: n_ctx_train      = 2048
0.00.039.279 I print_info: n_embd           = 2048
0.00.039.280 I print_info: n_layer          = 24
0.00.039.284 I print_info: n_head           = 16
0.00.039.285 I print_info: n_head_kv        = 16
0.00.039.285 I print_info: n_rot            = 32
0.00.039.285 I print_info: n_swa            = 0
0.00.039.285 I print_info: n_embd_head_k    = 128
0.00.039.285 I print_info: n_embd_head_v    = 128
0.00.039.286 I print_info: n_gqa            = 1
0.00.039.287 I print_info: n_embd_k_gqa     = 2048
0.00.039.288 I print_info: n_embd_v_gqa     = 2048
0.00.039.288 I print_info: f_norm_eps       = 1.0e-05
0.00.039.288 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.289 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.289 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.289 I print_info: f_logit_scale    = 0.0e+00
0.00.039.290 I print_info: n_ff             = 8192
0.00.039.290 I print_info: n_expert         = 0
0.00.039.290 I print_info: n_expert_used    = 0
0.00.039.290 I print_info: causal attn      = 1
0.00.039.290 I print_info: pooling type     = 0
0.00.039.290 I print_info: rope type        = 2
0.00.039.291 I print_info: rope scaling     = linear
0.00.039.291 I print_info: freq_base_train  = 10000.0
0.00.039.291 I print_info: freq_scale_train = 1
0.00.039.291 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.291 I print_info: rope_finetuned   = unknown
0.00.039.292 I print_info: ssm_d_conv       = 0
0.00.039.292 I print_info: ssm_d_inner      = 0
0.00.039.295 I print_info: ssm_d_state      = 0
0.00.039.295 I print_info: ssm_dt_rank      = 0
0.00.039.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.295 I print_info: model type       = 1.4B
0.00.039.296 I print_info: model params     = 1.41 B
0.00.039.296 I print_info: general.name     = 1.4B
0.00.039.296 I print_info: vocab type       = BPE
0.00.039.297 I print_info: n_vocab          = 50304
0.00.039.297 I print_info: n_merges         = 50009
0.00.039.297 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.298 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.298 I print_info: LF token         = 187 'Ċ'
0.00.039.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.299 I print_info: max token length = 1024
0.00.640.774 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.791 I load_tensors: offloading output layer to GPU
0.00.640.792 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.824 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.640.826 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.642.127 I llama_context: n_seq_max     = 1
0.00.642.131 I llama_context: n_ctx         = 2048
0.00.642.132 I llama_context: n_ctx_per_seq = 2048
0.00.642.132 I llama_context: n_batch       = 2048
0.00.642.133 I llama_context: n_ubatch      = 512
0.00.642.133 I llama_context: flash_attn    = 0
0.00.642.135 I llama_context: freq_base     = 10000.0
0.00.642.136 I llama_context: freq_scale    = 1
0.00.642.138 I ggml_metal_init: allocating
0.00.642.223 I ggml_metal_init: found device: Apple M4
0.00.642.237 I ggml_metal_init: picking default device: Apple M4
0.00.643.956 I ggml_metal_init: using embedded metal library
0.00.650.656 I ggml_metal_init: GPU name:   Apple M4
0.00.650.661 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.664 I ggml_metal_init: simdgroup reduction   = true
0.00.650.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.664 I ggml_metal_init: has residency sets    = true
0.00.650.665 I ggml_metal_init: has bfloat            = true
0.00.650.665 I ggml_metal_init: use bfloat            = true
0.00.650.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.297 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.530 I init:      Metal KV buffer size =   384.00 MiB
0.00.722.536 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.559 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.727.022 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.727.024 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.727.025 I llama_context: graph nodes  = 967
0.00.727.025 I llama_context: graph splits = 2
0.00.727.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.160 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.579 I main: llama threadpool init, n_threads = 4
0.00.775.619 I 
0.00.775.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.640 I 
0.00.775.740 I sampler seed: 1234
0.00.775.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.759 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.613.131 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.613.131 I llama_perf_context_print:        load time =     766.02 ms
0.01.613.132 I llama_perf_context_print: prompt eval time =      41.90 ms /     7 tokens (    5.99 ms per token,   167.07 tokens per second)
0.01.613.134 I llama_perf_context_print:        eval time =     792.52 ms /    63 runs   (   12.58 ms per token,    79.49 tokens per second)
0.01.613.134 I llama_perf_context_print:       total time =     838.48 ms /    70 tokens
0.01.617.027 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.109s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.512 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.515 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.515 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.516 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.516 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.518 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.350 I llama_model_loader: - type  f32:  194 tensors
0.00.024.351 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.351 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.351 I print_info: file format = GGUF V3 (latest)
0.00.024.352 I print_info: file type   = Q5_1
0.00.024.354 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.202 I load: special tokens cache size = 25
0.00.038.155 I load: token to piece cache size = 0.2984 MB
0.00.038.158 I print_info: arch             = gptneox
0.00.038.158 I print_info: vocab_only       = 0
0.00.038.158 I print_info: n_ctx_train      = 2048
0.00.038.158 I print_info: n_embd           = 2048
0.00.038.159 I print_info: n_layer          = 24
0.00.038.162 I print_info: n_head           = 16
0.00.038.163 I print_info: n_head_kv        = 16
0.00.038.163 I print_info: n_rot            = 32
0.00.038.163 I print_info: n_swa            = 0
0.00.038.163 I print_info: n_embd_head_k    = 128
0.00.038.163 I print_info: n_embd_head_v    = 128
0.00.038.164 I print_info: n_gqa            = 1
0.00.038.165 I print_info: n_embd_k_gqa     = 2048
0.00.038.165 I print_info: n_embd_v_gqa     = 2048
0.00.038.166 I print_info: f_norm_eps       = 1.0e-05
0.00.038.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.169 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.169 I print_info: f_logit_scale    = 0.0e+00
0.00.038.170 I print_info: n_ff             = 8192
0.00.038.170 I print_info: n_expert         = 0
0.00.038.170 I print_info: n_expert_used    = 0
0.00.038.170 I print_info: causal attn      = 1
0.00.038.170 I print_info: pooling type     = 0
0.00.038.172 I print_info: rope type        = 2
0.00.038.172 I print_info: rope scaling     = linear
0.00.038.172 I print_info: freq_base_train  = 10000.0
0.00.038.173 I print_info: freq_scale_train = 1
0.00.038.173 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.173 I print_info: rope_finetuned   = unknown
0.00.038.173 I print_info: ssm_d_conv       = 0
0.00.038.173 I print_info: ssm_d_inner      = 0
0.00.038.173 I print_info: ssm_d_state      = 0
0.00.038.174 I print_info: ssm_dt_rank      = 0
0.00.038.174 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.174 I print_info: model type       = 1.4B
0.00.038.174 I print_info: model params     = 1.41 B
0.00.038.174 I print_info: general.name     = 1.4B
0.00.038.175 I print_info: vocab type       = BPE
0.00.038.175 I print_info: n_vocab          = 50304
0.00.038.175 I print_info: n_merges         = 50009
0.00.038.176 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.178 I print_info: LF token         = 187 'Ċ'
0.00.038.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.178 I print_info: max token length = 1024
0.00.643.515 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.526 I load_tensors: offloading output layer to GPU
0.00.643.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.561 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.643.563 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.645.048 I llama_context: n_seq_max     = 1
0.00.645.051 I llama_context: n_ctx         = 128
0.00.645.051 I llama_context: n_ctx_per_seq = 128
0.00.645.052 I llama_context: n_batch       = 128
0.00.645.052 I llama_context: n_ubatch      = 128
0.00.645.052 I llama_context: flash_attn    = 0
0.00.645.053 I llama_context: freq_base     = 10000.0
0.00.645.054 I llama_context: freq_scale    = 1
0.00.645.055 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.057 I ggml_metal_init: allocating
0.00.645.069 I ggml_metal_init: found device: Apple M4
0.00.645.078 I ggml_metal_init: picking default device: Apple M4
0.00.646.364 I ggml_metal_init: using embedded metal library
0.00.652.915 I ggml_metal_init: GPU name:   Apple M4
0.00.652.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.921 I ggml_metal_init: simdgroup reduction   = true
0.00.652.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.922 I ggml_metal_init: has residency sets    = true
0.00.652.922 I ggml_metal_init: has bfloat            = true
0.00.652.922 I ggml_metal_init: use bfloat            = true
0.00.652.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.616 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.100 I init:      Metal KV buffer size =    24.00 MiB
0.00.673.103 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.134 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.676.516 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.676.518 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.676.519 I llama_context: graph nodes  = 967
0.00.676.519 I llama_context: graph splits = 2
0.00.676.522 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.477 I 
0.00.705.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.581 I perplexity: tokenizing the input ..
0.00.713.059 I perplexity: tokenization took 7.473 ms
0.00.713.067 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.875 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.863.216 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.863.246 I llama_perf_context_print:        load time =     696.73 ms
0.00.863.247 I llama_perf_context_print: prompt eval time =     147.85 ms /   128 tokens (    1.16 ms per token,   865.73 tokens per second)
0.00.863.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.863.248 I llama_perf_context_print:       total time =     157.77 ms /   129 tokens
0.00.863.817 I ggml_metal_free: deallocating

real	0m0.878s
user	0m0.078s
sys	0m0.132s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.788 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.270 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.271 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.272 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.029 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.029 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.030 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.030 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.031 I llama_model_loader: - type  f32:  194 tensors
0.00.025.031 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.031 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.032 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.032 I print_info: file format = GGUF V3 (latest)
0.00.025.033 I print_info: file type   = Q2_K - Medium
0.00.025.034 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.088 I load: special tokens cache size = 25
0.00.039.108 I load: token to piece cache size = 0.2984 MB
0.00.039.111 I print_info: arch             = gptneox
0.00.039.111 I print_info: vocab_only       = 0
0.00.039.112 I print_info: n_ctx_train      = 2048
0.00.039.112 I print_info: n_embd           = 2048
0.00.039.112 I print_info: n_layer          = 24
0.00.039.114 I print_info: n_head           = 16
0.00.039.115 I print_info: n_head_kv        = 16
0.00.039.115 I print_info: n_rot            = 32
0.00.039.116 I print_info: n_swa            = 0
0.00.039.116 I print_info: n_embd_head_k    = 128
0.00.039.118 I print_info: n_embd_head_v    = 128
0.00.039.119 I print_info: n_gqa            = 1
0.00.039.120 I print_info: n_embd_k_gqa     = 2048
0.00.039.120 I print_info: n_embd_v_gqa     = 2048
0.00.039.121 I print_info: f_norm_eps       = 1.0e-05
0.00.039.121 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.121 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.122 I print_info: f_logit_scale    = 0.0e+00
0.00.039.122 I print_info: n_ff             = 8192
0.00.039.123 I print_info: n_expert         = 0
0.00.039.125 I print_info: n_expert_used    = 0
0.00.039.125 I print_info: causal attn      = 1
0.00.039.125 I print_info: pooling type     = 0
0.00.039.125 I print_info: rope type        = 2
0.00.039.125 I print_info: rope scaling     = linear
0.00.039.126 I print_info: freq_base_train  = 10000.0
0.00.039.126 I print_info: freq_scale_train = 1
0.00.039.126 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.126 I print_info: rope_finetuned   = unknown
0.00.039.126 I print_info: ssm_d_conv       = 0
0.00.039.127 I print_info: ssm_d_inner      = 0
0.00.039.127 I print_info: ssm_d_state      = 0
0.00.039.127 I print_info: ssm_dt_rank      = 0
0.00.039.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.127 I print_info: model type       = 1.4B
0.00.039.127 I print_info: model params     = 1.41 B
0.00.039.128 I print_info: general.name     = 1.4B
0.00.039.128 I print_info: vocab type       = BPE
0.00.039.129 I print_info: n_vocab          = 50304
0.00.039.129 I print_info: n_merges         = 50009
0.00.039.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: LF token         = 187 'Ċ'
0.00.039.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: max token length = 1024
0.00.354.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.418 I load_tensors: offloading output layer to GPU
0.00.354.419 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.452 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.453 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.356.043 I llama_context: n_seq_max     = 1
0.00.356.052 I llama_context: n_ctx         = 2048
0.00.356.053 I llama_context: n_ctx_per_seq = 2048
0.00.356.053 I llama_context: n_batch       = 2048
0.00.356.053 I llama_context: n_ubatch      = 512
0.00.356.054 I llama_context: flash_attn    = 0
0.00.356.055 I llama_context: freq_base     = 10000.0
0.00.356.060 I llama_context: freq_scale    = 1
0.00.356.063 I ggml_metal_init: allocating
0.00.356.175 I ggml_metal_init: found device: Apple M4
0.00.356.188 I ggml_metal_init: picking default device: Apple M4
0.00.358.002 I ggml_metal_init: using embedded metal library
0.00.363.407 I ggml_metal_init: GPU name:   Apple M4
0.00.363.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.430 I ggml_metal_init: simdgroup reduction   = true
0.00.363.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.430 I ggml_metal_init: has residency sets    = true
0.00.363.431 I ggml_metal_init: has bfloat            = true
0.00.363.431 I ggml_metal_init: use bfloat            = true
0.00.363.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.384.817 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.839 I init:      Metal KV buffer size =   384.00 MiB
0.00.444.846 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.444.879 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.448.885 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.448.887 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.448.887 I llama_context: graph nodes  = 967
0.00.448.887 I llama_context: graph splits = 2
0.00.448.892 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.449.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.449.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.825 I main: llama threadpool init, n_threads = 4
0.00.509.869 I 
0.00.509.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.894 I 
0.00.510.049 I sampler seed: 1234
0.00.510.054 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.069 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.069 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.194.161 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.194.162 I llama_perf_context_print:        load time =     499.11 ms
0.01.194.162 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.53 tokens per second)
0.01.194.163 I llama_perf_context_print:        eval time =     637.11 ms /    63 runs   (   10.11 ms per token,    98.88 tokens per second)
0.01.194.163 I llama_perf_context_print:       total time =     685.26 ms /    70 tokens
0.01.198.005 I ggml_metal_free: deallocating

real	0m1.215s
user	0m0.111s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.588 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.470 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.471 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.471 I llama_model_loader: - type  f32:  194 tensors
0.00.026.471 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.472 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.472 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.472 I print_info: file format = GGUF V3 (latest)
0.00.026.473 I print_info: file type   = Q2_K - Medium
0.00.026.474 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.517 I load: special tokens cache size = 25
0.00.040.471 I load: token to piece cache size = 0.2984 MB
0.00.040.474 I print_info: arch             = gptneox
0.00.040.474 I print_info: vocab_only       = 0
0.00.040.474 I print_info: n_ctx_train      = 2048
0.00.040.475 I print_info: n_embd           = 2048
0.00.040.475 I print_info: n_layer          = 24
0.00.040.478 I print_info: n_head           = 16
0.00.040.479 I print_info: n_head_kv        = 16
0.00.040.479 I print_info: n_rot            = 32
0.00.040.479 I print_info: n_swa            = 0
0.00.040.479 I print_info: n_embd_head_k    = 128
0.00.040.480 I print_info: n_embd_head_v    = 128
0.00.040.480 I print_info: n_gqa            = 1
0.00.040.483 I print_info: n_embd_k_gqa     = 2048
0.00.040.484 I print_info: n_embd_v_gqa     = 2048
0.00.040.484 I print_info: f_norm_eps       = 1.0e-05
0.00.040.485 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.485 I print_info: f_logit_scale    = 0.0e+00
0.00.040.486 I print_info: n_ff             = 8192
0.00.040.486 I print_info: n_expert         = 0
0.00.040.486 I print_info: n_expert_used    = 0
0.00.040.487 I print_info: causal attn      = 1
0.00.040.487 I print_info: pooling type     = 0
0.00.040.487 I print_info: rope type        = 2
0.00.040.487 I print_info: rope scaling     = linear
0.00.040.487 I print_info: freq_base_train  = 10000.0
0.00.040.488 I print_info: freq_scale_train = 1
0.00.040.488 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.488 I print_info: rope_finetuned   = unknown
0.00.040.488 I print_info: ssm_d_conv       = 0
0.00.040.488 I print_info: ssm_d_inner      = 0
0.00.040.489 I print_info: ssm_d_state      = 0
0.00.040.489 I print_info: ssm_dt_rank      = 0
0.00.040.490 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.490 I print_info: model type       = 1.4B
0.00.040.491 I print_info: model params     = 1.41 B
0.00.040.491 I print_info: general.name     = 1.4B
0.00.040.491 I print_info: vocab type       = BPE
0.00.040.492 I print_info: n_vocab          = 50304
0.00.040.492 I print_info: n_merges         = 50009
0.00.040.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.493 I print_info: LF token         = 187 'Ċ'
0.00.040.497 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: max token length = 1024
0.00.369.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.369.809 I load_tensors: offloading output layer to GPU
0.00.369.810 I load_tensors: offloaded 25/25 layers to GPU
0.00.369.840 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.369.841 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.371.227 I llama_context: n_seq_max     = 1
0.00.371.231 I llama_context: n_ctx         = 128
0.00.371.232 I llama_context: n_ctx_per_seq = 128
0.00.371.232 I llama_context: n_batch       = 128
0.00.371.233 I llama_context: n_ubatch      = 128
0.00.371.233 I llama_context: flash_attn    = 0
0.00.371.234 I llama_context: freq_base     = 10000.0
0.00.371.234 I llama_context: freq_scale    = 1
0.00.371.235 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.371.237 I ggml_metal_init: allocating
0.00.371.288 I ggml_metal_init: found device: Apple M4
0.00.371.301 I ggml_metal_init: picking default device: Apple M4
0.00.372.959 I ggml_metal_init: using embedded metal library
0.00.378.952 I ggml_metal_init: GPU name:   Apple M4
0.00.378.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.378.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.378.962 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.378.962 I ggml_metal_init: simdgroup reduction   = true
0.00.378.963 I ggml_metal_init: simdgroup matrix mul. = true
0.00.378.963 I ggml_metal_init: has residency sets    = true
0.00.378.972 I ggml_metal_init: has bfloat            = true
0.00.378.972 I ggml_metal_init: use bfloat            = true
0.00.378.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.378.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.401.605 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.405.272 I init:      Metal KV buffer size =    24.00 MiB
0.00.405.279 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.405.317 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.408.621 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.408.623 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.408.623 I llama_context: graph nodes  = 967
0.00.408.624 I llama_context: graph splits = 2
0.00.408.627 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.408.627 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.406 I 
0.00.440.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.505 I perplexity: tokenizing the input ..
0.00.447.280 I perplexity: tokenization took 6.772 ms
0.00.447.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.589.606 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.590.942 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.590.962 I llama_perf_context_print:        load time =     429.45 ms
0.00.590.963 I llama_perf_context_print: prompt eval time =     141.39 ms /   128 tokens (    1.10 ms per token,   905.28 tokens per second)
0.00.590.964 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.590.964 I llama_perf_context_print:       total time =     150.56 ms /   129 tokens
0.00.591.557 I ggml_metal_free: deallocating

real	0m0.607s
user	0m0.081s
sys	0m0.109s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.675 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.044 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.770 I llama_model_loader: - type  f32:  194 tensors
0.00.024.771 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.771 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.771 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.772 I print_info: file format = GGUF V3 (latest)
0.00.024.773 I print_info: file type   = Q3_K - Medium
0.00.024.773 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.540 I load: special tokens cache size = 25
0.00.038.296 I load: token to piece cache size = 0.2984 MB
0.00.038.299 I print_info: arch             = gptneox
0.00.038.299 I print_info: vocab_only       = 0
0.00.038.300 I print_info: n_ctx_train      = 2048
0.00.038.300 I print_info: n_embd           = 2048
0.00.038.300 I print_info: n_layer          = 24
0.00.038.303 I print_info: n_head           = 16
0.00.038.305 I print_info: n_head_kv        = 16
0.00.038.305 I print_info: n_rot            = 32
0.00.038.306 I print_info: n_swa            = 0
0.00.038.306 I print_info: n_embd_head_k    = 128
0.00.038.306 I print_info: n_embd_head_v    = 128
0.00.038.307 I print_info: n_gqa            = 1
0.00.038.312 I print_info: n_embd_k_gqa     = 2048
0.00.038.312 I print_info: n_embd_v_gqa     = 2048
0.00.038.313 I print_info: f_norm_eps       = 1.0e-05
0.00.038.313 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.315 I print_info: f_logit_scale    = 0.0e+00
0.00.038.315 I print_info: n_ff             = 8192
0.00.038.316 I print_info: n_expert         = 0
0.00.038.316 I print_info: n_expert_used    = 0
0.00.038.316 I print_info: causal attn      = 1
0.00.038.317 I print_info: pooling type     = 0
0.00.038.317 I print_info: rope type        = 2
0.00.038.318 I print_info: rope scaling     = linear
0.00.038.318 I print_info: freq_base_train  = 10000.0
0.00.038.319 I print_info: freq_scale_train = 1
0.00.038.319 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.319 I print_info: rope_finetuned   = unknown
0.00.038.319 I print_info: ssm_d_conv       = 0
0.00.038.319 I print_info: ssm_d_inner      = 0
0.00.038.319 I print_info: ssm_d_state      = 0
0.00.038.320 I print_info: ssm_dt_rank      = 0
0.00.038.321 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.321 I print_info: model type       = 1.4B
0.00.038.321 I print_info: model params     = 1.41 B
0.00.038.322 I print_info: general.name     = 1.4B
0.00.038.322 I print_info: vocab type       = BPE
0.00.038.322 I print_info: n_vocab          = 50304
0.00.038.322 I print_info: n_merges         = 50009
0.00.038.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.323 I print_info: LF token         = 187 'Ċ'
0.00.038.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.324 I print_info: max token length = 1024
0.00.439.040 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.056 I load_tensors: offloading output layer to GPU
0.00.439.056 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.091 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.097 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.440.512 I llama_context: n_seq_max     = 1
0.00.440.516 I llama_context: n_ctx         = 2048
0.00.440.517 I llama_context: n_ctx_per_seq = 2048
0.00.440.517 I llama_context: n_batch       = 2048
0.00.440.518 I llama_context: n_ubatch      = 512
0.00.440.518 I llama_context: flash_attn    = 0
0.00.440.520 I llama_context: freq_base     = 10000.0
0.00.440.521 I llama_context: freq_scale    = 1
0.00.440.523 I ggml_metal_init: allocating
0.00.440.600 I ggml_metal_init: found device: Apple M4
0.00.440.614 I ggml_metal_init: picking default device: Apple M4
0.00.442.384 I ggml_metal_init: using embedded metal library
0.00.448.664 I ggml_metal_init: GPU name:   Apple M4
0.00.448.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.671 I ggml_metal_init: simdgroup reduction   = true
0.00.448.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.672 I ggml_metal_init: has residency sets    = true
0.00.448.672 I ggml_metal_init: has bfloat            = true
0.00.448.673 I ggml_metal_init: use bfloat            = true
0.00.448.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.957 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.852 I init:      Metal KV buffer size =   384.00 MiB
0.00.525.859 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.525.885 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.529.920 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.529.921 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.529.922 I llama_context: graph nodes  = 967
0.00.529.922 I llama_context: graph splits = 2
0.00.529.927 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.530.051 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.530.051 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.307 I main: llama threadpool init, n_threads = 4
0.00.584.351 I 
0.00.584.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.377 I 
0.00.584.529 I sampler seed: 1234
0.00.584.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.549 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.549 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.549 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.325.653 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.325.654 I llama_perf_context_print:        load time =     574.67 ms
0.01.325.655 I llama_perf_context_print: prompt eval time =      40.11 ms /     7 tokens (    5.73 ms per token,   174.53 tokens per second)
0.01.325.655 I llama_perf_context_print:        eval time =     698.17 ms /    63 runs   (   11.08 ms per token,    90.24 tokens per second)
0.01.325.656 I llama_perf_context_print:       total time =     742.30 ms /    70 tokens
0.01.329.570 I ggml_metal_free: deallocating

real	0m1.345s
user	0m0.108s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.249 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.223 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.225 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.225 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.981 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.982 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.982 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.983 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.984 I llama_model_loader: - type  f32:  194 tensors
0.00.024.984 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.984 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.984 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.984 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.985 I print_info: file format = GGUF V3 (latest)
0.00.024.985 I print_info: file type   = Q3_K - Medium
0.00.024.986 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.051 I load: special tokens cache size = 25
0.00.039.032 I load: token to piece cache size = 0.2984 MB
0.00.039.035 I print_info: arch             = gptneox
0.00.039.035 I print_info: vocab_only       = 0
0.00.039.035 I print_info: n_ctx_train      = 2048
0.00.039.035 I print_info: n_embd           = 2048
0.00.039.036 I print_info: n_layer          = 24
0.00.039.038 I print_info: n_head           = 16
0.00.039.039 I print_info: n_head_kv        = 16
0.00.039.039 I print_info: n_rot            = 32
0.00.039.039 I print_info: n_swa            = 0
0.00.039.041 I print_info: n_embd_head_k    = 128
0.00.039.041 I print_info: n_embd_head_v    = 128
0.00.039.042 I print_info: n_gqa            = 1
0.00.039.043 I print_info: n_embd_k_gqa     = 2048
0.00.039.043 I print_info: n_embd_v_gqa     = 2048
0.00.039.044 I print_info: f_norm_eps       = 1.0e-05
0.00.039.044 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.045 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.045 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.045 I print_info: f_logit_scale    = 0.0e+00
0.00.039.046 I print_info: n_ff             = 8192
0.00.039.046 I print_info: n_expert         = 0
0.00.039.046 I print_info: n_expert_used    = 0
0.00.039.046 I print_info: causal attn      = 1
0.00.039.046 I print_info: pooling type     = 0
0.00.039.046 I print_info: rope type        = 2
0.00.039.047 I print_info: rope scaling     = linear
0.00.039.047 I print_info: freq_base_train  = 10000.0
0.00.039.047 I print_info: freq_scale_train = 1
0.00.039.047 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.047 I print_info: rope_finetuned   = unknown
0.00.039.048 I print_info: ssm_d_conv       = 0
0.00.039.048 I print_info: ssm_d_inner      = 0
0.00.039.048 I print_info: ssm_d_state      = 0
0.00.039.048 I print_info: ssm_dt_rank      = 0
0.00.039.048 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.048 I print_info: model type       = 1.4B
0.00.039.049 I print_info: model params     = 1.41 B
0.00.039.049 I print_info: general.name     = 1.4B
0.00.039.049 I print_info: vocab type       = BPE
0.00.039.050 I print_info: n_vocab          = 50304
0.00.039.050 I print_info: n_merges         = 50009
0.00.039.050 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: LF token         = 187 'Ċ'
0.00.039.051 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.051 I print_info: max token length = 1024
0.00.437.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.216 I load_tensors: offloading output layer to GPU
0.00.437.217 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.251 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.253 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.438.786 I llama_context: n_seq_max     = 1
0.00.438.797 I llama_context: n_ctx         = 128
0.00.438.797 I llama_context: n_ctx_per_seq = 128
0.00.438.798 I llama_context: n_batch       = 128
0.00.438.798 I llama_context: n_ubatch      = 128
0.00.438.799 I llama_context: flash_attn    = 0
0.00.438.801 I llama_context: freq_base     = 10000.0
0.00.438.801 I llama_context: freq_scale    = 1
0.00.438.802 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.438.808 I ggml_metal_init: allocating
0.00.438.936 I ggml_metal_init: found device: Apple M4
0.00.438.950 I ggml_metal_init: picking default device: Apple M4
0.00.440.806 I ggml_metal_init: using embedded metal library
0.00.446.371 I ggml_metal_init: GPU name:   Apple M4
0.00.446.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.377 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.378 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.379 I ggml_metal_init: simdgroup reduction   = true
0.00.446.379 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.380 I ggml_metal_init: has residency sets    = true
0.00.446.380 I ggml_metal_init: has bfloat            = true
0.00.446.380 I ggml_metal_init: use bfloat            = true
0.00.446.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.495 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.059 I init:      Metal KV buffer size =    24.00 MiB
0.00.469.063 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.092 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.472.405 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.472.407 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.472.407 I llama_context: graph nodes  = 967
0.00.472.408 I llama_context: graph splits = 2
0.00.472.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.654 I 
0.00.500.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.740 I perplexity: tokenizing the input ..
0.00.507.935 I perplexity: tokenization took 7.191 ms
0.00.507.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.377 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.703 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.728 I llama_perf_context_print:        load time =     491.39 ms
0.00.655.729 I llama_perf_context_print: prompt eval time =     145.47 ms /   128 tokens (    1.14 ms per token,   879.92 tokens per second)
0.00.655.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.730 I llama_perf_context_print:       total time =     155.08 ms /   129 tokens
0.00.656.299 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.079s
sys	0m0.110s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.579 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.218 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.199 I llama_model_loader: - type  f32:  194 tensors
0.00.025.200 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.200 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.200 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.201 I print_info: file format = GGUF V3 (latest)
0.00.025.201 I print_info: file type   = Q4_K - Medium
0.00.025.206 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.362 I load: special tokens cache size = 25
0.00.039.423 I load: token to piece cache size = 0.2984 MB
0.00.039.426 I print_info: arch             = gptneox
0.00.039.426 I print_info: vocab_only       = 0
0.00.039.426 I print_info: n_ctx_train      = 2048
0.00.039.426 I print_info: n_embd           = 2048
0.00.039.427 I print_info: n_layer          = 24
0.00.039.429 I print_info: n_head           = 16
0.00.039.430 I print_info: n_head_kv        = 16
0.00.039.430 I print_info: n_rot            = 32
0.00.039.431 I print_info: n_swa            = 0
0.00.039.431 I print_info: n_embd_head_k    = 128
0.00.039.431 I print_info: n_embd_head_v    = 128
0.00.039.432 I print_info: n_gqa            = 1
0.00.039.432 I print_info: n_embd_k_gqa     = 2048
0.00.039.433 I print_info: n_embd_v_gqa     = 2048
0.00.039.434 I print_info: f_norm_eps       = 1.0e-05
0.00.039.434 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.434 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.434 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.434 I print_info: f_logit_scale    = 0.0e+00
0.00.039.435 I print_info: n_ff             = 8192
0.00.039.435 I print_info: n_expert         = 0
0.00.039.435 I print_info: n_expert_used    = 0
0.00.039.436 I print_info: causal attn      = 1
0.00.039.436 I print_info: pooling type     = 0
0.00.039.438 I print_info: rope type        = 2
0.00.039.438 I print_info: rope scaling     = linear
0.00.039.438 I print_info: freq_base_train  = 10000.0
0.00.039.440 I print_info: freq_scale_train = 1
0.00.039.440 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.441 I print_info: rope_finetuned   = unknown
0.00.039.441 I print_info: ssm_d_conv       = 0
0.00.039.441 I print_info: ssm_d_inner      = 0
0.00.039.441 I print_info: ssm_d_state      = 0
0.00.039.441 I print_info: ssm_dt_rank      = 0
0.00.039.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.441 I print_info: model type       = 1.4B
0.00.039.442 I print_info: model params     = 1.41 B
0.00.039.442 I print_info: general.name     = 1.4B
0.00.039.442 I print_info: vocab type       = BPE
0.00.039.443 I print_info: n_vocab          = 50304
0.00.039.443 I print_info: n_merges         = 50009
0.00.039.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.447 I print_info: LF token         = 187 'Ċ'
0.00.039.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: max token length = 1024
0.00.517.063 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.080 I load_tensors: offloading output layer to GPU
0.00.517.081 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.114 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.116 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.518.461 I llama_context: n_seq_max     = 1
0.00.518.465 I llama_context: n_ctx         = 2048
0.00.518.466 I llama_context: n_ctx_per_seq = 2048
0.00.518.466 I llama_context: n_batch       = 2048
0.00.518.466 I llama_context: n_ubatch      = 512
0.00.518.467 I llama_context: flash_attn    = 0
0.00.518.469 I llama_context: freq_base     = 10000.0
0.00.518.470 I llama_context: freq_scale    = 1
0.00.518.472 I ggml_metal_init: allocating
0.00.518.547 I ggml_metal_init: found device: Apple M4
0.00.518.561 I ggml_metal_init: picking default device: Apple M4
0.00.520.384 I ggml_metal_init: using embedded metal library
0.00.527.088 I ggml_metal_init: GPU name:   Apple M4
0.00.527.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.095 I ggml_metal_init: simdgroup reduction   = true
0.00.527.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.096 I ggml_metal_init: has residency sets    = true
0.00.527.097 I ggml_metal_init: has bfloat            = true
0.00.527.097 I ggml_metal_init: use bfloat            = true
0.00.527.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.398 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.317 I init:      Metal KV buffer size =   384.00 MiB
0.00.608.324 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.347 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.612.710 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.612.711 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.612.712 I llama_context: graph nodes  = 967
0.00.612.712 I llama_context: graph splits = 2
0.00.612.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.839 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.134 I main: llama threadpool init, n_threads = 4
0.00.670.177 I 
0.00.670.202 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.202 I 
0.00.670.354 I sampler seed: 1234
0.00.670.359 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.374 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.377 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.377 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.439.459 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.439.460 I llama_perf_context_print:        load time =     660.59 ms
0.01.439.460 I llama_perf_context_print: prompt eval time =      56.55 ms /     7 tokens (    8.08 ms per token,   123.78 tokens per second)
0.01.439.461 I llama_perf_context_print:        eval time =     709.46 ms /    63 runs   (   11.26 ms per token,    88.80 tokens per second)
0.01.439.462 I llama_perf_context_print:       total time =     770.28 ms /    70 tokens
0.01.443.523 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.110s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.685 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.690 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.620 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.502 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.503 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.504 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.504 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.505 I llama_model_loader: - type  f32:  194 tensors
0.00.024.506 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.506 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.506 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.507 I print_info: file format = GGUF V3 (latest)
0.00.024.507 I print_info: file type   = Q4_K - Medium
0.00.024.508 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.214 I load: special tokens cache size = 25
0.00.038.265 I load: token to piece cache size = 0.2984 MB
0.00.038.268 I print_info: arch             = gptneox
0.00.038.269 I print_info: vocab_only       = 0
0.00.038.269 I print_info: n_ctx_train      = 2048
0.00.038.269 I print_info: n_embd           = 2048
0.00.038.269 I print_info: n_layer          = 24
0.00.038.272 I print_info: n_head           = 16
0.00.038.273 I print_info: n_head_kv        = 16
0.00.038.273 I print_info: n_rot            = 32
0.00.038.273 I print_info: n_swa            = 0
0.00.038.274 I print_info: n_embd_head_k    = 128
0.00.038.274 I print_info: n_embd_head_v    = 128
0.00.038.274 I print_info: n_gqa            = 1
0.00.038.275 I print_info: n_embd_k_gqa     = 2048
0.00.038.276 I print_info: n_embd_v_gqa     = 2048
0.00.038.276 I print_info: f_norm_eps       = 1.0e-05
0.00.038.277 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.279 I print_info: f_logit_scale    = 0.0e+00
0.00.038.280 I print_info: n_ff             = 8192
0.00.038.280 I print_info: n_expert         = 0
0.00.038.280 I print_info: n_expert_used    = 0
0.00.038.280 I print_info: causal attn      = 1
0.00.038.281 I print_info: pooling type     = 0
0.00.038.281 I print_info: rope type        = 2
0.00.038.281 I print_info: rope scaling     = linear
0.00.038.281 I print_info: freq_base_train  = 10000.0
0.00.038.282 I print_info: freq_scale_train = 1
0.00.038.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.282 I print_info: rope_finetuned   = unknown
0.00.038.282 I print_info: ssm_d_conv       = 0
0.00.038.282 I print_info: ssm_d_inner      = 0
0.00.038.283 I print_info: ssm_d_state      = 0
0.00.038.283 I print_info: ssm_dt_rank      = 0
0.00.038.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.283 I print_info: model type       = 1.4B
0.00.038.283 I print_info: model params     = 1.41 B
0.00.038.284 I print_info: general.name     = 1.4B
0.00.038.284 I print_info: vocab type       = BPE
0.00.038.286 I print_info: n_vocab          = 50304
0.00.038.286 I print_info: n_merges         = 50009
0.00.038.287 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.288 I print_info: LF token         = 187 'Ċ'
0.00.038.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.289 I print_info: max token length = 1024
0.00.510.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.510.949 I load_tensors: offloading output layer to GPU
0.00.510.949 I load_tensors: offloaded 25/25 layers to GPU
0.00.510.983 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.510.988 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.512.560 I llama_context: n_seq_max     = 1
0.00.512.564 I llama_context: n_ctx         = 128
0.00.512.565 I llama_context: n_ctx_per_seq = 128
0.00.512.565 I llama_context: n_batch       = 128
0.00.512.566 I llama_context: n_ubatch      = 128
0.00.512.566 I llama_context: flash_attn    = 0
0.00.512.569 I llama_context: freq_base     = 10000.0
0.00.512.569 I llama_context: freq_scale    = 1
0.00.512.570 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.512.572 I ggml_metal_init: allocating
0.00.512.643 I ggml_metal_init: found device: Apple M4
0.00.512.657 I ggml_metal_init: picking default device: Apple M4
0.00.514.342 I ggml_metal_init: using embedded metal library
0.00.521.028 I ggml_metal_init: GPU name:   Apple M4
0.00.521.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.036 I ggml_metal_init: simdgroup reduction   = true
0.00.521.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.036 I ggml_metal_init: has residency sets    = true
0.00.521.036 I ggml_metal_init: has bfloat            = true
0.00.521.036 I ggml_metal_init: use bfloat            = true
0.00.521.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.799 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.309 I init:      Metal KV buffer size =    24.00 MiB
0.00.542.313 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.542.340 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.545.789 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.545.791 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.545.792 I llama_context: graph nodes  = 967
0.00.545.792 I llama_context: graph splits = 2
0.00.545.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.545.795 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.602 I 
0.00.576.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.708 I perplexity: tokenizing the input ..
0.00.583.765 I perplexity: tokenization took 7.053 ms
0.00.583.771 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.055 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.730.585 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.730.614 I llama_perf_context_print:        load time =     567.93 ms
0.00.730.615 I llama_perf_context_print: prompt eval time =     144.39 ms /   128 tokens (    1.13 ms per token,   886.49 tokens per second)
0.00.730.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.730.616 I llama_perf_context_print:       total time =     154.02 ms /   129 tokens
0.00.731.139 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.078s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.577 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.583 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.478 I llama_model_loader: - type  f32:  194 tensors
0.00.025.478 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.479 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.479 I print_info: file format = GGUF V3 (latest)
0.00.025.480 I print_info: file type   = Q5_K - Medium
0.00.025.481 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.184 I load: special tokens cache size = 25
0.00.039.163 I load: token to piece cache size = 0.2984 MB
0.00.039.165 I print_info: arch             = gptneox
0.00.039.165 I print_info: vocab_only       = 0
0.00.039.166 I print_info: n_ctx_train      = 2048
0.00.039.166 I print_info: n_embd           = 2048
0.00.039.166 I print_info: n_layer          = 24
0.00.039.168 I print_info: n_head           = 16
0.00.039.169 I print_info: n_head_kv        = 16
0.00.039.169 I print_info: n_rot            = 32
0.00.039.169 I print_info: n_swa            = 0
0.00.039.169 I print_info: n_embd_head_k    = 128
0.00.039.170 I print_info: n_embd_head_v    = 128
0.00.039.170 I print_info: n_gqa            = 1
0.00.039.171 I print_info: n_embd_k_gqa     = 2048
0.00.039.172 I print_info: n_embd_v_gqa     = 2048
0.00.039.172 I print_info: f_norm_eps       = 1.0e-05
0.00.039.173 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.173 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.173 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.173 I print_info: f_logit_scale    = 0.0e+00
0.00.039.174 I print_info: n_ff             = 8192
0.00.039.174 I print_info: n_expert         = 0
0.00.039.174 I print_info: n_expert_used    = 0
0.00.039.174 I print_info: causal attn      = 1
0.00.039.174 I print_info: pooling type     = 0
0.00.039.175 I print_info: rope type        = 2
0.00.039.175 I print_info: rope scaling     = linear
0.00.039.175 I print_info: freq_base_train  = 10000.0
0.00.039.175 I print_info: freq_scale_train = 1
0.00.039.176 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.176 I print_info: rope_finetuned   = unknown
0.00.039.176 I print_info: ssm_d_conv       = 0
0.00.039.176 I print_info: ssm_d_inner      = 0
0.00.039.176 I print_info: ssm_d_state      = 0
0.00.039.176 I print_info: ssm_dt_rank      = 0
0.00.039.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.177 I print_info: model type       = 1.4B
0.00.039.177 I print_info: model params     = 1.41 B
0.00.039.177 I print_info: general.name     = 1.4B
0.00.039.178 I print_info: vocab type       = BPE
0.00.039.178 I print_info: n_vocab          = 50304
0.00.039.178 I print_info: n_merges         = 50009
0.00.039.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.179 I print_info: LF token         = 187 'Ċ'
0.00.039.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.180 I print_info: max token length = 1024
0.00.596.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.393 I load_tensors: offloading output layer to GPU
0.00.596.394 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.431 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.432 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.597.926 I llama_context: n_seq_max     = 1
0.00.597.931 I llama_context: n_ctx         = 2048
0.00.597.931 I llama_context: n_ctx_per_seq = 2048
0.00.597.932 I llama_context: n_batch       = 2048
0.00.597.932 I llama_context: n_ubatch      = 512
0.00.597.932 I llama_context: flash_attn    = 0
0.00.597.934 I llama_context: freq_base     = 10000.0
0.00.597.935 I llama_context: freq_scale    = 1
0.00.597.937 I ggml_metal_init: allocating
0.00.598.037 I ggml_metal_init: found device: Apple M4
0.00.598.055 I ggml_metal_init: picking default device: Apple M4
0.00.599.955 I ggml_metal_init: using embedded metal library
0.00.607.220 I ggml_metal_init: GPU name:   Apple M4
0.00.607.224 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.227 I ggml_metal_init: simdgroup reduction   = true
0.00.607.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.228 I ggml_metal_init: has residency sets    = true
0.00.607.228 I ggml_metal_init: has bfloat            = true
0.00.607.228 I ggml_metal_init: use bfloat            = true
0.00.607.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.215 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.126 I init:      Metal KV buffer size =   384.00 MiB
0.00.692.137 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.161 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.696.386 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.696.388 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.696.388 I llama_context: graph nodes  = 967
0.00.696.388 I llama_context: graph splits = 2
0.00.696.395 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.170 I main: llama threadpool init, n_threads = 4
0.00.761.216 I 
0.00.761.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.242 I 
0.00.761.416 I sampler seed: 1234
0.00.761.421 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.468 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.471 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.471 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.608.243 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.608.243 I llama_perf_context_print:        load time =     750.19 ms
0.01.608.244 I llama_perf_context_print: prompt eval time =      51.48 ms /     7 tokens (    7.35 ms per token,   135.97 tokens per second)
0.01.608.247 I llama_perf_context_print:        eval time =     792.31 ms /    63 runs   (   12.58 ms per token,    79.51 tokens per second)
0.01.608.248 I llama_perf_context_print:       total time =     848.02 ms /    70 tokens
0.01.612.131 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.113s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.370 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.381 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.381 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.381 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.382 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.382 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.387 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.387 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.201 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.202 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.203 I llama_model_loader: - type  f32:  194 tensors
0.00.025.203 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.203 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.204 I print_info: file format = GGUF V3 (latest)
0.00.025.205 I print_info: file type   = Q5_K - Medium
0.00.025.206 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.961 I load: special tokens cache size = 25
0.00.038.834 I load: token to piece cache size = 0.2984 MB
0.00.038.837 I print_info: arch             = gptneox
0.00.038.837 I print_info: vocab_only       = 0
0.00.038.837 I print_info: n_ctx_train      = 2048
0.00.038.838 I print_info: n_embd           = 2048
0.00.038.838 I print_info: n_layer          = 24
0.00.038.841 I print_info: n_head           = 16
0.00.038.842 I print_info: n_head_kv        = 16
0.00.038.842 I print_info: n_rot            = 32
0.00.038.842 I print_info: n_swa            = 0
0.00.038.842 I print_info: n_embd_head_k    = 128
0.00.038.842 I print_info: n_embd_head_v    = 128
0.00.038.846 I print_info: n_gqa            = 1
0.00.038.846 I print_info: n_embd_k_gqa     = 2048
0.00.038.847 I print_info: n_embd_v_gqa     = 2048
0.00.038.848 I print_info: f_norm_eps       = 1.0e-05
0.00.038.848 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.848 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.849 I print_info: f_logit_scale    = 0.0e+00
0.00.038.849 I print_info: n_ff             = 8192
0.00.038.849 I print_info: n_expert         = 0
0.00.038.850 I print_info: n_expert_used    = 0
0.00.038.850 I print_info: causal attn      = 1
0.00.038.850 I print_info: pooling type     = 0
0.00.038.850 I print_info: rope type        = 2
0.00.038.850 I print_info: rope scaling     = linear
0.00.038.852 I print_info: freq_base_train  = 10000.0
0.00.038.852 I print_info: freq_scale_train = 1
0.00.038.852 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.853 I print_info: rope_finetuned   = unknown
0.00.038.853 I print_info: ssm_d_conv       = 0
0.00.038.854 I print_info: ssm_d_inner      = 0
0.00.038.854 I print_info: ssm_d_state      = 0
0.00.038.854 I print_info: ssm_dt_rank      = 0
0.00.038.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.855 I print_info: model type       = 1.4B
0.00.038.855 I print_info: model params     = 1.41 B
0.00.038.855 I print_info: general.name     = 1.4B
0.00.038.856 I print_info: vocab type       = BPE
0.00.038.856 I print_info: n_vocab          = 50304
0.00.038.856 I print_info: n_merges         = 50009
0.00.038.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.859 I print_info: LF token         = 187 'Ċ'
0.00.038.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.859 I print_info: max token length = 1024
0.00.621.097 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.109 I load_tensors: offloading output layer to GPU
0.00.621.110 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.139 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.621.141 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.622.607 I llama_context: n_seq_max     = 1
0.00.622.613 I llama_context: n_ctx         = 128
0.00.622.614 I llama_context: n_ctx_per_seq = 128
0.00.622.614 I llama_context: n_batch       = 128
0.00.622.615 I llama_context: n_ubatch      = 128
0.00.622.616 I llama_context: flash_attn    = 0
0.00.622.619 I llama_context: freq_base     = 10000.0
0.00.622.620 I llama_context: freq_scale    = 1
0.00.622.622 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.625 I ggml_metal_init: allocating
0.00.622.641 I ggml_metal_init: found device: Apple M4
0.00.622.651 I ggml_metal_init: picking default device: Apple M4
0.00.624.061 I ggml_metal_init: using embedded metal library
0.00.630.304 I ggml_metal_init: GPU name:   Apple M4
0.00.630.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.310 I ggml_metal_init: simdgroup reduction   = true
0.00.630.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.310 I ggml_metal_init: has residency sets    = true
0.00.630.310 I ggml_metal_init: has bfloat            = true
0.00.630.311 I ggml_metal_init: use bfloat            = true
0.00.630.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.920 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.651.510 I init:      Metal KV buffer size =    24.00 MiB
0.00.651.517 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.651.565 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.654.808 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.654.809 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.654.810 I llama_context: graph nodes  = 967
0.00.654.810 I llama_context: graph splits = 2
0.00.654.814 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.654.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.247 I 
0.00.689.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.345 I perplexity: tokenizing the input ..
0.00.696.239 I perplexity: tokenization took 6.89 ms
0.00.696.245 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.270 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.839.653 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.839.681 I llama_perf_context_print:        load time =     679.41 ms
0.00.839.682 I llama_perf_context_print: prompt eval time =     141.06 ms /   128 tokens (    1.10 ms per token,   907.42 tokens per second)
0.00.839.683 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.839.683 I llama_perf_context_print:       total time =     150.44 ms /   129 tokens
0.00.840.225 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.078s
sys	0m0.158s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.146 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.715 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.716 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.716 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.717 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.717 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.718 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.718 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.720 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.721 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.626 I llama_model_loader: - type  f32:  194 tensors
0.00.025.626 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.627 I print_info: file format = GGUF V3 (latest)
0.00.025.628 I print_info: file type   = Q6_K
0.00.025.628 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.463 I load: special tokens cache size = 25
0.00.039.468 I load: token to piece cache size = 0.2984 MB
0.00.039.470 I print_info: arch             = gptneox
0.00.039.471 I print_info: vocab_only       = 0
0.00.039.471 I print_info: n_ctx_train      = 2048
0.00.039.471 I print_info: n_embd           = 2048
0.00.039.471 I print_info: n_layer          = 24
0.00.039.474 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.475 I print_info: n_swa            = 0
0.00.039.475 I print_info: n_embd_head_k    = 128
0.00.039.475 I print_info: n_embd_head_v    = 128
0.00.039.477 I print_info: n_gqa            = 1
0.00.039.478 I print_info: n_embd_k_gqa     = 2048
0.00.039.479 I print_info: n_embd_v_gqa     = 2048
0.00.039.479 I print_info: f_norm_eps       = 1.0e-05
0.00.039.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.480 I print_info: f_logit_scale    = 0.0e+00
0.00.039.481 I print_info: n_ff             = 8192
0.00.039.481 I print_info: n_expert         = 0
0.00.039.482 I print_info: n_expert_used    = 0
0.00.039.482 I print_info: causal attn      = 1
0.00.039.482 I print_info: pooling type     = 0
0.00.039.482 I print_info: rope type        = 2
0.00.039.482 I print_info: rope scaling     = linear
0.00.039.483 I print_info: freq_base_train  = 10000.0
0.00.039.483 I print_info: freq_scale_train = 1
0.00.039.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.483 I print_info: rope_finetuned   = unknown
0.00.039.483 I print_info: ssm_d_conv       = 0
0.00.039.484 I print_info: ssm_d_inner      = 0
0.00.039.486 I print_info: ssm_d_state      = 0
0.00.039.486 I print_info: ssm_dt_rank      = 0
0.00.039.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.486 I print_info: model type       = 1.4B
0.00.039.486 I print_info: model params     = 1.41 B
0.00.039.487 I print_info: general.name     = 1.4B
0.00.039.487 I print_info: vocab type       = BPE
0.00.039.487 I print_info: n_vocab          = 50304
0.00.039.487 I print_info: n_merges         = 50009
0.00.039.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.488 I print_info: LF token         = 187 'Ċ'
0.00.039.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.489 I print_info: max token length = 1024
0.00.643.344 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.348 I load_tensors: offloading output layer to GPU
0.00.643.348 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.372 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.643.376 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.644.548 I llama_context: n_seq_max     = 1
0.00.644.551 I llama_context: n_ctx         = 2048
0.00.644.551 I llama_context: n_ctx_per_seq = 2048
0.00.644.551 I llama_context: n_batch       = 2048
0.00.644.552 I llama_context: n_ubatch      = 512
0.00.644.553 I llama_context: flash_attn    = 0
0.00.644.553 I llama_context: freq_base     = 10000.0
0.00.644.554 I llama_context: freq_scale    = 1
0.00.644.555 I ggml_metal_init: allocating
0.00.644.608 I ggml_metal_init: found device: Apple M4
0.00.644.621 I ggml_metal_init: picking default device: Apple M4
0.00.646.017 I ggml_metal_init: using embedded metal library
0.00.652.139 I ggml_metal_init: GPU name:   Apple M4
0.00.652.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.145 I ggml_metal_init: simdgroup reduction   = true
0.00.652.146 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.146 I ggml_metal_init: has residency sets    = true
0.00.652.146 I ggml_metal_init: has bfloat            = true
0.00.652.147 I ggml_metal_init: use bfloat            = true
0.00.652.147 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.701 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.094 I init:      Metal KV buffer size =   384.00 MiB
0.00.725.102 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.143 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.729.699 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.729.701 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.729.702 I llama_context: graph nodes  = 967
0.00.729.702 I llama_context: graph splits = 2
0.00.729.707 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.741 I main: llama threadpool init, n_threads = 4
0.00.799.784 I 
0.00.799.807 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.808 I 
0.00.799.974 I sampler seed: 1234
0.00.799.978 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.994 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.994 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.994 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.679.662 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.679.663 I llama_perf_context_print:        load time =     788.68 ms
0.01.679.663 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.679.664 I llama_perf_context_print:        eval time =     822.22 ms /    63 runs   (   13.05 ms per token,    76.62 tokens per second)
0.01.679.664 I llama_perf_context_print:       total time =     880.83 ms /    70 tokens
0.01.683.596 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4628 (3e23be79) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.383 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.389 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.354 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.269 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.270 I print_info: file format = GGUF V3 (latest)
0.00.024.271 I print_info: file type   = Q6_K
0.00.024.272 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.100 I load: special tokens cache size = 25
0.00.038.154 I load: token to piece cache size = 0.2984 MB
0.00.038.166 I print_info: arch             = gptneox
0.00.038.167 I print_info: vocab_only       = 0
0.00.038.167 I print_info: n_ctx_train      = 2048
0.00.038.168 I print_info: n_embd           = 2048
0.00.038.168 I print_info: n_layer          = 24
0.00.038.174 I print_info: n_head           = 16
0.00.038.174 I print_info: n_head_kv        = 16
0.00.038.175 I print_info: n_rot            = 32
0.00.038.175 I print_info: n_swa            = 0
0.00.038.175 I print_info: n_embd_head_k    = 128
0.00.038.175 I print_info: n_embd_head_v    = 128
0.00.038.176 I print_info: n_gqa            = 1
0.00.038.177 I print_info: n_embd_k_gqa     = 2048
0.00.038.178 I print_info: n_embd_v_gqa     = 2048
0.00.038.178 I print_info: f_norm_eps       = 1.0e-05
0.00.038.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.180 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.181 I print_info: f_logit_scale    = 0.0e+00
0.00.038.181 I print_info: n_ff             = 8192
0.00.038.181 I print_info: n_expert         = 0
0.00.038.182 I print_info: n_expert_used    = 0
0.00.038.182 I print_info: causal attn      = 1
0.00.038.182 I print_info: pooling type     = 0
0.00.038.182 I print_info: rope type        = 2
0.00.038.182 I print_info: rope scaling     = linear
0.00.038.182 I print_info: freq_base_train  = 10000.0
0.00.038.183 I print_info: freq_scale_train = 1
0.00.038.184 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.184 I print_info: rope_finetuned   = unknown
0.00.038.184 I print_info: ssm_d_conv       = 0
0.00.038.184 I print_info: ssm_d_inner      = 0
0.00.038.185 I print_info: ssm_d_state      = 0
0.00.038.185 I print_info: ssm_dt_rank      = 0
0.00.038.185 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.185 I print_info: model type       = 1.4B
0.00.038.185 I print_info: model params     = 1.41 B
0.00.038.187 I print_info: general.name     = 1.4B
0.00.038.187 I print_info: vocab type       = BPE
0.00.038.187 I print_info: n_vocab          = 50304
0.00.038.188 I print_info: n_merges         = 50009
0.00.038.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: LF token         = 187 'Ċ'
0.00.038.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.190 I print_info: max token length = 1024
0.00.374.260 I load_tensors: offloading 24 repeating layers to GPU
0.00.374.266 I load_tensors: offloading output layer to GPU
0.00.374.267 I load_tensors: offloaded 25/25 layers to GPU
0.00.374.291 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.374.292 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.375.499 I llama_context: n_seq_max     = 1
0.00.375.502 I llama_context: n_ctx         = 128
0.00.375.503 I llama_context: n_ctx_per_seq = 128
0.00.375.503 I llama_context: n_batch       = 128
0.00.375.503 I llama_context: n_ubatch      = 128
0.00.375.504 I llama_context: flash_attn    = 0
0.00.375.505 I llama_context: freq_base     = 10000.0
0.00.375.505 I llama_context: freq_scale    = 1
0.00.375.506 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.375.507 I ggml_metal_init: allocating
0.00.375.536 I ggml_metal_init: found device: Apple M4
0.00.375.545 I ggml_metal_init: picking default device: Apple M4
0.00.376.909 I ggml_metal_init: using embedded metal library
0.00.382.659 I ggml_metal_init: GPU name:   Apple M4
0.00.382.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.382.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.382.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.382.664 I ggml_metal_init: simdgroup reduction   = true
0.00.382.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.382.664 I ggml_metal_init: has residency sets    = true
0.00.382.665 I ggml_metal_init: has bfloat            = true
0.00.382.665 I ggml_metal_init: use bfloat            = true
0.00.382.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.382.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.399.116 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.402.701 I init:      Metal KV buffer size =    24.00 MiB
0.00.402.705 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.402.733 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.405.988 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.405.989 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.405.990 I llama_context: graph nodes  = 967
0.00.405.990 I llama_context: graph splits = 2
0.00.405.993 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.405.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.376 I 
0.00.437.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.478 I perplexity: tokenizing the input ..
0.00.444.774 I perplexity: tokenization took 7.294 ms
0.00.444.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.586.185 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.587.536 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.587.555 I llama_perf_context_print:        load time =     428.47 ms
0.00.587.556 I llama_perf_context_print: prompt eval time =     140.41 ms /   128 tokens (    1.10 ms per token,   911.60 tokens per second)
0.00.587.556 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.587.557 I llama_perf_context_print:       total time =     150.18 ms /   129 tokens
0.00.588.108 I ggml_metal_free: deallocating

real	0m0.602s
user	0m0.077s
sys	0m0.106s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4628 (3e23be79)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1483067e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148306e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148309f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14830a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14830aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14830af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14830b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14830ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14830bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14830bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14830c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14830c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14830d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14830dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14830e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14830eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14830f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14830f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148310100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1483108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148311710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148311e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1483126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148312df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1483130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1483136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148314330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148314870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148314b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148314fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148315290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148315b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148316060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148316320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1483167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148316c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148317100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1483175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148317a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148317ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148318380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148318820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148318cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148318f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148319590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148319ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14831a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14831aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14831b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14831b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14831bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14831c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14831c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14831d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14831d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14831da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14831dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14831e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14831eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14831edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14831f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14831f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14831fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148320050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1483204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148320990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148320e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1483212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148321770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148321c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1483220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148322550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148322aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148322ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148323540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148323a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148323fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148324530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148324a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148324fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148325520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148325a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148325fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148326510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148326a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148326fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148327500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148327a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148327fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1483284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148328a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148328f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1483294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148329a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148329f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14832a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14831a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14832a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14832b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14832b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14832bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14832c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14832c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14832cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14832d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14832d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14832db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14832e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14832e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14832eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14832f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14832f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14832faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14832ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1483303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148330880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148330d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1483311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148331660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148331b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148331fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148332440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1483328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148332d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148333220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1483336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148333b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148334000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1483344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148334940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148334de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148335280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148335720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148335bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148336060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148336500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1483369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148336e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1483372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148337780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148337c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1483380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148338560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148338a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148338ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148339340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1483397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148339c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14833a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14833a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14833aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14833af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14833b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14833b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14833bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14833c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14833c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14833cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14833cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14833d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14833d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14833dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14833e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14833e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14833eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14833efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14833f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14833f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14833fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148340240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1483406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148340b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148341020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1483414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148341960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148341e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1483422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148342740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148342be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148343080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148343520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1483439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148343e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148344300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1483447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148344c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1483450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148345580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148345a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148345ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148346360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148346800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148346d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1483472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1483477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148347d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148348000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148348610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148348c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148349230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148349a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148349ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14834a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14834a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14834ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14834b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14834ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14834bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14834c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14834cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14834d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14834d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14834db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14834e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14834e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14834eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14834f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14834f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14834faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148350040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148350590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148350ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148351030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148351580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148351ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148352020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148352570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148352ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148353010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148353560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148353ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148354000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148354550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148354aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148354ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148355540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148355a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148355fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148356530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148356a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148356fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148357520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148357a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148357fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148358510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148358a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148358fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148359500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148359a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148359fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14835a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148404eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148405320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148405790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148405c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148406070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1484064e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148406950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148406dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148407230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1484076a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148407b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148407f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1484083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148408860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148408cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148409140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1484095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148409a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148409e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14840a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14840a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14840abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14840b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14840b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14840b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14840bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14840c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14840c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14840caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14840cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14840d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14840de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14840e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14840ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14840f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14840f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14840fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1484100d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1484106e0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.662.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.662.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1483482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1483488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148348ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14831bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14831b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148319e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14831a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14831ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148319850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148319240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14831c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148312370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148307110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148305e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14831cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14831e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14832ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148315680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1483494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148313c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148313f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14835a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14835aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14835ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14835aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14835b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14835b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14835b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14835baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14835bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14835c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14835c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14835c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14835c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14835cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14835ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14835d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14835d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14835d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14835d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14835dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14835deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14835e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14835e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14835e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14835e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14835ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14835ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14835f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14835f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14835f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14835fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14835fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14835ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148360270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148360530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1483607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148360ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148360d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148361030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1483612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1483615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148361870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148361b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148361df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1483620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148362370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148362630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1483628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148362bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148362e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148363130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1483633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1483636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148363970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148363c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148363ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1483641b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148364470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148364730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1483649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148364cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148364f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148365230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1483654f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1483657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148365a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148365d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148365ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1483662b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148366570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148366830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148366af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148366db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148367070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148367330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1483675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1483678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148367b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148367e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1483680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1483683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148368670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148368930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148368bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148368eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148369170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148369430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1483696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1483699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148369c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148369f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14836a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14836a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14836a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14836aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14836acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14836afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14836b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14836b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14836b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14836bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14836bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14836c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14836c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14836c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14836c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14836cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14836cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14836d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14836d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14836d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14836d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14836dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14836de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14836e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14836e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14836e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14836e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14836ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14836eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14836f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14836f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14836f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14836f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14836fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14836ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148370230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1483704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1483707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148370a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148370d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148370ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1483712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148371570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148371830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148371af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148371db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148372070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148372330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1483725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1483728b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148372b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148372e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1483730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1483733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148373670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148373930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148373bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148373eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148374170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148374430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1483746f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1483749b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148374c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148374f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1483751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1483754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148375770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148375a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148375cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148375fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148376270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148376530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1483767f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148376ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148376d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148377030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1483772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1483775b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148377870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148377b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148377df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1483780b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148378370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148378630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1483788f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148378bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148378e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148379130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1483793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1483796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148379970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148379eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14837a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14837a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14837aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14837af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14837b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14837bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14837c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14837c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14837cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14837d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14837d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14837dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14837e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14837e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14837ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14837f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14837f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14837fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148380160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1483806b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148380c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148381150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1483816a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148381bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148382140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148382690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148382be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148383130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148383680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148383bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148384120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148384670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148384bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148385110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148385660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148385bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148386100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148386650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148386ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1483870f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148387640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148387b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1483880e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148388630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148388b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1483890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148389620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148389b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14838a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14838a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14838ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14838b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14838b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14838bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14838c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14838c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14838cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14838d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14838d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14838db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14838e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14838e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14838e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14838ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14838f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14838f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14838fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1483900e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148390580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148390a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148390ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148391360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148391800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148391ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148392140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1483925e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148392b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148393250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148393970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148394090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1483947b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148394a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148395260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148395520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148395b30 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148410390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148412340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148412f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1484131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148413480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148413740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148413a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148413cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148413f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148414240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148414500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1484149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1484154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148415c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1484164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148416bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1484172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148417a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148418120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1484188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148419010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148419730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148419e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14841a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14841ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14841af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14841b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14841b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14841baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14841bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14841c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14841c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14841cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14841d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14841d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14841d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143504080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1435044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143504960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143504dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143505240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1435056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143505b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143505f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143506400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143506870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143506ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143507150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1435075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143507a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143507ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143508310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143508780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143508bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143509060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1435094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143509a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143509f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14350a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14350a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14350acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14350b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14350b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14350ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14350be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14350c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14350c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14350cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14350d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14350d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14350d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14350dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14350e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14350e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14350eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14350ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14350f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14350f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14350fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143510110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143510580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1435109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143510e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1435112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143511740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143511bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143512020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143512490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143512900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143512d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1435131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143513650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143513ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143513f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1435143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143514810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143514c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1435150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143515560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1435159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143515e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1435162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143516720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143516cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143517140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1435175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143517a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143517e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143518300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143518770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143518be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143519050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1435194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143519930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143519da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14351a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14351a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14351aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14351af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14351b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14351b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14351bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14351c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14351c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14351ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14351ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14351d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14351d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14351dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14351e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14351e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14351e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14351ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14351f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14351f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14351fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14351ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1435203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143520820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143520c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143521100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143521570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1435219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143521e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1435222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143522730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143522ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143523010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143523480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1435238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143523d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1435241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143524640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143524ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143524f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143525390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143525800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143525c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1435260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143526550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1435269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143526e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1435272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143527710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143527b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143527ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143528460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1435288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143528d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1435291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143529620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143529a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143529f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14352a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14352a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14352ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14352b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14352b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14352b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14352be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14352c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14352c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14352cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14352cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14352d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14352d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14352dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14352e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14352e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14352ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14352eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14352f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14352f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14352fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1435300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143530510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143530980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143530df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143531260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1435316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143531b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143531fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143532420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143532890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143532d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143533170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1435335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143533b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143533fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143534450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143534fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143535260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143535520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143535990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143535e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143536270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1435366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143536b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143536fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143537430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1435378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143537d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143538180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1435385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143538a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143538ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143539340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1435397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143539c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14353a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14353a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14353a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14353ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14353b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14353b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14353bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14353bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14353c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14353c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14353ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14353d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14353d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14353da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14353deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14353e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14353e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14353ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14353f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14353f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14353f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14353fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143540230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1435406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143540b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143540f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1435413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143541860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143541cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143542140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1435425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143542a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143542e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143543300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143543770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143543be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143544050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1435444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143544930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143544da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143545210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143545680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143545af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143545f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1435463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143546840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143546cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143547120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143547590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143547a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143547e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1435482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143548750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143548bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143549630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143549d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14354a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14354ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14354ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14354b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14354b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14354bed0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.728s
user	0m0.276s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4628 (3e23be79)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b00b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b00bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b00c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b00c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b00cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b00d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b00d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b00dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b00e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b00e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b00ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b00f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b00fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b010450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b010c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b011380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b0121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b0128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b0130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b0137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b013ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b014eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b0155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b015890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b015ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b016b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b017050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b017310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b0177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b017a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b018300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b018840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b018b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b018fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b019440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b0198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b019d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b01a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b01a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b01ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b01b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b01b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b01b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b01bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b01c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b01cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b01d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b01d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b01ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b01e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b01eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b01f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b01f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b01fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b020230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b0204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b020b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b0212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b0215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b021a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b021ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b022390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b022830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b022cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b023610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b023ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b023f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b0243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b024d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b025280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b0257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b025d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b026270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b0267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b026d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b027260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b0277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b027d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b028250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b0287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b028cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b029240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b029790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b029ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b02a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b02a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b02acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b02b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b02b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b02bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b02c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b02c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b02ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b01c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b02d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b02d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b02de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b02e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b02e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b02ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b02f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b02f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b02fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b030350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b0308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b030df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b031340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b031890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b031de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b032280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b032720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b032bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b033060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b033500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b0339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b033e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b0342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b034780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b034c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b0350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b035560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b035a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b035ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b036340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b0367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b037120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b0375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b037a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b037f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b0383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b038840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b038ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b039180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b039620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b039ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b039f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b03a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b03a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b03ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b03b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b03b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b03bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b03bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b03c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b03c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b03cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b03d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b03d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b03db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b03e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b03e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b03e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b03ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b03f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b03f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b03fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b040080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b0409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b040e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b041300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b0417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b041c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b0420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b042a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b042ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b043360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b043800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b043ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b044140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b0445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b044a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b044f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b0453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b045860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b045d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b0461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b046640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b046ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b046f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b047420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b0478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b047d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b048200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b0486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b048b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b048fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b049530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b049a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b049fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b04a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b04a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b04adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b04b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b04ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b04c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b04c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b04c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b04cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b04d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b04dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b04e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b04e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b04eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b04f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b04f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b04fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b0502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b050840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b050d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b0512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b051830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b051d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b0522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b052820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b052d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b0532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b053d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b0542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b054800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b054d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b0552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b0557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b055d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b056290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b0567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b056d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b057280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b0577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b057d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b058270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b0587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b058d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b059260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b0597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b059d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b05a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b05a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b05acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b05b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b05b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b05bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b05c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b05c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b05ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b05d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b05d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b05dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b05e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b05e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b05ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b05f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b05f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b05fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b0601f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b060740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b060c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b0611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b061730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b061c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b062120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b0625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b062a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b062f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b0633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b063840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b063ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b064180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b064620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b064ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b064f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b065400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b0658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b065d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b0661e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b066730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b066e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b067570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b067c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b0683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b068670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b068e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b069120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b069730 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.101.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.101.194 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
prepare: reserving a worst case graph
main : serialized state into 988319 out of a maximum of 988319 bytes
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b0693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b04cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b04aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b04b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b01e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b01e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b0207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b04d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b015b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b01c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b01cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b01d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b01ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b01db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b014b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b020dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b02d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b068930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b017d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b04d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b04bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b016160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b016420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b0166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b069b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b069e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b06a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b06a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b06a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b06a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b06ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b06aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b06b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b06b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b06b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b06b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b06bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b06bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b06c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b06c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b06c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b06ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b06cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b06cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b06d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b06d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b06d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b06dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b06dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b06e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b06e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b06e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b06e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b06eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b06ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b06f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b06f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b06f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b06f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b06fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b06fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b070150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b070410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b0706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b070990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b070c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b070f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b0711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b071490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b071750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b071a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b071cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b071f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b072250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b072510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b0727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b072a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b072d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b073010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b0732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b073590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b073850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b073b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b073dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b074090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b074350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b074610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b0748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b074b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b074e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b075110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b0753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b075690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b075950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b075c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b075ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b076190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b076450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b076710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b0769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b076c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b076f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b077210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b0774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b077790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b077a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b077d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b077fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b078290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b078550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b078810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b078ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b078d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b079050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b079310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b0795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b079890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b079b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b079e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b07a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b07a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b07a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b07a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b07abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b07ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b07b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b07b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b07b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b07b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b07bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b07bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b07c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b07c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b07c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b07ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b07ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b07cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b07d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b07d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b07d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b07da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b07dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b07e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b07e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b07e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b07e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b07eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b07edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b07f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b07f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b07f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b07f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b07fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b07fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b080110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b0803d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b080690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b080950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b080c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b080ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b081190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b081450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b081710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b0819d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b081c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b081f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b082210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b0824d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b082790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b082a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b082d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b082fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b083290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b083550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b083810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b083ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b083d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b084050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b084310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b0845d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b084890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b084b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b084e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b0850d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b085390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b085650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b085910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b085bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b085e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b086150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b086410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b0866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b086990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b086c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b086f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b0871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b087490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b087750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b087a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b087cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b087f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b088250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b088510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b0887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b088d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b088fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b089470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b089910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b089db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b08a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b08a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b08aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b08af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b08b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b08b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b08bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b08c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b08c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b08c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b08ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b08d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b08d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b08dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b08e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b08e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b08e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b08ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b08f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b08f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b08fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b08ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b0903a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b090810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b090c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b0910f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b091560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b0919d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b091e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b0922b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b092720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b092b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b093000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b093470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b0938e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b093d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b0941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b094630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b094aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b094f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b095380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b0957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b095c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b0960d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b096540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b0969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b096e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b097290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b097700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b097b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b097fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b098450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b0988c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b098d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b0991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b099610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b099a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b099ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b09a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b09a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b09ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b09b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b09b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b09b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b09be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b09c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b09c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b09cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b09cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b09d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b09d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b09dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b09e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b09ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b09f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b09fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b0a0150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b0a0410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b0a0c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b0a0ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b0a14d0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b105dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b106240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b1066b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b106b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b106f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b107400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b107870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b107ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b108150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b1085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b108a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b109150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b109c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b10a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b10ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b10b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b10ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b10c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b10c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b10cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b10d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b10de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b10e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b10ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b10f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b10f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b10f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b10fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b1101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b110650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b110ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b110ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b111460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b111720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b111b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b112000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b112470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b1128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b112d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b1131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b113630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b113aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b113f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b114380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b1147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b114c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b1150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b115540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b1159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b115e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b116290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b116700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b116b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b116fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b117450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b1178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b117e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b118330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b1187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b118c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b119080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b1194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b119960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b119dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b11a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b11a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b11ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b11af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b11b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b11b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b11bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b11c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b11c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b11ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b11cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b11d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b11d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b11dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b11e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b11e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b11e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b11edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b11f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b11f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b11fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b11ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b1203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b120850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b120cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b121130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b1215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b121a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b121e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b1222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b122760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b122bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b123040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b1234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b123920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b123d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b124200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b124670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b124ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b125370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b125630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b125aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b125f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b126380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b1267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b126c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b1270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b127540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b1279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b127e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b128290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b128700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b128b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b128fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b129450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b1298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b129d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b12a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b12a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b12aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b12aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b12b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b12b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b12bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b12c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b12c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b12c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b12ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b12d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b12d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b12db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b12dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b12e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b12e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b12ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b12f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b12f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b12fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b12fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b130340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b1307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b130c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b131090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b131500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b131970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b131de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b132250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b1326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b132b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b132fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b133410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b133880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b133cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b134160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b1345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b134a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b134eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b135320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b135790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b135c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b136070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b1364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b136950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b136dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b137230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b1376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b137b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b137f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b1383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b138860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b138cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b139140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b1395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b139a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b139e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b13a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b13a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b13abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b13b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b13b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b13b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b13bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b13c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b13c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b13caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b13cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b13d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b13d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b13dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b13e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b13e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b13ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b13ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b13f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b13f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b13fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b140030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b1404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b140910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b140d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b1411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b141660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b141ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b141f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b1423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b142820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b1433a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b143660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b143920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b143d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b144200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b144670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b144ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b144f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b1453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b145830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b145ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b146110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b146580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b1469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b146e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b1472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b147740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b147bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b148020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b148490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b148900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b148d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b1491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b149650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b149ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b149f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b14a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b14a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b14ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b14b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b14b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b14b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b14be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b14c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b14c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b14cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b14d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b14d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b14d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b14dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b14e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b14e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b14eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b14ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b14f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b14f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b14fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b1500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b150540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b1509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b150e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b151290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b151700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b151b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b151fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b152450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b1528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b152d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b1531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b153610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b153a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b153ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b154360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b1547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b154c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b1550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b155520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b155990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b155e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b156270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b1566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b156b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b156fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b157a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b158150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b158870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b158f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b159250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b1596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b159cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b15a2d0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph
prepare: reserving a worst case graph

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.969s
user	0m0.237s
sys	0m0.192s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.52 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.95 sec*proc (2 tests)

Total Test time (real) =   1.96 sec
        1.98 real         0.52 user         0.24 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
