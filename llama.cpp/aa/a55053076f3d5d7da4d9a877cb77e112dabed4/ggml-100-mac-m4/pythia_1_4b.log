Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.593s
user	0m0.894s
sys	0m1.255s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target xxhash
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-chat
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-chat-template
[ 63%] Built target test-gguf
[ 63%] Built target test-backend-ops
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-rope
[ 64%] Built target test-autorelease
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-bench
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-infill
[ 74%] Built target llama-gritlm
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-merge
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-passkey
[ 83%] Built target llama-parallel
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Built target llama-quantize
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Built target llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.129s
user	0m6.554s
sys	0m10.145s

main: quantize time =  5615.05 ms
main:    total time =  5615.05 ms

main: quantize time =  5279.57 ms
main:    total time =  5279.57 ms

main: quantize time =  3878.45 ms
main:    total time =  3878.45 ms

main: quantize time =  3153.11 ms
main:    total time =  3153.11 ms

main: quantize time =  3908.25 ms
main:    total time =  3908.25 ms

main: quantize time =  5294.36 ms
main:    total time =  5294.36 ms

main: quantize time =  6037.95 ms
main:    total time =  6037.95 ms

main: quantize time =  7386.27 ms
main:    total time =  7386.27 ms

main: quantize time =  6410.67 ms
main:    total time =  6410.67 ms

main: quantize time =  4541.70 ms
main:    total time =  4541.70 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.221 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.385 I main: llama backend init
0.00.000.392 I main: load the model and apply lora adapter, if any
0.00.097.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.110.391 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.110.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.110.409 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.110.410 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.110.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.110.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.110.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.110.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.110.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.110.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.110.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.110.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.110.423 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.110.423 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.110.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.110.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.110.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.117.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.119.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.126.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.126.116 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.126.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.126.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.126.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.126.119 I llama_model_loader: - type  f32:  194 tensors
0.00.126.120 I llama_model_loader: - type  f16:   98 tensors
0.00.126.121 I print_info: file format = GGUF V3 (latest)
0.00.126.128 I print_info: file type   = all F32 (guessed)
0.00.126.131 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.144.611 I load: special tokens cache size = 25
0.00.155.105 I load: token to piece cache size = 0.2984 MB
0.00.155.111 I print_info: arch             = gptneox
0.00.155.111 I print_info: vocab_only       = 0
0.00.155.113 I print_info: n_ctx_train      = 2048
0.00.155.114 I print_info: n_embd           = 2048
0.00.155.114 I print_info: n_layer          = 24
0.00.155.119 I print_info: n_head           = 16
0.00.155.120 I print_info: n_head_kv        = 16
0.00.155.120 I print_info: n_rot            = 32
0.00.155.121 I print_info: n_swa            = 0
0.00.155.121 I print_info: n_embd_head_k    = 128
0.00.155.122 I print_info: n_embd_head_v    = 128
0.00.155.123 I print_info: n_gqa            = 1
0.00.155.124 I print_info: n_embd_k_gqa     = 2048
0.00.155.127 I print_info: n_embd_v_gqa     = 2048
0.00.155.127 I print_info: f_norm_eps       = 1.0e-05
0.00.155.128 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.155.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.155.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.155.129 I print_info: f_logit_scale    = 0.0e+00
0.00.155.130 I print_info: n_ff             = 8192
0.00.155.130 I print_info: n_expert         = 0
0.00.155.130 I print_info: n_expert_used    = 0
0.00.155.130 I print_info: causal attn      = 1
0.00.155.130 I print_info: pooling type     = 0
0.00.155.130 I print_info: rope type        = 2
0.00.155.131 I print_info: rope scaling     = linear
0.00.155.131 I print_info: freq_base_train  = 10000.0
0.00.155.132 I print_info: freq_scale_train = 1
0.00.155.132 I print_info: n_ctx_orig_yarn  = 2048
0.00.155.132 I print_info: rope_finetuned   = unknown
0.00.155.132 I print_info: ssm_d_conv       = 0
0.00.155.133 I print_info: ssm_d_inner      = 0
0.00.155.133 I print_info: ssm_d_state      = 0
0.00.155.133 I print_info: ssm_dt_rank      = 0
0.00.155.133 I print_info: ssm_dt_b_c_rms   = 0
0.00.155.134 I print_info: model type       = 1.4B
0.00.155.134 I print_info: model params     = 1.41 B
0.00.155.134 I print_info: general.name     = 1.4B
0.00.155.135 I print_info: vocab type       = BPE
0.00.155.135 I print_info: n_vocab          = 50304
0.00.155.135 I print_info: n_merges         = 50009
0.00.155.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.155.136 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.155.136 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.155.136 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.155.137 I print_info: LF token         = 187 'Ċ'
0.00.155.137 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.155.137 I print_info: max token length = 1024
0.00.155.138 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.208.492 I load_tensors: offloading 24 repeating layers to GPU
0.00.208.496 I load_tensors: offloading output layer to GPU
0.00.208.496 I load_tensors: offloaded 25/25 layers to GPU
0.00.208.521 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.208.522 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.209.112 I llama_init_from_model: n_seq_max     = 1
0.00.209.113 I llama_init_from_model: n_ctx         = 2048
0.00.209.113 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.209.114 I llama_init_from_model: n_batch       = 2048
0.00.209.114 I llama_init_from_model: n_ubatch      = 512
0.00.209.114 I llama_init_from_model: flash_attn    = 0
0.00.209.114 I llama_init_from_model: freq_base     = 10000.0
0.00.209.114 I llama_init_from_model: freq_scale    = 1
0.00.209.115 I ggml_metal_init: allocating
0.00.209.138 I ggml_metal_init: found device: Apple M4
0.00.209.143 I ggml_metal_init: picking default device: Apple M4
0.00.209.735 I ggml_metal_init: using embedded metal library
0.00.223.045 I ggml_metal_init: GPU name:   Apple M4
0.00.223.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.223.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.223.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.223.047 I ggml_metal_init: simdgroup reduction   = true
0.00.223.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.223.047 I ggml_metal_init: has residency sets    = true
0.00.223.048 I ggml_metal_init: has bfloat            = true
0.00.223.048 I ggml_metal_init: use bfloat            = true
0.00.223.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.223.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.254.872 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.283.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.283.737 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.283.762 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.287.831 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.287.833 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.287.833 I llama_init_from_model: graph nodes  = 967
0.00.287.834 I llama_init_from_model: graph splits = 2
0.00.287.841 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.287.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.287.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.428 I main: llama threadpool init, n_threads = 4
0.00.354.470 I 
0.00.354.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.509 I 
0.00.354.653 I sampler seed: 1234
0.00.354.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.354.682 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.354.684 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.354.685 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.180.073 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.180.074 I llama_perf_context_print:        load time =     255.68 ms
0.02.180.075 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.04 tokens per second)
0.02.180.076 I llama_perf_context_print:        eval time =    1778.80 ms /    63 runs   (   28.23 ms per token,    35.42 tokens per second)
0.02.180.077 I llama_perf_context_print:       total time =    1826.48 ms /    70 tokens
0.02.180.302 I ggml_metal_free: deallocating

real	0m2.505s
user	0m0.134s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.101 I main: llama backend init
0.00.000.104 I main: load the model and apply lora adapter, if any
0.00.010.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.257 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.258 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.261 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.264 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.028 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.741 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.742 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.745 I llama_model_loader: - type  f32:  194 tensors
0.00.036.745 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.746 I print_info: file format = GGUF V3 (latest)
0.00.036.746 I print_info: file type   = Q8_0
0.00.036.748 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.639 I load: special tokens cache size = 25
0.00.052.339 I load: token to piece cache size = 0.2984 MB
0.00.052.343 I print_info: arch             = gptneox
0.00.052.343 I print_info: vocab_only       = 0
0.00.052.344 I print_info: n_ctx_train      = 2048
0.00.052.344 I print_info: n_embd           = 2048
0.00.052.344 I print_info: n_layer          = 24
0.00.052.349 I print_info: n_head           = 16
0.00.052.352 I print_info: n_head_kv        = 16
0.00.052.352 I print_info: n_rot            = 32
0.00.052.352 I print_info: n_swa            = 0
0.00.052.352 I print_info: n_embd_head_k    = 128
0.00.052.352 I print_info: n_embd_head_v    = 128
0.00.052.354 I print_info: n_gqa            = 1
0.00.052.354 I print_info: n_embd_k_gqa     = 2048
0.00.052.355 I print_info: n_embd_v_gqa     = 2048
0.00.052.356 I print_info: f_norm_eps       = 1.0e-05
0.00.052.356 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.356 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.359 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.359 I print_info: f_logit_scale    = 0.0e+00
0.00.052.359 I print_info: n_ff             = 8192
0.00.052.360 I print_info: n_expert         = 0
0.00.052.360 I print_info: n_expert_used    = 0
0.00.052.360 I print_info: causal attn      = 1
0.00.052.360 I print_info: pooling type     = 0
0.00.052.360 I print_info: rope type        = 2
0.00.052.361 I print_info: rope scaling     = linear
0.00.052.361 I print_info: freq_base_train  = 10000.0
0.00.052.361 I print_info: freq_scale_train = 1
0.00.052.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.362 I print_info: rope_finetuned   = unknown
0.00.052.362 I print_info: ssm_d_conv       = 0
0.00.052.362 I print_info: ssm_d_inner      = 0
0.00.052.362 I print_info: ssm_d_state      = 0
0.00.052.362 I print_info: ssm_dt_rank      = 0
0.00.052.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.363 I print_info: model type       = 1.4B
0.00.052.363 I print_info: model params     = 1.41 B
0.00.052.363 I print_info: general.name     = 1.4B
0.00.052.364 I print_info: vocab type       = BPE
0.00.052.364 I print_info: n_vocab          = 50304
0.00.052.364 I print_info: n_merges         = 50009
0.00.052.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.365 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.367 I print_info: LF token         = 187 'Ċ'
0.00.052.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.367 I print_info: max token length = 1024
0.00.052.368 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.297.432 I load_tensors: offloading 24 repeating layers to GPU
0.01.297.437 I load_tensors: offloading output layer to GPU
0.01.297.439 I load_tensors: offloaded 25/25 layers to GPU
0.01.297.463 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.297.464 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.298.540 I llama_init_from_model: n_seq_max     = 1
0.01.298.542 I llama_init_from_model: n_ctx         = 2048
0.01.298.542 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.298.542 I llama_init_from_model: n_batch       = 2048
0.01.298.543 I llama_init_from_model: n_ubatch      = 512
0.01.298.543 I llama_init_from_model: flash_attn    = 0
0.01.298.544 I llama_init_from_model: freq_base     = 10000.0
0.01.298.544 I llama_init_from_model: freq_scale    = 1
0.01.298.545 I ggml_metal_init: allocating
0.01.298.555 I ggml_metal_init: found device: Apple M4
0.01.298.563 I ggml_metal_init: picking default device: Apple M4
0.01.299.811 I ggml_metal_init: using embedded metal library
0.01.305.096 I ggml_metal_init: GPU name:   Apple M4
0.01.305.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.305.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.305.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.305.102 I ggml_metal_init: simdgroup reduction   = true
0.01.305.102 I ggml_metal_init: simdgroup matrix mul. = true
0.01.305.102 I ggml_metal_init: has residency sets    = true
0.01.305.102 I ggml_metal_init: has bfloat            = true
0.01.305.102 I ggml_metal_init: use bfloat            = true
0.01.305.103 I ggml_metal_init: hasUnifiedMemory      = true
0.01.305.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.321.720 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.376.344 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.376.351 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.376.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.381.429 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.381.430 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.381.431 I llama_init_from_model: graph nodes  = 967
0.01.381.431 I llama_init_from_model: graph splits = 2
0.01.381.440 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.381.552 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.381.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.439.917 I main: llama threadpool init, n_threads = 4
0.01.439.957 I 
0.01.439.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.439.982 I 
0.01.440.149 I sampler seed: 1234
0.01.440.154 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.440.165 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.440.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.440.165 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.525.570 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.525.570 I llama_perf_context_print:        load time =    1429.04 ms
0.02.525.571 I llama_perf_context_print: prompt eval time =      49.25 ms /     7 tokens (    7.04 ms per token,   142.13 tokens per second)
0.02.525.572 I llama_perf_context_print:        eval time =    1033.31 ms /    63 runs   (   16.40 ms per token,    60.97 tokens per second)
0.02.525.572 I llama_perf_context_print:       total time =    1086.37 ms /    70 tokens
0.02.525.843 I ggml_metal_free: deallocating

real	0m2.542s
user	0m0.109s
sys	0m0.280s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.019.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.039.823 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.806 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.165 I llama_model_loader: - type  f32:  194 tensors
0.00.051.165 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.167 I print_info: file format = GGUF V3 (latest)
0.00.051.167 I print_info: file type   = Q4_0
0.00.051.168 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.063.734 I load: special tokens cache size = 25
0.00.077.989 I load: token to piece cache size = 0.2984 MB
0.00.077.994 I print_info: arch             = gptneox
0.00.077.995 I print_info: vocab_only       = 0
0.00.077.995 I print_info: n_ctx_train      = 2048
0.00.077.995 I print_info: n_embd           = 2048
0.00.077.996 I print_info: n_layer          = 24
0.00.078.001 I print_info: n_head           = 16
0.00.078.003 I print_info: n_head_kv        = 16
0.00.078.003 I print_info: n_rot            = 32
0.00.078.003 I print_info: n_swa            = 0
0.00.078.004 I print_info: n_embd_head_k    = 128
0.00.078.004 I print_info: n_embd_head_v    = 128
0.00.078.005 I print_info: n_gqa            = 1
0.00.078.007 I print_info: n_embd_k_gqa     = 2048
0.00.078.012 I print_info: n_embd_v_gqa     = 2048
0.00.078.013 I print_info: f_norm_eps       = 1.0e-05
0.00.078.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.016 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.017 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.017 I print_info: f_logit_scale    = 0.0e+00
0.00.078.018 I print_info: n_ff             = 8192
0.00.078.018 I print_info: n_expert         = 0
0.00.078.019 I print_info: n_expert_used    = 0
0.00.078.019 I print_info: causal attn      = 1
0.00.078.019 I print_info: pooling type     = 0
0.00.078.022 I print_info: rope type        = 2
0.00.078.030 I print_info: rope scaling     = linear
0.00.078.031 I print_info: freq_base_train  = 10000.0
0.00.078.032 I print_info: freq_scale_train = 1
0.00.078.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.033 I print_info: rope_finetuned   = unknown
0.00.078.033 I print_info: ssm_d_conv       = 0
0.00.078.033 I print_info: ssm_d_inner      = 0
0.00.078.033 I print_info: ssm_d_state      = 0
0.00.078.034 I print_info: ssm_dt_rank      = 0
0.00.078.034 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.035 I print_info: model type       = 1.4B
0.00.078.038 I print_info: model params     = 1.41 B
0.00.078.039 I print_info: general.name     = 1.4B
0.00.078.039 I print_info: vocab type       = BPE
0.00.078.040 I print_info: n_vocab          = 50304
0.00.078.040 I print_info: n_merges         = 50009
0.00.078.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.044 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.044 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.044 I print_info: LF token         = 187 'Ċ'
0.00.078.045 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.045 I print_info: max token length = 1024
0.00.078.046 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.984.872 I load_tensors: offloading 24 repeating layers to GPU
0.00.984.883 I load_tensors: offloading output layer to GPU
0.00.984.884 I load_tensors: offloaded 25/25 layers to GPU
0.00.984.919 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.984.920 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.986.396 I llama_init_from_model: n_seq_max     = 1
0.00.986.398 I llama_init_from_model: n_ctx         = 2048
0.00.986.399 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.986.399 I llama_init_from_model: n_batch       = 2048
0.00.986.400 I llama_init_from_model: n_ubatch      = 512
0.00.986.400 I llama_init_from_model: flash_attn    = 0
0.00.986.403 I llama_init_from_model: freq_base     = 10000.0
0.00.986.403 I llama_init_from_model: freq_scale    = 1
0.00.986.405 I ggml_metal_init: allocating
0.00.986.478 I ggml_metal_init: found device: Apple M4
0.00.986.492 I ggml_metal_init: picking default device: Apple M4
0.00.988.425 I ggml_metal_init: using embedded metal library
0.00.993.977 I ggml_metal_init: GPU name:   Apple M4
0.00.993.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.993.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.993.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.993.984 I ggml_metal_init: simdgroup reduction   = true
0.00.993.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.993.985 I ggml_metal_init: has residency sets    = true
0.00.993.985 I ggml_metal_init: has bfloat            = true
0.00.993.986 I ggml_metal_init: use bfloat            = true
0.00.993.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.993.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.013.406 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.068.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.068.303 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.068.334 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.072.459 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.072.462 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.072.462 I llama_init_from_model: graph nodes  = 967
0.01.072.462 I llama_init_from_model: graph splits = 2
0.01.072.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.072.582 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.072.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.130.563 I main: llama threadpool init, n_threads = 4
0.01.130.603 I 
0.01.130.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.130.630 I 
0.01.130.781 I sampler seed: 1234
0.01.130.785 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.130.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.130.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.130.798 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.809.296 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.809.296 I llama_perf_context_print:        load time =    1109.96 ms
0.01.809.297 I llama_perf_context_print: prompt eval time =      49.33 ms /     7 tokens (    7.05 ms per token,   141.89 tokens per second)
0.01.809.298 I llama_perf_context_print:        eval time =     626.20 ms /    63 runs   (    9.94 ms per token,   100.61 tokens per second)
0.01.809.298 I llama_perf_context_print:       total time =     679.44 ms /    70 tokens
0.01.809.535 I ggml_metal_free: deallocating

real	0m1.844s
user	0m0.128s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.769 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.610 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.407 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.408 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.409 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.410 I llama_model_loader: - type  f32:  194 tensors
0.00.025.411 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.412 I print_info: file format = GGUF V3 (latest)
0.00.025.412 I print_info: file type   = Q4_1
0.00.025.413 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.141 I load: special tokens cache size = 25
0.00.038.879 I load: token to piece cache size = 0.2984 MB
0.00.038.882 I print_info: arch             = gptneox
0.00.038.882 I print_info: vocab_only       = 0
0.00.038.882 I print_info: n_ctx_train      = 2048
0.00.038.882 I print_info: n_embd           = 2048
0.00.038.882 I print_info: n_layer          = 24
0.00.038.885 I print_info: n_head           = 16
0.00.038.886 I print_info: n_head_kv        = 16
0.00.038.886 I print_info: n_rot            = 32
0.00.038.889 I print_info: n_swa            = 0
0.00.038.889 I print_info: n_embd_head_k    = 128
0.00.038.889 I print_info: n_embd_head_v    = 128
0.00.038.890 I print_info: n_gqa            = 1
0.00.038.891 I print_info: n_embd_k_gqa     = 2048
0.00.038.891 I print_info: n_embd_v_gqa     = 2048
0.00.038.892 I print_info: f_norm_eps       = 1.0e-05
0.00.038.892 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.893 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.893 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.893 I print_info: f_logit_scale    = 0.0e+00
0.00.038.893 I print_info: n_ff             = 8192
0.00.038.894 I print_info: n_expert         = 0
0.00.038.894 I print_info: n_expert_used    = 0
0.00.038.899 I print_info: causal attn      = 1
0.00.038.901 I print_info: pooling type     = 0
0.00.038.903 I print_info: rope type        = 2
0.00.038.903 I print_info: rope scaling     = linear
0.00.038.905 I print_info: freq_base_train  = 10000.0
0.00.038.905 I print_info: freq_scale_train = 1
0.00.038.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.906 I print_info: rope_finetuned   = unknown
0.00.038.906 I print_info: ssm_d_conv       = 0
0.00.038.906 I print_info: ssm_d_inner      = 0
0.00.038.907 I print_info: ssm_d_state      = 0
0.00.038.907 I print_info: ssm_dt_rank      = 0
0.00.038.907 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.907 I print_info: model type       = 1.4B
0.00.038.907 I print_info: model params     = 1.41 B
0.00.038.908 I print_info: general.name     = 1.4B
0.00.038.908 I print_info: vocab type       = BPE
0.00.038.908 I print_info: n_vocab          = 50304
0.00.038.908 I print_info: n_merges         = 50009
0.00.038.909 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: LF token         = 187 'Ċ'
0.00.038.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.910 I print_info: max token length = 1024
0.00.038.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.678.853 I load_tensors: offloading 24 repeating layers to GPU
0.00.678.871 I load_tensors: offloading output layer to GPU
0.00.678.872 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.906 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.678.908 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.680.564 I llama_init_from_model: n_seq_max     = 1
0.00.680.566 I llama_init_from_model: n_ctx         = 2048
0.00.680.567 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.568 I llama_init_from_model: n_batch       = 2048
0.00.680.568 I llama_init_from_model: n_ubatch      = 512
0.00.680.569 I llama_init_from_model: flash_attn    = 0
0.00.680.571 I llama_init_from_model: freq_base     = 10000.0
0.00.680.572 I llama_init_from_model: freq_scale    = 1
0.00.680.574 I ggml_metal_init: allocating
0.00.680.653 I ggml_metal_init: found device: Apple M4
0.00.680.666 I ggml_metal_init: picking default device: Apple M4
0.00.682.577 I ggml_metal_init: using embedded metal library
0.00.689.681 I ggml_metal_init: GPU name:   Apple M4
0.00.689.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.689 I ggml_metal_init: simdgroup reduction   = true
0.00.689.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.689 I ggml_metal_init: has residency sets    = true
0.00.689.690 I ggml_metal_init: has bfloat            = true
0.00.689.690 I ggml_metal_init: use bfloat            = true
0.00.689.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.707.963 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.299 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.763.306 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.763.330 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.526 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.767.528 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.767.528 I llama_init_from_model: graph nodes  = 967
0.00.767.528 I llama_init_from_model: graph splits = 2
0.00.767.533 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.767.647 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.767.647 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.823.711 I main: llama threadpool init, n_threads = 4
0.00.823.756 I 
0.00.823.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.823.780 I 
0.00.823.934 I sampler seed: 1234
0.00.823.938 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.823.985 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.823.989 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.823.989 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.548.032 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.01.548.032 I llama_perf_context_print:        load time =     814.20 ms
0.01.548.033 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.01.548.035 I llama_perf_context_print:        eval time =     672.16 ms /    63 runs   (   10.67 ms per token,    93.73 tokens per second)
0.01.548.035 I llama_perf_context_print:       total time =     725.03 ms /    70 tokens
0.01.548.335 I ggml_metal_free: deallocating

real	0m1.564s
user	0m0.108s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.119 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.860 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.860 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.861 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.861 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.862 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.862 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.862 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.863 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.864 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.398 I llama_model_loader: - type  f32:  194 tensors
0.00.027.399 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.399 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.400 I print_info: file format = GGUF V3 (latest)
0.00.027.400 I print_info: file type   = Q5_0
0.00.027.405 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.100 I load: special tokens cache size = 25
0.00.041.217 I load: token to piece cache size = 0.2984 MB
0.00.041.220 I print_info: arch             = gptneox
0.00.041.220 I print_info: vocab_only       = 0
0.00.041.221 I print_info: n_ctx_train      = 2048
0.00.041.221 I print_info: n_embd           = 2048
0.00.041.221 I print_info: n_layer          = 24
0.00.041.223 I print_info: n_head           = 16
0.00.041.224 I print_info: n_head_kv        = 16
0.00.041.224 I print_info: n_rot            = 32
0.00.041.225 I print_info: n_swa            = 0
0.00.041.225 I print_info: n_embd_head_k    = 128
0.00.041.225 I print_info: n_embd_head_v    = 128
0.00.041.226 I print_info: n_gqa            = 1
0.00.041.227 I print_info: n_embd_k_gqa     = 2048
0.00.041.229 I print_info: n_embd_v_gqa     = 2048
0.00.041.230 I print_info: f_norm_eps       = 1.0e-05
0.00.041.230 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.231 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.231 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.231 I print_info: f_logit_scale    = 0.0e+00
0.00.041.232 I print_info: n_ff             = 8192
0.00.041.233 I print_info: n_expert         = 0
0.00.041.233 I print_info: n_expert_used    = 0
0.00.041.233 I print_info: causal attn      = 1
0.00.041.233 I print_info: pooling type     = 0
0.00.041.235 I print_info: rope type        = 2
0.00.041.236 I print_info: rope scaling     = linear
0.00.041.237 I print_info: freq_base_train  = 10000.0
0.00.041.237 I print_info: freq_scale_train = 1
0.00.041.237 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.237 I print_info: rope_finetuned   = unknown
0.00.041.238 I print_info: ssm_d_conv       = 0
0.00.041.238 I print_info: ssm_d_inner      = 0
0.00.041.238 I print_info: ssm_d_state      = 0
0.00.041.238 I print_info: ssm_dt_rank      = 0
0.00.041.238 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.238 I print_info: model type       = 1.4B
0.00.041.239 I print_info: model params     = 1.41 B
0.00.041.239 I print_info: general.name     = 1.4B
0.00.041.240 I print_info: vocab type       = BPE
0.00.041.243 I print_info: n_vocab          = 50304
0.00.041.243 I print_info: n_merges         = 50009
0.00.041.243 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.245 I print_info: LF token         = 187 'Ċ'
0.00.041.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.246 I print_info: max token length = 1024
0.00.041.246 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.728.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.728.178 I load_tensors: offloading output layer to GPU
0.00.728.179 I load_tensors: offloaded 25/25 layers to GPU
0.00.728.211 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.728.212 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.729.816 I llama_init_from_model: n_seq_max     = 1
0.00.729.823 I llama_init_from_model: n_ctx         = 2048
0.00.729.824 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.729.824 I llama_init_from_model: n_batch       = 2048
0.00.729.825 I llama_init_from_model: n_ubatch      = 512
0.00.729.825 I llama_init_from_model: flash_attn    = 0
0.00.729.826 I llama_init_from_model: freq_base     = 10000.0
0.00.729.826 I llama_init_from_model: freq_scale    = 1
0.00.729.829 I ggml_metal_init: allocating
0.00.729.882 I ggml_metal_init: found device: Apple M4
0.00.729.893 I ggml_metal_init: picking default device: Apple M4
0.00.732.038 I ggml_metal_init: using embedded metal library
0.00.738.862 I ggml_metal_init: GPU name:   Apple M4
0.00.738.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.738.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.738.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.738.870 I ggml_metal_init: simdgroup reduction   = true
0.00.738.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.738.870 I ggml_metal_init: has residency sets    = true
0.00.738.870 I ggml_metal_init: has bfloat            = true
0.00.738.871 I ggml_metal_init: use bfloat            = true
0.00.738.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.738.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.757.225 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.812.864 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.812.870 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.812.938 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.467 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.817.469 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.817.469 I llama_init_from_model: graph nodes  = 967
0.00.817.470 I llama_init_from_model: graph splits = 2
0.00.817.475 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.817.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.817.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.877.283 I main: llama threadpool init, n_threads = 4
0.00.877.322 I 
0.00.877.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.877.346 I 
0.00.877.518 I sampler seed: 1234
0.00.877.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.877.532 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.877.534 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.877.534 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.666.819 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.666.820 I llama_perf_context_print:        load time =     865.42 ms
0.01.666.821 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.27 tokens per second)
0.01.666.821 I llama_perf_context_print:        eval time =     733.43 ms /    63 runs   (   11.64 ms per token,    85.90 tokens per second)
0.01.666.822 I llama_perf_context_print:       total time =     790.28 ms /    70 tokens
0.01.667.043 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.389 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.390 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.392 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.392 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.393 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.393 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.399 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.134 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.877 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.878 I llama_model_loader: - type  f32:  194 tensors
0.00.024.878 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.878 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.879 I print_info: file format = GGUF V3 (latest)
0.00.024.880 I print_info: file type   = Q5_1
0.00.024.881 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.892 I load: special tokens cache size = 25
0.00.038.910 I load: token to piece cache size = 0.2984 MB
0.00.038.913 I print_info: arch             = gptneox
0.00.038.913 I print_info: vocab_only       = 0
0.00.038.913 I print_info: n_ctx_train      = 2048
0.00.038.913 I print_info: n_embd           = 2048
0.00.038.914 I print_info: n_layer          = 24
0.00.038.917 I print_info: n_head           = 16
0.00.038.918 I print_info: n_head_kv        = 16
0.00.038.918 I print_info: n_rot            = 32
0.00.038.918 I print_info: n_swa            = 0
0.00.038.918 I print_info: n_embd_head_k    = 128
0.00.038.920 I print_info: n_embd_head_v    = 128
0.00.038.921 I print_info: n_gqa            = 1
0.00.038.922 I print_info: n_embd_k_gqa     = 2048
0.00.038.922 I print_info: n_embd_v_gqa     = 2048
0.00.038.923 I print_info: f_norm_eps       = 1.0e-05
0.00.038.923 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.923 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.924 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.924 I print_info: f_logit_scale    = 0.0e+00
0.00.038.924 I print_info: n_ff             = 8192
0.00.038.925 I print_info: n_expert         = 0
0.00.038.925 I print_info: n_expert_used    = 0
0.00.038.925 I print_info: causal attn      = 1
0.00.038.925 I print_info: pooling type     = 0
0.00.038.926 I print_info: rope type        = 2
0.00.038.928 I print_info: rope scaling     = linear
0.00.038.928 I print_info: freq_base_train  = 10000.0
0.00.038.928 I print_info: freq_scale_train = 1
0.00.038.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.929 I print_info: rope_finetuned   = unknown
0.00.038.929 I print_info: ssm_d_conv       = 0
0.00.038.929 I print_info: ssm_d_inner      = 0
0.00.038.929 I print_info: ssm_d_state      = 0
0.00.038.929 I print_info: ssm_dt_rank      = 0
0.00.038.929 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.930 I print_info: model type       = 1.4B
0.00.038.934 I print_info: model params     = 1.41 B
0.00.038.934 I print_info: general.name     = 1.4B
0.00.038.935 I print_info: vocab type       = BPE
0.00.038.935 I print_info: n_vocab          = 50304
0.00.038.935 I print_info: n_merges         = 50009
0.00.038.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.936 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.936 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: LF token         = 187 'Ċ'
0.00.038.937 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: max token length = 1024
0.00.038.938 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.394 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.401 I load_tensors: offloading output layer to GPU
0.00.608.402 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.434 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.608.435 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.100 I llama_init_from_model: n_seq_max     = 1
0.00.610.103 I llama_init_from_model: n_ctx         = 2048
0.00.610.104 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.104 I llama_init_from_model: n_batch       = 2048
0.00.610.105 I llama_init_from_model: n_ubatch      = 512
0.00.610.105 I llama_init_from_model: flash_attn    = 0
0.00.610.107 I llama_init_from_model: freq_base     = 10000.0
0.00.610.108 I llama_init_from_model: freq_scale    = 1
0.00.610.110 I ggml_metal_init: allocating
0.00.610.194 I ggml_metal_init: found device: Apple M4
0.00.610.209 I ggml_metal_init: picking default device: Apple M4
0.00.611.885 I ggml_metal_init: using embedded metal library
0.00.618.355 I ggml_metal_init: GPU name:   Apple M4
0.00.618.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.359 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.361 I ggml_metal_init: simdgroup reduction   = true
0.00.618.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.361 I ggml_metal_init: has residency sets    = true
0.00.618.361 I ggml_metal_init: has bfloat            = true
0.00.618.362 I ggml_metal_init: use bfloat            = true
0.00.618.363 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.909 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.692.331 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.692.338 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.692.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.697.223 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.697.225 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.697.225 I llama_init_from_model: graph nodes  = 967
0.00.697.226 I llama_init_from_model: graph splits = 2
0.00.697.236 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.697.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.697.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.188 I main: llama threadpool init, n_threads = 4
0.00.757.228 I 
0.00.757.250 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.252 I 
0.00.757.424 I sampler seed: 1234
0.00.757.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.448 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.609.194 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47144.75 tokens per second)
0.01.609.194 I llama_perf_context_print:        load time =     747.72 ms
0.01.609.195 I llama_perf_context_print: prompt eval time =      52.00 ms /     7 tokens (    7.43 ms per token,   134.62 tokens per second)
0.01.609.197 I llama_perf_context_print:        eval time =     797.19 ms /    63 runs   (   12.65 ms per token,    79.03 tokens per second)
0.01.609.197 I llama_perf_context_print:       total time =     852.70 ms /    70 tokens
0.01.609.487 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.528 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.384 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.390 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.391 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.392 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.393 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.398 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.202 I llama_model_loader: - type  f32:  194 tensors
0.00.026.202 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.202 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.203 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.203 I print_info: file format = GGUF V3 (latest)
0.00.026.204 I print_info: file type   = Q2_K - Medium
0.00.026.205 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.547 I load: special tokens cache size = 25
0.00.040.558 I load: token to piece cache size = 0.2984 MB
0.00.040.562 I print_info: arch             = gptneox
0.00.040.562 I print_info: vocab_only       = 0
0.00.040.562 I print_info: n_ctx_train      = 2048
0.00.040.563 I print_info: n_embd           = 2048
0.00.040.563 I print_info: n_layer          = 24
0.00.040.567 I print_info: n_head           = 16
0.00.040.568 I print_info: n_head_kv        = 16
0.00.040.568 I print_info: n_rot            = 32
0.00.040.569 I print_info: n_swa            = 0
0.00.040.569 I print_info: n_embd_head_k    = 128
0.00.040.569 I print_info: n_embd_head_v    = 128
0.00.040.570 I print_info: n_gqa            = 1
0.00.040.570 I print_info: n_embd_k_gqa     = 2048
0.00.040.574 I print_info: n_embd_v_gqa     = 2048
0.00.040.574 I print_info: f_norm_eps       = 1.0e-05
0.00.040.574 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.575 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.575 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.575 I print_info: f_logit_scale    = 0.0e+00
0.00.040.575 I print_info: n_ff             = 8192
0.00.040.576 I print_info: n_expert         = 0
0.00.040.577 I print_info: n_expert_used    = 0
0.00.040.577 I print_info: causal attn      = 1
0.00.040.577 I print_info: pooling type     = 0
0.00.040.577 I print_info: rope type        = 2
0.00.040.577 I print_info: rope scaling     = linear
0.00.040.578 I print_info: freq_base_train  = 10000.0
0.00.040.578 I print_info: freq_scale_train = 1
0.00.040.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.578 I print_info: rope_finetuned   = unknown
0.00.040.578 I print_info: ssm_d_conv       = 0
0.00.040.580 I print_info: ssm_d_inner      = 0
0.00.040.580 I print_info: ssm_d_state      = 0
0.00.040.580 I print_info: ssm_dt_rank      = 0
0.00.040.580 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.580 I print_info: model type       = 1.4B
0.00.040.581 I print_info: model params     = 1.41 B
0.00.040.581 I print_info: general.name     = 1.4B
0.00.040.581 I print_info: vocab type       = BPE
0.00.040.583 I print_info: n_vocab          = 50304
0.00.040.583 I print_info: n_merges         = 50009
0.00.040.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.583 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.584 I print_info: LF token         = 187 'Ċ'
0.00.040.585 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.585 I print_info: max token length = 1024
0.00.040.585 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.388.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.388.595 I load_tensors: offloading output layer to GPU
0.00.388.595 I load_tensors: offloaded 25/25 layers to GPU
0.00.388.608 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.388.609 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.389.543 I llama_init_from_model: n_seq_max     = 1
0.00.389.546 I llama_init_from_model: n_ctx         = 2048
0.00.389.547 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.389.547 I llama_init_from_model: n_batch       = 2048
0.00.389.547 I llama_init_from_model: n_ubatch      = 512
0.00.389.548 I llama_init_from_model: flash_attn    = 0
0.00.389.549 I llama_init_from_model: freq_base     = 10000.0
0.00.389.549 I llama_init_from_model: freq_scale    = 1
0.00.389.550 I ggml_metal_init: allocating
0.00.389.593 I ggml_metal_init: found device: Apple M4
0.00.389.604 I ggml_metal_init: picking default device: Apple M4
0.00.390.668 I ggml_metal_init: using embedded metal library
0.00.395.074 I ggml_metal_init: GPU name:   Apple M4
0.00.395.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.395.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.395.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.395.083 I ggml_metal_init: simdgroup reduction   = true
0.00.395.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.395.083 I ggml_metal_init: has residency sets    = true
0.00.395.083 I ggml_metal_init: has bfloat            = true
0.00.395.084 I ggml_metal_init: use bfloat            = true
0.00.395.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.395.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.412.671 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.441.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.441.331 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.441.351 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.445.739 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.445.741 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.445.741 I llama_init_from_model: graph nodes  = 967
0.00.445.741 I llama_init_from_model: graph splits = 2
0.00.445.747 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.445.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.445.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.824 I main: llama threadpool init, n_threads = 4
0.00.500.857 I 
0.00.500.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.874 I 
0.00.501.009 I sampler seed: 1234
0.00.501.013 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.023 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.023 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.568 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50932.57 tokens per second)
0.01.172.569 I llama_perf_context_print:        load time =     489.60 ms
0.01.172.569 I llama_perf_context_print: prompt eval time =      35.43 ms /     7 tokens (    5.06 ms per token,   197.56 tokens per second)
0.01.172.570 I llama_perf_context_print:        eval time =     633.28 ms /    63 runs   (   10.05 ms per token,    99.48 tokens per second)
0.01.172.570 I llama_perf_context_print:       total time =     672.44 ms /    70 tokens
0.01.172.770 I ggml_metal_free: deallocating

real	0m1.193s
user	0m0.108s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.147 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.148 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.150 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.512 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.515 I llama_model_loader: - type  f32:  194 tensors
0.00.024.515 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.515 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.515 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.515 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.516 I print_info: file format = GGUF V3 (latest)
0.00.024.516 I print_info: file type   = Q3_K - Medium
0.00.024.517 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.211 I load: special tokens cache size = 25
0.00.038.314 I load: token to piece cache size = 0.2984 MB
0.00.038.317 I print_info: arch             = gptneox
0.00.038.317 I print_info: vocab_only       = 0
0.00.038.317 I print_info: n_ctx_train      = 2048
0.00.038.317 I print_info: n_embd           = 2048
0.00.038.318 I print_info: n_layer          = 24
0.00.038.320 I print_info: n_head           = 16
0.00.038.321 I print_info: n_head_kv        = 16
0.00.038.321 I print_info: n_rot            = 32
0.00.038.321 I print_info: n_swa            = 0
0.00.038.322 I print_info: n_embd_head_k    = 128
0.00.038.322 I print_info: n_embd_head_v    = 128
0.00.038.324 I print_info: n_gqa            = 1
0.00.038.324 I print_info: n_embd_k_gqa     = 2048
0.00.038.325 I print_info: n_embd_v_gqa     = 2048
0.00.038.326 I print_info: f_norm_eps       = 1.0e-05
0.00.038.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.328 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.328 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.328 I print_info: f_logit_scale    = 0.0e+00
0.00.038.329 I print_info: n_ff             = 8192
0.00.038.329 I print_info: n_expert         = 0
0.00.038.330 I print_info: n_expert_used    = 0
0.00.038.331 I print_info: causal attn      = 1
0.00.038.332 I print_info: pooling type     = 0
0.00.038.333 I print_info: rope type        = 2
0.00.038.333 I print_info: rope scaling     = linear
0.00.038.333 I print_info: freq_base_train  = 10000.0
0.00.038.334 I print_info: freq_scale_train = 1
0.00.038.334 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.334 I print_info: rope_finetuned   = unknown
0.00.038.334 I print_info: ssm_d_conv       = 0
0.00.038.334 I print_info: ssm_d_inner      = 0
0.00.038.334 I print_info: ssm_d_state      = 0
0.00.038.334 I print_info: ssm_dt_rank      = 0
0.00.038.335 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.335 I print_info: model type       = 1.4B
0.00.038.335 I print_info: model params     = 1.41 B
0.00.038.336 I print_info: general.name     = 1.4B
0.00.038.337 I print_info: vocab type       = BPE
0.00.038.337 I print_info: n_vocab          = 50304
0.00.038.337 I print_info: n_merges         = 50009
0.00.038.337 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.337 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.338 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.338 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.338 I print_info: LF token         = 187 'Ċ'
0.00.038.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: max token length = 1024
0.00.038.339 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.436.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.436.904 I load_tensors: offloading output layer to GPU
0.00.436.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.436.940 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.436.941 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.596 I llama_init_from_model: n_seq_max     = 1
0.00.438.599 I llama_init_from_model: n_ctx         = 2048
0.00.438.599 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.438.600 I llama_init_from_model: n_batch       = 2048
0.00.438.600 I llama_init_from_model: n_ubatch      = 512
0.00.438.600 I llama_init_from_model: flash_attn    = 0
0.00.438.603 I llama_init_from_model: freq_base     = 10000.0
0.00.438.603 I llama_init_from_model: freq_scale    = 1
0.00.438.608 I ggml_metal_init: allocating
0.00.438.686 I ggml_metal_init: found device: Apple M4
0.00.438.700 I ggml_metal_init: picking default device: Apple M4
0.00.440.612 I ggml_metal_init: using embedded metal library
0.00.446.121 I ggml_metal_init: GPU name:   Apple M4
0.00.446.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.129 I ggml_metal_init: simdgroup reduction   = true
0.00.446.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.130 I ggml_metal_init: has residency sets    = true
0.00.446.130 I ggml_metal_init: has bfloat            = true
0.00.446.131 I ggml_metal_init: use bfloat            = true
0.00.446.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.465.674 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.271 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.281 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.304 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.531.400 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.531.402 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.531.403 I llama_init_from_model: graph nodes  = 967
0.00.531.403 I llama_init_from_model: graph splits = 2
0.00.531.413 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.749 I main: llama threadpool init, n_threads = 4
0.00.587.794 I 
0.00.587.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.819 I 
0.00.587.994 I sampler seed: 1234
0.00.587.998 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.588.040 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.588.043 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.588.043 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.340.088 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49477.35 tokens per second)
0.01.340.089 I llama_perf_context_print:        load time =     578.27 ms
0.01.340.089 I llama_perf_context_print: prompt eval time =      49.98 ms /     7 tokens (    7.14 ms per token,   140.06 tokens per second)
0.01.340.090 I llama_perf_context_print:        eval time =     699.18 ms /    63 runs   (   11.10 ms per token,    90.11 tokens per second)
0.01.340.090 I llama_perf_context_print:       total time =     753.03 ms /    70 tokens
0.01.340.328 I ggml_metal_free: deallocating

real	0m1.355s
user	0m0.110s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.008.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.246 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.246 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.615 I llama_model_loader: - type  f32:  194 tensors
0.00.024.615 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.615 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.616 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.616 I print_info: file format = GGUF V3 (latest)
0.00.024.617 I print_info: file type   = Q4_K - Medium
0.00.024.617 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.450 I load: special tokens cache size = 25
0.00.038.679 I load: token to piece cache size = 0.2984 MB
0.00.038.682 I print_info: arch             = gptneox
0.00.038.683 I print_info: vocab_only       = 0
0.00.038.683 I print_info: n_ctx_train      = 2048
0.00.038.683 I print_info: n_embd           = 2048
0.00.038.683 I print_info: n_layer          = 24
0.00.038.686 I print_info: n_head           = 16
0.00.038.687 I print_info: n_head_kv        = 16
0.00.038.687 I print_info: n_rot            = 32
0.00.038.687 I print_info: n_swa            = 0
0.00.038.688 I print_info: n_embd_head_k    = 128
0.00.038.688 I print_info: n_embd_head_v    = 128
0.00.038.689 I print_info: n_gqa            = 1
0.00.038.690 I print_info: n_embd_k_gqa     = 2048
0.00.038.691 I print_info: n_embd_v_gqa     = 2048
0.00.038.691 I print_info: f_norm_eps       = 1.0e-05
0.00.038.692 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.692 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.692 I print_info: f_logit_scale    = 0.0e+00
0.00.038.693 I print_info: n_ff             = 8192
0.00.038.693 I print_info: n_expert         = 0
0.00.038.693 I print_info: n_expert_used    = 0
0.00.038.694 I print_info: causal attn      = 1
0.00.038.694 I print_info: pooling type     = 0
0.00.038.694 I print_info: rope type        = 2
0.00.038.694 I print_info: rope scaling     = linear
0.00.038.695 I print_info: freq_base_train  = 10000.0
0.00.038.695 I print_info: freq_scale_train = 1
0.00.038.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.695 I print_info: rope_finetuned   = unknown
0.00.038.697 I print_info: ssm_d_conv       = 0
0.00.038.697 I print_info: ssm_d_inner      = 0
0.00.038.697 I print_info: ssm_d_state      = 0
0.00.038.697 I print_info: ssm_dt_rank      = 0
0.00.038.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.698 I print_info: model type       = 1.4B
0.00.038.698 I print_info: model params     = 1.41 B
0.00.038.698 I print_info: general.name     = 1.4B
0.00.038.699 I print_info: vocab type       = BPE
0.00.038.699 I print_info: n_vocab          = 50304
0.00.038.699 I print_info: n_merges         = 50009
0.00.038.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.700 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: LF token         = 187 'Ċ'
0.00.038.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: max token length = 1024
0.00.038.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.519.778 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.790 I load_tensors: offloading output layer to GPU
0.00.519.791 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.826 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.832 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.521.195 I llama_init_from_model: n_seq_max     = 1
0.00.521.197 I llama_init_from_model: n_ctx         = 2048
0.00.521.198 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.521.199 I llama_init_from_model: n_batch       = 2048
0.00.521.199 I llama_init_from_model: n_ubatch      = 512
0.00.521.199 I llama_init_from_model: flash_attn    = 0
0.00.521.201 I llama_init_from_model: freq_base     = 10000.0
0.00.521.201 I llama_init_from_model: freq_scale    = 1
0.00.521.204 I ggml_metal_init: allocating
0.00.521.280 I ggml_metal_init: found device: Apple M4
0.00.521.294 I ggml_metal_init: picking default device: Apple M4
0.00.523.078 I ggml_metal_init: using embedded metal library
0.00.530.161 I ggml_metal_init: GPU name:   Apple M4
0.00.530.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.530.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.530.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.530.171 I ggml_metal_init: simdgroup reduction   = true
0.00.530.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.530.172 I ggml_metal_init: has residency sets    = true
0.00.530.172 I ggml_metal_init: has bfloat            = true
0.00.530.173 I ggml_metal_init: use bfloat            = true
0.00.530.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.530.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.547.802 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.608 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.604.615 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.604.638 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.563 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.608.565 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.608.565 I llama_init_from_model: graph nodes  = 967
0.00.608.565 I llama_init_from_model: graph splits = 2
0.00.608.570 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.862 I main: llama threadpool init, n_threads = 4
0.00.665.904 I 
0.00.665.926 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.926 I 
0.00.666.079 I sampler seed: 1234
0.00.666.084 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.133 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.136 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.136 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.436.587 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.436.588 I llama_perf_context_print:        load time =     656.31 ms
0.01.436.589 I llama_perf_context_print: prompt eval time =      56.95 ms /     7 tokens (    8.14 ms per token,   122.91 tokens per second)
0.01.436.590 I llama_perf_context_print:        eval time =     710.52 ms /    63 runs   (   11.28 ms per token,    88.67 tokens per second)
0.01.436.590 I llama_perf_context_print:       total time =     771.42 ms /    70 tokens
0.01.436.876 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.011.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.730 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.338 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.339 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.339 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.340 I llama_model_loader: - type  f32:  194 tensors
0.00.027.340 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.341 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.341 I print_info: file format = GGUF V3 (latest)
0.00.027.342 I print_info: file type   = Q5_K - Medium
0.00.027.345 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.003 I load: special tokens cache size = 25
0.00.040.891 I load: token to piece cache size = 0.2984 MB
0.00.040.894 I print_info: arch             = gptneox
0.00.040.894 I print_info: vocab_only       = 0
0.00.040.894 I print_info: n_ctx_train      = 2048
0.00.040.894 I print_info: n_embd           = 2048
0.00.040.895 I print_info: n_layer          = 24
0.00.040.897 I print_info: n_head           = 16
0.00.040.900 I print_info: n_head_kv        = 16
0.00.040.900 I print_info: n_rot            = 32
0.00.040.900 I print_info: n_swa            = 0
0.00.040.900 I print_info: n_embd_head_k    = 128
0.00.040.900 I print_info: n_embd_head_v    = 128
0.00.040.903 I print_info: n_gqa            = 1
0.00.040.904 I print_info: n_embd_k_gqa     = 2048
0.00.040.904 I print_info: n_embd_v_gqa     = 2048
0.00.040.905 I print_info: f_norm_eps       = 1.0e-05
0.00.040.906 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.907 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.907 I print_info: f_logit_scale    = 0.0e+00
0.00.040.908 I print_info: n_ff             = 8192
0.00.040.908 I print_info: n_expert         = 0
0.00.040.908 I print_info: n_expert_used    = 0
0.00.040.908 I print_info: causal attn      = 1
0.00.040.908 I print_info: pooling type     = 0
0.00.040.910 I print_info: rope type        = 2
0.00.040.911 I print_info: rope scaling     = linear
0.00.040.912 I print_info: freq_base_train  = 10000.0
0.00.040.912 I print_info: freq_scale_train = 1
0.00.040.912 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.912 I print_info: rope_finetuned   = unknown
0.00.040.912 I print_info: ssm_d_conv       = 0
0.00.040.912 I print_info: ssm_d_inner      = 0
0.00.040.913 I print_info: ssm_d_state      = 0
0.00.040.913 I print_info: ssm_dt_rank      = 0
0.00.040.913 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.916 I print_info: model type       = 1.4B
0.00.040.917 I print_info: model params     = 1.41 B
0.00.040.917 I print_info: general.name     = 1.4B
0.00.040.917 I print_info: vocab type       = BPE
0.00.040.918 I print_info: n_vocab          = 50304
0.00.040.918 I print_info: n_merges         = 50009
0.00.040.918 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.918 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.918 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.919 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.919 I print_info: LF token         = 187 'Ċ'
0.00.040.919 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.923 I print_info: max token length = 1024
0.00.040.924 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.603 I load_tensors: offloading output layer to GPU
0.00.589.604 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.638 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.640 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.356 I llama_init_from_model: n_seq_max     = 1
0.00.591.359 I llama_init_from_model: n_ctx         = 2048
0.00.591.359 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.360 I llama_init_from_model: n_batch       = 2048
0.00.591.360 I llama_init_from_model: n_ubatch      = 512
0.00.591.361 I llama_init_from_model: flash_attn    = 0
0.00.591.363 I llama_init_from_model: freq_base     = 10000.0
0.00.591.363 I llama_init_from_model: freq_scale    = 1
0.00.591.366 I ggml_metal_init: allocating
0.00.591.442 I ggml_metal_init: found device: Apple M4
0.00.591.455 I ggml_metal_init: picking default device: Apple M4
0.00.593.039 I ggml_metal_init: using embedded metal library
0.00.599.517 I ggml_metal_init: GPU name:   Apple M4
0.00.599.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.523 I ggml_metal_init: simdgroup reduction   = true
0.00.599.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.524 I ggml_metal_init: has residency sets    = true
0.00.599.524 I ggml_metal_init: has bfloat            = true
0.00.599.524 I ggml_metal_init: use bfloat            = true
0.00.599.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.597 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.498 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.668 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.670 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.670 I llama_init_from_model: graph nodes  = 967
0.00.676.670 I llama_init_from_model: graph splits = 2
0.00.676.676 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.372 I main: llama threadpool init, n_threads = 4
0.00.736.416 I 
0.00.736.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.440 I 
0.00.736.618 I sampler seed: 1234
0.00.736.623 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.634 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.634 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.636 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.585.857 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.585.858 I llama_perf_context_print:        load time =     724.23 ms
0.01.585.858 I llama_perf_context_print: prompt eval time =      51.22 ms /     7 tokens (    7.32 ms per token,   136.68 tokens per second)
0.01.585.859 I llama_perf_context_print:        eval time =     795.04 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.585.859 I llama_perf_context_print:       total time =     850.18 ms /    70 tokens
0.01.586.125 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.626 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.628 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.897 I llama_model_loader: - type  f32:  194 tensors
0.00.024.898 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.898 I print_info: file format = GGUF V3 (latest)
0.00.024.899 I print_info: file type   = Q6_K
0.00.024.900 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.653 I load: special tokens cache size = 25
0.00.038.591 I load: token to piece cache size = 0.2984 MB
0.00.038.594 I print_info: arch             = gptneox
0.00.038.594 I print_info: vocab_only       = 0
0.00.038.594 I print_info: n_ctx_train      = 2048
0.00.038.595 I print_info: n_embd           = 2048
0.00.038.595 I print_info: n_layer          = 24
0.00.038.598 I print_info: n_head           = 16
0.00.038.599 I print_info: n_head_kv        = 16
0.00.038.599 I print_info: n_rot            = 32
0.00.038.599 I print_info: n_swa            = 0
0.00.038.601 I print_info: n_embd_head_k    = 128
0.00.038.601 I print_info: n_embd_head_v    = 128
0.00.038.602 I print_info: n_gqa            = 1
0.00.038.602 I print_info: n_embd_k_gqa     = 2048
0.00.038.603 I print_info: n_embd_v_gqa     = 2048
0.00.038.604 I print_info: f_norm_eps       = 1.0e-05
0.00.038.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.604 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.604 I print_info: f_logit_scale    = 0.0e+00
0.00.038.605 I print_info: n_ff             = 8192
0.00.038.605 I print_info: n_expert         = 0
0.00.038.605 I print_info: n_expert_used    = 0
0.00.038.605 I print_info: causal attn      = 1
0.00.038.605 I print_info: pooling type     = 0
0.00.038.605 I print_info: rope type        = 2
0.00.038.606 I print_info: rope scaling     = linear
0.00.038.606 I print_info: freq_base_train  = 10000.0
0.00.038.606 I print_info: freq_scale_train = 1
0.00.038.606 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.607 I print_info: rope_finetuned   = unknown
0.00.038.607 I print_info: ssm_d_conv       = 0
0.00.038.607 I print_info: ssm_d_inner      = 0
0.00.038.607 I print_info: ssm_d_state      = 0
0.00.038.607 I print_info: ssm_dt_rank      = 0
0.00.038.607 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.607 I print_info: model type       = 1.4B
0.00.038.608 I print_info: model params     = 1.41 B
0.00.038.608 I print_info: general.name     = 1.4B
0.00.038.609 I print_info: vocab type       = BPE
0.00.038.609 I print_info: n_vocab          = 50304
0.00.038.610 I print_info: n_merges         = 50009
0.00.038.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.610 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.610 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: LF token         = 187 'Ċ'
0.00.038.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: max token length = 1024
0.00.038.612 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.864 I load_tensors: offloading output layer to GPU
0.00.637.865 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.900 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.902 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.552 I llama_init_from_model: n_seq_max     = 1
0.00.639.554 I llama_init_from_model: n_ctx         = 2048
0.00.639.554 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.555 I llama_init_from_model: n_batch       = 2048
0.00.639.556 I llama_init_from_model: n_ubatch      = 512
0.00.639.556 I llama_init_from_model: flash_attn    = 0
0.00.639.557 I llama_init_from_model: freq_base     = 10000.0
0.00.639.557 I llama_init_from_model: freq_scale    = 1
0.00.639.559 I ggml_metal_init: allocating
0.00.639.612 I ggml_metal_init: found device: Apple M4
0.00.639.622 I ggml_metal_init: picking default device: Apple M4
0.00.641.122 I ggml_metal_init: using embedded metal library
0.00.647.191 I ggml_metal_init: GPU name:   Apple M4
0.00.647.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.197 I ggml_metal_init: simdgroup reduction   = true
0.00.647.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.198 I ggml_metal_init: has residency sets    = true
0.00.647.198 I ggml_metal_init: has bfloat            = true
0.00.647.199 I ggml_metal_init: use bfloat            = true
0.00.647.199 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.927 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.979 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.001 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.121 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.124 I llama_init_from_model: graph nodes  = 967
0.00.726.124 I llama_init_from_model: graph splits = 2
0.00.726.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.253 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.193 I main: llama threadpool init, n_threads = 4
0.00.795.239 I 
0.00.795.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.262 I 
0.00.795.440 I sampler seed: 1234
0.00.795.445 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.456 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.676.043 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.676.044 I llama_perf_context_print:        load time =     785.76 ms
0.01.676.045 I llama_perf_context_print: prompt eval time =      54.11 ms /     7 tokens (    7.73 ms per token,   129.38 tokens per second)
0.01.676.046 I llama_perf_context_print:        eval time =     823.49 ms /    63 runs   (   13.07 ms per token,    76.50 tokens per second)
0.01.676.046 I llama_perf_context_print:       total time =     881.56 ms /    70 tokens
0.01.676.279 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.107s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.756 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.260 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.279 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.281 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.282 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.290 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.825 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.826 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.826 I llama_model_loader: - type  f32:  194 tensors
0.00.056.827 I llama_model_loader: - type  f16:   98 tensors
0.00.056.828 I print_info: file format = GGUF V3 (latest)
0.00.056.829 I print_info: file type   = all F32 (guessed)
0.00.056.830 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.518 I load: special tokens cache size = 25
0.00.077.938 I load: token to piece cache size = 0.2984 MB
0.00.077.941 I print_info: arch             = gptneox
0.00.077.941 I print_info: vocab_only       = 0
0.00.077.941 I print_info: n_ctx_train      = 2048
0.00.077.942 I print_info: n_embd           = 2048
0.00.077.942 I print_info: n_layer          = 24
0.00.077.945 I print_info: n_head           = 16
0.00.077.946 I print_info: n_head_kv        = 16
0.00.077.946 I print_info: n_rot            = 32
0.00.077.946 I print_info: n_swa            = 0
0.00.077.947 I print_info: n_embd_head_k    = 128
0.00.077.947 I print_info: n_embd_head_v    = 128
0.00.077.948 I print_info: n_gqa            = 1
0.00.077.948 I print_info: n_embd_k_gqa     = 2048
0.00.077.949 I print_info: n_embd_v_gqa     = 2048
0.00.077.950 I print_info: f_norm_eps       = 1.0e-05
0.00.077.950 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.952 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.953 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.953 I print_info: f_logit_scale    = 0.0e+00
0.00.077.953 I print_info: n_ff             = 8192
0.00.077.954 I print_info: n_expert         = 0
0.00.077.954 I print_info: n_expert_used    = 0
0.00.077.954 I print_info: causal attn      = 1
0.00.077.954 I print_info: pooling type     = 0
0.00.077.954 I print_info: rope type        = 2
0.00.077.956 I print_info: rope scaling     = linear
0.00.077.956 I print_info: freq_base_train  = 10000.0
0.00.077.956 I print_info: freq_scale_train = 1
0.00.077.956 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.957 I print_info: rope_finetuned   = unknown
0.00.077.957 I print_info: ssm_d_conv       = 0
0.00.077.957 I print_info: ssm_d_inner      = 0
0.00.077.957 I print_info: ssm_d_state      = 0
0.00.077.957 I print_info: ssm_dt_rank      = 0
0.00.077.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.958 I print_info: model type       = 1.4B
0.00.077.958 I print_info: model params     = 1.41 B
0.00.077.958 I print_info: general.name     = 1.4B
0.00.077.959 I print_info: vocab type       = BPE
0.00.077.959 I print_info: n_vocab          = 50304
0.00.077.959 I print_info: n_merges         = 50009
0.00.077.960 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.960 I print_info: LF token         = 187 'Ċ'
0.00.077.961 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.961 I print_info: max token length = 1024
0.00.077.961 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.415.044 I load_tensors: offloading 24 repeating layers to GPU
0.01.415.048 I load_tensors: offloading output layer to GPU
0.01.415.048 I load_tensors: offloaded 25/25 layers to GPU
0.01.415.071 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.415.072 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.415.706 I llama_init_from_model: n_seq_max     = 1
0.01.415.707 I llama_init_from_model: n_ctx         = 128
0.01.415.708 I llama_init_from_model: n_ctx_per_seq = 128
0.01.415.708 I llama_init_from_model: n_batch       = 128
0.01.415.708 I llama_init_from_model: n_ubatch      = 128
0.01.415.709 I llama_init_from_model: flash_attn    = 0
0.01.415.709 I llama_init_from_model: freq_base     = 10000.0
0.01.415.709 I llama_init_from_model: freq_scale    = 1
0.01.415.710 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.415.710 I ggml_metal_init: allocating
0.01.415.746 I ggml_metal_init: found device: Apple M4
0.01.415.751 I ggml_metal_init: picking default device: Apple M4
0.01.416.795 I ggml_metal_init: using embedded metal library
0.01.420.754 I ggml_metal_init: GPU name:   Apple M4
0.01.420.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.420.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.420.757 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.420.758 I ggml_metal_init: simdgroup reduction   = true
0.01.420.758 I ggml_metal_init: simdgroup matrix mul. = true
0.01.420.758 I ggml_metal_init: has residency sets    = true
0.01.420.758 I ggml_metal_init: has bfloat            = true
0.01.420.758 I ggml_metal_init: use bfloat            = true
0.01.420.759 I ggml_metal_init: hasUnifiedMemory      = true
0.01.420.762 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.431.526 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.433.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.433.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.433.253 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.434.915 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.434.916 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.434.916 I llama_init_from_model: graph nodes  = 967
0.01.434.917 I llama_init_from_model: graph splits = 2
0.01.434.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.434.918 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.468.620 I 
0.01.468.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.468.659 I perplexity: tokenizing the input ..
0.01.473.654 I perplexity: tokenization took 4.994 ms
0.01.473.659 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.592.738 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.595.479 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.595.535 I llama_perf_context_print:        load time =    1443.71 ms
0.01.595.537 I llama_perf_context_print: prompt eval time =     118.81 ms /   128 tokens (    0.93 ms per token,  1077.36 tokens per second)
0.01.595.538 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.595.539 I llama_perf_context_print:       total time =     126.91 ms /   129 tokens
0.01.596.118 I ggml_metal_free: deallocating

real	0m1.786s
user	0m0.104s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.309 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.555 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.503 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.390 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.390 I llama_model_loader: - type  f32:  194 tensors
0.00.025.390 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.391 I print_info: file format = GGUF V3 (latest)
0.00.025.392 I print_info: file type   = Q8_0
0.00.025.393 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.643 I load: special tokens cache size = 25
0.00.039.890 I load: token to piece cache size = 0.2984 MB
0.00.039.894 I print_info: arch             = gptneox
0.00.039.895 I print_info: vocab_only       = 0
0.00.039.895 I print_info: n_ctx_train      = 2048
0.00.039.895 I print_info: n_embd           = 2048
0.00.039.895 I print_info: n_layer          = 24
0.00.039.900 I print_info: n_head           = 16
0.00.039.901 I print_info: n_head_kv        = 16
0.00.039.901 I print_info: n_rot            = 32
0.00.039.901 I print_info: n_swa            = 0
0.00.039.901 I print_info: n_embd_head_k    = 128
0.00.039.903 I print_info: n_embd_head_v    = 128
0.00.039.904 I print_info: n_gqa            = 1
0.00.039.904 I print_info: n_embd_k_gqa     = 2048
0.00.039.905 I print_info: n_embd_v_gqa     = 2048
0.00.039.905 I print_info: f_norm_eps       = 1.0e-05
0.00.039.906 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.906 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.906 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.908 I print_info: f_logit_scale    = 0.0e+00
0.00.039.908 I print_info: n_ff             = 8192
0.00.039.908 I print_info: n_expert         = 0
0.00.039.909 I print_info: n_expert_used    = 0
0.00.039.909 I print_info: causal attn      = 1
0.00.039.909 I print_info: pooling type     = 0
0.00.039.909 I print_info: rope type        = 2
0.00.039.910 I print_info: rope scaling     = linear
0.00.039.911 I print_info: freq_base_train  = 10000.0
0.00.039.911 I print_info: freq_scale_train = 1
0.00.039.911 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.912 I print_info: rope_finetuned   = unknown
0.00.039.912 I print_info: ssm_d_conv       = 0
0.00.039.912 I print_info: ssm_d_inner      = 0
0.00.039.912 I print_info: ssm_d_state      = 0
0.00.039.913 I print_info: ssm_dt_rank      = 0
0.00.039.913 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.914 I print_info: model type       = 1.4B
0.00.039.914 I print_info: model params     = 1.41 B
0.00.039.914 I print_info: general.name     = 1.4B
0.00.039.915 I print_info: vocab type       = BPE
0.00.039.916 I print_info: n_vocab          = 50304
0.00.039.916 I print_info: n_merges         = 50009
0.00.039.916 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.917 I print_info: LF token         = 187 'Ċ'
0.00.039.917 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.917 I print_info: max token length = 1024
0.00.039.917 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.922.481 I load_tensors: offloading 24 repeating layers to GPU
0.00.922.486 I load_tensors: offloading output layer to GPU
0.00.922.486 I load_tensors: offloaded 25/25 layers to GPU
0.00.922.507 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.922.508 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.923.478 I llama_init_from_model: n_seq_max     = 1
0.00.923.483 I llama_init_from_model: n_ctx         = 128
0.00.923.483 I llama_init_from_model: n_ctx_per_seq = 128
0.00.923.483 I llama_init_from_model: n_batch       = 128
0.00.923.484 I llama_init_from_model: n_ubatch      = 128
0.00.923.484 I llama_init_from_model: flash_attn    = 0
0.00.923.485 I llama_init_from_model: freq_base     = 10000.0
0.00.923.486 I llama_init_from_model: freq_scale    = 1
0.00.923.486 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.923.487 I ggml_metal_init: allocating
0.00.923.534 I ggml_metal_init: found device: Apple M4
0.00.923.545 I ggml_metal_init: picking default device: Apple M4
0.00.924.598 I ggml_metal_init: using embedded metal library
0.00.928.778 I ggml_metal_init: GPU name:   Apple M4
0.00.928.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.928.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.928.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.928.783 I ggml_metal_init: simdgroup reduction   = true
0.00.928.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.928.783 I ggml_metal_init: has residency sets    = true
0.00.928.783 I ggml_metal_init: has bfloat            = true
0.00.928.783 I ggml_metal_init: use bfloat            = true
0.00.928.784 I ggml_metal_init: hasUnifiedMemory      = true
0.00.928.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.939.090 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.940.807 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.940.810 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.940.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.942.452 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.942.453 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.942.453 I llama_init_from_model: graph nodes  = 967
0.00.942.453 I llama_init_from_model: graph splits = 2
0.00.942.454 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.942.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.967.627 I 
0.00.967.664 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.967.667 I perplexity: tokenizing the input ..
0.00.971.433 I perplexity: tokenization took 3.764 ms
0.00.971.436 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.105.691 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.107.597 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.107.624 I llama_perf_context_print:        load time =     958.31 ms
0.01.107.625 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   955.02 tokens per second)
0.01.107.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.107.626 I llama_perf_context_print:       total time =     140.00 ms /   129 tokens
0.01.108.006 I ggml_metal_free: deallocating

real	0m1.126s
user	0m0.067s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.331 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.869 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.870 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.879 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.882 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.358 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.359 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.360 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.360 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.361 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.361 I llama_model_loader: - type  f32:  194 tensors
0.00.027.361 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.362 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.362 I print_info: file format = GGUF V3 (latest)
0.00.027.363 I print_info: file type   = Q4_0
0.00.027.368 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.192 I load: special tokens cache size = 25
0.00.041.220 I load: token to piece cache size = 0.2984 MB
0.00.041.224 I print_info: arch             = gptneox
0.00.041.224 I print_info: vocab_only       = 0
0.00.041.225 I print_info: n_ctx_train      = 2048
0.00.041.225 I print_info: n_embd           = 2048
0.00.041.225 I print_info: n_layer          = 24
0.00.041.229 I print_info: n_head           = 16
0.00.041.230 I print_info: n_head_kv        = 16
0.00.041.232 I print_info: n_rot            = 32
0.00.041.233 I print_info: n_swa            = 0
0.00.041.233 I print_info: n_embd_head_k    = 128
0.00.041.233 I print_info: n_embd_head_v    = 128
0.00.041.234 I print_info: n_gqa            = 1
0.00.041.234 I print_info: n_embd_k_gqa     = 2048
0.00.041.235 I print_info: n_embd_v_gqa     = 2048
0.00.041.236 I print_info: f_norm_eps       = 1.0e-05
0.00.041.236 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.236 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.236 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.237 I print_info: f_logit_scale    = 0.0e+00
0.00.041.237 I print_info: n_ff             = 8192
0.00.041.237 I print_info: n_expert         = 0
0.00.041.237 I print_info: n_expert_used    = 0
0.00.041.238 I print_info: causal attn      = 1
0.00.041.250 I print_info: pooling type     = 0
0.00.041.252 I print_info: rope type        = 2
0.00.041.253 I print_info: rope scaling     = linear
0.00.041.253 I print_info: freq_base_train  = 10000.0
0.00.041.253 I print_info: freq_scale_train = 1
0.00.041.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.254 I print_info: rope_finetuned   = unknown
0.00.041.254 I print_info: ssm_d_conv       = 0
0.00.041.256 I print_info: ssm_d_inner      = 0
0.00.041.256 I print_info: ssm_d_state      = 0
0.00.041.256 I print_info: ssm_dt_rank      = 0
0.00.041.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.256 I print_info: model type       = 1.4B
0.00.041.258 I print_info: model params     = 1.41 B
0.00.041.259 I print_info: general.name     = 1.4B
0.00.041.259 I print_info: vocab type       = BPE
0.00.041.259 I print_info: n_vocab          = 50304
0.00.041.259 I print_info: n_merges         = 50009
0.00.041.260 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.260 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.260 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.260 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.260 I print_info: LF token         = 187 'Ċ'
0.00.041.262 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: max token length = 1024
0.00.041.263 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.596 I load_tensors: offloading output layer to GPU
0.00.591.596 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.632 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.591.634 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.592.911 I llama_init_from_model: n_seq_max     = 1
0.00.592.915 I llama_init_from_model: n_ctx         = 128
0.00.592.916 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.916 I llama_init_from_model: n_batch       = 128
0.00.592.916 I llama_init_from_model: n_ubatch      = 128
0.00.592.917 I llama_init_from_model: flash_attn    = 0
0.00.592.919 I llama_init_from_model: freq_base     = 10000.0
0.00.592.919 I llama_init_from_model: freq_scale    = 1
0.00.592.920 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.922 I ggml_metal_init: allocating
0.00.593.037 I ggml_metal_init: found device: Apple M4
0.00.593.051 I ggml_metal_init: picking default device: Apple M4
0.00.594.870 I ggml_metal_init: using embedded metal library
0.00.600.195 I ggml_metal_init: GPU name:   Apple M4
0.00.600.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.205 I ggml_metal_init: simdgroup reduction   = true
0.00.600.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.206 I ggml_metal_init: has residency sets    = true
0.00.600.206 I ggml_metal_init: has bfloat            = true
0.00.600.206 I ggml_metal_init: use bfloat            = true
0.00.600.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.133 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.743 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.769 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.068 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.070 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.071 I llama_init_from_model: graph nodes  = 967
0.00.627.071 I llama_init_from_model: graph splits = 2
0.00.627.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.793 I 
0.00.650.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.882 I perplexity: tokenizing the input ..
0.00.657.621 I perplexity: tokenization took 6.738 ms
0.00.657.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.667 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.782.075 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.782.099 I llama_perf_context_print:        load time =     639.45 ms
0.00.782.100 I llama_perf_context_print: prompt eval time =     122.49 ms /   128 tokens (    0.96 ms per token,  1044.98 tokens per second)
0.00.782.101 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.101 I llama_perf_context_print:       total time =     131.31 ms /   129 tokens
0.00.782.450 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.079s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.848 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.129 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.145 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.147 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.259 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.260 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.261 I llama_model_loader: - type  f32:  194 tensors
0.00.025.262 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.262 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.263 I print_info: file format = GGUF V3 (latest)
0.00.025.263 I print_info: file type   = Q4_1
0.00.025.265 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.267 I load: special tokens cache size = 25
0.00.039.465 I load: token to piece cache size = 0.2984 MB
0.00.039.469 I print_info: arch             = gptneox
0.00.039.470 I print_info: vocab_only       = 0
0.00.039.470 I print_info: n_ctx_train      = 2048
0.00.039.470 I print_info: n_embd           = 2048
0.00.039.470 I print_info: n_layer          = 24
0.00.039.474 I print_info: n_head           = 16
0.00.039.475 I print_info: n_head_kv        = 16
0.00.039.475 I print_info: n_rot            = 32
0.00.039.475 I print_info: n_swa            = 0
0.00.039.476 I print_info: n_embd_head_k    = 128
0.00.039.476 I print_info: n_embd_head_v    = 128
0.00.039.476 I print_info: n_gqa            = 1
0.00.039.477 I print_info: n_embd_k_gqa     = 2048
0.00.039.478 I print_info: n_embd_v_gqa     = 2048
0.00.039.478 I print_info: f_norm_eps       = 1.0e-05
0.00.039.479 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.482 I print_info: f_logit_scale    = 0.0e+00
0.00.039.483 I print_info: n_ff             = 8192
0.00.039.483 I print_info: n_expert         = 0
0.00.039.483 I print_info: n_expert_used    = 0
0.00.039.483 I print_info: causal attn      = 1
0.00.039.483 I print_info: pooling type     = 0
0.00.039.485 I print_info: rope type        = 2
0.00.039.485 I print_info: rope scaling     = linear
0.00.039.485 I print_info: freq_base_train  = 10000.0
0.00.039.486 I print_info: freq_scale_train = 1
0.00.039.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.486 I print_info: rope_finetuned   = unknown
0.00.039.486 I print_info: ssm_d_conv       = 0
0.00.039.486 I print_info: ssm_d_inner      = 0
0.00.039.486 I print_info: ssm_d_state      = 0
0.00.039.487 I print_info: ssm_dt_rank      = 0
0.00.039.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.487 I print_info: model type       = 1.4B
0.00.039.487 I print_info: model params     = 1.41 B
0.00.039.487 I print_info: general.name     = 1.4B
0.00.039.493 I print_info: vocab type       = BPE
0.00.039.494 I print_info: n_vocab          = 50304
0.00.039.494 I print_info: n_merges         = 50009
0.00.039.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: LF token         = 187 'Ċ'
0.00.039.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.496 I print_info: max token length = 1024
0.00.039.496 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.661.860 I load_tensors: offloading 24 repeating layers to GPU
0.00.661.876 I load_tensors: offloading output layer to GPU
0.00.661.877 I load_tensors: offloaded 25/25 layers to GPU
0.00.661.919 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.661.921 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.663.408 I llama_init_from_model: n_seq_max     = 1
0.00.663.411 I llama_init_from_model: n_ctx         = 128
0.00.663.412 I llama_init_from_model: n_ctx_per_seq = 128
0.00.663.412 I llama_init_from_model: n_batch       = 128
0.00.663.412 I llama_init_from_model: n_ubatch      = 128
0.00.663.413 I llama_init_from_model: flash_attn    = 0
0.00.663.416 I llama_init_from_model: freq_base     = 10000.0
0.00.663.416 I llama_init_from_model: freq_scale    = 1
0.00.663.417 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.663.419 I ggml_metal_init: allocating
0.00.663.573 I ggml_metal_init: found device: Apple M4
0.00.663.590 I ggml_metal_init: picking default device: Apple M4
0.00.665.616 I ggml_metal_init: using embedded metal library
0.00.672.315 I ggml_metal_init: GPU name:   Apple M4
0.00.672.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.323 I ggml_metal_init: simdgroup reduction   = true
0.00.672.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.323 I ggml_metal_init: has residency sets    = true
0.00.672.323 I ggml_metal_init: has bfloat            = true
0.00.672.324 I ggml_metal_init: use bfloat            = true
0.00.672.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.689.771 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.693.162 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.693.166 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.693.196 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.465 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.696.467 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.696.467 I llama_init_from_model: graph nodes  = 967
0.00.696.467 I llama_init_from_model: graph splits = 2
0.00.696.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.696.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.423 I 
0.00.727.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.502 I perplexity: tokenizing the input ..
0.00.734.271 I perplexity: tokenization took 6.767 ms
0.00.734.277 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.093 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.872.448 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.872.471 I llama_perf_context_print:        load time =     718.57 ms
0.00.872.472 I llama_perf_context_print: prompt eval time =     136.27 ms /   128 tokens (    1.06 ms per token,   939.32 tokens per second)
0.00.872.473 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.473 I llama_perf_context_print:       total time =     145.05 ms /   129 tokens
0.00.872.896 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.079s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.185 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.537 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.546 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.548 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.549 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.550 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.552 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.552 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.552 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.296 I llama_model_loader: - type  f32:  194 tensors
0.00.027.297 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.298 I print_info: file format = GGUF V3 (latest)
0.00.027.298 I print_info: file type   = Q5_0
0.00.027.299 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.503 I load: special tokens cache size = 25
0.00.041.560 I load: token to piece cache size = 0.2984 MB
0.00.041.563 I print_info: arch             = gptneox
0.00.041.563 I print_info: vocab_only       = 0
0.00.041.563 I print_info: n_ctx_train      = 2048
0.00.041.564 I print_info: n_embd           = 2048
0.00.041.564 I print_info: n_layer          = 24
0.00.041.568 I print_info: n_head           = 16
0.00.041.569 I print_info: n_head_kv        = 16
0.00.041.569 I print_info: n_rot            = 32
0.00.041.569 I print_info: n_swa            = 0
0.00.041.569 I print_info: n_embd_head_k    = 128
0.00.041.569 I print_info: n_embd_head_v    = 128
0.00.041.570 I print_info: n_gqa            = 1
0.00.041.571 I print_info: n_embd_k_gqa     = 2048
0.00.041.572 I print_info: n_embd_v_gqa     = 2048
0.00.041.572 I print_info: f_norm_eps       = 1.0e-05
0.00.041.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.573 I print_info: f_logit_scale    = 0.0e+00
0.00.041.577 I print_info: n_ff             = 8192
0.00.041.577 I print_info: n_expert         = 0
0.00.041.577 I print_info: n_expert_used    = 0
0.00.041.577 I print_info: causal attn      = 1
0.00.041.577 I print_info: pooling type     = 0
0.00.041.577 I print_info: rope type        = 2
0.00.041.578 I print_info: rope scaling     = linear
0.00.041.578 I print_info: freq_base_train  = 10000.0
0.00.041.578 I print_info: freq_scale_train = 1
0.00.041.579 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.579 I print_info: rope_finetuned   = unknown
0.00.041.579 I print_info: ssm_d_conv       = 0
0.00.041.579 I print_info: ssm_d_inner      = 0
0.00.041.579 I print_info: ssm_d_state      = 0
0.00.041.579 I print_info: ssm_dt_rank      = 0
0.00.041.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.584 I print_info: model type       = 1.4B
0.00.041.584 I print_info: model params     = 1.41 B
0.00.041.584 I print_info: general.name     = 1.4B
0.00.041.585 I print_info: vocab type       = BPE
0.00.041.585 I print_info: n_vocab          = 50304
0.00.041.586 I print_info: n_merges         = 50009
0.00.041.587 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.587 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.587 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.587 I print_info: LF token         = 187 'Ċ'
0.00.041.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.588 I print_info: max token length = 1024
0.00.041.588 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.734.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.734.856 I load_tensors: offloading output layer to GPU
0.00.734.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.734.889 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.734.896 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.736.513 I llama_init_from_model: n_seq_max     = 1
0.00.736.516 I llama_init_from_model: n_ctx         = 128
0.00.736.516 I llama_init_from_model: n_ctx_per_seq = 128
0.00.736.517 I llama_init_from_model: n_batch       = 128
0.00.736.517 I llama_init_from_model: n_ubatch      = 128
0.00.736.518 I llama_init_from_model: flash_attn    = 0
0.00.736.520 I llama_init_from_model: freq_base     = 10000.0
0.00.736.520 I llama_init_from_model: freq_scale    = 1
0.00.736.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.736.523 I ggml_metal_init: allocating
0.00.736.598 I ggml_metal_init: found device: Apple M4
0.00.736.611 I ggml_metal_init: picking default device: Apple M4
0.00.738.419 I ggml_metal_init: using embedded metal library
0.00.745.125 I ggml_metal_init: GPU name:   Apple M4
0.00.745.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.745.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.745.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.745.132 I ggml_metal_init: simdgroup reduction   = true
0.00.745.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.745.132 I ggml_metal_init: has residency sets    = true
0.00.745.133 I ggml_metal_init: has bfloat            = true
0.00.745.133 I ggml_metal_init: use bfloat            = true
0.00.745.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.745.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.483 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.958 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.765.964 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.765.995 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.187 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.769.189 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.769.189 I llama_init_from_model: graph nodes  = 967
0.00.769.190 I llama_init_from_model: graph splits = 2
0.00.769.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.769.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.624 I 
0.00.797.695 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.703 I perplexity: tokenizing the input ..
0.00.804.658 I perplexity: tokenization took 6.952 ms
0.00.804.668 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.940.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.942.186 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.942.209 I llama_perf_context_print:        load time =     786.43 ms
0.00.942.209 I llama_perf_context_print: prompt eval time =     135.19 ms /   128 tokens (    1.06 ms per token,   946.83 tokens per second)
0.00.942.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.942.210 I llama_perf_context_print:       total time =     144.59 ms /   129 tokens
0.00.942.582 I ggml_metal_free: deallocating

real	0m0.958s
user	0m0.079s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.790 I llama_model_loader: - type  f32:  194 tensors
0.00.024.791 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.792 I print_info: file format = GGUF V3 (latest)
0.00.024.792 I print_info: file type   = Q5_1
0.00.024.793 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.064 I load: special tokens cache size = 25
0.00.039.096 I load: token to piece cache size = 0.2984 MB
0.00.039.100 I print_info: arch             = gptneox
0.00.039.100 I print_info: vocab_only       = 0
0.00.039.100 I print_info: n_ctx_train      = 2048
0.00.039.100 I print_info: n_embd           = 2048
0.00.039.101 I print_info: n_layer          = 24
0.00.039.104 I print_info: n_head           = 16
0.00.039.105 I print_info: n_head_kv        = 16
0.00.039.105 I print_info: n_rot            = 32
0.00.039.106 I print_info: n_swa            = 0
0.00.039.106 I print_info: n_embd_head_k    = 128
0.00.039.106 I print_info: n_embd_head_v    = 128
0.00.039.107 I print_info: n_gqa            = 1
0.00.039.107 I print_info: n_embd_k_gqa     = 2048
0.00.039.108 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.110 I print_info: f_logit_scale    = 0.0e+00
0.00.039.110 I print_info: n_ff             = 8192
0.00.039.111 I print_info: n_expert         = 0
0.00.039.111 I print_info: n_expert_used    = 0
0.00.039.111 I print_info: causal attn      = 1
0.00.039.111 I print_info: pooling type     = 0
0.00.039.111 I print_info: rope type        = 2
0.00.039.114 I print_info: rope scaling     = linear
0.00.039.114 I print_info: freq_base_train  = 10000.0
0.00.039.115 I print_info: freq_scale_train = 1
0.00.039.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.115 I print_info: rope_finetuned   = unknown
0.00.039.115 I print_info: ssm_d_conv       = 0
0.00.039.115 I print_info: ssm_d_inner      = 0
0.00.039.115 I print_info: ssm_d_state      = 0
0.00.039.116 I print_info: ssm_dt_rank      = 0
0.00.039.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.116 I print_info: model type       = 1.4B
0.00.039.116 I print_info: model params     = 1.41 B
0.00.039.116 I print_info: general.name     = 1.4B
0.00.039.117 I print_info: vocab type       = BPE
0.00.039.117 I print_info: n_vocab          = 50304
0.00.039.117 I print_info: n_merges         = 50009
0.00.039.117 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.119 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.119 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: LF token         = 187 'Ċ'
0.00.039.120 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.120 I print_info: max token length = 1024
0.00.039.121 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.624.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.846 I load_tensors: offloading output layer to GPU
0.00.624.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.874 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.624.876 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.626.403 I llama_init_from_model: n_seq_max     = 1
0.00.626.409 I llama_init_from_model: n_ctx         = 128
0.00.626.409 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.410 I llama_init_from_model: n_batch       = 128
0.00.626.410 I llama_init_from_model: n_ubatch      = 128
0.00.626.410 I llama_init_from_model: flash_attn    = 0
0.00.626.412 I llama_init_from_model: freq_base     = 10000.0
0.00.626.412 I llama_init_from_model: freq_scale    = 1
0.00.626.413 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.415 I ggml_metal_init: allocating
0.00.626.470 I ggml_metal_init: found device: Apple M4
0.00.626.483 I ggml_metal_init: picking default device: Apple M4
0.00.628.210 I ggml_metal_init: using embedded metal library
0.00.634.753 I ggml_metal_init: GPU name:   Apple M4
0.00.634.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.634.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.634.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.634.759 I ggml_metal_init: simdgroup reduction   = true
0.00.634.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.634.759 I ggml_metal_init: has residency sets    = true
0.00.634.760 I ggml_metal_init: has bfloat            = true
0.00.634.760 I ggml_metal_init: use bfloat            = true
0.00.634.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.634.762 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.257 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.655.925 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.655.973 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.659.216 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.659.218 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.659.219 I llama_init_from_model: graph nodes  = 967
0.00.659.219 I llama_init_from_model: graph splits = 2
0.00.659.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.659.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.867 I 
0.00.689.950 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.960 I perplexity: tokenizing the input ..
0.00.697.067 I perplexity: tokenization took 7.105 ms
0.00.697.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.394 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.834.748 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.834.774 I llama_perf_context_print:        load time =     680.89 ms
0.00.834.774 I llama_perf_context_print: prompt eval time =     135.35 ms /   128 tokens (    1.06 ms per token,   945.72 tokens per second)
0.00.834.775 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.775 I llama_perf_context_print:       total time =     144.91 ms /   129 tokens
0.00.835.148 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.080s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.502 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.503 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.229 I llama_model_loader: - type  f32:  194 tensors
0.00.027.230 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.230 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.231 I print_info: file format = GGUF V3 (latest)
0.00.027.232 I print_info: file type   = Q2_K - Medium
0.00.027.233 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.289 I load: special tokens cache size = 25
0.00.041.240 I load: token to piece cache size = 0.2984 MB
0.00.041.243 I print_info: arch             = gptneox
0.00.041.243 I print_info: vocab_only       = 0
0.00.041.244 I print_info: n_ctx_train      = 2048
0.00.041.244 I print_info: n_embd           = 2048
0.00.041.244 I print_info: n_layer          = 24
0.00.041.248 I print_info: n_head           = 16
0.00.041.249 I print_info: n_head_kv        = 16
0.00.041.249 I print_info: n_rot            = 32
0.00.041.249 I print_info: n_swa            = 0
0.00.041.249 I print_info: n_embd_head_k    = 128
0.00.041.249 I print_info: n_embd_head_v    = 128
0.00.041.250 I print_info: n_gqa            = 1
0.00.041.251 I print_info: n_embd_k_gqa     = 2048
0.00.041.252 I print_info: n_embd_v_gqa     = 2048
0.00.041.252 I print_info: f_norm_eps       = 1.0e-05
0.00.041.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.253 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.253 I print_info: f_logit_scale    = 0.0e+00
0.00.041.254 I print_info: n_ff             = 8192
0.00.041.254 I print_info: n_expert         = 0
0.00.041.254 I print_info: n_expert_used    = 0
0.00.041.254 I print_info: causal attn      = 1
0.00.041.254 I print_info: pooling type     = 0
0.00.041.254 I print_info: rope type        = 2
0.00.041.256 I print_info: rope scaling     = linear
0.00.041.258 I print_info: freq_base_train  = 10000.0
0.00.041.258 I print_info: freq_scale_train = 1
0.00.041.258 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.259 I print_info: rope_finetuned   = unknown
0.00.041.259 I print_info: ssm_d_conv       = 0
0.00.041.259 I print_info: ssm_d_inner      = 0
0.00.041.259 I print_info: ssm_d_state      = 0
0.00.041.259 I print_info: ssm_dt_rank      = 0
0.00.041.259 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.259 I print_info: model type       = 1.4B
0.00.041.260 I print_info: model params     = 1.41 B
0.00.041.260 I print_info: general.name     = 1.4B
0.00.041.261 I print_info: vocab type       = BPE
0.00.041.261 I print_info: n_vocab          = 50304
0.00.041.261 I print_info: n_merges         = 50009
0.00.041.261 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.261 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: LF token         = 187 'Ċ'
0.00.041.262 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: max token length = 1024
0.00.041.264 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.350.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.350.309 I load_tensors: offloading output layer to GPU
0.00.350.309 I load_tensors: offloaded 25/25 layers to GPU
0.00.350.348 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.350.350 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.351.885 I llama_init_from_model: n_seq_max     = 1
0.00.351.889 I llama_init_from_model: n_ctx         = 128
0.00.351.890 I llama_init_from_model: n_ctx_per_seq = 128
0.00.351.890 I llama_init_from_model: n_batch       = 128
0.00.351.890 I llama_init_from_model: n_ubatch      = 128
0.00.351.891 I llama_init_from_model: flash_attn    = 0
0.00.351.893 I llama_init_from_model: freq_base     = 10000.0
0.00.351.893 I llama_init_from_model: freq_scale    = 1
0.00.351.894 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.351.896 I ggml_metal_init: allocating
0.00.351.968 I ggml_metal_init: found device: Apple M4
0.00.351.981 I ggml_metal_init: picking default device: Apple M4
0.00.353.703 I ggml_metal_init: using embedded metal library
0.00.359.319 I ggml_metal_init: GPU name:   Apple M4
0.00.359.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.333 I ggml_metal_init: simdgroup reduction   = true
0.00.359.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.334 I ggml_metal_init: has residency sets    = true
0.00.359.334 I ggml_metal_init: has bfloat            = true
0.00.359.334 I ggml_metal_init: use bfloat            = true
0.00.359.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.562 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.384.242 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.384.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.384.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.387.662 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.387.664 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.387.664 I llama_init_from_model: graph nodes  = 967
0.00.387.665 I llama_init_from_model: graph splits = 2
0.00.387.668 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.387.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.423 I 
0.00.418.504 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.511 I perplexity: tokenizing the input ..
0.00.424.184 I perplexity: tokenization took 5.672 ms
0.00.424.188 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.556.264 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.557.599 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.557.622 I llama_perf_context_print:        load time =     406.66 ms
0.00.557.623 I llama_perf_context_print: prompt eval time =     131.85 ms /   128 tokens (    1.03 ms per token,   970.80 tokens per second)
0.00.557.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.557.624 I llama_perf_context_print:       total time =     139.20 ms /   129 tokens
0.00.558.000 I ggml_metal_free: deallocating

real	0m0.574s
user	0m0.079s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.879 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.162 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.163 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.163 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.819 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.820 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.820 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.821 I print_info: file format = GGUF V3 (latest)
0.00.024.822 I print_info: file type   = Q3_K - Medium
0.00.024.823 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.955 I load: special tokens cache size = 25
0.00.039.051 I load: token to piece cache size = 0.2984 MB
0.00.039.054 I print_info: arch             = gptneox
0.00.039.054 I print_info: vocab_only       = 0
0.00.039.054 I print_info: n_ctx_train      = 2048
0.00.039.054 I print_info: n_embd           = 2048
0.00.039.055 I print_info: n_layer          = 24
0.00.039.059 I print_info: n_head           = 16
0.00.039.059 I print_info: n_head_kv        = 16
0.00.039.060 I print_info: n_rot            = 32
0.00.039.060 I print_info: n_swa            = 0
0.00.039.060 I print_info: n_embd_head_k    = 128
0.00.039.060 I print_info: n_embd_head_v    = 128
0.00.039.061 I print_info: n_gqa            = 1
0.00.039.062 I print_info: n_embd_k_gqa     = 2048
0.00.039.062 I print_info: n_embd_v_gqa     = 2048
0.00.039.063 I print_info: f_norm_eps       = 1.0e-05
0.00.039.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.067 I print_info: f_logit_scale    = 0.0e+00
0.00.039.068 I print_info: n_ff             = 8192
0.00.039.068 I print_info: n_expert         = 0
0.00.039.068 I print_info: n_expert_used    = 0
0.00.039.068 I print_info: causal attn      = 1
0.00.039.068 I print_info: pooling type     = 0
0.00.039.068 I print_info: rope type        = 2
0.00.039.069 I print_info: rope scaling     = linear
0.00.039.069 I print_info: freq_base_train  = 10000.0
0.00.039.071 I print_info: freq_scale_train = 1
0.00.039.071 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.071 I print_info: rope_finetuned   = unknown
0.00.039.071 I print_info: ssm_d_conv       = 0
0.00.039.071 I print_info: ssm_d_inner      = 0
0.00.039.071 I print_info: ssm_d_state      = 0
0.00.039.072 I print_info: ssm_dt_rank      = 0
0.00.039.072 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.072 I print_info: model type       = 1.4B
0.00.039.072 I print_info: model params     = 1.41 B
0.00.039.073 I print_info: general.name     = 1.4B
0.00.039.073 I print_info: vocab type       = BPE
0.00.039.073 I print_info: n_vocab          = 50304
0.00.039.073 I print_info: n_merges         = 50009
0.00.039.077 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: LF token         = 187 'Ċ'
0.00.039.078 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: max token length = 1024
0.00.039.079 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.667 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.682 I load_tensors: offloading output layer to GPU
0.00.437.683 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.721 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.722 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.439.508 I llama_init_from_model: n_seq_max     = 1
0.00.439.511 I llama_init_from_model: n_ctx         = 128
0.00.439.512 I llama_init_from_model: n_ctx_per_seq = 128
0.00.439.512 I llama_init_from_model: n_batch       = 128
0.00.439.512 I llama_init_from_model: n_ubatch      = 128
0.00.439.513 I llama_init_from_model: flash_attn    = 0
0.00.439.515 I llama_init_from_model: freq_base     = 10000.0
0.00.439.516 I llama_init_from_model: freq_scale    = 1
0.00.439.517 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.439.522 I ggml_metal_init: allocating
0.00.439.642 I ggml_metal_init: found device: Apple M4
0.00.439.654 I ggml_metal_init: picking default device: Apple M4
0.00.441.581 I ggml_metal_init: using embedded metal library
0.00.447.909 I ggml_metal_init: GPU name:   Apple M4
0.00.447.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.447.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.447.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.447.918 I ggml_metal_init: simdgroup reduction   = true
0.00.447.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.447.919 I ggml_metal_init: has residency sets    = true
0.00.447.919 I ggml_metal_init: has bfloat            = true
0.00.447.919 I ggml_metal_init: use bfloat            = true
0.00.447.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.447.922 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.192 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.692 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.469.696 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.469.724 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.472.883 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.472.885 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.472.885 I llama_init_from_model: graph nodes  = 967
0.00.472.886 I llama_init_from_model: graph splits = 2
0.00.472.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.472.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.715 I 
0.00.498.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.804 I perplexity: tokenizing the input ..
0.00.506.002 I perplexity: tokenization took 7.195 ms
0.00.506.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.639.224 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.640.547 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.640.572 I llama_perf_context_print:        load time =     489.83 ms
0.00.640.573 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.37 tokens per second)
0.00.640.573 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.640.574 I llama_perf_context_print:       total time =     141.86 ms /   129 tokens
0.00.640.943 I ggml_metal_free: deallocating

real	0m0.654s
user	0m0.079s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.741 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.791 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.603 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.361 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.365 I llama_model_loader: - type  f32:  194 tensors
0.00.024.365 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.365 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.365 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.366 I print_info: file format = GGUF V3 (latest)
0.00.024.367 I print_info: file type   = Q4_K - Medium
0.00.024.368 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.288 I load: special tokens cache size = 25
0.00.038.440 I load: token to piece cache size = 0.2984 MB
0.00.038.445 I print_info: arch             = gptneox
0.00.038.445 I print_info: vocab_only       = 0
0.00.038.445 I print_info: n_ctx_train      = 2048
0.00.038.445 I print_info: n_embd           = 2048
0.00.038.445 I print_info: n_layer          = 24
0.00.038.449 I print_info: n_head           = 16
0.00.038.450 I print_info: n_head_kv        = 16
0.00.038.451 I print_info: n_rot            = 32
0.00.038.451 I print_info: n_swa            = 0
0.00.038.451 I print_info: n_embd_head_k    = 128
0.00.038.451 I print_info: n_embd_head_v    = 128
0.00.038.452 I print_info: n_gqa            = 1
0.00.038.453 I print_info: n_embd_k_gqa     = 2048
0.00.038.453 I print_info: n_embd_v_gqa     = 2048
0.00.038.454 I print_info: f_norm_eps       = 1.0e-05
0.00.038.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.458 I print_info: f_logit_scale    = 0.0e+00
0.00.038.459 I print_info: n_ff             = 8192
0.00.038.459 I print_info: n_expert         = 0
0.00.038.459 I print_info: n_expert_used    = 0
0.00.038.459 I print_info: causal attn      = 1
0.00.038.459 I print_info: pooling type     = 0
0.00.038.459 I print_info: rope type        = 2
0.00.038.460 I print_info: rope scaling     = linear
0.00.038.460 I print_info: freq_base_train  = 10000.0
0.00.038.460 I print_info: freq_scale_train = 1
0.00.038.461 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.461 I print_info: rope_finetuned   = unknown
0.00.038.461 I print_info: ssm_d_conv       = 0
0.00.038.461 I print_info: ssm_d_inner      = 0
0.00.038.461 I print_info: ssm_d_state      = 0
0.00.038.462 I print_info: ssm_dt_rank      = 0
0.00.038.462 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.462 I print_info: model type       = 1.4B
0.00.038.462 I print_info: model params     = 1.41 B
0.00.038.463 I print_info: general.name     = 1.4B
0.00.038.463 I print_info: vocab type       = BPE
0.00.038.463 I print_info: n_vocab          = 50304
0.00.038.463 I print_info: n_merges         = 50009
0.00.038.464 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.465 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.465 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.466 I print_info: LF token         = 187 'Ċ'
0.00.038.466 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.466 I print_info: max token length = 1024
0.00.038.467 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.533.945 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.956 I load_tensors: offloading output layer to GPU
0.00.533.956 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.986 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.988 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.535.638 I llama_init_from_model: n_seq_max     = 1
0.00.535.643 I llama_init_from_model: n_ctx         = 128
0.00.535.644 I llama_init_from_model: n_ctx_per_seq = 128
0.00.535.644 I llama_init_from_model: n_batch       = 128
0.00.535.645 I llama_init_from_model: n_ubatch      = 128
0.00.535.645 I llama_init_from_model: flash_attn    = 0
0.00.535.646 I llama_init_from_model: freq_base     = 10000.0
0.00.535.647 I llama_init_from_model: freq_scale    = 1
0.00.535.648 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.535.650 I ggml_metal_init: allocating
0.00.535.705 I ggml_metal_init: found device: Apple M4
0.00.535.722 I ggml_metal_init: picking default device: Apple M4
0.00.537.398 I ggml_metal_init: using embedded metal library
0.00.543.513 I ggml_metal_init: GPU name:   Apple M4
0.00.543.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.522 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.522 I ggml_metal_init: simdgroup reduction   = true
0.00.543.523 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.523 I ggml_metal_init: has residency sets    = true
0.00.543.523 I ggml_metal_init: has bfloat            = true
0.00.543.524 I ggml_metal_init: use bfloat            = true
0.00.543.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.539 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.193 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.565.912 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.565.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.565.956 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.569.264 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.569.266 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.569.266 I llama_init_from_model: graph nodes  = 967
0.00.569.267 I llama_init_from_model: graph splits = 2
0.00.569.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.569.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.404 I 
0.00.602.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.491 I perplexity: tokenizing the input ..
0.00.609.604 I perplexity: tokenization took 7.111 ms
0.00.609.614 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.755.134 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.756.471 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.756.496 I llama_perf_context_print:        load time =     593.65 ms
0.00.756.497 I llama_perf_context_print: prompt eval time =     144.59 ms /   128 tokens (    1.13 ms per token,   885.28 tokens per second)
0.00.756.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.756.498 I llama_perf_context_print:       total time =     154.10 ms /   129 tokens
0.00.756.995 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.079s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.075 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.076 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.408 I llama_model_loader: - type  f32:  194 tensors
0.00.025.408 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.408 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.409 I print_info: file format = GGUF V3 (latest)
0.00.025.409 I print_info: file type   = Q5_K - Medium
0.00.025.410 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.115 I load: special tokens cache size = 25
0.00.039.046 I load: token to piece cache size = 0.2984 MB
0.00.039.049 I print_info: arch             = gptneox
0.00.039.049 I print_info: vocab_only       = 0
0.00.039.050 I print_info: n_ctx_train      = 2048
0.00.039.050 I print_info: n_embd           = 2048
0.00.039.050 I print_info: n_layer          = 24
0.00.039.053 I print_info: n_head           = 16
0.00.039.054 I print_info: n_head_kv        = 16
0.00.039.054 I print_info: n_rot            = 32
0.00.039.054 I print_info: n_swa            = 0
0.00.039.055 I print_info: n_embd_head_k    = 128
0.00.039.055 I print_info: n_embd_head_v    = 128
0.00.039.057 I print_info: n_gqa            = 1
0.00.039.058 I print_info: n_embd_k_gqa     = 2048
0.00.039.058 I print_info: n_embd_v_gqa     = 2048
0.00.039.059 I print_info: f_norm_eps       = 1.0e-05
0.00.039.059 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.060 I print_info: f_logit_scale    = 0.0e+00
0.00.039.060 I print_info: n_ff             = 8192
0.00.039.062 I print_info: n_expert         = 0
0.00.039.062 I print_info: n_expert_used    = 0
0.00.039.063 I print_info: causal attn      = 1
0.00.039.063 I print_info: pooling type     = 0
0.00.039.063 I print_info: rope type        = 2
0.00.039.063 I print_info: rope scaling     = linear
0.00.039.063 I print_info: freq_base_train  = 10000.0
0.00.039.064 I print_info: freq_scale_train = 1
0.00.039.064 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.064 I print_info: rope_finetuned   = unknown
0.00.039.064 I print_info: ssm_d_conv       = 0
0.00.039.064 I print_info: ssm_d_inner      = 0
0.00.039.064 I print_info: ssm_d_state      = 0
0.00.039.065 I print_info: ssm_dt_rank      = 0
0.00.039.065 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.065 I print_info: model type       = 1.4B
0.00.039.065 I print_info: model params     = 1.41 B
0.00.039.070 I print_info: general.name     = 1.4B
0.00.039.070 I print_info: vocab type       = BPE
0.00.039.070 I print_info: n_vocab          = 50304
0.00.039.070 I print_info: n_merges         = 50009
0.00.039.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: LF token         = 187 'Ċ'
0.00.039.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: max token length = 1024
0.00.039.072 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.587.678 I load_tensors: offloading 24 repeating layers to GPU
0.00.587.694 I load_tensors: offloading output layer to GPU
0.00.587.694 I load_tensors: offloaded 25/25 layers to GPU
0.00.587.725 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.587.727 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.400 I llama_init_from_model: n_seq_max     = 1
0.00.589.404 I llama_init_from_model: n_ctx         = 128
0.00.589.404 I llama_init_from_model: n_ctx_per_seq = 128
0.00.589.405 I llama_init_from_model: n_batch       = 128
0.00.589.405 I llama_init_from_model: n_ubatch      = 128
0.00.589.406 I llama_init_from_model: flash_attn    = 0
0.00.589.408 I llama_init_from_model: freq_base     = 10000.0
0.00.589.409 I llama_init_from_model: freq_scale    = 1
0.00.589.409 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.589.412 I ggml_metal_init: allocating
0.00.589.476 I ggml_metal_init: found device: Apple M4
0.00.589.490 I ggml_metal_init: picking default device: Apple M4
0.00.591.001 I ggml_metal_init: using embedded metal library
0.00.597.372 I ggml_metal_init: GPU name:   Apple M4
0.00.597.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.376 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.378 I ggml_metal_init: simdgroup reduction   = true
0.00.597.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.378 I ggml_metal_init: has residency sets    = true
0.00.597.378 I ggml_metal_init: has bfloat            = true
0.00.597.379 I ggml_metal_init: use bfloat            = true
0.00.597.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.520 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.523 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.552 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.804 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.806 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.807 I llama_init_from_model: graph nodes  = 967
0.00.620.807 I llama_init_from_model: graph splits = 2
0.00.620.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.069 I 
0.00.655.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.171 I perplexity: tokenizing the input ..
0.00.662.155 I perplexity: tokenization took 6.98 ms
0.00.662.162 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.803.975 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.316 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.338 I llama_perf_context_print:        load time =     644.59 ms
0.00.805.339 I llama_perf_context_print: prompt eval time =     140.89 ms /   128 tokens (    1.10 ms per token,   908.49 tokens per second)
0.00.805.340 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.341 I llama_perf_context_print:       total time =     150.28 ms /   129 tokens
0.00.805.747 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.557 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.375 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.377 I llama_model_loader: - type  f32:  194 tensors
0.00.025.378 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.378 I print_info: file format = GGUF V3 (latest)
0.00.025.379 I print_info: file type   = Q6_K
0.00.025.380 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.749 I load: special tokens cache size = 25
0.00.039.901 I load: token to piece cache size = 0.2984 MB
0.00.039.905 I print_info: arch             = gptneox
0.00.039.906 I print_info: vocab_only       = 0
0.00.039.906 I print_info: n_ctx_train      = 2048
0.00.039.906 I print_info: n_embd           = 2048
0.00.039.906 I print_info: n_layer          = 24
0.00.039.910 I print_info: n_head           = 16
0.00.039.913 I print_info: n_head_kv        = 16
0.00.039.913 I print_info: n_rot            = 32
0.00.039.913 I print_info: n_swa            = 0
0.00.039.914 I print_info: n_embd_head_k    = 128
0.00.039.914 I print_info: n_embd_head_v    = 128
0.00.039.914 I print_info: n_gqa            = 1
0.00.039.915 I print_info: n_embd_k_gqa     = 2048
0.00.039.916 I print_info: n_embd_v_gqa     = 2048
0.00.039.917 I print_info: f_norm_eps       = 1.0e-05
0.00.039.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.920 I print_info: f_logit_scale    = 0.0e+00
0.00.039.920 I print_info: n_ff             = 8192
0.00.039.920 I print_info: n_expert         = 0
0.00.039.921 I print_info: n_expert_used    = 0
0.00.039.922 I print_info: causal attn      = 1
0.00.039.922 I print_info: pooling type     = 0
0.00.039.922 I print_info: rope type        = 2
0.00.039.922 I print_info: rope scaling     = linear
0.00.039.923 I print_info: freq_base_train  = 10000.0
0.00.039.923 I print_info: freq_scale_train = 1
0.00.039.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.924 I print_info: rope_finetuned   = unknown
0.00.039.924 I print_info: ssm_d_conv       = 0
0.00.039.924 I print_info: ssm_d_inner      = 0
0.00.039.924 I print_info: ssm_d_state      = 0
0.00.039.924 I print_info: ssm_dt_rank      = 0
0.00.039.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.925 I print_info: model type       = 1.4B
0.00.039.925 I print_info: model params     = 1.41 B
0.00.039.926 I print_info: general.name     = 1.4B
0.00.039.929 I print_info: vocab type       = BPE
0.00.039.930 I print_info: n_vocab          = 50304
0.00.039.930 I print_info: n_merges         = 50009
0.00.039.930 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.931 I print_info: LF token         = 187 'Ċ'
0.00.039.931 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.931 I print_info: max token length = 1024
0.00.039.931 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.160 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.166 I load_tensors: offloading output layer to GPU
0.00.594.167 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.191 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.594.194 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.595.697 I llama_init_from_model: n_seq_max     = 1
0.00.595.699 I llama_init_from_model: n_ctx         = 128
0.00.595.699 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.700 I llama_init_from_model: n_batch       = 128
0.00.595.700 I llama_init_from_model: n_ubatch      = 128
0.00.595.700 I llama_init_from_model: flash_attn    = 0
0.00.595.701 I llama_init_from_model: freq_base     = 10000.0
0.00.595.702 I llama_init_from_model: freq_scale    = 1
0.00.595.703 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.704 I ggml_metal_init: allocating
0.00.595.719 I ggml_metal_init: found device: Apple M4
0.00.595.727 I ggml_metal_init: picking default device: Apple M4
0.00.597.129 I ggml_metal_init: using embedded metal library
0.00.603.101 I ggml_metal_init: GPU name:   Apple M4
0.00.603.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.106 I ggml_metal_init: simdgroup reduction   = true
0.00.603.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.107 I ggml_metal_init: has residency sets    = true
0.00.603.107 I ggml_metal_init: has bfloat            = true
0.00.603.107 I ggml_metal_init: use bfloat            = true
0.00.603.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.503 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.003 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.007 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.033 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.353 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.354 I llama_init_from_model: graph nodes  = 967
0.00.626.354 I llama_init_from_model: graph splits = 2
0.00.626.357 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.897 I 
0.00.656.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.988 I perplexity: tokenizing the input ..
0.00.664.385 I perplexity: tokenization took 7.394 ms
0.00.664.394 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.815 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.807.262 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.807.289 I llama_perf_context_print:        load time =     647.37 ms
0.00.807.290 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.78 tokens per second)
0.00.807.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.292 I llama_perf_context_print:       total time =     150.40 ms /   129 tokens
0.00.807.647 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.078s
sys	0m0.138s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.287 I build: 4673 (aaa55053) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.033 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.948 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.956 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.960 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.964 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.205 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.515 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.517 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.519 I llama_model_loader: - type  f32:  194 tensors
0.00.058.519 I llama_model_loader: - type  f16:   98 tensors
0.00.058.520 I print_info: file format = GGUF V3 (latest)
0.00.058.520 I print_info: file type   = all F32 (guessed)
0.00.058.521 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.913 I load: special tokens cache size = 25
0.00.077.157 I load: token to piece cache size = 0.2984 MB
0.00.077.160 I print_info: arch             = gptneox
0.00.077.161 I print_info: vocab_only       = 0
0.00.077.161 I print_info: n_ctx_train      = 2048
0.00.077.161 I print_info: n_embd           = 2048
0.00.077.161 I print_info: n_layer          = 24
0.00.077.164 I print_info: n_head           = 16
0.00.077.165 I print_info: n_head_kv        = 16
0.00.077.165 I print_info: n_rot            = 32
0.00.077.165 I print_info: n_swa            = 0
0.00.077.165 I print_info: n_embd_head_k    = 128
0.00.077.166 I print_info: n_embd_head_v    = 128
0.00.077.166 I print_info: n_gqa            = 1
0.00.077.167 I print_info: n_embd_k_gqa     = 2048
0.00.077.168 I print_info: n_embd_v_gqa     = 2048
0.00.077.168 I print_info: f_norm_eps       = 1.0e-05
0.00.077.169 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.171 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.171 I print_info: f_logit_scale    = 0.0e+00
0.00.077.172 I print_info: n_ff             = 8192
0.00.077.172 I print_info: n_expert         = 0
0.00.077.172 I print_info: n_expert_used    = 0
0.00.077.172 I print_info: causal attn      = 1
0.00.077.173 I print_info: pooling type     = 0
0.00.077.173 I print_info: rope type        = 2
0.00.077.174 I print_info: rope scaling     = linear
0.00.077.174 I print_info: freq_base_train  = 10000.0
0.00.077.174 I print_info: freq_scale_train = 1
0.00.077.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.175 I print_info: rope_finetuned   = unknown
0.00.077.175 I print_info: ssm_d_conv       = 0
0.00.077.175 I print_info: ssm_d_inner      = 0
0.00.077.175 I print_info: ssm_d_state      = 0
0.00.077.175 I print_info: ssm_dt_rank      = 0
0.00.077.175 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.175 I print_info: model type       = 1.4B
0.00.077.176 I print_info: model params     = 1.41 B
0.00.077.176 I print_info: general.name     = 1.4B
0.00.077.176 I print_info: vocab type       = BPE
0.00.077.177 I print_info: n_vocab          = 50304
0.00.077.177 I print_info: n_merges         = 50009
0.00.077.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.177 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.178 I print_info: LF token         = 187 'Ċ'
0.00.077.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.178 I print_info: max token length = 1024
0.00.077.179 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.376.945 I load_tensors: offloading 24 repeating layers to GPU
0.01.376.951 I load_tensors: offloading output layer to GPU
0.01.376.951 I load_tensors: offloaded 25/25 layers to GPU
0.01.376.972 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.376.974 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.377.594 I llama_init_from_model: n_seq_max     = 1
0.01.377.595 I llama_init_from_model: n_ctx         = 128
0.01.377.595 I llama_init_from_model: n_ctx_per_seq = 128
0.01.377.595 I llama_init_from_model: n_batch       = 128
0.01.377.595 I llama_init_from_model: n_ubatch      = 128
0.01.377.596 I llama_init_from_model: flash_attn    = 0
0.01.377.596 I llama_init_from_model: freq_base     = 10000.0
0.01.377.596 I llama_init_from_model: freq_scale    = 1
0.01.377.597 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.377.597 I ggml_metal_init: allocating
0.01.377.634 I ggml_metal_init: found device: Apple M4
0.01.377.640 I ggml_metal_init: picking default device: Apple M4
0.01.378.297 I ggml_metal_init: using embedded metal library
0.01.381.778 I ggml_metal_init: GPU name:   Apple M4
0.01.381.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.381.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.381.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.381.781 I ggml_metal_init: simdgroup reduction   = true
0.01.381.781 I ggml_metal_init: simdgroup matrix mul. = true
0.01.381.781 I ggml_metal_init: has residency sets    = true
0.01.381.782 I ggml_metal_init: has bfloat            = true
0.01.381.782 I ggml_metal_init: use bfloat            = true
0.01.381.782 I ggml_metal_init: hasUnifiedMemory      = true
0.01.381.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.396.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.398.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.398.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.398.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.400.291 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.400.293 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.400.293 I llama_init_from_model: graph nodes  = 967
0.01.400.293 I llama_init_from_model: graph splits = 2
0.01.400.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.400.296 I 
0.01.400.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.400.342 I compute_imatrix: tokenizing the input ..
0.01.405.784 I compute_imatrix: tokenization took 5.441 ms
0.01.405.787 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.683.549 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.685.887 I llama_perf_context_print:        load time =    1661.51 ms
0.01.685.888 I llama_perf_context_print: prompt eval time =     275.76 ms /   128 tokens (    2.15 ms per token,   464.18 tokens per second)
0.01.685.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.685.889 I llama_perf_context_print:       total time =    1663.85 ms /   129 tokens
0.01.686.407 I ggml_metal_free: deallocating

real	0m1.872s
user	0m0.142s
sys	0m0.226s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4673 (aaa55053)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1414083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141408ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141409090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141409640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141409bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14140a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14140a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14140ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14140b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14140b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14140bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14140c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14140ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14140d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14140dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14140e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14140ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14140f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14140f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1414100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141410800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141410f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141411640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141411ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141412600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1414128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141412ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141413b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141414080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141414340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1414147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141414aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141415330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141415870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141415b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141415fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141416470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141416910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141416db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141417250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1414176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141417b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141418030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1414184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141418790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141418da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1414193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141419cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14141a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14141a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14141af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14141b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14141bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14141c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14141c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14141cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14141d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14141d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14141db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14141e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14141e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14141ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14141ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14141f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14141f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14141fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1414201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141420640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141420ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141420f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141421420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1414218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141421d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1414222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141422800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141422d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1414232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1414237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141423d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141424290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1414247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141424d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141425280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1414257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141425d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141426270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1414267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141426d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141427260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1414277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141427d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141428250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1414287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141428cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141429240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141429790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141429ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1414199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14142a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14142a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14142ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14142b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14142b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14142be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14142c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14142c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14142ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14142d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14142d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14142de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14142e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14142e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14142ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14142f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14142f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14142fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141430090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141430530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1414309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141430e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141431310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1414317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141431c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1414320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141432590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141432a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141432ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141433370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141433810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141433cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141434150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1414345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141434a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141434f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1414353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141435870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141435d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1414361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141436650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141436af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141436f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141437430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1414378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141437d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141438210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1414386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141438b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141438ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141439490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141439930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141439dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14143a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14143a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14143abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14143b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14143b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14143b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14143be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14143c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14143c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14143cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14143d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14143d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14143d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14143de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14143e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14143e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14143ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14143f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14143f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14143fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14143fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141440390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141440830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141440cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141441170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141441610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141441ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141441f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1414423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141442890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141442d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1414431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141443670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141443b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141443fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141444450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1414448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141444d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141445230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1414456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141445b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141446010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141446560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141446ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141447000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141447550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141447810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141447e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141448430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141448a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141449230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1414496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141449990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141449fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14144a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14144ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14144b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14144b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14144bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14144c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14144c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14144cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14144d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14144d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14144ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14144e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14144e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14144edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14144f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14144f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14144fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1414502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141450840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141450d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1414512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141451830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141451d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1414522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141452820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141452d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1414532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141453810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141453d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1414542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141454800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141454d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1414552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1414557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141455d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141456290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1414567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141456d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141457280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1414577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141457d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141458270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1414587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141458d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141459260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1414597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141459d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14145a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14145a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14145acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14145b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14145b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14145bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14145c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14145c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14145ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14145d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14145d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14145dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14145e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14145e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14145ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14145f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14145f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14145fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14145ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1414603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141460870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141460d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1414611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141461650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141461af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141461f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141462430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1414628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141462d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141463210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141463760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141463e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1414645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141464cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1414653e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1414656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141465e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141466150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141466760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.743.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141506120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141506590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141506a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141506e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1415072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141507750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141507bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141508030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1415084a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141508910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141508d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141509470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141509f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14150a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14150af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14150b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14150bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14150c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14150cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14150d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14150da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14150e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14150e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14150ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14150f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14150f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14150fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141510090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141510500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141510970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141510de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141511310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141511780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141511a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141511eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141512320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141512790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141512c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141513070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1415134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141513950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141513dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141514230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1415146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141514b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141514f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1415153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141515860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141515cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141516140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1415165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141516a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141516e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141517300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141517770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141517be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141518150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141518650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141518ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141518f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1415193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141519810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141519c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14151a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14151a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14151a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14151ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14151b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14151b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14151bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14151c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14151c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14151c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14151cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14151d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14151d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14151daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14151df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14151e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14151e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14151ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14151f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14151f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14151f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14151fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141520290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141520700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141520b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141520fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141521450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1415218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141521d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1415221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141522610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141522a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141522ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141523360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1415237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141523c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1415240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141524520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141524990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141524e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141525270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1415256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141525b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141525fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141526430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1415268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141526d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141527180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1415275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141527a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141527ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141528340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1415287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141528c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141529090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141529500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141529970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141529de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14152a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14152a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14152ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14152afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14152b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14152b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14152bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14152c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14152c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14152ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14152ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14152d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14152d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14152dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14152e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14152e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14152e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14152edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14152f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14152f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14152fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14152ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1415303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141530860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141530cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141531140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1415315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141531a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141531e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141532300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141532770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141532be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141533050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1415334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141533930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141533da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141534210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141534680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141534af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141534f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1415353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141535840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141535cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141536120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141536590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1415371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141537480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141537740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141537bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141538020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141538490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141538900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141538d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1415391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141539650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141539ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141539f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14153a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14153a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14153ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14153b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14153b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14153b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14153be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14153c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14153c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14153cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14153d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14153d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14153d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14153dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14153e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14153e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14153eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14153ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14153f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14153f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14153fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1415400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141540540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1415409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141540f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141541420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141541890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141541d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141542170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1415425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141542b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141543010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141543b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141543e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141544400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1415449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141544f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141545540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141545b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1415460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141546680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141546c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141547200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1415477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141547d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141548340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141548900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141548ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141549480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141549a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14154a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14154a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14154ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14154b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14154b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14154bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14154c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14154c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14154ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14154d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14154d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14154df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14154e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14154eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14154f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14154f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14154fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1415501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141550780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141550d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141551300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1415518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141551e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141552440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141552a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141552fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141553580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141553b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141554100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1415546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141554c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141555240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141555800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141555dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141556380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141556940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141556f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1415574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141557a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141558040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141558540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141558a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141558f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141559440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141559940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141559e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14155a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14155a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14155ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14155b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14155b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14155bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14155c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14155c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14155cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14155d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14155dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14155e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14155eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14155ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14155f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14155f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14155fe30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141705a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141705e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141706300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141706770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141706be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141707050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1417074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141707930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141707da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1417082e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141708750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141708dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1417098f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14170a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14170a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14170afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14170b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14170be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14170c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14170cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14170d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14170db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14170e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14170e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14170f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14170f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14170f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14170fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14170ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141710370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141710870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141710d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1417111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1417114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141711920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141711d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1417122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1417127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141712cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1417131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1417136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141713bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1417140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1417145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141714af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141714f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1417153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141715840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141715cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141716120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141716590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141716a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141716e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1417172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141717750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141717f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1417183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141718680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141718c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141719480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141719920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141719dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14171a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14171a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14171aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14171b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14171b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14171b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14171be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14171c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14171c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14171cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14171d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14171d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14171db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14171e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14171e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14171eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14171f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14171f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14171fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141720070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1417205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141720b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141721060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1417215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141721b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141722050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1417225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141722af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141723040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141723590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141723ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141724030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141724580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141725020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141725570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141725ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141726010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141726560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141726ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141727000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141727550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141727aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141727ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141728540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141728fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141729a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141729fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14172a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14172a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14172ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14172b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14172b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14172bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14172c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14172c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14172ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14172cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14172d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14172d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14172dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14172e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14172e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14172ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14172ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14172f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14172f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14172fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1417301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141730640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141730ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141730f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1417318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141731d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141732200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1417326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141732b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141732fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141733480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141733920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141733dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141734260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141734700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141734ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1417354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141735980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141735e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1417362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141736760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141736c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1417370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141737540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1417379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141738320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1417387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141738c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141739100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1417395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141739a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141739ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14173a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14173a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14173acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14173b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14173b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14173baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14173bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14173c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14173c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14173cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14173d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14173d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14173db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14173dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14173e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14173e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14173ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14173f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14173f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14173fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141740000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1417404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141740940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141740de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141741280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141741720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141741c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1417421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141742710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141742c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141744150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141744940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141744de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1417450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1417456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141745cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1417464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141746950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141746df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141747290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141747a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141747f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1417484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141748a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141748f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1417494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141749a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141749f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14174a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14174aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14174af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14174b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14174ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14174bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14174c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14174c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14174cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14174d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14174d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14174df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14174e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14174e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14174ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14174f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14174f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14174ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141750460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1417509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141750f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141751450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1417519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141751ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141752990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141753980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141753ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141754420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141754ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141755410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141755eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141756400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141756950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1417573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141757e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1417583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141758930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141758e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1417593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141759920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14175a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14175a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14175ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14175b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14175b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14175bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14175bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14175c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14175c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14175cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14175d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14175d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14175db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14175dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14175e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14175e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14175ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14175f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14175fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1417603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141760db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1417615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141761860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141761e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.795s
user	0m0.279s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4673 (aaa55053)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15860b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15860bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15860c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15860c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15860cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15860d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15860d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15860dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15860e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15860e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15860ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15860f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15860fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158610510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158610d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158611440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158611b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158612280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1586129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158613170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158613fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1586146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158614f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158615690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158617110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1586173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158617b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1586183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158619500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1586199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15861a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15861a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15861ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15861b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15861b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15861b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15861be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15861c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15861cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15861d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15861d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15861df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15861e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15861ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15861f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15861f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15861fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1586202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1586205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158620bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1586213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158621b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158622450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1586228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158622d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158623230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1586236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158623b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158624010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1586244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158624950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158624df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158626330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158627320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158627dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158628310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158629850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15862a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15862a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15862ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15862b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15862b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15862bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15862c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15862c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15862cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15861ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15862d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15862d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15862dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15862e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15862e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15862eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15862f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15862f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15862fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158631ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158632340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1586327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158632c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158633120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1586335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158633a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158633f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1586343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158634840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158634ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158635620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158636400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1586368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158636d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1586371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158637fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158638900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158638da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158639240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1586396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158639b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15863a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15863a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15863a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15863ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15863b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15863b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15863bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15863c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15863c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15863c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15863ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15863d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15863d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15863dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15863e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15863e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15863ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15863eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15863f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15863f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15863fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158640140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1586405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1586413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158641d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1586421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158642640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158642ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158643420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1586438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158644200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1586446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158644b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158644fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158645920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158645dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158646700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158646ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158647040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1586474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158647980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158647e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1586482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158648c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1586490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1586495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158649b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15864a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15864a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15864a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15864aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15864b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15864bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15864c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15864c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15864ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15864d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15864d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15864de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15864e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15864e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15864ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15864f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15864f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15864fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1586503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158650900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158650e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1586513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1586518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158651e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1586528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158652e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158653380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1586538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158653e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1586548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158654e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158655360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1586558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158656350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1586568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158657340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158657890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158657de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158658330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158658880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158658dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158659dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15865a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15865a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15865adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15865b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15865b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15865bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15865c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15865c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15865cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15865d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15865d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15865dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15865e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15865e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15865ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15865f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15865f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15865fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1586602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158660800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158660d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1586612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1586617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158661d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1586621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158662680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158662b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158662fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158663460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158663900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158663da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158664240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1586646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158664b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158665020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1586654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158665960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158665e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1586662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1586667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158666f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158667d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158668470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158668730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158668f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1586691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1586697f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1586694a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15864b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15864ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15864b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15861e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15861e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158620870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15864d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158615c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15861c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15861d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15861d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15861bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15861dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158614c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15860aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15861f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15862d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1586689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158617df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1586180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15864d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15864bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158616220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1586164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1586167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158669c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158669f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15866a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15866a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15866a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15866aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15866acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15866af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15866b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15866b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15866b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15866ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15866bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15866c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15866c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15866c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15866c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15866cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15866cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15866d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15866d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15866d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15866d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15866db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15866de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15866e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15866e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15866e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15866e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15866ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15866eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15866f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15866f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15866f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15866f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15866fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15866ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158670210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1586704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158670790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158670a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158670d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158670fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158671290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158671550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158671810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158671ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158671d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158672050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158672310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1586725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158672890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158672b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158672e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1586730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158673390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158673650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158673910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158673bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158673e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158674150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158674410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1586746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158674990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158674c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158674f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1586751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158675490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158675750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158675a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158675cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158675f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158676250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158676510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1586767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158676a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158676d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158677010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1586772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158677590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158677850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158677b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158677dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158678090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158678350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158678610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1586788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158678b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158678e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158679110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1586793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158679690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158679950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158679c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158679ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15867a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15867a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15867a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15867a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15867ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15867af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15867b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15867b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15867b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15867ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15867bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15867bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15867c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15867c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15867c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15867cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15867cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15867d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15867d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15867d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15867d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15867db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15867de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15867e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15867e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15867e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15867e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15867ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15867ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15867f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15867f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15867f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15867f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15867fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15867ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1586801d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158680490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158680750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158680a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158680cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158680f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158681250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158681510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1586817d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158681a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158681d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158682010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1586822d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158682590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158682850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158682b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158682dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158683090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158683350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158683610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1586838d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158683b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158683e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158684110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1586843d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158684690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158684950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158684c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158684ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158685190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158685450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158685710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1586859d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158685c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158685f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158686210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1586864d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158686790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158686a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158686d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158686fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158687290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158687550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158687810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158687ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158687d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158688050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158688310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1586885d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158688890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158688b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158688e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1586890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1586896a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158689960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158689c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158689ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15868a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15868a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15868a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15868a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15868aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15868af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15868b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15868b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15868b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15868ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15868bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15868bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15868c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15868c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15868c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15868cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15868cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15868d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15868d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15868d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15868d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15868db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15868de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15868e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15868e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15868e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15868e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15868ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15868eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15868f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15868f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15868fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1586903e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158690930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158690e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1586913d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158691920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158691e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1586923c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158692910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158692e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1586933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158693900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158693e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1586943a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1586948f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158694e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158695390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1586958e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158695e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158696380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1586968d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158696e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1586970e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1586973a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1586978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158697da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1586982a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1586987a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158698ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1586991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1586996a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158699ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15869a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15869a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15869aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15869afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15869b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15869b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15869c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15869cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15869d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15869d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15869dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15869e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15869e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15869ec90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15869e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15864cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15869de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15869f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15869f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15869f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15869f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15869fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15869feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1586a0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1586a0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1586a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1586a0cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1586a1290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1586a18c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1586a1b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1586a1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1586a2100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1586a23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1586a2680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1586a2940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1586a2c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1586a2ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1586a3180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1586a3440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1586a3700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1586a39c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1586a3c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1586a3f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1586a4200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1586a44c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1586a4780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1586a4a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1586a4d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1586a4fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1586a5280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1586a5540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1586a5800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1586a5ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1586a5d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1586a6040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1586a6300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1586a65c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1586a6880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1586a6b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1586a6e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1586a70c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1586a7380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1586a7640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1586a7900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1586a7bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1586a7e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1586a8140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1586a8400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1586a86c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1586a8980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1586a8c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1586a8f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1586a91c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1586a9480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1586a9740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1586a9a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1586a9cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1586a9f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1586aa240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1586aa500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1586aa7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1586aaa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1586aad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1586ab000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1586ab2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1586ab580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1586ab840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1586abb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1586abdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1586ac080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1586ac340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1586ac600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1586ac8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1586acb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1586ace40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1586ad100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1586ad3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1586ad680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1586ad940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1586adc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1586adec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1586ae180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1586ae440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1586ae700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1586ae9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1586aec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1586aef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1586af200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1586af4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1586af780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1586afa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1586afd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1586affc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1586b0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1586b0540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1586b0800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1586b0ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1586b0d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1586b1040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1586b1300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1586b15c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1586b1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1586b1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1586b1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1586b20c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1586b2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1586b2640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1586b2900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1586b2bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1586b2e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1586b3140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1586b3400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1586b36c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1586b3980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1586b3c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1586b3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1586b41c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1586b4480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1586b4740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1586b4a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1586b4cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1586b4f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1586b5240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1586b5500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1586b57c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1586b5a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1586b5d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1586b6000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1586b62c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1586b6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1586b6840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1586b6b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1586b6dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1586b7080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1586b7340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1586b7600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1586b78c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1586b7b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1586b7e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1586b8100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1586b83c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1586b8680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1586b8940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1586b8c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1586b8ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1586b9180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1586b9440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1586b9700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1586b99c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1586b9c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1586b9f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1586ba200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1586ba4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1586ba780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1586baa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1586bad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1586bafc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1586bb280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1586bb540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1586bb800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1586bbac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1586bbd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1586bc040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1586bc300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1586bc5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1586bc880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1586bcb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1586bce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1586bd0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1586bd380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1586bd640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1586bd900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1586bdbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1586bde80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1586be140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1586be400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1586be6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1586be980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1586bec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1586bef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1586bf1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1586bf480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1586bf740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1586bfa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1586bfcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1586bff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1586c0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1586c0500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1586c07c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1586c0a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1586c0d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1586c1000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1586c12c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1586c1580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1586c1840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1586c1b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1586c1dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1586c2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1586c2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1586c2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1586c28c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1586c2b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1586c2e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1586c3100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1586c36d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1586c3990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1586c3c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1586c3f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1586c41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1586c4490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1586c4750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1586c4a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1586c4cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1586c4f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1586c5250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1586c5510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1586c57d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1586c5a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1586c5d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1586c6010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1586c62d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1586c6590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1586c6850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1586c6b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1586c6dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1586c7090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1586c7350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1586c7610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1586c78d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1586c7b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1586c7e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1586c8110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1586c83d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1586c8690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1586c8950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1586c8c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1586c8ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1586c9190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1586c9450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1586c9710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1586c99d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1586c9c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1586c9f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1586ca210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1586ca4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1586ca790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1586caa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1586cad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1586cafd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1586cb290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1586cb550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1586cb810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1586cbad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1586cbd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1586cc050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1586cc310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1586cc5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1586cc890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1586ccb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1586cce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1586cd0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1586cd390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1586cd650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1586cd910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1586cdbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1586cde90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1586ce150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1586ce410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1586ce6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1586cead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1586ced90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1586cf290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1586cf790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1586cfc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1586d0190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1586d0690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1586d0b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1586d15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1586d1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1586d23e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1586d2b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1586d2dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1586d35b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1586d3870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1586d3e80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.947s
user	0m0.229s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
