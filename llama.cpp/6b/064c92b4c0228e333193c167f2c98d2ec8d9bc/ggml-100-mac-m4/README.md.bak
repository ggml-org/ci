### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.52 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.82 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.30 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.49 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.81 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.92 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.34 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.35 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.58 sec*proc (28 tests)

Total Test time (real) = 224.60 sec

real	3m44.719s
user	7m46.404s
sys	0m6.366s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.23 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.30 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.23 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.15 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.53 sec*proc (28 tests)

Total Test time (real) =  51.54 sec

real	0m51.555s
user	1m11.543s
sys	0m5.628s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.109 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.709 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.590 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.605 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.606 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.607 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.608 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.610 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.611 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.612 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.612 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.616 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.622 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.623 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.623 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.624 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.625 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.625 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.626 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.581 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.584 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.585 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.585 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.586 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.033.586 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.587 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.033.588 I llama_model_loader: - type  f32:  124 tensors
0.00.033.588 I llama_model_loader: - type  f16:   73 tensors
0.00.038.950 I llm_load_vocab: special tokens cache size = 5
0.00.041.794 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.801 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.802 I llm_load_print_meta: arch             = bert
0.00.041.802 I llm_load_print_meta: vocab type       = WPM
0.00.041.803 I llm_load_print_meta: n_vocab          = 30522
0.00.041.803 I llm_load_print_meta: n_merges         = 0
0.00.041.803 I llm_load_print_meta: vocab_only       = 0
0.00.041.804 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.804 I llm_load_print_meta: n_embd           = 384
0.00.041.804 I llm_load_print_meta: n_layer          = 12
0.00.041.821 I llm_load_print_meta: n_head           = 12
0.00.041.822 I llm_load_print_meta: n_head_kv        = 12
0.00.041.823 I llm_load_print_meta: n_rot            = 32
0.00.041.823 I llm_load_print_meta: n_swa            = 0
0.00.041.823 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.824 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.825 I llm_load_print_meta: n_gqa            = 1
0.00.041.826 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.827 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.828 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.829 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.829 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.829 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.830 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.831 I llm_load_print_meta: n_ff             = 1536
0.00.041.831 I llm_load_print_meta: n_expert         = 0
0.00.041.832 I llm_load_print_meta: n_expert_used    = 0
0.00.041.832 I llm_load_print_meta: causal attn      = 0
0.00.041.832 I llm_load_print_meta: pooling type     = 2
0.00.041.832 I llm_load_print_meta: rope type        = 2
0.00.041.833 I llm_load_print_meta: rope scaling     = linear
0.00.041.834 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.834 I llm_load_print_meta: freq_scale_train = 1
0.00.041.835 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.835 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.835 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.836 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.836 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.836 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.838 I llm_load_print_meta: model type       = 33M
0.00.041.841 I llm_load_print_meta: model ftype      = F16
0.00.041.842 I llm_load_print_meta: model params     = 33.21 M
0.00.041.843 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.843 I llm_load_print_meta: general.name     = Bge Small
0.00.041.844 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.844 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.845 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.845 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.845 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.854 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.855 I llm_load_print_meta: max token length = 21
0.00.044.080 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.044.083 I llm_load_tensors: offloading output layer to GPU
0.00.044.084 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.044.114 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.116 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.799 I llama_new_context_with_model: n_ctx         = 512
0.00.044.799 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.800 I llama_new_context_with_model: n_batch       = 2048
0.00.044.800 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.800 I llama_new_context_with_model: flash_attn    = 0
0.00.044.801 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.801 I llama_new_context_with_model: freq_scale    = 1
0.00.044.802 I ggml_metal_init: allocating
0.00.044.815 I ggml_metal_init: found device: Apple M4
0.00.044.819 I ggml_metal_init: picking default device: Apple M4
0.00.045.837 I ggml_metal_init: using embedded metal library
0.00.053.047 I ggml_metal_init: GPU name:   Apple M4
0.00.053.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.051 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.052 I ggml_metal_init: simdgroup reduction   = true
0.00.053.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.052 I ggml_metal_init: has bfloat            = true
0.00.053.052 I ggml_metal_init: use bfloat            = true
0.00.053.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.017 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.068.020 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.068.021 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.068.911 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.068.912 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.068.913 I llama_new_context_with_model: graph nodes  = 429
0.00.068.913 I llama_new_context_with_model: graph splits = 2
0.00.068.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.068.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.076.446 I 
0.00.076.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.077.299 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.082.672 I llama_perf_context_print:        load time =      53.73 ms
0.00.082.673 I llama_perf_context_print: prompt eval time =       5.20 ms /     9 tokens (    0.58 ms per token,  1730.77 tokens per second)
0.00.082.674 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.082.676 I llama_perf_context_print:       total time =       6.23 ms /    10 tokens
0.00.082.826 I ggml_metal_free: deallocating

real	0m0.284s
user	0m0.054s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.134 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.139 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.140 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.140 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.144 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.145 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.146 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.146 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.146 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.147 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.149 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.150 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.150 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.150 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.151 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.153 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.153 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.696 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.697 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.697 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.697 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.698 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.698 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.698 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.699 I llama_model_loader: - type  f32:  124 tensors
0.00.015.699 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.262 I llm_load_vocab: special tokens cache size = 5
0.00.019.650 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.653 I llm_load_print_meta: arch             = bert
0.00.019.653 I llm_load_print_meta: vocab type       = WPM
0.00.019.653 I llm_load_print_meta: n_vocab          = 30522
0.00.019.653 I llm_load_print_meta: n_merges         = 0
0.00.019.653 I llm_load_print_meta: vocab_only       = 0
0.00.019.654 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.654 I llm_load_print_meta: n_embd           = 384
0.00.019.654 I llm_load_print_meta: n_layer          = 12
0.00.019.663 I llm_load_print_meta: n_head           = 12
0.00.019.663 I llm_load_print_meta: n_head_kv        = 12
0.00.019.666 I llm_load_print_meta: n_rot            = 32
0.00.019.666 I llm_load_print_meta: n_swa            = 0
0.00.019.666 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.666 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.667 I llm_load_print_meta: n_gqa            = 1
0.00.019.667 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.668 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.668 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.669 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.669 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.669 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.669 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.670 I llm_load_print_meta: n_ff             = 1536
0.00.019.670 I llm_load_print_meta: n_expert         = 0
0.00.019.670 I llm_load_print_meta: n_expert_used    = 0
0.00.019.671 I llm_load_print_meta: causal attn      = 0
0.00.019.671 I llm_load_print_meta: pooling type     = 2
0.00.019.671 I llm_load_print_meta: rope type        = 2
0.00.019.671 I llm_load_print_meta: rope scaling     = linear
0.00.019.671 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.672 I llm_load_print_meta: freq_scale_train = 1
0.00.019.672 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.672 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.672 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.672 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.672 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.673 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.673 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.673 I llm_load_print_meta: model type       = 33M
0.00.019.674 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.674 I llm_load_print_meta: model params     = 33.21 M
0.00.019.675 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.675 I llm_load_print_meta: general.name     = Bge Small
0.00.019.675 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.675 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.676 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.676 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.676 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.676 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.676 I llm_load_print_meta: max token length = 21
0.00.021.073 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.021.076 I llm_load_tensors: offloading output layer to GPU
0.00.021.077 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.021.086 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.087 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.464 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.465 I llama_new_context_with_model: n_ctx         = 512
0.00.021.465 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.465 I llama_new_context_with_model: n_batch       = 2048
0.00.021.466 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.466 I llama_new_context_with_model: flash_attn    = 0
0.00.021.466 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.467 I llama_new_context_with_model: freq_scale    = 1
0.00.021.467 I ggml_metal_init: allocating
0.00.021.473 I ggml_metal_init: found device: Apple M4
0.00.021.475 I ggml_metal_init: picking default device: Apple M4
0.00.022.117 I ggml_metal_init: using embedded metal library
0.00.024.786 I ggml_metal_init: GPU name:   Apple M4
0.00.024.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.788 I ggml_metal_init: simdgroup reduction   = true
0.00.024.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.789 I ggml_metal_init: has bfloat            = true
0.00.024.789 I ggml_metal_init: use bfloat            = true
0.00.024.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.984 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.986 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.988 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.618 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.618 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.619 I llama_new_context_with_model: graph nodes  = 429
0.00.035.619 I llama_new_context_with_model: graph splits = 2
0.00.035.632 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.204 I 
0.00.040.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.787 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.208 I llama_perf_context_print:        load time =      30.29 ms
0.00.045.209 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2091.56 tokens per second)
0.00.045.210 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.210 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.045.399 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.169 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.774 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.198 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.206 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.210 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.214 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.215 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.216 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.217 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.217 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.218 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.219 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.222 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.223 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.224 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.225 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.042.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.209 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.209 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.210 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.047.210 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.210 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.211 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.211 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.212 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.047.212 I llama_model_loader: - type  f32:   41 tensors
0.00.047.212 I llama_model_loader: - type  f16:   29 tensors
0.00.066.090 W llm_load_vocab: empty token at index 5
0.00.070.950 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.348 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.378 I llm_load_vocab: special tokens cache size = 5
0.00.338.689 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.338.694 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.338.695 I llm_load_print_meta: arch             = jina-bert-v2
0.00.338.695 I llm_load_print_meta: vocab type       = BPE
0.00.338.696 I llm_load_print_meta: n_vocab          = 61056
0.00.338.699 I llm_load_print_meta: n_merges         = 39382
0.00.338.699 I llm_load_print_meta: vocab_only       = 0
0.00.338.699 I llm_load_print_meta: n_ctx_train      = 8192
0.00.338.699 I llm_load_print_meta: n_embd           = 384
0.00.338.700 I llm_load_print_meta: n_layer          = 4
0.00.338.729 I llm_load_print_meta: n_head           = 12
0.00.338.730 I llm_load_print_meta: n_head_kv        = 12
0.00.338.730 I llm_load_print_meta: n_rot            = 32
0.00.338.731 I llm_load_print_meta: n_swa            = 0
0.00.338.731 I llm_load_print_meta: n_embd_head_k    = 32
0.00.338.731 I llm_load_print_meta: n_embd_head_v    = 32
0.00.338.731 I llm_load_print_meta: n_gqa            = 1
0.00.338.732 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.338.732 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.338.733 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.338.734 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.338.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.338.734 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.338.734 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.338.735 I llm_load_print_meta: n_ff             = 1536
0.00.338.735 I llm_load_print_meta: n_expert         = 0
0.00.338.739 I llm_load_print_meta: n_expert_used    = 0
0.00.338.739 I llm_load_print_meta: causal attn      = 0
0.00.338.740 I llm_load_print_meta: pooling type     = -1
0.00.338.740 I llm_load_print_meta: rope type        = -1
0.00.338.740 I llm_load_print_meta: rope scaling     = linear
0.00.338.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.338.741 I llm_load_print_meta: freq_scale_train = 1
0.00.338.741 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.338.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.338.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.338.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.338.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.338.742 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.338.742 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.338.742 I llm_load_print_meta: model type       = 33M
0.00.338.743 I llm_load_print_meta: model ftype      = F16
0.00.338.743 I llm_load_print_meta: model params     = 32.90 M
0.00.338.744 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.338.744 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.338.745 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.338.745 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.338.745 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.338.745 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.338.745 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.338.746 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.338.746 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.338.746 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.338.746 I llm_load_print_meta: max token length = 45
0.00.339.765 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.339.766 I llm_load_tensors: offloading output layer to GPU
0.00.339.766 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.339.790 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.339.791 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.340.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.340.536 I llama_new_context_with_model: n_ctx         = 8192
0.00.340.536 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.340.536 I llama_new_context_with_model: n_batch       = 2048
0.00.340.536 I llama_new_context_with_model: n_ubatch      = 2048
0.00.340.537 I llama_new_context_with_model: flash_attn    = 0
0.00.340.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.340.537 I llama_new_context_with_model: freq_scale    = 1
0.00.340.538 I ggml_metal_init: allocating
0.00.340.541 I ggml_metal_init: found device: Apple M4
0.00.340.543 I ggml_metal_init: picking default device: Apple M4
0.00.341.345 I ggml_metal_init: using embedded metal library
0.00.344.197 I ggml_metal_init: GPU name:   Apple M4
0.00.344.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.200 I ggml_metal_init: simdgroup reduction   = true
0.00.344.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.200 I ggml_metal_init: has bfloat            = true
0.00.344.200 I ggml_metal_init: use bfloat            = true
0.00.344.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.356.032 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.356.035 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.356.036 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.356.614 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.356.615 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.356.615 I llama_new_context_with_model: graph nodes  = 154
0.00.356.615 I llama_new_context_with_model: graph splits = 2
0.00.356.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.356.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.026 I 
0.00.368.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.368.214 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.368.214 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.368.217 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.368.217 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.222 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.222 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.756 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.282 I llama_perf_context_print:        load time =     343.24 ms
0.00.372.283 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17628.66 tokens per second)
0.00.372.283 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.284 I llama_perf_context_print:       total time =       4.26 ms /    63 tokens
0.00.372.473 I ggml_metal_free: deallocating

real	0m1.182s
user	0m0.343s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.073 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.162 I main: llama backend init
0.00.000.168 I main: load the model and apply lora adapter, if any
0.00.018.824 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.788 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.029.797 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.800 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.801 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.802 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.803 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.803 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.804 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.804 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.805 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.806 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.809 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.809 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.810 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.649 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.650 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.651 I llama_model_loader: - type  f32:  194 tensors
0.00.038.651 I llama_model_loader: - type  f16:   98 tensors
0.00.060.749 I llm_load_vocab: special tokens cache size = 25
0.00.067.159 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.164 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.164 I llm_load_print_meta: arch             = gptneox
0.00.067.164 I llm_load_print_meta: vocab type       = BPE
0.00.067.165 I llm_load_print_meta: n_vocab          = 50304
0.00.067.165 I llm_load_print_meta: n_merges         = 50009
0.00.067.165 I llm_load_print_meta: vocab_only       = 0
0.00.067.165 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.165 I llm_load_print_meta: n_embd           = 2048
0.00.067.170 I llm_load_print_meta: n_layer          = 24
0.00.067.183 I llm_load_print_meta: n_head           = 16
0.00.067.185 I llm_load_print_meta: n_head_kv        = 16
0.00.067.185 I llm_load_print_meta: n_rot            = 32
0.00.067.185 I llm_load_print_meta: n_swa            = 0
0.00.067.185 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.185 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.186 I llm_load_print_meta: n_gqa            = 1
0.00.067.187 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.187 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.188 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.188 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.188 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.188 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.189 I llm_load_print_meta: n_ff             = 8192
0.00.067.189 I llm_load_print_meta: n_expert         = 0
0.00.067.190 I llm_load_print_meta: n_expert_used    = 0
0.00.067.190 I llm_load_print_meta: causal attn      = 1
0.00.067.190 I llm_load_print_meta: pooling type     = 0
0.00.067.190 I llm_load_print_meta: rope type        = 2
0.00.067.190 I llm_load_print_meta: rope scaling     = linear
0.00.067.190 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.191 I llm_load_print_meta: freq_scale_train = 1
0.00.067.191 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.191 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.191 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.191 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.191 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.192 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.192 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.192 I llm_load_print_meta: model type       = 1.4B
0.00.067.193 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.067.194 I llm_load_print_meta: model params     = 1.41 B
0.00.067.194 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.067.194 I llm_load_print_meta: general.name     = 1.4B
0.00.067.194 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.195 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.195 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.195 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.196 I llm_load_print_meta: LF token         = 128 ''
0.00.067.196 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.196 I llm_load_print_meta: max token length = 1024
0.00.069.677 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.678 I llm_load_tensors: offloading output layer to GPU
0.00.069.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.697 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.069.699 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.070.609 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.610 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.611 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.611 I llama_new_context_with_model: n_batch       = 2048
0.00.070.611 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.611 I llama_new_context_with_model: flash_attn    = 0
0.00.070.611 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.612 I llama_new_context_with_model: freq_scale    = 1
0.00.070.612 I ggml_metal_init: allocating
0.00.070.616 I ggml_metal_init: found device: Apple M4
0.00.070.618 I ggml_metal_init: picking default device: Apple M4
0.00.071.306 I ggml_metal_init: using embedded metal library
0.00.110.838 I ggml_metal_init: GPU name:   Apple M4
0.00.110.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.843 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.844 I ggml_metal_init: simdgroup reduction   = true
0.00.110.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.844 I ggml_metal_init: has bfloat            = true
0.00.110.844 I ggml_metal_init: use bfloat            = true
0.00.110.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.231.382 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.231.389 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.231.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.232.425 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.232.427 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.232.427 I llama_new_context_with_model: graph nodes  = 967
0.00.232.427 I llama_new_context_with_model: graph splits = 2
0.00.232.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.232.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.232.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.312.909 I main: llama threadpool init, n_threads = 4
0.00.312.953 I 
0.00.312.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.312.991 I 
0.00.313.059 I sampler seed: 1234
0.00.313.064 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.313.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.313.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.313.099 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.156.933 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.02.156.934 I llama_perf_context_print:        load time =     294.08 ms
0.02.156.935 I llama_perf_context_print: prompt eval time =      43.86 ms /     7 tokens (    6.27 ms per token,   159.60 tokens per second)
0.02.156.935 I llama_perf_context_print:        eval time =    1796.95 ms /    63 runs   (   28.52 ms per token,    35.06 tokens per second)
0.02.156.936 I llama_perf_context_print:       total time =    1844.03 ms /    70 tokens
0.02.157.087 I ggml_metal_free: deallocating

real	0m2.697s
user	0m0.128s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.592 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.931 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.052 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.065 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.078 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.540 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.727 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.729 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.731 I llama_model_loader: - type  f32:  194 tensors
0.00.053.732 I llama_model_loader: - type  f16:   98 tensors
0.00.083.681 I llm_load_vocab: special tokens cache size = 25
0.00.090.623 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.625 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.626 I llm_load_print_meta: arch             = gptneox
0.00.090.626 I llm_load_print_meta: vocab type       = BPE
0.00.090.626 I llm_load_print_meta: n_vocab          = 50304
0.00.090.626 I llm_load_print_meta: n_merges         = 50009
0.00.090.627 I llm_load_print_meta: vocab_only       = 0
0.00.090.627 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.627 I llm_load_print_meta: n_embd           = 2048
0.00.090.627 I llm_load_print_meta: n_layer          = 24
0.00.090.641 I llm_load_print_meta: n_head           = 16
0.00.090.643 I llm_load_print_meta: n_head_kv        = 16
0.00.090.643 I llm_load_print_meta: n_rot            = 32
0.00.090.643 I llm_load_print_meta: n_swa            = 0
0.00.090.643 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.643 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.644 I llm_load_print_meta: n_gqa            = 1
0.00.090.645 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.645 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.646 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.646 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.647 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.647 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.647 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.648 I llm_load_print_meta: n_ff             = 8192
0.00.090.648 I llm_load_print_meta: n_expert         = 0
0.00.090.648 I llm_load_print_meta: n_expert_used    = 0
0.00.090.648 I llm_load_print_meta: causal attn      = 1
0.00.090.648 I llm_load_print_meta: pooling type     = 0
0.00.090.648 I llm_load_print_meta: rope type        = 2
0.00.090.648 I llm_load_print_meta: rope scaling     = linear
0.00.090.649 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.649 I llm_load_print_meta: freq_scale_train = 1
0.00.090.649 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.652 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.652 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.652 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.652 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.652 I llm_load_print_meta: model type       = 1.4B
0.00.090.653 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.653 I llm_load_print_meta: model params     = 1.41 B
0.00.090.654 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.654 I llm_load_print_meta: general.name     = 1.4B
0.00.090.654 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.654 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.655 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.655 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.655 I llm_load_print_meta: LF token         = 128 ''
0.00.090.655 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.657 I llm_load_print_meta: max token length = 1024
0.00.093.200 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.200 I llm_load_tensors: offloading output layer to GPU
0.00.093.200 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.211 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.212 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.141 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.142 I llama_new_context_with_model: n_ctx         = 128
0.00.094.142 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.142 I llama_new_context_with_model: n_batch       = 128
0.00.094.142 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.142 I llama_new_context_with_model: flash_attn    = 0
0.00.094.143 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.143 I llama_new_context_with_model: freq_scale    = 1
0.00.094.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.144 I ggml_metal_init: allocating
0.00.094.146 I ggml_metal_init: found device: Apple M4
0.00.094.148 I ggml_metal_init: picking default device: Apple M4
0.00.094.751 I ggml_metal_init: using embedded metal library
0.00.097.336 I ggml_metal_init: GPU name:   Apple M4
0.00.097.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.338 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.339 I ggml_metal_init: simdgroup reduction   = true
0.00.097.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.339 I ggml_metal_init: has bfloat            = true
0.00.097.339 I ggml_metal_init: use bfloat            = true
0.00.097.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.172 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.185 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.023 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.024 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.024 I llama_new_context_with_model: graph nodes  = 967
0.00.109.025 I llama_new_context_with_model: graph splits = 2
0.00.109.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.972.782 I 
0.00.972.902 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.972.957 I perplexity: tokenizing the input ..
0.00.987.756 I perplexity: tokenization took 14.797 ms
0.00.987.763 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.108.875 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.110.734 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.110.750 I llama_perf_context_print:        load time =     950.82 ms
0.01.110.752 I llama_perf_context_print: prompt eval time =     120.20 ms /   128 tokens (    0.94 ms per token,  1064.86 tokens per second)
0.01.110.753 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.110.754 I llama_perf_context_print:       total time =     137.99 ms /   129 tokens
0.01.111.536 I ggml_metal_free: deallocating

real	0m1.321s
user	0m0.128s
sys	0m0.194s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.137 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.182 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.182 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.183 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.183 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.186 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.190 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.187 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.189 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.190 I llama_model_loader: - type  f32:  194 tensors
0.00.033.190 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.731 I llm_load_vocab: special tokens cache size = 25
0.00.060.851 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.854 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.855 I llm_load_print_meta: arch             = gptneox
0.00.060.855 I llm_load_print_meta: vocab type       = BPE
0.00.060.855 I llm_load_print_meta: n_vocab          = 50304
0.00.060.856 I llm_load_print_meta: n_merges         = 50009
0.00.060.856 I llm_load_print_meta: vocab_only       = 0
0.00.060.856 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.858 I llm_load_print_meta: n_embd           = 2048
0.00.060.858 I llm_load_print_meta: n_layer          = 24
0.00.060.875 I llm_load_print_meta: n_head           = 16
0.00.060.877 I llm_load_print_meta: n_head_kv        = 16
0.00.060.877 I llm_load_print_meta: n_rot            = 32
0.00.060.877 I llm_load_print_meta: n_swa            = 0
0.00.060.877 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.877 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.878 I llm_load_print_meta: n_gqa            = 1
0.00.060.878 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.880 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.880 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.881 I llm_load_print_meta: n_ff             = 8192
0.00.060.881 I llm_load_print_meta: n_expert         = 0
0.00.060.883 I llm_load_print_meta: n_expert_used    = 0
0.00.060.883 I llm_load_print_meta: causal attn      = 1
0.00.060.883 I llm_load_print_meta: pooling type     = 0
0.00.060.883 I llm_load_print_meta: rope type        = 2
0.00.060.883 I llm_load_print_meta: rope scaling     = linear
0.00.060.884 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.884 I llm_load_print_meta: freq_scale_train = 1
0.00.060.884 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.884 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.884 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.885 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.885 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.885 I llm_load_print_meta: model type       = 1.4B
0.00.060.886 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.886 I llm_load_print_meta: model params     = 1.41 B
0.00.060.887 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.887 I llm_load_print_meta: general.name     = 1.4B
0.00.060.887 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.887 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.888 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.888 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.888 I llm_load_print_meta: LF token         = 128 ''
0.00.060.888 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.888 I llm_load_print_meta: max token length = 1024
0.00.063.417 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.417 I llm_load_tensors: offloading output layer to GPU
0.00.063.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.429 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.430 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.383 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.384 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.384 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.384 I llama_new_context_with_model: n_batch       = 2048
0.00.064.384 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.384 I llama_new_context_with_model: flash_attn    = 0
0.00.064.385 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.385 I llama_new_context_with_model: freq_scale    = 1
0.00.064.386 I ggml_metal_init: allocating
0.00.064.392 I ggml_metal_init: found device: Apple M4
0.00.064.395 I ggml_metal_init: picking default device: Apple M4
0.00.065.117 I ggml_metal_init: using embedded metal library
0.00.067.688 I ggml_metal_init: GPU name:   Apple M4
0.00.067.690 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.691 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.691 I ggml_metal_init: simdgroup reduction   = true
0.00.067.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.691 I ggml_metal_init: has bfloat            = true
0.00.067.691 I ggml_metal_init: use bfloat            = true
0.00.067.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.693 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.057 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.068 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.088 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.221 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.222 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.223 I llama_new_context_with_model: graph nodes  = 967
0.00.103.223 I llama_new_context_with_model: graph splits = 2
0.00.103.240 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.285.074 I main: llama threadpool init, n_threads = 4
0.01.285.120 I 
0.01.285.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.285.178 I 
0.01.285.534 I sampler seed: 1234
0.01.285.542 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.285.581 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.285.583 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.285.583 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.386.570 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.02.386.570 I llama_perf_context_print:        load time =    1274.93 ms
0.02.386.571 I llama_perf_context_print: prompt eval time =      50.10 ms /     7 tokens (    7.16 ms per token,   139.72 tokens per second)
0.02.386.572 I llama_perf_context_print:        eval time =    1048.00 ms /    63 runs   (   16.63 ms per token,    60.11 tokens per second)
0.02.386.575 I llama_perf_context_print:       total time =    1101.50 ms /    70 tokens
0.02.386.771 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.121s
sys	0m0.243s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.128 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.706 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.256 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.687 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.092 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.094 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.096 I llama_model_loader: - type  f32:  194 tensors
0.00.032.097 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.833 I llm_load_vocab: special tokens cache size = 25
0.00.064.529 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.532 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.532 I llm_load_print_meta: arch             = gptneox
0.00.064.532 I llm_load_print_meta: vocab type       = BPE
0.00.064.533 I llm_load_print_meta: n_vocab          = 50304
0.00.064.533 I llm_load_print_meta: n_merges         = 50009
0.00.064.533 I llm_load_print_meta: vocab_only       = 0
0.00.064.533 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.533 I llm_load_print_meta: n_embd           = 2048
0.00.064.533 I llm_load_print_meta: n_layer          = 24
0.00.064.549 I llm_load_print_meta: n_head           = 16
0.00.064.550 I llm_load_print_meta: n_head_kv        = 16
0.00.064.550 I llm_load_print_meta: n_rot            = 32
0.00.064.550 I llm_load_print_meta: n_swa            = 0
0.00.064.550 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.551 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.551 I llm_load_print_meta: n_gqa            = 1
0.00.064.552 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.553 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.553 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.554 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.554 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.554 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.554 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.555 I llm_load_print_meta: n_ff             = 8192
0.00.064.555 I llm_load_print_meta: n_expert         = 0
0.00.064.555 I llm_load_print_meta: n_expert_used    = 0
0.00.064.557 I llm_load_print_meta: causal attn      = 1
0.00.064.557 I llm_load_print_meta: pooling type     = 0
0.00.064.557 I llm_load_print_meta: rope type        = 2
0.00.064.557 I llm_load_print_meta: rope scaling     = linear
0.00.064.558 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.562 I llm_load_print_meta: freq_scale_train = 1
0.00.064.562 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.564 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.564 I llm_load_print_meta: model type       = 1.4B
0.00.064.565 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.565 I llm_load_print_meta: model params     = 1.41 B
0.00.064.565 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.566 I llm_load_print_meta: general.name     = 1.4B
0.00.064.567 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.567 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.567 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.567 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.568 I llm_load_print_meta: LF token         = 128 ''
0.00.064.568 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.568 I llm_load_print_meta: max token length = 1024
0.00.066.857 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.857 I llm_load_tensors: offloading output layer to GPU
0.00.066.857 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.868 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.869 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.818 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.819 I llama_new_context_with_model: n_ctx         = 128
0.00.067.819 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.819 I llama_new_context_with_model: n_batch       = 128
0.00.067.820 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.820 I llama_new_context_with_model: flash_attn    = 0
0.00.067.820 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.821 I llama_new_context_with_model: freq_scale    = 1
0.00.067.821 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.822 I ggml_metal_init: allocating
0.00.067.828 I ggml_metal_init: found device: Apple M4
0.00.067.831 I ggml_metal_init: picking default device: Apple M4
0.00.068.472 I ggml_metal_init: using embedded metal library
0.00.070.986 I ggml_metal_init: GPU name:   Apple M4
0.00.070.988 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.989 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.989 I ggml_metal_init: simdgroup reduction   = true
0.00.070.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.990 I ggml_metal_init: has bfloat            = true
0.00.070.990 I ggml_metal_init: use bfloat            = true
0.00.070.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.991 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.120 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.068 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.083.069 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.083.069 I llama_new_context_with_model: graph nodes  = 967
0.00.083.069 I llama_new_context_with_model: graph splits = 2
0.00.083.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.912.323 I 
0.00.912.351 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.912.362 I perplexity: tokenizing the input ..
0.00.919.997 I perplexity: tokenization took 7.634 ms
0.00.920.005 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.044.399 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.045.623 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.045.640 I llama_perf_context_print:        load time =     900.76 ms
0.01.045.642 I llama_perf_context_print: prompt eval time =     124.14 ms /   128 tokens (    0.97 ms per token,  1031.05 tokens per second)
0.01.045.643 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.045.643 I llama_perf_context_print:       total time =     133.32 ms /   129 tokens
0.01.046.061 I ggml_metal_free: deallocating

real	0m1.064s
user	0m0.092s
sys	0m0.142s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.015.939 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.222 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.867 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.258 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.825 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.826 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.826 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.827 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.827 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.827 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.828 I llama_model_loader: - type  f32:  194 tensors
0.00.035.828 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.828 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.640 I llm_load_vocab: special tokens cache size = 25
0.00.075.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.707 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.708 I llm_load_print_meta: arch             = gptneox
0.00.075.709 I llm_load_print_meta: vocab type       = BPE
0.00.075.709 I llm_load_print_meta: n_vocab          = 50304
0.00.075.709 I llm_load_print_meta: n_merges         = 50009
0.00.075.709 I llm_load_print_meta: vocab_only       = 0
0.00.075.710 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.710 I llm_load_print_meta: n_embd           = 2048
0.00.075.712 I llm_load_print_meta: n_layer          = 24
0.00.075.729 I llm_load_print_meta: n_head           = 16
0.00.075.732 I llm_load_print_meta: n_head_kv        = 16
0.00.075.732 I llm_load_print_meta: n_rot            = 32
0.00.075.732 I llm_load_print_meta: n_swa            = 0
0.00.075.732 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.733 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.734 I llm_load_print_meta: n_gqa            = 1
0.00.075.735 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.736 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.736 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.737 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.737 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.739 I llm_load_print_meta: n_ff             = 8192
0.00.075.739 I llm_load_print_meta: n_expert         = 0
0.00.075.739 I llm_load_print_meta: n_expert_used    = 0
0.00.075.739 I llm_load_print_meta: causal attn      = 1
0.00.075.740 I llm_load_print_meta: pooling type     = 0
0.00.075.740 I llm_load_print_meta: rope type        = 2
0.00.075.740 I llm_load_print_meta: rope scaling     = linear
0.00.075.741 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.741 I llm_load_print_meta: freq_scale_train = 1
0.00.075.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.742 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.742 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.742 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.742 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.743 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.743 I llm_load_print_meta: model type       = 1.4B
0.00.075.744 I llm_load_print_meta: model ftype      = Q4_0
0.00.075.745 I llm_load_print_meta: model params     = 1.41 B
0.00.075.746 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.075.746 I llm_load_print_meta: general.name     = 1.4B
0.00.075.746 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.747 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.747 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.747 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.748 I llm_load_print_meta: LF token         = 128 ''
0.00.075.750 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.750 I llm_load_print_meta: max token length = 1024
0.00.078.945 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.078.945 I llm_load_tensors: offloading output layer to GPU
0.00.078.946 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.078.958 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.960 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.526 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.526 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.526 I llama_new_context_with_model: n_batch       = 2048
0.00.080.527 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.527 I llama_new_context_with_model: flash_attn    = 0
0.00.080.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.528 I llama_new_context_with_model: freq_scale    = 1
0.00.080.529 I ggml_metal_init: allocating
0.00.080.534 I ggml_metal_init: found device: Apple M4
0.00.080.536 I ggml_metal_init: picking default device: Apple M4
0.00.081.513 I ggml_metal_init: using embedded metal library
0.00.085.280 I ggml_metal_init: GPU name:   Apple M4
0.00.085.282 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.284 I ggml_metal_init: simdgroup reduction   = true
0.00.085.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.284 I ggml_metal_init: has bfloat            = true
0.00.085.284 I ggml_metal_init: use bfloat            = true
0.00.085.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.122.832 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.845 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.871 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.946 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.948 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.948 I llama_new_context_with_model: graph nodes  = 967
0.00.123.948 I llama_new_context_with_model: graph splits = 2
0.00.123.962 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.095 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.613 I main: llama threadpool init, n_threads = 4
0.00.790.650 I 
0.00.790.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.682 I 
0.00.790.900 I sampler seed: 1234
0.00.790.905 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.966 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.478.955 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.478.956 I llama_perf_context_print:        load time =     774.67 ms
0.01.478.957 I llama_perf_context_print: prompt eval time =      45.05 ms /     7 tokens (    6.44 ms per token,   155.40 tokens per second)
0.01.478.959 I llama_perf_context_print:        eval time =     639.93 ms /    63 runs   (   10.16 ms per token,    98.45 tokens per second)
0.01.478.959 I llama_perf_context_print:       total time =     688.35 ms /    70 tokens
0.01.479.156 I ggml_metal_free: deallocating

real	0m1.506s
user	0m0.131s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.188 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.936 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.938 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.941 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.794 I llm_load_vocab: special tokens cache size = 25
0.00.051.874 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.877 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.877 I llm_load_print_meta: arch             = gptneox
0.00.051.878 I llm_load_print_meta: vocab type       = BPE
0.00.051.878 I llm_load_print_meta: n_vocab          = 50304
0.00.051.878 I llm_load_print_meta: n_merges         = 50009
0.00.051.878 I llm_load_print_meta: vocab_only       = 0
0.00.051.878 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.879 I llm_load_print_meta: n_embd           = 2048
0.00.051.879 I llm_load_print_meta: n_layer          = 24
0.00.051.888 I llm_load_print_meta: n_head           = 16
0.00.051.889 I llm_load_print_meta: n_head_kv        = 16
0.00.051.889 I llm_load_print_meta: n_rot            = 32
0.00.051.889 I llm_load_print_meta: n_swa            = 0
0.00.051.892 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.892 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.893 I llm_load_print_meta: n_gqa            = 1
0.00.051.894 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.894 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.895 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.895 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.896 I llm_load_print_meta: n_ff             = 8192
0.00.051.897 I llm_load_print_meta: n_expert         = 0
0.00.051.897 I llm_load_print_meta: n_expert_used    = 0
0.00.051.897 I llm_load_print_meta: causal attn      = 1
0.00.051.897 I llm_load_print_meta: pooling type     = 0
0.00.051.897 I llm_load_print_meta: rope type        = 2
0.00.051.897 I llm_load_print_meta: rope scaling     = linear
0.00.051.898 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.898 I llm_load_print_meta: freq_scale_train = 1
0.00.051.898 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.899 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.901 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.901 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.901 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.901 I llm_load_print_meta: model type       = 1.4B
0.00.051.901 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.902 I llm_load_print_meta: model params     = 1.41 B
0.00.051.902 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.902 I llm_load_print_meta: general.name     = 1.4B
0.00.051.903 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.903 I llm_load_print_meta: LF token         = 128 ''
0.00.051.905 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.905 I llm_load_print_meta: max token length = 1024
0.00.053.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.700 I llm_load_tensors: offloading output layer to GPU
0.00.053.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.706 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.707 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.593 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.594 I llama_new_context_with_model: n_ctx         = 128
0.00.054.594 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.594 I llama_new_context_with_model: n_batch       = 128
0.00.054.594 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.594 I llama_new_context_with_model: flash_attn    = 0
0.00.054.595 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.595 I llama_new_context_with_model: freq_scale    = 1
0.00.054.595 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.596 I ggml_metal_init: allocating
0.00.054.599 I ggml_metal_init: found device: Apple M4
0.00.054.601 I ggml_metal_init: picking default device: Apple M4
0.00.055.165 I ggml_metal_init: using embedded metal library
0.00.057.473 I ggml_metal_init: GPU name:   Apple M4
0.00.057.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.475 I ggml_metal_init: simdgroup reduction   = true
0.00.057.475 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.475 I ggml_metal_init: has bfloat            = true
0.00.057.475 I ggml_metal_init: use bfloat            = true
0.00.057.476 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.803 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.805 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.819 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.737 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.738 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.739 I llama_new_context_with_model: graph nodes  = 967
0.00.069.739 I llama_new_context_with_model: graph splits = 2
0.00.069.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.748 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.497 I 
0.00.605.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.596 I perplexity: tokenizing the input ..
0.00.613.401 I perplexity: tokenization took 7.804 ms
0.00.613.405 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.226 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.737.379 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.737.392 I llama_perf_context_print:        load time =     595.29 ms
0.00.737.394 I llama_perf_context_print: prompt eval time =     122.59 ms /   128 tokens (    0.96 ms per token,  1044.09 tokens per second)
0.00.737.394 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.395 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.00.737.915 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.079s
sys	0m0.095s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.679 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.278 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.279 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.281 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.282 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.282 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.282 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.284 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.144 I llama_model_loader: - type  f32:  194 tensors
0.00.023.144 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.291 I llm_load_vocab: special tokens cache size = 25
0.00.050.166 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.169 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.169 I llm_load_print_meta: arch             = gptneox
0.00.050.169 I llm_load_print_meta: vocab type       = BPE
0.00.050.169 I llm_load_print_meta: n_vocab          = 50304
0.00.050.170 I llm_load_print_meta: n_merges         = 50009
0.00.050.170 I llm_load_print_meta: vocab_only       = 0
0.00.050.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.170 I llm_load_print_meta: n_embd           = 2048
0.00.050.170 I llm_load_print_meta: n_layer          = 24
0.00.050.185 I llm_load_print_meta: n_head           = 16
0.00.050.185 I llm_load_print_meta: n_head_kv        = 16
0.00.050.186 I llm_load_print_meta: n_rot            = 32
0.00.050.186 I llm_load_print_meta: n_swa            = 0
0.00.050.186 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.186 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.187 I llm_load_print_meta: n_gqa            = 1
0.00.050.188 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.188 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.189 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.189 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.190 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.190 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.190 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.191 I llm_load_print_meta: n_ff             = 8192
0.00.050.191 I llm_load_print_meta: n_expert         = 0
0.00.050.191 I llm_load_print_meta: n_expert_used    = 0
0.00.050.191 I llm_load_print_meta: causal attn      = 1
0.00.050.191 I llm_load_print_meta: pooling type     = 0
0.00.050.191 I llm_load_print_meta: rope type        = 2
0.00.050.192 I llm_load_print_meta: rope scaling     = linear
0.00.050.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.193 I llm_load_print_meta: freq_scale_train = 1
0.00.050.193 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.193 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.193 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.194 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.194 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.194 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.194 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.194 I llm_load_print_meta: model type       = 1.4B
0.00.050.194 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.195 I llm_load_print_meta: model params     = 1.41 B
0.00.050.195 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.197 I llm_load_print_meta: general.name     = 1.4B
0.00.050.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.197 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.197 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.197 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.198 I llm_load_print_meta: LF token         = 128 ''
0.00.050.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.199 I llm_load_print_meta: max token length = 1024
0.00.052.221 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.221 I llm_load_tensors: offloading output layer to GPU
0.00.052.222 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.232 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.233 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.131 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.132 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.132 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.132 I llama_new_context_with_model: n_batch       = 2048
0.00.053.133 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.133 I llama_new_context_with_model: flash_attn    = 0
0.00.053.133 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.133 I llama_new_context_with_model: freq_scale    = 1
0.00.053.134 I ggml_metal_init: allocating
0.00.053.137 I ggml_metal_init: found device: Apple M4
0.00.053.139 I ggml_metal_init: picking default device: Apple M4
0.00.053.730 I ggml_metal_init: using embedded metal library
0.00.056.073 I ggml_metal_init: GPU name:   Apple M4
0.00.056.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.075 I ggml_metal_init: simdgroup reduction   = true
0.00.056.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.075 I ggml_metal_init: has bfloat            = true
0.00.056.075 I ggml_metal_init: use bfloat            = true
0.00.056.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.844 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.849 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.869 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.853 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.854 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.855 I llama_new_context_with_model: graph nodes  = 967
0.00.086.855 I llama_new_context_with_model: graph splits = 2
0.00.086.871 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.531 I main: llama threadpool init, n_threads = 4
0.00.721.573 I 
0.00.721.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.622 I 
0.00.721.867 I sampler seed: 1234
0.00.721.873 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.891 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.891 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.449.505 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.449.506 I llama_perf_context_print:        load time =     712.85 ms
0.01.449.508 I llama_perf_context_print: prompt eval time =      42.79 ms /     7 tokens (    6.11 ms per token,   163.59 tokens per second)
0.01.449.509 I llama_perf_context_print:        eval time =     682.20 ms /    63 runs   (   10.83 ms per token,    92.35 tokens per second)
0.01.449.509 I llama_perf_context_print:       total time =     727.98 ms /    70 tokens
0.01.449.759 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.111s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.691 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.312 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.315 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.318 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.318 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.320 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.321 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.098 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.869 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.870 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.871 I llama_model_loader: - type  f32:  194 tensors
0.00.022.871 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.871 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.169 I llm_load_vocab: special tokens cache size = 25
0.00.049.100 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.103 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.103 I llm_load_print_meta: arch             = gptneox
0.00.049.104 I llm_load_print_meta: vocab type       = BPE
0.00.049.104 I llm_load_print_meta: n_vocab          = 50304
0.00.049.104 I llm_load_print_meta: n_merges         = 50009
0.00.049.104 I llm_load_print_meta: vocab_only       = 0
0.00.049.104 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.105 I llm_load_print_meta: n_embd           = 2048
0.00.049.105 I llm_load_print_meta: n_layer          = 24
0.00.049.118 I llm_load_print_meta: n_head           = 16
0.00.049.119 I llm_load_print_meta: n_head_kv        = 16
0.00.049.119 I llm_load_print_meta: n_rot            = 32
0.00.049.119 I llm_load_print_meta: n_swa            = 0
0.00.049.121 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.121 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.122 I llm_load_print_meta: n_gqa            = 1
0.00.049.123 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.124 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.124 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.125 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.126 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.126 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.126 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.127 I llm_load_print_meta: n_ff             = 8192
0.00.049.128 I llm_load_print_meta: n_expert         = 0
0.00.049.128 I llm_load_print_meta: n_expert_used    = 0
0.00.049.128 I llm_load_print_meta: causal attn      = 1
0.00.049.128 I llm_load_print_meta: pooling type     = 0
0.00.049.128 I llm_load_print_meta: rope type        = 2
0.00.049.128 I llm_load_print_meta: rope scaling     = linear
0.00.049.129 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.130 I llm_load_print_meta: freq_scale_train = 1
0.00.049.130 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.130 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.130 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.131 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.131 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.131 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.132 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.132 I llm_load_print_meta: model type       = 1.4B
0.00.049.132 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.133 I llm_load_print_meta: model params     = 1.41 B
0.00.049.134 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.134 I llm_load_print_meta: general.name     = 1.4B
0.00.049.134 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.134 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.135 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.135 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.135 I llm_load_print_meta: LF token         = 128 ''
0.00.049.136 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.136 I llm_load_print_meta: max token length = 1024
0.00.050.690 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.690 I llm_load_tensors: offloading output layer to GPU
0.00.050.690 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.700 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.702 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.526 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.527 I llama_new_context_with_model: n_ctx         = 128
0.00.051.527 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.527 I llama_new_context_with_model: n_batch       = 128
0.00.051.527 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.528 I llama_new_context_with_model: flash_attn    = 0
0.00.051.528 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.528 I llama_new_context_with_model: freq_scale    = 1
0.00.051.529 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.529 I ggml_metal_init: allocating
0.00.051.535 I ggml_metal_init: found device: Apple M4
0.00.051.537 I ggml_metal_init: picking default device: Apple M4
0.00.052.120 I ggml_metal_init: using embedded metal library
0.00.054.485 I ggml_metal_init: GPU name:   Apple M4
0.00.054.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.487 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.488 I ggml_metal_init: simdgroup reduction   = true
0.00.054.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.488 I ggml_metal_init: has bfloat            = true
0.00.054.488 I ggml_metal_init: use bfloat            = true
0.00.054.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.238 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.150 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.151 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.152 I llama_new_context_with_model: graph nodes  = 967
0.00.066.152 I llama_new_context_with_model: graph splits = 2
0.00.066.164 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.443 I 
0.00.661.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.505 I perplexity: tokenizing the input ..
0.00.669.133 I perplexity: tokenization took 7.627 ms
0.00.669.137 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.785 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.792.946 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.792.969 I llama_perf_context_print:        load time =     652.75 ms
0.00.792.970 I llama_perf_context_print: prompt eval time =     122.42 ms /   128 tokens (    0.96 ms per token,  1045.56 tokens per second)
0.00.792.971 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.971 I llama_perf_context_print:       total time =     131.53 ms /   129 tokens
0.00.793.464 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.078s
sys	0m0.107s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.258 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.285 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.209 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.209 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.715 I llm_load_vocab: special tokens cache size = 25
0.00.051.648 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.651 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.652 I llm_load_print_meta: arch             = gptneox
0.00.051.652 I llm_load_print_meta: vocab type       = BPE
0.00.051.652 I llm_load_print_meta: n_vocab          = 50304
0.00.051.652 I llm_load_print_meta: n_merges         = 50009
0.00.051.653 I llm_load_print_meta: vocab_only       = 0
0.00.051.653 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.653 I llm_load_print_meta: n_embd           = 2048
0.00.051.653 I llm_load_print_meta: n_layer          = 24
0.00.051.667 I llm_load_print_meta: n_head           = 16
0.00.051.670 I llm_load_print_meta: n_head_kv        = 16
0.00.051.671 I llm_load_print_meta: n_rot            = 32
0.00.051.671 I llm_load_print_meta: n_swa            = 0
0.00.051.671 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.671 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.672 I llm_load_print_meta: n_gqa            = 1
0.00.051.673 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.675 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.675 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.676 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.677 I llm_load_print_meta: n_ff             = 8192
0.00.051.677 I llm_load_print_meta: n_expert         = 0
0.00.051.678 I llm_load_print_meta: n_expert_used    = 0
0.00.051.679 I llm_load_print_meta: causal attn      = 1
0.00.051.680 I llm_load_print_meta: pooling type     = 0
0.00.051.681 I llm_load_print_meta: rope type        = 2
0.00.051.681 I llm_load_print_meta: rope scaling     = linear
0.00.051.681 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.681 I llm_load_print_meta: freq_scale_train = 1
0.00.051.681 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.682 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.685 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.685 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.685 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.685 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.686 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.686 I llm_load_print_meta: model type       = 1.4B
0.00.051.686 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.687 I llm_load_print_meta: model params     = 1.41 B
0.00.051.687 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.687 I llm_load_print_meta: general.name     = 1.4B
0.00.051.688 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: LF token         = 128 ''
0.00.051.689 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.689 I llm_load_print_meta: max token length = 1024
0.00.053.806 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.806 I llm_load_tensors: offloading output layer to GPU
0.00.053.806 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.817 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.818 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.747 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.747 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.748 I llama_new_context_with_model: n_batch       = 2048
0.00.054.748 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.748 I llama_new_context_with_model: flash_attn    = 0
0.00.054.748 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.749 I llama_new_context_with_model: freq_scale    = 1
0.00.054.749 I ggml_metal_init: allocating
0.00.054.753 I ggml_metal_init: found device: Apple M4
0.00.054.755 I ggml_metal_init: picking default device: Apple M4
0.00.055.345 I ggml_metal_init: using embedded metal library
0.00.057.666 I ggml_metal_init: GPU name:   Apple M4
0.00.057.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.668 I ggml_metal_init: simdgroup reduction   = true
0.00.057.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.668 I ggml_metal_init: has bfloat            = true
0.00.057.668 I ggml_metal_init: use bfloat            = true
0.00.057.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.328 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.334 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.352 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.375 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.376 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.377 I llama_new_context_with_model: graph nodes  = 967
0.00.088.377 I llama_new_context_with_model: graph splits = 2
0.00.088.391 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.912 I main: llama threadpool init, n_threads = 4
0.00.752.949 I 
0.00.752.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.982 I 
0.00.753.207 I sampler seed: 1234
0.00.753.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.244 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.246 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.550.427 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.550.428 I llama_perf_context_print:        load time =     743.65 ms
0.01.550.429 I llama_perf_context_print: prompt eval time =      46.33 ms /     7 tokens (    6.62 ms per token,   151.08 tokens per second)
0.01.550.429 I llama_perf_context_print:        eval time =     747.72 ms /    63 runs   (   11.87 ms per token,    84.26 tokens per second)
0.01.550.430 I llama_perf_context_print:       total time =     797.52 ms /    70 tokens
0.01.550.617 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.098 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.171 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.054 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.867 I llama_model_loader: - type  f32:  194 tensors
0.00.023.867 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.258 I llm_load_vocab: special tokens cache size = 25
0.00.050.053 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.056 I llm_load_print_meta: arch             = gptneox
0.00.050.056 I llm_load_print_meta: vocab type       = BPE
0.00.050.056 I llm_load_print_meta: n_vocab          = 50304
0.00.050.056 I llm_load_print_meta: n_merges         = 50009
0.00.050.057 I llm_load_print_meta: vocab_only       = 0
0.00.050.057 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.057 I llm_load_print_meta: n_embd           = 2048
0.00.050.057 I llm_load_print_meta: n_layer          = 24
0.00.050.071 I llm_load_print_meta: n_head           = 16
0.00.050.072 I llm_load_print_meta: n_head_kv        = 16
0.00.050.074 I llm_load_print_meta: n_rot            = 32
0.00.050.074 I llm_load_print_meta: n_swa            = 0
0.00.050.074 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.074 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.075 I llm_load_print_meta: n_gqa            = 1
0.00.050.076 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.076 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.077 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.077 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.077 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.078 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.078 I llm_load_print_meta: n_ff             = 8192
0.00.050.079 I llm_load_print_meta: n_expert         = 0
0.00.050.079 I llm_load_print_meta: n_expert_used    = 0
0.00.050.079 I llm_load_print_meta: causal attn      = 1
0.00.050.079 I llm_load_print_meta: pooling type     = 0
0.00.050.079 I llm_load_print_meta: rope type        = 2
0.00.050.080 I llm_load_print_meta: rope scaling     = linear
0.00.050.081 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.081 I llm_load_print_meta: freq_scale_train = 1
0.00.050.081 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.081 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.081 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.083 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.083 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.083 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.083 I llm_load_print_meta: model type       = 1.4B
0.00.050.083 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.084 I llm_load_print_meta: model params     = 1.41 B
0.00.050.085 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.085 I llm_load_print_meta: general.name     = 1.4B
0.00.050.085 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.085 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.086 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.086 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.086 I llm_load_print_meta: LF token         = 128 ''
0.00.050.086 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.086 I llm_load_print_meta: max token length = 1024
0.00.052.082 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.082 I llm_load_tensors: offloading output layer to GPU
0.00.052.083 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.093 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.094 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.982 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.983 I llama_new_context_with_model: n_ctx         = 128
0.00.052.983 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.983 I llama_new_context_with_model: n_batch       = 128
0.00.052.983 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.983 I llama_new_context_with_model: flash_attn    = 0
0.00.052.984 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.984 I llama_new_context_with_model: freq_scale    = 1
0.00.052.984 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.985 I ggml_metal_init: allocating
0.00.052.988 I ggml_metal_init: found device: Apple M4
0.00.052.990 I ggml_metal_init: picking default device: Apple M4
0.00.053.565 I ggml_metal_init: using embedded metal library
0.00.055.894 I ggml_metal_init: GPU name:   Apple M4
0.00.055.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.897 I ggml_metal_init: simdgroup reduction   = true
0.00.055.897 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.897 I ggml_metal_init: has bfloat            = true
0.00.055.897 I ggml_metal_init: use bfloat            = true
0.00.055.898 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.894 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.897 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.911 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.826 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.827 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.827 I llama_new_context_with_model: graph nodes  = 967
0.00.067.827 I llama_new_context_with_model: graph splits = 2
0.00.067.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.950 I 
0.00.710.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.019 I perplexity: tokenizing the input ..
0.00.718.559 I perplexity: tokenization took 7.539 ms
0.00.718.563 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.418 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.902 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.933 I llama_perf_context_print:        load time =     701.85 ms
0.00.854.933 I llama_perf_context_print: prompt eval time =     134.61 ms /   128 tokens (    1.05 ms per token,   950.87 tokens per second)
0.00.854.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.935 I llama_perf_context_print:       total time =     143.98 ms /   129 tokens
0.00.855.465 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.078s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.008.630 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.626 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.628 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.629 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.637 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.637 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.557 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.539 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.541 I llama_model_loader: - type  f32:  194 tensors
0.00.024.541 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.541 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.028 I llm_load_vocab: special tokens cache size = 25
0.00.050.839 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.841 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.842 I llm_load_print_meta: arch             = gptneox
0.00.050.842 I llm_load_print_meta: vocab type       = BPE
0.00.050.842 I llm_load_print_meta: n_vocab          = 50304
0.00.050.842 I llm_load_print_meta: n_merges         = 50009
0.00.050.843 I llm_load_print_meta: vocab_only       = 0
0.00.050.843 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.843 I llm_load_print_meta: n_embd           = 2048
0.00.050.843 I llm_load_print_meta: n_layer          = 24
0.00.050.857 I llm_load_print_meta: n_head           = 16
0.00.050.858 I llm_load_print_meta: n_head_kv        = 16
0.00.050.858 I llm_load_print_meta: n_rot            = 32
0.00.050.858 I llm_load_print_meta: n_swa            = 0
0.00.050.859 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.859 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.861 I llm_load_print_meta: n_gqa            = 1
0.00.050.862 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.863 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.864 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.865 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.865 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.865 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.866 I llm_load_print_meta: n_ff             = 8192
0.00.050.866 I llm_load_print_meta: n_expert         = 0
0.00.050.866 I llm_load_print_meta: n_expert_used    = 0
0.00.050.868 I llm_load_print_meta: causal attn      = 1
0.00.050.869 I llm_load_print_meta: pooling type     = 0
0.00.050.869 I llm_load_print_meta: rope type        = 2
0.00.050.869 I llm_load_print_meta: rope scaling     = linear
0.00.050.870 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.870 I llm_load_print_meta: freq_scale_train = 1
0.00.050.870 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.871 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.871 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.871 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.871 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.871 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.871 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.872 I llm_load_print_meta: model type       = 1.4B
0.00.050.872 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.872 I llm_load_print_meta: model params     = 1.41 B
0.00.050.873 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.873 I llm_load_print_meta: general.name     = 1.4B
0.00.050.873 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.874 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.874 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.874 I llm_load_print_meta: LF token         = 128 ''
0.00.050.874 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.875 I llm_load_print_meta: max token length = 1024
0.00.052.899 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.899 I llm_load_tensors: offloading output layer to GPU
0.00.052.900 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.910 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.911 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.798 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.798 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.798 I llama_new_context_with_model: n_batch       = 2048
0.00.053.799 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.799 I llama_new_context_with_model: flash_attn    = 0
0.00.053.799 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.800 I llama_new_context_with_model: freq_scale    = 1
0.00.053.800 I ggml_metal_init: allocating
0.00.053.803 I ggml_metal_init: found device: Apple M4
0.00.053.805 I ggml_metal_init: picking default device: Apple M4
0.00.054.404 I ggml_metal_init: using embedded metal library
0.00.056.733 I ggml_metal_init: GPU name:   Apple M4
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.736 I ggml_metal_init: simdgroup reduction   = true
0.00.056.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.737 I ggml_metal_init: has bfloat            = true
0.00.056.737 I ggml_metal_init: use bfloat            = true
0.00.056.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.318 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.324 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.344 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.393 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.394 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.395 I llama_new_context_with_model: graph nodes  = 967
0.00.087.395 I llama_new_context_with_model: graph splits = 2
0.00.087.410 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.544 I main: llama threadpool init, n_threads = 4
0.00.725.580 I 
0.00.725.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.628 I 
0.00.725.862 I sampler seed: 1234
0.00.725.867 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.882 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.883 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.884 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.565.194 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.01.565.194 I llama_perf_context_print:        load time =     716.91 ms
0.01.565.195 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.36 tokens per second)
0.01.565.196 I llama_perf_context_print:        eval time =     794.03 ms /    63 runs   (   12.60 ms per token,    79.34 tokens per second)
0.01.565.200 I llama_perf_context_print:       total time =     839.65 ms /    70 tokens
0.01.565.403 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.110s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.432 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.018 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.019 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.019 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.020 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.021 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.022 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.025 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.025 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.228 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.336 I llama_model_loader: - type  f32:  194 tensors
0.00.026.337 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.542 I llm_load_vocab: special tokens cache size = 25
0.00.054.671 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.676 I llm_load_print_meta: arch             = gptneox
0.00.054.677 I llm_load_print_meta: vocab type       = BPE
0.00.054.677 I llm_load_print_meta: n_vocab          = 50304
0.00.054.677 I llm_load_print_meta: n_merges         = 50009
0.00.054.679 I llm_load_print_meta: vocab_only       = 0
0.00.054.679 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.680 I llm_load_print_meta: n_embd           = 2048
0.00.054.680 I llm_load_print_meta: n_layer          = 24
0.00.054.697 I llm_load_print_meta: n_head           = 16
0.00.054.698 I llm_load_print_meta: n_head_kv        = 16
0.00.054.698 I llm_load_print_meta: n_rot            = 32
0.00.054.698 I llm_load_print_meta: n_swa            = 0
0.00.054.698 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.699 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.700 I llm_load_print_meta: n_gqa            = 1
0.00.054.700 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.701 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.701 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.702 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.702 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.703 I llm_load_print_meta: n_ff             = 8192
0.00.054.703 I llm_load_print_meta: n_expert         = 0
0.00.054.703 I llm_load_print_meta: n_expert_used    = 0
0.00.054.703 I llm_load_print_meta: causal attn      = 1
0.00.054.703 I llm_load_print_meta: pooling type     = 0
0.00.054.703 I llm_load_print_meta: rope type        = 2
0.00.054.704 I llm_load_print_meta: rope scaling     = linear
0.00.054.705 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.705 I llm_load_print_meta: freq_scale_train = 1
0.00.054.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.705 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.706 I llm_load_print_meta: model type       = 1.4B
0.00.054.708 I llm_load_print_meta: model ftype      = Q5_1
0.00.054.708 I llm_load_print_meta: model params     = 1.41 B
0.00.054.709 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.054.709 I llm_load_print_meta: general.name     = 1.4B
0.00.054.709 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: LF token         = 128 ''
0.00.054.710 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.710 I llm_load_print_meta: max token length = 1024
0.00.056.782 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.782 I llm_load_tensors: offloading output layer to GPU
0.00.056.782 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.793 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.794 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.057.781 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.781 I llama_new_context_with_model: n_ctx         = 128
0.00.057.782 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.782 I llama_new_context_with_model: n_batch       = 128
0.00.057.782 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.782 I llama_new_context_with_model: flash_attn    = 0
0.00.057.783 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.783 I llama_new_context_with_model: freq_scale    = 1
0.00.057.783 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.784 I ggml_metal_init: allocating
0.00.057.787 I ggml_metal_init: found device: Apple M4
0.00.057.789 I ggml_metal_init: picking default device: Apple M4
0.00.058.391 I ggml_metal_init: using embedded metal library
0.00.060.849 I ggml_metal_init: GPU name:   Apple M4
0.00.060.851 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.851 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.852 I ggml_metal_init: simdgroup reduction   = true
0.00.060.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.852 I ggml_metal_init: has bfloat            = true
0.00.060.852 I ggml_metal_init: use bfloat            = true
0.00.060.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.853 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.610 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.625 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.592 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.594 I llama_new_context_with_model: graph nodes  = 967
0.00.073.594 I llama_new_context_with_model: graph splits = 2
0.00.073.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.294 I 
0.00.656.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.365 I perplexity: tokenizing the input ..
0.00.663.936 I perplexity: tokenization took 7.571 ms
0.00.663.940 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.037 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.800.334 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.800.349 I llama_perf_context_print:        load time =     646.86 ms
0.00.800.350 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.08 tokens per second)
0.00.800.351 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.351 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.00.800.687 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.081s
sys	0m0.118s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.556 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.383 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.239 I llama_model_loader: - type  f32:  194 tensors
0.00.024.240 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.240 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.240 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.412 I llm_load_vocab: special tokens cache size = 25
0.00.051.393 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.396 I llm_load_print_meta: arch             = gptneox
0.00.051.396 I llm_load_print_meta: vocab type       = BPE
0.00.051.396 I llm_load_print_meta: n_vocab          = 50304
0.00.051.397 I llm_load_print_meta: n_merges         = 50009
0.00.051.397 I llm_load_print_meta: vocab_only       = 0
0.00.051.397 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.397 I llm_load_print_meta: n_embd           = 2048
0.00.051.397 I llm_load_print_meta: n_layer          = 24
0.00.051.411 I llm_load_print_meta: n_head           = 16
0.00.051.412 I llm_load_print_meta: n_head_kv        = 16
0.00.051.412 I llm_load_print_meta: n_rot            = 32
0.00.051.412 I llm_load_print_meta: n_swa            = 0
0.00.051.413 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.413 I llm_load_print_meta: n_gqa            = 1
0.00.051.414 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.416 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.419 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.419 I llm_load_print_meta: n_ff             = 8192
0.00.051.419 I llm_load_print_meta: n_expert         = 0
0.00.051.419 I llm_load_print_meta: n_expert_used    = 0
0.00.051.421 I llm_load_print_meta: causal attn      = 1
0.00.051.421 I llm_load_print_meta: pooling type     = 0
0.00.051.421 I llm_load_print_meta: rope type        = 2
0.00.051.421 I llm_load_print_meta: rope scaling     = linear
0.00.051.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.422 I llm_load_print_meta: freq_scale_train = 1
0.00.051.423 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.423 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.423 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.423 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.423 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.423 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.424 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.424 I llm_load_print_meta: model type       = 1.4B
0.00.051.424 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.424 I llm_load_print_meta: model params     = 1.41 B
0.00.051.425 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.425 I llm_load_print_meta: general.name     = 1.4B
0.00.051.425 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.426 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.426 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.427 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.427 I llm_load_print_meta: LF token         = 128 ''
0.00.051.427 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.427 I llm_load_print_meta: max token length = 1024
0.00.053.361 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.361 I llm_load_tensors: offloading output layer to GPU
0.00.053.361 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.372 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.373 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.298 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.298 I llama_new_context_with_model: n_batch       = 2048
0.00.054.299 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.299 I llama_new_context_with_model: flash_attn    = 0
0.00.054.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.300 I llama_new_context_with_model: freq_scale    = 1
0.00.054.300 I ggml_metal_init: allocating
0.00.054.306 I ggml_metal_init: found device: Apple M4
0.00.054.308 I ggml_metal_init: picking default device: Apple M4
0.00.054.880 I ggml_metal_init: using embedded metal library
0.00.057.186 I ggml_metal_init: GPU name:   Apple M4
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.188 I ggml_metal_init: simdgroup reduction   = true
0.00.057.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.188 I ggml_metal_init: has bfloat            = true
0.00.057.188 I ggml_metal_init: use bfloat            = true
0.00.057.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.831 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.836 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.853 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.899 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.900 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.901 I llama_new_context_with_model: graph nodes  = 967
0.00.086.901 I llama_new_context_with_model: graph splits = 2
0.00.086.917 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.115 I main: llama threadpool init, n_threads = 4
0.00.441.152 I 
0.00.441.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.184 I 
0.00.441.419 I sampler seed: 1234
0.00.441.425 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.465 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.469 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.469 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.120.932 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.120.933 I llama_perf_context_print:        load time =     431.55 ms
0.01.120.934 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.67 tokens per second)
0.01.120.934 I llama_perf_context_print:        eval time =     640.75 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.120.934 I llama_perf_context_print:       total time =     679.82 ms /    70 tokens
0.01.121.128 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.110s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.894 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.375 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.385 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.385 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.390 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.396 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.344 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.168 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.171 I llama_model_loader: - type  f32:  194 tensors
0.00.024.171 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.172 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.172 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.524 I llm_load_vocab: special tokens cache size = 25
0.00.050.536 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.539 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.539 I llm_load_print_meta: arch             = gptneox
0.00.050.540 I llm_load_print_meta: vocab type       = BPE
0.00.050.540 I llm_load_print_meta: n_vocab          = 50304
0.00.050.540 I llm_load_print_meta: n_merges         = 50009
0.00.050.540 I llm_load_print_meta: vocab_only       = 0
0.00.050.541 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.541 I llm_load_print_meta: n_embd           = 2048
0.00.050.541 I llm_load_print_meta: n_layer          = 24
0.00.050.556 I llm_load_print_meta: n_head           = 16
0.00.050.557 I llm_load_print_meta: n_head_kv        = 16
0.00.050.557 I llm_load_print_meta: n_rot            = 32
0.00.050.557 I llm_load_print_meta: n_swa            = 0
0.00.050.557 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.557 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.558 I llm_load_print_meta: n_gqa            = 1
0.00.050.559 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.559 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.560 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.560 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.560 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.561 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.561 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.561 I llm_load_print_meta: n_ff             = 8192
0.00.050.562 I llm_load_print_meta: n_expert         = 0
0.00.050.562 I llm_load_print_meta: n_expert_used    = 0
0.00.050.562 I llm_load_print_meta: causal attn      = 1
0.00.050.562 I llm_load_print_meta: pooling type     = 0
0.00.050.562 I llm_load_print_meta: rope type        = 2
0.00.050.562 I llm_load_print_meta: rope scaling     = linear
0.00.050.563 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.563 I llm_load_print_meta: freq_scale_train = 1
0.00.050.563 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.564 I llm_load_print_meta: model type       = 1.4B
0.00.050.566 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.566 I llm_load_print_meta: model params     = 1.41 B
0.00.050.567 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.567 I llm_load_print_meta: general.name     = 1.4B
0.00.050.568 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.568 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.568 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.568 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.569 I llm_load_print_meta: LF token         = 128 ''
0.00.050.569 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.570 I llm_load_print_meta: max token length = 1024
0.00.052.479 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.479 I llm_load_tensors: offloading output layer to GPU
0.00.052.480 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.490 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.491 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.405 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.405 I llama_new_context_with_model: n_ctx         = 128
0.00.053.405 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.406 I llama_new_context_with_model: n_batch       = 128
0.00.053.406 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.406 I llama_new_context_with_model: flash_attn    = 0
0.00.053.406 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.406 I llama_new_context_with_model: freq_scale    = 1
0.00.053.407 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.407 I ggml_metal_init: allocating
0.00.053.410 I ggml_metal_init: found device: Apple M4
0.00.053.412 I ggml_metal_init: picking default device: Apple M4
0.00.053.997 I ggml_metal_init: using embedded metal library
0.00.056.302 I ggml_metal_init: GPU name:   Apple M4
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.304 I ggml_metal_init: simdgroup reduction   = true
0.00.056.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.304 I ggml_metal_init: has bfloat            = true
0.00.056.304 I ggml_metal_init: use bfloat            = true
0.00.056.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.226 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.228 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.242 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.153 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.154 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.154 I llama_new_context_with_model: graph nodes  = 967
0.00.068.154 I llama_new_context_with_model: graph splits = 2
0.00.068.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.564 I 
0.00.388.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.601 I perplexity: tokenizing the input ..
0.00.396.227 I perplexity: tokenization took 7.625 ms
0.00.396.230 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.875 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.530.036 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.530.058 I llama_perf_context_print:        load time =     378.67 ms
0.00.530.059 I llama_perf_context_print: prompt eval time =     132.42 ms /   128 tokens (    1.03 ms per token,   966.64 tokens per second)
0.00.530.060 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.530.060 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.530.595 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.078s
sys	0m0.073s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.081 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.681 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.685 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.824 I llama_model_loader: - type  f32:  194 tensors
0.00.023.824 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.824 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.825 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.162 I llm_load_vocab: special tokens cache size = 25
0.00.051.180 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.182 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.183 I llm_load_print_meta: arch             = gptneox
0.00.051.183 I llm_load_print_meta: vocab type       = BPE
0.00.051.183 I llm_load_print_meta: n_vocab          = 50304
0.00.051.184 I llm_load_print_meta: n_merges         = 50009
0.00.051.184 I llm_load_print_meta: vocab_only       = 0
0.00.051.184 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.184 I llm_load_print_meta: n_embd           = 2048
0.00.051.184 I llm_load_print_meta: n_layer          = 24
0.00.051.198 I llm_load_print_meta: n_head           = 16
0.00.051.199 I llm_load_print_meta: n_head_kv        = 16
0.00.051.199 I llm_load_print_meta: n_rot            = 32
0.00.051.200 I llm_load_print_meta: n_swa            = 0
0.00.051.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.201 I llm_load_print_meta: n_gqa            = 1
0.00.051.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.203 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.203 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.204 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.204 I llm_load_print_meta: n_ff             = 8192
0.00.051.204 I llm_load_print_meta: n_expert         = 0
0.00.051.205 I llm_load_print_meta: n_expert_used    = 0
0.00.051.205 I llm_load_print_meta: causal attn      = 1
0.00.051.205 I llm_load_print_meta: pooling type     = 0
0.00.051.205 I llm_load_print_meta: rope type        = 2
0.00.051.206 I llm_load_print_meta: rope scaling     = linear
0.00.051.206 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.207 I llm_load_print_meta: freq_scale_train = 1
0.00.051.207 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.207 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.207 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.207 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.209 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.209 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.209 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.209 I llm_load_print_meta: model type       = 1.4B
0.00.051.210 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.210 I llm_load_print_meta: model params     = 1.41 B
0.00.051.211 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.211 I llm_load_print_meta: general.name     = 1.4B
0.00.051.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.211 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.212 I llm_load_print_meta: LF token         = 128 ''
0.00.051.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.213 I llm_load_print_meta: max token length = 1024
0.00.053.166 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.166 I llm_load_tensors: offloading output layer to GPU
0.00.053.166 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.177 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.178 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.062 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.063 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.063 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.063 I llama_new_context_with_model: n_batch       = 2048
0.00.054.063 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.063 I llama_new_context_with_model: flash_attn    = 0
0.00.054.064 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.064 I llama_new_context_with_model: freq_scale    = 1
0.00.054.064 I ggml_metal_init: allocating
0.00.054.068 I ggml_metal_init: found device: Apple M4
0.00.054.070 I ggml_metal_init: picking default device: Apple M4
0.00.054.669 I ggml_metal_init: using embedded metal library
0.00.057.023 I ggml_metal_init: GPU name:   Apple M4
0.00.057.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.025 I ggml_metal_init: simdgroup reduction   = true
0.00.057.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.026 I ggml_metal_init: has bfloat            = true
0.00.057.026 I ggml_metal_init: use bfloat            = true
0.00.057.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.669 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.675 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.696 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.748 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.749 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.749 I llama_new_context_with_model: graph nodes  = 967
0.00.088.749 I llama_new_context_with_model: graph splits = 2
0.00.088.765 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.906 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.029 I main: llama threadpool init, n_threads = 4
0.00.550.074 I 
0.00.550.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.104 I 
0.00.550.349 I sampler seed: 1234
0.00.550.355 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.550.371 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.550.371 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.550.371 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.300.757 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.300.758 I llama_perf_context_print:        load time =     540.94 ms
0.01.300.759 I llama_perf_context_print: prompt eval time =      46.58 ms /     7 tokens (    6.65 ms per token,   150.30 tokens per second)
0.01.300.760 I llama_perf_context_print:        eval time =     700.74 ms /    63 runs   (   11.12 ms per token,    89.91 tokens per second)
0.01.300.760 I llama_perf_context_print:       total time =     750.73 ms /    70 tokens
0.01.300.993 I ggml_metal_free: deallocating

real	0m1.318s
user	0m0.112s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.723 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.199 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.111 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.056 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.057 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.057 I llama_model_loader: - type  f32:  194 tensors
0.00.023.058 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.058 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.058 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.058 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.308 I llm_load_vocab: special tokens cache size = 25
0.00.049.215 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.218 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.218 I llm_load_print_meta: arch             = gptneox
0.00.049.219 I llm_load_print_meta: vocab type       = BPE
0.00.049.219 I llm_load_print_meta: n_vocab          = 50304
0.00.049.219 I llm_load_print_meta: n_merges         = 50009
0.00.049.219 I llm_load_print_meta: vocab_only       = 0
0.00.049.220 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.220 I llm_load_print_meta: n_embd           = 2048
0.00.049.220 I llm_load_print_meta: n_layer          = 24
0.00.049.234 I llm_load_print_meta: n_head           = 16
0.00.049.235 I llm_load_print_meta: n_head_kv        = 16
0.00.049.235 I llm_load_print_meta: n_rot            = 32
0.00.049.235 I llm_load_print_meta: n_swa            = 0
0.00.049.236 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.237 I llm_load_print_meta: n_gqa            = 1
0.00.049.237 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.238 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.239 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.239 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.239 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.239 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.242 I llm_load_print_meta: n_ff             = 8192
0.00.049.242 I llm_load_print_meta: n_expert         = 0
0.00.049.242 I llm_load_print_meta: n_expert_used    = 0
0.00.049.242 I llm_load_print_meta: causal attn      = 1
0.00.049.242 I llm_load_print_meta: pooling type     = 0
0.00.049.244 I llm_load_print_meta: rope type        = 2
0.00.049.244 I llm_load_print_meta: rope scaling     = linear
0.00.049.244 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.244 I llm_load_print_meta: freq_scale_train = 1
0.00.049.245 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.245 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.245 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.245 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.245 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.246 I llm_load_print_meta: model type       = 1.4B
0.00.049.246 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.246 I llm_load_print_meta: model params     = 1.41 B
0.00.049.247 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.247 I llm_load_print_meta: general.name     = 1.4B
0.00.049.248 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.248 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.249 I llm_load_print_meta: LF token         = 128 ''
0.00.049.249 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.249 I llm_load_print_meta: max token length = 1024
0.00.051.196 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.197 I llm_load_tensors: offloading output layer to GPU
0.00.051.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.208 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.209 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.108 I llama_new_context_with_model: n_ctx         = 128
0.00.052.108 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.108 I llama_new_context_with_model: n_batch       = 128
0.00.052.109 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.109 I llama_new_context_with_model: flash_attn    = 0
0.00.052.109 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.109 I llama_new_context_with_model: freq_scale    = 1
0.00.052.110 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.110 I ggml_metal_init: allocating
0.00.052.113 I ggml_metal_init: found device: Apple M4
0.00.052.115 I ggml_metal_init: picking default device: Apple M4
0.00.052.676 I ggml_metal_init: using embedded metal library
0.00.054.953 I ggml_metal_init: GPU name:   Apple M4
0.00.054.954 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.955 I ggml_metal_init: simdgroup reduction   = true
0.00.054.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.955 I ggml_metal_init: has bfloat            = true
0.00.054.956 I ggml_metal_init: use bfloat            = true
0.00.054.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.787 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.792 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.723 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.724 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.724 I llama_new_context_with_model: graph nodes  = 967
0.00.066.725 I llama_new_context_with_model: graph splits = 2
0.00.066.737 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.046 I 
0.00.499.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.132 I perplexity: tokenizing the input ..
0.00.506.822 I perplexity: tokenization took 7.689 ms
0.00.506.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.639.057 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.640.224 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.640.241 I llama_perf_context_print:        load time =     490.32 ms
0.00.640.242 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.76 tokens per second)
0.00.640.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.640.244 I llama_perf_context_print:       total time =     141.20 ms /   129 tokens
0.00.640.794 I ggml_metal_free: deallocating

real	0m0.655s
user	0m0.078s
sys	0m0.099s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.011.750 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.350 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.354 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.355 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.355 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.362 I llama_model_loader: - type  f32:  194 tensors
0.00.026.363 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.363 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.363 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.984 I llm_load_vocab: special tokens cache size = 25
0.00.052.789 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.792 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.792 I llm_load_print_meta: arch             = gptneox
0.00.052.792 I llm_load_print_meta: vocab type       = BPE
0.00.052.793 I llm_load_print_meta: n_vocab          = 50304
0.00.052.793 I llm_load_print_meta: n_merges         = 50009
0.00.052.793 I llm_load_print_meta: vocab_only       = 0
0.00.052.793 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.793 I llm_load_print_meta: n_embd           = 2048
0.00.052.793 I llm_load_print_meta: n_layer          = 24
0.00.052.808 I llm_load_print_meta: n_head           = 16
0.00.052.809 I llm_load_print_meta: n_head_kv        = 16
0.00.052.809 I llm_load_print_meta: n_rot            = 32
0.00.052.809 I llm_load_print_meta: n_swa            = 0
0.00.052.809 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.810 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.810 I llm_load_print_meta: n_gqa            = 1
0.00.052.811 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.814 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.814 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.814 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.815 I llm_load_print_meta: n_ff             = 8192
0.00.052.815 I llm_load_print_meta: n_expert         = 0
0.00.052.816 I llm_load_print_meta: n_expert_used    = 0
0.00.052.816 I llm_load_print_meta: causal attn      = 1
0.00.052.816 I llm_load_print_meta: pooling type     = 0
0.00.052.816 I llm_load_print_meta: rope type        = 2
0.00.052.816 I llm_load_print_meta: rope scaling     = linear
0.00.052.817 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.817 I llm_load_print_meta: freq_scale_train = 1
0.00.052.817 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.817 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.817 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.817 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.818 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.818 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.818 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.819 I llm_load_print_meta: model type       = 1.4B
0.00.052.819 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.819 I llm_load_print_meta: model params     = 1.41 B
0.00.052.820 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.820 I llm_load_print_meta: general.name     = 1.4B
0.00.052.820 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.820 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.820 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.821 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.821 I llm_load_print_meta: LF token         = 128 ''
0.00.052.821 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.821 I llm_load_print_meta: max token length = 1024
0.00.054.807 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.807 I llm_load_tensors: offloading output layer to GPU
0.00.054.807 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.818 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.819 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.748 I llama_new_context_with_model: n_batch       = 2048
0.00.055.748 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.748 I llama_new_context_with_model: flash_attn    = 0
0.00.055.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.749 I llama_new_context_with_model: freq_scale    = 1
0.00.055.749 I ggml_metal_init: allocating
0.00.055.753 I ggml_metal_init: found device: Apple M4
0.00.055.755 I ggml_metal_init: picking default device: Apple M4
0.00.056.340 I ggml_metal_init: using embedded metal library
0.00.058.711 I ggml_metal_init: GPU name:   Apple M4
0.00.058.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.714 I ggml_metal_init: simdgroup reduction   = true
0.00.058.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.714 I ggml_metal_init: has bfloat            = true
0.00.058.714 I ggml_metal_init: use bfloat            = true
0.00.058.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.291 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.297 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.315 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.399 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.401 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.401 I llama_new_context_with_model: graph nodes  = 967
0.00.089.401 I llama_new_context_with_model: graph splits = 2
0.00.089.412 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.560 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.142 I main: llama threadpool init, n_threads = 4
0.00.615.183 I 
0.00.615.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.212 I 
0.00.615.442 I sampler seed: 1234
0.00.615.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.494 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.494 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.379.059 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.379.060 I llama_perf_context_print:        load time =     603.39 ms
0.01.379.061 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.39 tokens per second)
0.01.379.061 I llama_perf_context_print:        eval time =     713.57 ms /    63 runs   (   11.33 ms per token,    88.29 tokens per second)
0.01.379.061 I llama_perf_context_print:       total time =     763.92 ms /    70 tokens
0.01.379.299 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.316 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.874 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.888 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.836 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.836 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.837 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.837 I llama_model_loader: - type  f32:  194 tensors
0.00.023.838 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.838 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.838 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.916 I llm_load_vocab: special tokens cache size = 25
0.00.050.923 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.926 I llm_load_print_meta: arch             = gptneox
0.00.050.926 I llm_load_print_meta: vocab type       = BPE
0.00.050.927 I llm_load_print_meta: n_vocab          = 50304
0.00.050.927 I llm_load_print_meta: n_merges         = 50009
0.00.050.927 I llm_load_print_meta: vocab_only       = 0
0.00.050.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.927 I llm_load_print_meta: n_embd           = 2048
0.00.050.927 I llm_load_print_meta: n_layer          = 24
0.00.050.941 I llm_load_print_meta: n_head           = 16
0.00.050.942 I llm_load_print_meta: n_head_kv        = 16
0.00.050.945 I llm_load_print_meta: n_rot            = 32
0.00.050.945 I llm_load_print_meta: n_swa            = 0
0.00.050.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.945 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.946 I llm_load_print_meta: n_gqa            = 1
0.00.050.947 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.947 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.948 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.948 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.950 I llm_load_print_meta: n_ff             = 8192
0.00.050.950 I llm_load_print_meta: n_expert         = 0
0.00.050.950 I llm_load_print_meta: n_expert_used    = 0
0.00.050.950 I llm_load_print_meta: causal attn      = 1
0.00.050.950 I llm_load_print_meta: pooling type     = 0
0.00.050.950 I llm_load_print_meta: rope type        = 2
0.00.050.952 I llm_load_print_meta: rope scaling     = linear
0.00.050.953 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.953 I llm_load_print_meta: freq_scale_train = 1
0.00.050.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.954 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.955 I llm_load_print_meta: model type       = 1.4B
0.00.050.956 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.956 I llm_load_print_meta: model params     = 1.41 B
0.00.050.956 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.956 I llm_load_print_meta: general.name     = 1.4B
0.00.050.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: LF token         = 128 ''
0.00.050.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: max token length = 1024
0.00.052.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.921 I llm_load_tensors: offloading output layer to GPU
0.00.052.921 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.932 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.933 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.868 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.868 I llama_new_context_with_model: n_ctx         = 128
0.00.053.869 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.869 I llama_new_context_with_model: n_batch       = 128
0.00.053.869 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.869 I llama_new_context_with_model: flash_attn    = 0
0.00.053.870 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.870 I llama_new_context_with_model: freq_scale    = 1
0.00.053.870 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.871 I ggml_metal_init: allocating
0.00.053.877 I ggml_metal_init: found device: Apple M4
0.00.053.879 I ggml_metal_init: picking default device: Apple M4
0.00.054.454 I ggml_metal_init: using embedded metal library
0.00.056.810 I ggml_metal_init: GPU name:   Apple M4
0.00.056.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.812 I ggml_metal_init: simdgroup reduction   = true
0.00.056.812 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.813 I ggml_metal_init: has bfloat            = true
0.00.056.813 I ggml_metal_init: use bfloat            = true
0.00.056.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.034 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.030 I llama_new_context_with_model: graph nodes  = 967
0.00.069.030 I llama_new_context_with_model: graph splits = 2
0.00.069.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.044 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.551.967 I 
0.00.552.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.552.015 I perplexity: tokenizing the input ..
0.00.559.747 I perplexity: tokenization took 7.731 ms
0.00.559.755 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.694.262 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.695.499 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.695.513 I llama_perf_context_print:        load time =     542.65 ms
0.00.695.514 I llama_perf_context_print: prompt eval time =     134.28 ms /   128 tokens (    1.05 ms per token,   953.23 tokens per second)
0.00.695.514 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.515 I llama_perf_context_print:       total time =     143.55 ms /   129 tokens
0.00.695.966 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.079s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.915 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.445 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.452 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.453 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.458 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.460 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.462 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.462 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.518 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.519 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.519 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.520 I llama_model_loader: - type  f32:  194 tensors
0.00.023.520 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.520 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.405 I llm_load_vocab: special tokens cache size = 25
0.00.051.548 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.551 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.552 I llm_load_print_meta: arch             = gptneox
0.00.051.552 I llm_load_print_meta: vocab type       = BPE
0.00.051.552 I llm_load_print_meta: n_vocab          = 50304
0.00.051.552 I llm_load_print_meta: n_merges         = 50009
0.00.051.552 I llm_load_print_meta: vocab_only       = 0
0.00.051.553 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.553 I llm_load_print_meta: n_embd           = 2048
0.00.051.553 I llm_load_print_meta: n_layer          = 24
0.00.051.569 I llm_load_print_meta: n_head           = 16
0.00.051.570 I llm_load_print_meta: n_head_kv        = 16
0.00.051.571 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.571 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.571 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.572 I llm_load_print_meta: n_gqa            = 1
0.00.051.573 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.573 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.574 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.574 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.575 I llm_load_print_meta: n_ff             = 8192
0.00.051.577 I llm_load_print_meta: n_expert         = 0
0.00.051.577 I llm_load_print_meta: n_expert_used    = 0
0.00.051.577 I llm_load_print_meta: causal attn      = 1
0.00.051.577 I llm_load_print_meta: pooling type     = 0
0.00.051.577 I llm_load_print_meta: rope type        = 2
0.00.051.577 I llm_load_print_meta: rope scaling     = linear
0.00.051.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.578 I llm_load_print_meta: freq_scale_train = 1
0.00.051.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.578 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.579 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.579 I llm_load_print_meta: model type       = 1.4B
0.00.051.580 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.580 I llm_load_print_meta: model params     = 1.41 B
0.00.051.580 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.581 I llm_load_print_meta: general.name     = 1.4B
0.00.051.581 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: LF token         = 128 ''
0.00.051.582 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: max token length = 1024
0.00.053.648 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.648 I llm_load_tensors: offloading output layer to GPU
0.00.053.648 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.658 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.660 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.562 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.563 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.563 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.563 I llama_new_context_with_model: n_batch       = 2048
0.00.054.563 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.564 I llama_new_context_with_model: flash_attn    = 0
0.00.054.564 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.564 I llama_new_context_with_model: freq_scale    = 1
0.00.054.565 I ggml_metal_init: allocating
0.00.054.569 I ggml_metal_init: found device: Apple M4
0.00.054.571 I ggml_metal_init: picking default device: Apple M4
0.00.055.184 I ggml_metal_init: using embedded metal library
0.00.057.661 I ggml_metal_init: GPU name:   Apple M4
0.00.057.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.664 I ggml_metal_init: simdgroup reduction   = true
0.00.057.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.664 I ggml_metal_init: has bfloat            = true
0.00.057.665 I ggml_metal_init: use bfloat            = true
0.00.057.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.709 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.716 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.736 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.698 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.700 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.700 I llama_new_context_with_model: graph nodes  = 967
0.00.088.700 I llama_new_context_with_model: graph splits = 2
0.00.088.718 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.032 I main: llama threadpool init, n_threads = 4
0.00.689.065 I 
0.00.689.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.100 I 
0.00.689.320 I sampler seed: 1234
0.00.689.324 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.376 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.378 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.378 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.537.341 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.537.341 I llama_perf_context_print:        load time =     680.11 ms
0.01.537.343 I llama_perf_context_print: prompt eval time =      51.30 ms /     7 tokens (    7.33 ms per token,   136.47 tokens per second)
0.01.537.344 I llama_perf_context_print:        eval time =     793.74 ms /    63 runs   (   12.60 ms per token,    79.37 tokens per second)
0.01.537.344 I llama_perf_context_print:       total time =     848.31 ms /    70 tokens
0.01.537.535 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.450 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.067 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.078 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.056 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.100 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.101 I llama_model_loader: - type  f32:  194 tensors
0.00.024.101 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.102 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.285 I llm_load_vocab: special tokens cache size = 25
0.00.051.272 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.275 I llm_load_print_meta: arch             = gptneox
0.00.051.276 I llm_load_print_meta: vocab type       = BPE
0.00.051.276 I llm_load_print_meta: n_vocab          = 50304
0.00.051.276 I llm_load_print_meta: n_merges         = 50009
0.00.051.276 I llm_load_print_meta: vocab_only       = 0
0.00.051.276 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.277 I llm_load_print_meta: n_embd           = 2048
0.00.051.277 I llm_load_print_meta: n_layer          = 24
0.00.051.292 I llm_load_print_meta: n_head           = 16
0.00.051.293 I llm_load_print_meta: n_head_kv        = 16
0.00.051.294 I llm_load_print_meta: n_rot            = 32
0.00.051.294 I llm_load_print_meta: n_swa            = 0
0.00.051.294 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.294 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.295 I llm_load_print_meta: n_gqa            = 1
0.00.051.296 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.296 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.297 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.297 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.299 I llm_load_print_meta: n_ff             = 8192
0.00.051.300 I llm_load_print_meta: n_expert         = 0
0.00.051.300 I llm_load_print_meta: n_expert_used    = 0
0.00.051.300 I llm_load_print_meta: causal attn      = 1
0.00.051.300 I llm_load_print_meta: pooling type     = 0
0.00.051.300 I llm_load_print_meta: rope type        = 2
0.00.051.300 I llm_load_print_meta: rope scaling     = linear
0.00.051.301 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.301 I llm_load_print_meta: freq_scale_train = 1
0.00.051.301 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.301 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.301 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.303 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.303 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.303 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.303 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.303 I llm_load_print_meta: model type       = 1.4B
0.00.051.304 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.304 I llm_load_print_meta: model params     = 1.41 B
0.00.051.304 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.305 I llm_load_print_meta: general.name     = 1.4B
0.00.051.305 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.305 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.305 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.305 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.306 I llm_load_print_meta: LF token         = 128 ''
0.00.051.306 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.306 I llm_load_print_meta: max token length = 1024
0.00.053.360 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.360 I llm_load_tensors: offloading output layer to GPU
0.00.053.360 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.371 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.372 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.355 I llama_new_context_with_model: n_ctx         = 128
0.00.054.356 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.356 I llama_new_context_with_model: n_batch       = 128
0.00.054.356 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.356 I llama_new_context_with_model: flash_attn    = 0
0.00.054.357 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.357 I llama_new_context_with_model: freq_scale    = 1
0.00.054.357 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.358 I ggml_metal_init: allocating
0.00.054.363 I ggml_metal_init: found device: Apple M4
0.00.054.365 I ggml_metal_init: picking default device: Apple M4
0.00.054.975 I ggml_metal_init: using embedded metal library
0.00.057.327 I ggml_metal_init: GPU name:   Apple M4
0.00.057.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.329 I ggml_metal_init: simdgroup reduction   = true
0.00.057.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.330 I ggml_metal_init: has bfloat            = true
0.00.057.330 I ggml_metal_init: use bfloat            = true
0.00.057.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.472 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.475 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.489 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.419 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.420 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.420 I llama_new_context_with_model: graph nodes  = 967
0.00.069.420 I llama_new_context_with_model: graph splits = 2
0.00.069.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.827 I 
0.00.636.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.874 I perplexity: tokenizing the input ..
0.00.644.544 I perplexity: tokenization took 7.668 ms
0.00.644.547 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.252 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.435 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.455 I llama_perf_context_print:        load time =     627.37 ms
0.00.786.456 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.17 tokens per second)
0.00.786.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.459 I llama_perf_context_print:       total time =     149.63 ms /   129 tokens
0.00.786.820 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.079s
sys	0m0.111s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.010.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.024 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.033 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.033 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.034 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.821 I llama_model_loader: - type  f32:  194 tensors
0.00.024.821 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.299 I llm_load_vocab: special tokens cache size = 25
0.00.051.230 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.233 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.233 I llm_load_print_meta: arch             = gptneox
0.00.051.233 I llm_load_print_meta: vocab type       = BPE
0.00.051.234 I llm_load_print_meta: n_vocab          = 50304
0.00.051.234 I llm_load_print_meta: n_merges         = 50009
0.00.051.234 I llm_load_print_meta: vocab_only       = 0
0.00.051.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.234 I llm_load_print_meta: n_embd           = 2048
0.00.051.235 I llm_load_print_meta: n_layer          = 24
0.00.051.249 I llm_load_print_meta: n_head           = 16
0.00.051.250 I llm_load_print_meta: n_head_kv        = 16
0.00.051.250 I llm_load_print_meta: n_rot            = 32
0.00.051.250 I llm_load_print_meta: n_swa            = 0
0.00.051.250 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.250 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.251 I llm_load_print_meta: n_gqa            = 1
0.00.051.252 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.252 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.253 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.253 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.255 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.255 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.255 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.256 I llm_load_print_meta: n_ff             = 8192
0.00.051.256 I llm_load_print_meta: n_expert         = 0
0.00.051.256 I llm_load_print_meta: n_expert_used    = 0
0.00.051.256 I llm_load_print_meta: causal attn      = 1
0.00.051.256 I llm_load_print_meta: pooling type     = 0
0.00.051.256 I llm_load_print_meta: rope type        = 2
0.00.051.258 I llm_load_print_meta: rope scaling     = linear
0.00.051.258 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.259 I llm_load_print_meta: freq_scale_train = 1
0.00.051.259 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.259 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.259 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.259 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.259 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.259 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.260 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.260 I llm_load_print_meta: model type       = 1.4B
0.00.051.260 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.260 I llm_load_print_meta: model params     = 1.41 B
0.00.051.261 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.261 I llm_load_print_meta: general.name     = 1.4B
0.00.051.261 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.263 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.263 I llm_load_print_meta: LF token         = 128 ''
0.00.051.264 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.264 I llm_load_print_meta: max token length = 1024
0.00.053.206 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.206 I llm_load_tensors: offloading output layer to GPU
0.00.053.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.217 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.218 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.099 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.099 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.099 I llama_new_context_with_model: n_batch       = 2048
0.00.054.100 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.100 I llama_new_context_with_model: flash_attn    = 0
0.00.054.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.100 I llama_new_context_with_model: freq_scale    = 1
0.00.054.101 I ggml_metal_init: allocating
0.00.054.106 I ggml_metal_init: found device: Apple M4
0.00.054.108 I ggml_metal_init: picking default device: Apple M4
0.00.054.733 I ggml_metal_init: using embedded metal library
0.00.057.065 I ggml_metal_init: GPU name:   Apple M4
0.00.057.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.067 I ggml_metal_init: simdgroup reduction   = true
0.00.057.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.068 I ggml_metal_init: has bfloat            = true
0.00.057.068 I ggml_metal_init: use bfloat            = true
0.00.057.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.710 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.717 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.734 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.762 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.763 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.764 I llama_new_context_with_model: graph nodes  = 967
0.00.087.764 I llama_new_context_with_model: graph splits = 2
0.00.087.779 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.907 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.458 I main: llama threadpool init, n_threads = 4
0.00.753.496 I 
0.00.753.539 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.540 I 
0.00.753.777 I sampler seed: 1234
0.00.753.810 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.863 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.865 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.865 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.632.342 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.632.343 I llama_perf_context_print:        load time =     742.92 ms
0.01.632.343 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.62 tokens per second)
0.01.632.344 I llama_perf_context_print:        eval time =     821.05 ms /    63 runs   (   13.03 ms per token,    76.73 tokens per second)
0.01.632.344 I llama_perf_context_print:       total time =     878.88 ms /    70 tokens
0.01.632.540 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4352 (6b064c92) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.283 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.712 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.725 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.659 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.554 I llama_model_loader: - type  f32:  194 tensors
0.00.023.555 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.734 I llm_load_vocab: special tokens cache size = 25
0.00.049.840 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.843 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.843 I llm_load_print_meta: arch             = gptneox
0.00.049.844 I llm_load_print_meta: vocab type       = BPE
0.00.049.844 I llm_load_print_meta: n_vocab          = 50304
0.00.049.844 I llm_load_print_meta: n_merges         = 50009
0.00.049.844 I llm_load_print_meta: vocab_only       = 0
0.00.049.844 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.844 I llm_load_print_meta: n_embd           = 2048
0.00.049.845 I llm_load_print_meta: n_layer          = 24
0.00.049.859 I llm_load_print_meta: n_head           = 16
0.00.049.860 I llm_load_print_meta: n_head_kv        = 16
0.00.049.860 I llm_load_print_meta: n_rot            = 32
0.00.049.860 I llm_load_print_meta: n_swa            = 0
0.00.049.860 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.861 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.862 I llm_load_print_meta: n_gqa            = 1
0.00.049.863 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.864 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.864 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.864 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.866 I llm_load_print_meta: n_ff             = 8192
0.00.049.866 I llm_load_print_meta: n_expert         = 0
0.00.049.867 I llm_load_print_meta: n_expert_used    = 0
0.00.049.867 I llm_load_print_meta: causal attn      = 1
0.00.049.867 I llm_load_print_meta: pooling type     = 0
0.00.049.867 I llm_load_print_meta: rope type        = 2
0.00.049.868 I llm_load_print_meta: rope scaling     = linear
0.00.049.868 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.868 I llm_load_print_meta: freq_scale_train = 1
0.00.049.868 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.870 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.870 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.870 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.870 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.870 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.870 I llm_load_print_meta: model type       = 1.4B
0.00.049.871 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.871 I llm_load_print_meta: model params     = 1.41 B
0.00.049.871 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.871 I llm_load_print_meta: general.name     = 1.4B
0.00.049.872 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.872 I llm_load_print_meta: LF token         = 128 ''
0.00.049.873 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.873 I llm_load_print_meta: max token length = 1024
0.00.051.876 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.876 I llm_load_tensors: offloading output layer to GPU
0.00.051.877 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.887 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.888 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.827 I llama_new_context_with_model: n_ctx         = 128
0.00.052.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.827 I llama_new_context_with_model: n_batch       = 128
0.00.052.827 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.827 I llama_new_context_with_model: flash_attn    = 0
0.00.052.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.828 I llama_new_context_with_model: freq_scale    = 1
0.00.052.828 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.829 I ggml_metal_init: allocating
0.00.052.832 I ggml_metal_init: found device: Apple M4
0.00.052.834 I ggml_metal_init: picking default device: Apple M4
0.00.053.409 I ggml_metal_init: using embedded metal library
0.00.055.726 I ggml_metal_init: GPU name:   Apple M4
0.00.055.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.728 I ggml_metal_init: simdgroup reduction   = true
0.00.055.728 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.728 I ggml_metal_init: has bfloat            = true
0.00.055.729 I ggml_metal_init: use bfloat            = true
0.00.055.729 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.705 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.709 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.724 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.647 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.648 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.648 I llama_new_context_with_model: graph nodes  = 967
0.00.067.648 I llama_new_context_with_model: graph splits = 2
0.00.067.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.241.764 I 
0.00.241.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.241.819 I perplexity: tokenizing the input ..
0.00.249.269 I perplexity: tokenization took 7.45 ms
0.00.249.273 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.389.624 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.390.860 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.390.874 I llama_perf_context_print:        load time =     232.48 ms
0.00.390.878 I llama_perf_context_print: prompt eval time =     140.12 ms /   128 tokens (    1.09 ms per token,   913.47 tokens per second)
0.00.390.880 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.390.881 I llama_perf_context_print:       total time =     149.11 ms /   129 tokens
0.00.391.305 I ggml_metal_free: deallocating

real	0m0.407s
user	0m0.078s
sys	0m0.051s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4352 (6b064c92)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11da0a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11da0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11da0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11da0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11da0baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11da0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11da0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11da0cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11da0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11da0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11da0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11da0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11da0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11da0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11da0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11da102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11da109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11da110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11da11810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11da11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11da12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11da12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11da13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11da13de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11da14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11da147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11da14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11da15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11da15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11da16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11da166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11da169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11da17230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11da17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11da17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11da17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11da18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11da18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11da18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11da19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11da195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11da19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11da19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11da1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11da1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11da1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11da1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11da1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11da1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11da1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11da1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11da1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11da1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11da1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11da1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11da1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11da1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11da1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11da1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11da20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11da204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11da20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11da20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11da212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11da21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11da21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11da220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11da22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11da229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11da22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11da23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11da237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11da23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11da241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11da24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11da24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11da251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11da256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11da25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11da26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11da266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11da26c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11da27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11da276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11da27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11da28170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11da286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11da28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11da29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11da296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11da29c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11da2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11da2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11da2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11da2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11da2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11da2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11da1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11da2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11da2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11da2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11da2d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11da2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11da2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11da2e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11da2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11da2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11da2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11da2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11da2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11da30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11da307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11da30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11da311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11da31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11da31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11da31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11da32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11da328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11da32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11da33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11da336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11da33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11da33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11da34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11da34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11da34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11da35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11da35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11da35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11da36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11da364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11da36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11da36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11da372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11da37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11da37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11da380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11da38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11da389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11da38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11da39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11da397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11da39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11da3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11da3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11da3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11da3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11da3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11da3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11da3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11da3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11da3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11da3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11da3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11da3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11da3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11da3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11da3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11da3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11da3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11da3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11da3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11da3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11da3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11da40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11da406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11da40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11da41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11da414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11da41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11da41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11da42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11da42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11da42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11da43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11da43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11da439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11da43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11da442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11da44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11da44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11da450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11da45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11da45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11da45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11da46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11da467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11da46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11da47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11da475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11da47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11da47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11da48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11da489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11da48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11da49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11da49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11da49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11da4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11da4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11da4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11da4b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11da4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11da4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11da4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11da4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11da4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11da4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11da4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11da4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11da4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11da4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11da4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11da4f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11da4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11da50210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11da50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11da50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11da51200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11da51750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11da51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11da521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11da52740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11da52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11da531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11da53730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11da53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11da541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11da54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11da54c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11da551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11da55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11da55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11da561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11da56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11da56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11da571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11da576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11da57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11da58190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11da586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11da58c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11da59180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11da596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11da59c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11da5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11da5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11da5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11da5b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11da5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11da5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11da5c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11da5c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11da5cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11da5d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11da5d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11da5dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11da5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11da5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11da5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11da5f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11da5f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11da5fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11da60110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11da60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11da60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11da61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11da614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11da61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11da61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11da622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11da62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11da62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11da630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11da63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11da639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11da63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11da64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11da647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11da64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11da65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11da65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11da65d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11da664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11da66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11da672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11da675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11da67d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11da68050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11da68660 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.978 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11da25420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11da25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11da25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11da26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11da265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11da26a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11da26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11da27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11da277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11da27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11da28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11da28660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11da28f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11da296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11da29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11da2a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11da2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11da2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11da2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11da2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11da2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11da2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11da2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11da2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11da2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11da2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11da2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11da2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11da2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11da2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11da30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11da305b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11da30a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11da30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11da31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11da315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11da31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11da31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11da32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11da32780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11da32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11da33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11da334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11da33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11da33db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11da34220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11da34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11da34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11da34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11da353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11da35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11da35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11da36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11da365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11da36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11da36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11da372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11da37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11da37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11da38040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11da384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11da38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11da38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11da39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11da39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11da39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11da39f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11da3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11da3a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11da3aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11da3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11da3b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11da3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11da3be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11da3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11da3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11da3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11da3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11da3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11da3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11da3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11da3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11da3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11da3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11da3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11da3f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11da3f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11da3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11da400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11da40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11da409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11da40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11da412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11da41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11da41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11da42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11da42470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11da428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11da42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11da431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11da43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11da43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11da43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11da44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11da447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11da44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11da450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11da45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11da459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11da45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11da46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11da46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11da46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11da46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11da47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11da478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11da47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11da481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11da48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11da48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11da48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11da49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11da497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11da49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11da4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11da4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11da4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11da4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11da4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11da4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11da4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11da4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11da4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11da4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11da4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11da4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11da4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11da4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11da4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11da4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11da4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11da4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11da4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11da4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11da4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11da4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11da50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11da506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11da50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11da50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11da51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11da51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11da51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11da52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11da525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11da52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11da52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11da53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11da53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11da53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11da54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11da544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11da54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11da54dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11da55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11da556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11da55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11da55f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11da563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11da56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11da56cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11da57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11da575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11da57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11da57e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11da58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11da58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11da58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11da59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11da594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11da59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11da59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11da5a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11da5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11da5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11da5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11da5b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11da5b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11da5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11da5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11da5c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11da5ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11da5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11da5d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11da5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11da5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11da5e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11da5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11da5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11da5ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11da5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11da5f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11da5fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11da5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11da603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11da60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11da60c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11da61100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11da61570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11da619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11da62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11da625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11da62a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11da62eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11da63320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11da63790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11da63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11da64070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11da644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11da64950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11da64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11da65230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11da656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11da65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11da65f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11da663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11da66860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11da66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11da67140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11da675b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11da67a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11da67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11da68300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11da68770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11da0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11da0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11da0a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11da17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11da17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11da17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11da18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11da18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11da18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11da19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11da194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11da19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11da19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11da1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11da1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11da1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11da1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11da1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11da1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11da1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11da1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11da1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11da1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11da1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11da1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11da1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11da1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11da1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11da1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11da1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11da1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11da1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11da1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11da1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11da1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11da203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11da20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11da20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11da21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11da21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11da219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11da21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11da222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11da22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11da22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11da23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11da23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11da238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11da23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11da24450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11da161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11da168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11da16fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11da0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11da0dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11da0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11da0e3c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11cd044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11cd04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11cd04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11cd05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11cd056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11cd05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11cd05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11cd063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11cd06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11cd06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11cd07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11cd07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11cd08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11cd08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11cd09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11cd09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11cd0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11cd0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11cd0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11cd0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11cd0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11cd0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11cd0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11cd0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11cd0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11cd0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11cd0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11cd0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11cd0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11cd0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11cd0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11cd0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11cd0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11cd0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11cd102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11cd10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11cd10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11cd10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11cd11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11cd118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11cd11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11cd121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11cd12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11cd12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11cd12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11cd13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11cd137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11cd13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11cd140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11cd14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11cd149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11cd14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11cd15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11cd156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11cd15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11cd15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11cd16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11cd16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11cd16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11cd17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11cd17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11cd17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11cd18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11cd184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11cd18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11cd18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11cd19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11cd196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11cd19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11cd19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11cd1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11cd1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11cd1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11cd1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11cd1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11cd1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11cd1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11cd1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11cd1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11cd1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11cd1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11cd1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11cd1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11cd1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11cd1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11cd1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11cd1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11cd1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11cd1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11cd1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11cd1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11cd20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11cd20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11cd20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11cd20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11cd212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11cd21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11cd21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11cd22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11cd224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11cd22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11cd22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11cd231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11cd23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11cd23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11cd23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11cd243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11cd24820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11cd24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11cd25100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11cd25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11cd259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11cd25e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11cd262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11cd26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11cd26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11cd27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11cd27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11cd278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11cd27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11cd281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11cd28640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11cd28ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11cd28f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11cd29390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11cd29800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11cd29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11cd2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11cd2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11cd2a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11cd2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11cd2b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11cd2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11cd2bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11cd2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11cd2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11cd2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11cd2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11cd2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11cd2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11cd2da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11cd2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11cd2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11cd2e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11cd2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11cd2f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11cd2f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11cd2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11cd2fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11cd30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11cd306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11cd30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11cd30fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11cd31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11cd318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11cd31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11cd32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11cd32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11cd32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11cd32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11cd33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11cd337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11cd33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11cd340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11cd34510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11cd34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11cd34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11cd35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11cd356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11cd35b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11cd35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11cd36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11cd36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11cd36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11cd37170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11cd375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11cd37a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11cd37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11cd38330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11cd387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11cd38c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11cd39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11cd394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11cd39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11cd39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11cd3a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11cd3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11cd3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11cd3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11cd3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11cd3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11cd3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11cd3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11cd3c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11cd3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11cd3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11cd3d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11cd3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11cd3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11cd3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11cd3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11cd3e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11cd3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11cd3f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11cd3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11cd3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11cd3ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11cd40500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11cd40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11cd40de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11cd41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11cd41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11cd41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11cd42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11cd42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11cd42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11cd43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11cd434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11cd43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11cd43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11cd44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11cd446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11cd44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11cd44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11cd453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11cd45860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11cd45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11cd46140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11cd465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11cd46a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11cd46e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11cd47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11cd47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11cd47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11cd48050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11cd484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11cd48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11cd48da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11cd49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11cd49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11cd49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11cd49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11cd4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11cd4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11cd4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11cd4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11cd4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11cd4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11cd4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11cd4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11cd4caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11cd4cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11cd4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11cd4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11cd4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11cd4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11cd4e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11cd4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11cd4ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11cd4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11cd4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11cd4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11cd4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11cd50450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11cd508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11cd50d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11cd511a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11cd51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11cd51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11cd51ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11cd52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11cd527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11cd52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11cd530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11cd53520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11cd53990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11cd53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11cd54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11cd546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11cd54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11cd54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11cd55430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11cd558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11cd56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11cd56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11cd57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11cd57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11cd57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11cd57fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11cd585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11cd58bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.843s
user	0m0.308s
sys	0m0.281s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4352 (6b064c92)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee0ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee0d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee0e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee0f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee10d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee11490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee14000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee16c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee17b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee19e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee1a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee1b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee1d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee1dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee21400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee22000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee23720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee23bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee26380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee26e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee27370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee28360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee29df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee2a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee2ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee2c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee2d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee2e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee2ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee30460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee30f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee44b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee47090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee48310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee4ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee4f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee5d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee5ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee5f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee60300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee60850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee60da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee61840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee62b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee63010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee64730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee64bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee65510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee65e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee66f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee67da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee68f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee69230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee69840 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.789 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x130004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x130004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1300053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x130005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x130005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x130006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x130006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1300069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x130006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1300072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x130007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x130007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x130008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1300090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x130009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13000a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13000a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13000ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13000b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13000bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13000c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13000cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13000d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13000d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13000e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13000e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13000e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13000eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13000ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13000f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13000f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13000fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1300101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x130010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x130010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x130010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1300111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x130011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x130011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x130011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1300123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x130012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x130012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1300130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x130013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1300139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x130013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1300142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x130014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x130014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x130015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x130015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1300158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x130015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1300161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x130016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x130016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1300170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x130017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x130017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x130018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1300186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x130018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x130018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x130019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x130019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x130019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13001a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13001a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13001aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13001aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13001b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13001b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13001bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13001c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13001c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13001c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13001cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13001d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13001d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13001db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13001df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13001e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13001e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13001ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13001f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13001f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13001fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13001fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x130020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x130020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x130020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x130021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1300214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x130021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x130021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x130022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x130022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x130022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x130022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1300233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x130023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x130023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x130024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1300245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x130024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x130024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1300252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x130025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x130025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x130026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1300264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x130026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x130026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x130027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x130027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x130027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x130027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1300283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x130028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x130028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x130029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x130029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1300299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x130029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13002a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13002a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13002abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13002b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13002b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13002b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13002bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13002c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13002c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13002cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13002cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13002d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13002d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13002dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13002e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13002e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13002e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13002ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13002f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13002f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13002fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x130030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x130030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1300308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x130030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1300311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x130031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x130031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x130032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1300327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x130032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1300330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x130033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1300339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x130033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x130034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x130034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x130034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x130035450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1300358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130035d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1300361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x130036610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130036a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x130036ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x130037360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1300377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x130037c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1300380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x130038520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x130038990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x130038e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x130039270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1300396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x130039b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x130039fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13003a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13003a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13003ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13003b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13003b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13003ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13003bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13003c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13003c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13003cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13003d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13003d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13003d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13003dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13003e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13003e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13003eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13003efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13003f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13003f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13003fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x130040160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1300405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x130040b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x130040fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x130041440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x130041f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x130042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130042510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x130042980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x130042df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x130043260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1300436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x130043b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x130043fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x130044420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x130044890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x130044d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x130045170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1300455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x130045a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x130045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x130046330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1300467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x130046c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x130047080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1300474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x130047960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x130047dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x130048240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1300486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130048b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x130048f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x130049400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x130049870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x130049ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13004a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13004a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13004aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13004aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13004b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13004b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13004bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13004c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13004c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13004c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13004cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13004d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13004d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13004db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13004df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13004e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13004e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13004ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13004f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13004f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13004fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13004fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1300502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x130050760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x130050bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x130051040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1300514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x130051920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x130051d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x130052200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x130052670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x130052ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130052f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1300533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x130053830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x130053ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x130054110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x130054580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1300549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x130054e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1300552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x130055740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x130055bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x130056620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x130056d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x130057460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x130057b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x130057e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1300582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1300588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x130058ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ef046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ef04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ef04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ef05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ef058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ef05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ef06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ef065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ef06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ef06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ef07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ef079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ef08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ef08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ef094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ef09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ef0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ef0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ef0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ef0b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ef0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ef0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ef0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ef0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ef0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ef0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ef0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ef0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ef0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ef0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ef0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ef0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ef0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ef10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ef104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ef10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ef10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ef11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ef11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ef11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ef11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ef123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ef12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ef12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ef13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ef13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ef13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ef13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ef142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ef14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ef14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ef15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ef154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ef15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ef15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ef161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ef16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ef16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ef170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ef17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ef179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ef17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ef18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ef18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ef18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ef18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ef19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ef198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ef19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ef1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ef1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ef1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ef1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11ef1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11ef1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11ef1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11ef1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11ef1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11ef1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11ef1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11ef1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11ef1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11ef1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11ef1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11ef1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11ef1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11ef1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11ef1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11ef1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11ef1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11ef1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11ef20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11ef207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11ef20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11ef21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11ef21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11ef21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11ef21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11ef22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11ef226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11ef22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11ef22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11ef23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11ef23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11ef23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11ef24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11ef245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11ef24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11ef24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11ef25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11ef25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11ef25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11ef26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ef264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ef26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ef26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ef27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ef276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ef27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ef27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ef283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ef28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ef28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ef29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ef295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ef29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ef29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ef2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ef2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ef2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ef2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ef2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ef2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ef2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ef2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ef2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ef2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ef2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ef2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ef2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ef2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ef2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ef2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ef2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ef2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ef2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ef2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ef2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ef30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ef304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ef30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ef30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ef311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ef31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ef31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ef31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ef323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ef32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ef32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ef33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ef33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ef339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ef33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ef342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ef34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ef34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ef35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ef35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ef358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ef35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ef361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ef36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ef36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ef36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ef37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ef37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ef37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ef380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ef38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ef389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ef38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ef392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ef39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ef39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ef39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ef3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ef3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ef3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ef3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ef3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ef3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ef3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ef3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ef3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ef3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ef3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ef3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ef3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ef3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ef3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ef3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ef3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ef3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ef3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ef3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ef3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ef40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ef40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ef40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ef41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ef41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ef41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ef420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ef42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ef429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ef42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ef43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ef43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ef43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ef43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ef44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ef448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ef44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ef451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ef45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ef45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ef45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ef46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ef467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ef46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ef470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ef47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ef47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ef47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ef48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ef486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ef48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ef48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ef49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ef498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ef49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ef4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ef4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ef4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ef4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ef4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ef4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ef4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ef4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ef4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ef4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ef4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ef4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ef4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ef4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ef4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ef4e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ef4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ef4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ef4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ef4f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ef4fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ef50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ef50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ef50ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ef50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ef513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ef51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ef51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ef52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ef52580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ef529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ef52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ef532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ef53740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ef53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ef54020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ef54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ef54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ef54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ef551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ef55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ef55ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ef56530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ef56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ef57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ef57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ef57d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ef581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ef587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ef58dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.913s
user	0m0.244s
sys	0m0.129s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.57 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.17 sec*proc (2 tests)

Total Test time (real) =   1.18 sec
        1.20 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.27 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.38 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.65 sec*proc (2 tests)

Total Test time (real) =   0.69 sec
        0.70 real         0.16 user         0.05 sys
```
