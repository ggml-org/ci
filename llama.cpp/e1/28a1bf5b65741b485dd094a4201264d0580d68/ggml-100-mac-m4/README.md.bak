### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.19 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.57 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.30 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.70 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.64 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.30 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.76 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.38 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.36 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.13 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.08 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  106.39 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.60 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.37 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 170.44 sec*proc (29 tests)

Total Test time (real) = 170.45 sec

real	2m50.461s
user	4m42.136s
sys	0m5.799s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.35 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.87 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.62 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.17 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.98 sec*proc (29 tests)

Total Test time (real) =  48.99 sec

real	0m49.003s
user	0m55.130s
sys	0m5.268s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.697 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.624 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.633 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.634 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.635 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.636 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.637 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.638 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.638 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.639 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.639 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.642 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.643 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.644 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.644 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.645 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.645 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.646 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.206 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.208 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.209 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.209 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.210 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.210 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.032.211 I llama_model_loader: - type  f32:  124 tensors
0.00.032.211 I llama_model_loader: - type  f16:   73 tensors
0.00.032.212 I print_info: file format = GGUF V3 (latest)
0.00.032.213 I print_info: file type   = F16
0.00.032.214 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.036.551 I load: special tokens cache size = 5
0.00.038.831 I load: token to piece cache size = 0.2032 MB
0.00.038.858 I print_info: arch             = bert
0.00.038.860 I print_info: vocab_only       = 0
0.00.038.860 I print_info: n_ctx_train      = 512
0.00.038.860 I print_info: n_embd           = 384
0.00.038.861 I print_info: n_layer          = 12
0.00.038.863 I print_info: n_head           = 12
0.00.038.864 I print_info: n_head_kv        = 12
0.00.038.864 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.865 I print_info: n_embd_head_k    = 32
0.00.038.865 I print_info: n_embd_head_v    = 32
0.00.038.866 I print_info: n_gqa            = 1
0.00.038.867 I print_info: n_embd_k_gqa     = 384
0.00.038.867 I print_info: n_embd_v_gqa     = 384
0.00.038.868 I print_info: f_norm_eps       = 1.0e-12
0.00.038.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.869 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.870 I print_info: f_logit_scale    = 0.0e+00
0.00.038.870 I print_info: n_ff             = 1536
0.00.038.871 I print_info: n_expert         = 0
0.00.038.871 I print_info: n_expert_used    = 0
0.00.038.871 I print_info: causal attn      = 0
0.00.038.871 I print_info: pooling type     = 2
0.00.038.871 I print_info: rope type        = 2
0.00.038.873 I print_info: rope scaling     = linear
0.00.038.874 I print_info: freq_base_train  = 10000.0
0.00.038.874 I print_info: freq_scale_train = 1
0.00.038.874 I print_info: n_ctx_orig_yarn  = 512
0.00.038.874 I print_info: rope_finetuned   = unknown
0.00.038.875 I print_info: ssm_d_conv       = 0
0.00.038.875 I print_info: ssm_d_inner      = 0
0.00.038.875 I print_info: ssm_d_state      = 0
0.00.038.875 I print_info: ssm_dt_rank      = 0
0.00.038.876 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.876 I print_info: model type       = 33M
0.00.038.876 I print_info: model params     = 33.21 M
0.00.038.877 I print_info: general.name     = Bge Small
0.00.038.877 I print_info: vocab type       = WPM
0.00.038.878 I print_info: n_vocab          = 30522
0.00.038.878 I print_info: n_merges         = 0
0.00.038.878 I print_info: BOS token        = 101 '[CLS]'
0.00.038.879 I print_info: UNK token        = 100 '[UNK]'
0.00.038.879 I print_info: SEP token        = 102 '[SEP]'
0.00.038.879 I print_info: PAD token        = 0 '[PAD]'
0.00.038.879 I print_info: MASK token       = 103 '[MASK]'
0.00.038.882 I print_info: LF token         = 0 '[PAD]'
0.00.038.882 I print_info: max token length = 21
0.00.038.883 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.041.829 I load_tensors: offloading 12 repeating layers to GPU
0.00.041.830 I load_tensors: offloading output layer to GPU
0.00.041.831 I load_tensors: offloaded 13/13 layers to GPU
0.00.041.854 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.855 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.123 I llama_init_from_model: n_seq_max     = 1
0.00.042.124 I llama_init_from_model: n_ctx         = 512
0.00.042.124 I llama_init_from_model: n_ctx_per_seq = 512
0.00.042.125 I llama_init_from_model: n_batch       = 2048
0.00.042.125 I llama_init_from_model: n_ubatch      = 2048
0.00.042.125 I llama_init_from_model: flash_attn    = 0
0.00.042.126 I llama_init_from_model: freq_base     = 10000.0
0.00.042.126 I llama_init_from_model: freq_scale    = 1
0.00.042.126 I ggml_metal_init: allocating
0.00.042.131 I ggml_metal_init: found device: Apple M4
0.00.042.135 I ggml_metal_init: picking default device: Apple M4
0.00.042.751 I ggml_metal_init: using embedded metal library
0.00.046.514 I ggml_metal_init: GPU name:   Apple M4
0.00.046.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.046.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.046.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.046.517 I ggml_metal_init: simdgroup reduction   = true
0.00.046.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.046.518 I ggml_metal_init: has residency sets    = true
0.00.046.518 I ggml_metal_init: has bfloat            = true
0.00.046.518 I ggml_metal_init: use bfloat            = true
0.00.046.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.046.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.797 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.474 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.476 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.477 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.560 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.562 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.562 I llama_init_from_model: graph nodes  = 429
0.00.060.562 I llama_init_from_model: graph splits = 2
0.00.060.564 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.435 I 
0.00.066.461 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.082 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.070.994 I llama_perf_context_print:        load time =      45.31 ms
0.00.070.996 I llama_perf_context_print: prompt eval time =       3.78 ms /     9 tokens (    0.42 ms per token,  2380.32 tokens per second)
0.00.070.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.070.996 I llama_perf_context_print:       total time =       4.56 ms /    10 tokens
0.00.071.134 I ggml_metal_free: deallocating

real	0m0.280s
user	0m0.050s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.441 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.084 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.088 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.090 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.090 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.091 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.091 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.092 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.092 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.093 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.093 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.093 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.095 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.096 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.096 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.097 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.097 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.097 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.406 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.058 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.060 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.060 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.060 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.060 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.061 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.061 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.061 I llama_model_loader: - type  f32:  124 tensors
0.00.015.062 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.062 I print_info: file format = GGUF V3 (latest)
0.00.015.063 I print_info: file type   = Q8_0
0.00.015.064 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.540 I load: special tokens cache size = 5
0.00.018.907 I load: token to piece cache size = 0.2032 MB
0.00.018.916 I print_info: arch             = bert
0.00.018.917 I print_info: vocab_only       = 0
0.00.018.917 I print_info: n_ctx_train      = 512
0.00.018.917 I print_info: n_embd           = 384
0.00.018.917 I print_info: n_layer          = 12
0.00.018.920 I print_info: n_head           = 12
0.00.018.921 I print_info: n_head_kv        = 12
0.00.018.921 I print_info: n_rot            = 32
0.00.018.921 I print_info: n_swa            = 0
0.00.018.921 I print_info: n_embd_head_k    = 32
0.00.018.921 I print_info: n_embd_head_v    = 32
0.00.018.922 I print_info: n_gqa            = 1
0.00.018.922 I print_info: n_embd_k_gqa     = 384
0.00.018.923 I print_info: n_embd_v_gqa     = 384
0.00.018.923 I print_info: f_norm_eps       = 1.0e-12
0.00.018.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.924 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.924 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.924 I print_info: f_logit_scale    = 0.0e+00
0.00.018.925 I print_info: n_ff             = 1536
0.00.018.925 I print_info: n_expert         = 0
0.00.018.925 I print_info: n_expert_used    = 0
0.00.018.925 I print_info: causal attn      = 0
0.00.018.925 I print_info: pooling type     = 2
0.00.018.925 I print_info: rope type        = 2
0.00.018.925 I print_info: rope scaling     = linear
0.00.018.926 I print_info: freq_base_train  = 10000.0
0.00.018.926 I print_info: freq_scale_train = 1
0.00.018.926 I print_info: n_ctx_orig_yarn  = 512
0.00.018.926 I print_info: rope_finetuned   = unknown
0.00.018.927 I print_info: ssm_d_conv       = 0
0.00.018.927 I print_info: ssm_d_inner      = 0
0.00.018.927 I print_info: ssm_d_state      = 0
0.00.018.927 I print_info: ssm_dt_rank      = 0
0.00.018.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.927 I print_info: model type       = 33M
0.00.018.928 I print_info: model params     = 33.21 M
0.00.018.928 I print_info: general.name     = Bge Small
0.00.018.928 I print_info: vocab type       = WPM
0.00.018.928 I print_info: n_vocab          = 30522
0.00.018.928 I print_info: n_merges         = 0
0.00.018.929 I print_info: BOS token        = 101 '[CLS]'
0.00.018.929 I print_info: UNK token        = 100 '[UNK]'
0.00.018.929 I print_info: SEP token        = 102 '[SEP]'
0.00.018.929 I print_info: PAD token        = 0 '[PAD]'
0.00.018.929 I print_info: MASK token       = 103 '[MASK]'
0.00.018.929 I print_info: LF token         = 0 '[PAD]'
0.00.018.930 I print_info: max token length = 21
0.00.018.930 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.838 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.839 I load_tensors: offloading output layer to GPU
0.00.020.839 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.848 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.848 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.089 I llama_init_from_model: n_seq_max     = 1
0.00.021.090 I llama_init_from_model: n_ctx         = 512
0.00.021.091 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.091 I llama_init_from_model: n_batch       = 2048
0.00.021.091 I llama_init_from_model: n_ubatch      = 2048
0.00.021.091 I llama_init_from_model: flash_attn    = 0
0.00.021.092 I llama_init_from_model: freq_base     = 10000.0
0.00.021.092 I llama_init_from_model: freq_scale    = 1
0.00.021.092 I ggml_metal_init: allocating
0.00.021.102 I ggml_metal_init: found device: Apple M4
0.00.021.105 I ggml_metal_init: picking default device: Apple M4
0.00.021.537 I ggml_metal_init: using embedded metal library
0.00.023.932 I ggml_metal_init: GPU name:   Apple M4
0.00.023.934 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.935 I ggml_metal_init: simdgroup reduction   = true
0.00.023.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.935 I ggml_metal_init: has residency sets    = true
0.00.023.935 I ggml_metal_init: has bfloat            = true
0.00.023.935 I ggml_metal_init: use bfloat            = true
0.00.023.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.557 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.185 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.190 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.192 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.240 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.241 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.242 I llama_init_from_model: graph nodes  = 429
0.00.036.242 I llama_init_from_model: graph splits = 2
0.00.036.243 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.731 I 
0.00.039.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.301 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.658 I llama_perf_context_print:        load time =      30.29 ms
0.00.043.659 I llama_perf_context_print: prompt eval time =       3.23 ms /     9 tokens (    0.36 ms per token,  2782.93 tokens per second)
0.00.043.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.660 I llama_perf_context_print:       total time =       3.93 ms /    10 tokens
0.00.043.847 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.240 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.135 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.236 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.243 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.245 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.246 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.246 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.247 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.248 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.249 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.249 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.250 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.253 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.254 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.255 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.701 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.701 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.701 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.702 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.702 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.702 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.703 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.703 I llama_model_loader: - type  f32:   40 tensors
0.00.051.704 I llama_model_loader: - type  f16:   30 tensors
0.00.051.706 I print_info: file format = GGUF V3 (latest)
0.00.051.707 I print_info: file type   = F16
0.00.051.708 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.043 W load: empty token at index 5
0.00.061.451 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.025 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.062 I load: special tokens cache size = 5
0.00.330.232 I load: token to piece cache size = 1.5060 MB
0.00.330.255 I print_info: arch             = jina-bert-v2
0.00.330.256 I print_info: vocab_only       = 0
0.00.330.256 I print_info: n_ctx_train      = 8192
0.00.330.256 I print_info: n_embd           = 384
0.00.330.256 I print_info: n_layer          = 4
0.00.330.259 I print_info: n_head           = 12
0.00.330.260 I print_info: n_head_kv        = 12
0.00.330.260 I print_info: n_rot            = 32
0.00.330.260 I print_info: n_swa            = 0
0.00.330.260 I print_info: n_embd_head_k    = 32
0.00.330.260 I print_info: n_embd_head_v    = 32
0.00.330.261 I print_info: n_gqa            = 1
0.00.330.262 I print_info: n_embd_k_gqa     = 384
0.00.330.262 I print_info: n_embd_v_gqa     = 384
0.00.330.263 I print_info: f_norm_eps       = 1.0e-12
0.00.330.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.330.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.330.264 I print_info: f_max_alibi_bias = 8.0e+00
0.00.330.264 I print_info: f_logit_scale    = 0.0e+00
0.00.330.264 I print_info: n_ff             = 1536
0.00.330.264 I print_info: n_expert         = 0
0.00.330.265 I print_info: n_expert_used    = 0
0.00.330.265 I print_info: causal attn      = 0
0.00.330.265 I print_info: pooling type     = -1
0.00.330.265 I print_info: rope type        = -1
0.00.330.265 I print_info: rope scaling     = linear
0.00.330.265 I print_info: freq_base_train  = 10000.0
0.00.330.266 I print_info: freq_scale_train = 1
0.00.330.266 I print_info: n_ctx_orig_yarn  = 8192
0.00.330.266 I print_info: rope_finetuned   = unknown
0.00.330.266 I print_info: ssm_d_conv       = 0
0.00.330.266 I print_info: ssm_d_inner      = 0
0.00.330.266 I print_info: ssm_d_state      = 0
0.00.330.267 I print_info: ssm_dt_rank      = 0
0.00.330.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.330.267 I print_info: model type       = 33M
0.00.330.267 I print_info: model params     = 32.90 M
0.00.330.268 I print_info: general.name     = Jina Bert Implementation
0.00.330.268 I print_info: vocab type       = BPE
0.00.330.268 I print_info: n_vocab          = 61056
0.00.330.268 I print_info: n_merges         = 39382
0.00.330.269 I print_info: BOS token        = 0 '<s>'
0.00.330.269 I print_info: EOS token        = 2 '</s>'
0.00.330.269 I print_info: UNK token        = 3 '<unk>'
0.00.330.269 I print_info: SEP token        = 2 '</s>'
0.00.330.269 I print_info: PAD token        = 1 '<pad>'
0.00.330.269 I print_info: MASK token       = 4 '<mask>'
0.00.330.270 I print_info: EOG token        = 2 '</s>'
0.00.330.270 I print_info: max token length = 45
0.00.330.270 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.331.912 I load_tensors: offloading 4 repeating layers to GPU
0.00.331.913 I load_tensors: offloading output layer to GPU
0.00.331.913 I load_tensors: offloaded 5/5 layers to GPU
0.00.331.933 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.934 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.332.241 I llama_init_from_model: n_seq_max     = 1
0.00.332.242 I llama_init_from_model: n_ctx         = 8192
0.00.332.242 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.332.242 I llama_init_from_model: n_batch       = 2048
0.00.332.242 I llama_init_from_model: n_ubatch      = 2048
0.00.332.242 I llama_init_from_model: flash_attn    = 0
0.00.332.243 I llama_init_from_model: freq_base     = 10000.0
0.00.332.243 I llama_init_from_model: freq_scale    = 1
0.00.332.243 I ggml_metal_init: allocating
0.00.332.247 I ggml_metal_init: found device: Apple M4
0.00.332.250 I ggml_metal_init: picking default device: Apple M4
0.00.332.806 I ggml_metal_init: using embedded metal library
0.00.335.952 I ggml_metal_init: GPU name:   Apple M4
0.00.335.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.958 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.959 I ggml_metal_init: simdgroup reduction   = true
0.00.335.959 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.959 I ggml_metal_init: has residency sets    = true
0.00.335.959 I ggml_metal_init: has bfloat            = true
0.00.335.959 I ggml_metal_init: use bfloat            = true
0.00.335.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.448 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.348.441 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.443 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.444 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.355.465 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.355.467 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.355.467 I llama_init_from_model: graph nodes  = 154
0.00.355.468 I llama_init_from_model: graph splits = 2
0.00.355.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.357 I 
0.00.362.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.721 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.362.722 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.362.733 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.362.733 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.362.736 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.362.737 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.363.222 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.366.689 I llama_perf_context_print:        load time =     338.22 ms
0.00.366.690 I llama_perf_context_print: prompt eval time =       3.46 ms /    62 tokens (    0.06 ms per token, 17924.26 tokens per second)
0.00.366.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.366.692 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.366.985 I ggml_metal_free: deallocating

real	0m1.207s
user	0m0.337s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.210 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.397 I main: llama backend init
0.00.000.404 I main: load the model and apply lora adapter, if any
0.00.073.091 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.085.429 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.085.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.085.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.085.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.085.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.085.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.085.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.085.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.085.452 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.085.453 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.085.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.085.455 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.085.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.085.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.085.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.085.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.085.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.092.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.094.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.103.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.103.105 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.103.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.103.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.103.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.103.108 I llama_model_loader: - type  f32:  194 tensors
0.00.103.109 I llama_model_loader: - type  f16:   98 tensors
0.00.103.113 I print_info: file format = GGUF V3 (latest)
0.00.103.115 I print_info: file type   = all F32 (guessed)
0.00.103.117 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.119.424 I load: special tokens cache size = 25
0.00.129.860 I load: token to piece cache size = 0.2984 MB
0.00.129.893 I print_info: arch             = gptneox
0.00.129.894 I print_info: vocab_only       = 0
0.00.129.894 I print_info: n_ctx_train      = 2048
0.00.129.894 I print_info: n_embd           = 2048
0.00.129.895 I print_info: n_layer          = 24
0.00.129.902 I print_info: n_head           = 16
0.00.129.906 I print_info: n_head_kv        = 16
0.00.129.906 I print_info: n_rot            = 32
0.00.129.906 I print_info: n_swa            = 0
0.00.129.906 I print_info: n_embd_head_k    = 128
0.00.129.907 I print_info: n_embd_head_v    = 128
0.00.129.908 I print_info: n_gqa            = 1
0.00.129.909 I print_info: n_embd_k_gqa     = 2048
0.00.129.916 I print_info: n_embd_v_gqa     = 2048
0.00.129.923 I print_info: f_norm_eps       = 1.0e-05
0.00.129.926 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.129.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.129.927 I print_info: f_max_alibi_bias = 0.0e+00
0.00.129.927 I print_info: f_logit_scale    = 0.0e+00
0.00.129.936 I print_info: n_ff             = 8192
0.00.129.937 I print_info: n_expert         = 0
0.00.129.937 I print_info: n_expert_used    = 0
0.00.129.938 I print_info: causal attn      = 1
0.00.129.940 I print_info: pooling type     = 0
0.00.129.940 I print_info: rope type        = 2
0.00.129.941 I print_info: rope scaling     = linear
0.00.129.941 I print_info: freq_base_train  = 10000.0
0.00.129.941 I print_info: freq_scale_train = 1
0.00.129.942 I print_info: n_ctx_orig_yarn  = 2048
0.00.129.942 I print_info: rope_finetuned   = unknown
0.00.129.944 I print_info: ssm_d_conv       = 0
0.00.129.944 I print_info: ssm_d_inner      = 0
0.00.129.944 I print_info: ssm_d_state      = 0
0.00.129.944 I print_info: ssm_dt_rank      = 0
0.00.129.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.129.945 I print_info: model type       = 1.4B
0.00.129.947 I print_info: model params     = 1.41 B
0.00.129.947 I print_info: general.name     = 1.4B
0.00.129.948 I print_info: vocab type       = BPE
0.00.129.949 I print_info: n_vocab          = 50304
0.00.129.949 I print_info: n_merges         = 50009
0.00.129.949 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.129.950 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.129.950 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.129.950 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.129.950 I print_info: LF token         = 187 ''
0.00.129.951 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.129.951 I print_info: max token length = 1024
0.00.129.952 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.191.636 I load_tensors: offloading 24 repeating layers to GPU
0.00.191.639 I load_tensors: offloading output layer to GPU
0.00.191.639 I load_tensors: offloaded 25/25 layers to GPU
0.00.191.665 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.191.666 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.192.246 I llama_init_from_model: n_seq_max     = 1
0.00.192.246 I llama_init_from_model: n_ctx         = 2048
0.00.192.247 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.192.247 I llama_init_from_model: n_batch       = 2048
0.00.192.247 I llama_init_from_model: n_ubatch      = 512
0.00.192.247 I llama_init_from_model: flash_attn    = 0
0.00.192.248 I llama_init_from_model: freq_base     = 10000.0
0.00.192.248 I llama_init_from_model: freq_scale    = 1
0.00.192.249 I ggml_metal_init: allocating
0.00.192.290 I ggml_metal_init: found device: Apple M4
0.00.192.294 I ggml_metal_init: picking default device: Apple M4
0.00.192.845 I ggml_metal_init: using embedded metal library
0.00.202.020 I ggml_metal_init: GPU name:   Apple M4
0.00.202.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.202.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.202.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.202.023 I ggml_metal_init: simdgroup reduction   = true
0.00.202.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.202.023 I ggml_metal_init: has residency sets    = true
0.00.202.024 I ggml_metal_init: has bfloat            = true
0.00.202.024 I ggml_metal_init: use bfloat            = true
0.00.202.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.202.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.228.641 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.258.541 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.258.550 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.258.569 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.262.261 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.262.264 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.262.265 I llama_init_from_model: graph nodes  = 967
0.00.262.265 I llama_init_from_model: graph splits = 2
0.00.262.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.262.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.262.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.326.568 I main: llama threadpool init, n_threads = 4
0.00.326.626 I 
0.00.326.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.326.655 I 
0.00.326.794 I sampler seed: 1234
0.00.326.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.326.831 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.326.833 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.326.833 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.169.483 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.02.169.483 I llama_perf_context_print:        load time =     252.56 ms
0.02.169.484 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.90 tokens per second)
0.02.169.488 I llama_perf_context_print:        eval time =    1795.93 ms /    63 runs   (   28.51 ms per token,    35.08 tokens per second)
0.02.169.489 I llama_perf_context_print:       total time =    1843.82 ms /    70 tokens
0.02.169.783 I ggml_metal_free: deallocating

real	0m2.499s
user	0m0.133s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.648 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.208 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.953 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.967 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.970 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.976 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.996 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.001 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.289 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.510 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.514 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.515 I llama_model_loader: - type  f32:  194 tensors
0.00.054.515 I llama_model_loader: - type  f16:   98 tensors
0.00.054.516 I print_info: file format = GGUF V3 (latest)
0.00.054.519 I print_info: file type   = all F32 (guessed)
0.00.054.522 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.027 I load: special tokens cache size = 25
0.00.077.136 I load: token to piece cache size = 0.2984 MB
0.00.077.152 I print_info: arch             = gptneox
0.00.077.153 I print_info: vocab_only       = 0
0.00.077.153 I print_info: n_ctx_train      = 2048
0.00.077.154 I print_info: n_embd           = 2048
0.00.077.154 I print_info: n_layer          = 24
0.00.077.157 I print_info: n_head           = 16
0.00.077.158 I print_info: n_head_kv        = 16
0.00.077.158 I print_info: n_rot            = 32
0.00.077.158 I print_info: n_swa            = 0
0.00.077.158 I print_info: n_embd_head_k    = 128
0.00.077.158 I print_info: n_embd_head_v    = 128
0.00.077.159 I print_info: n_gqa            = 1
0.00.077.160 I print_info: n_embd_k_gqa     = 2048
0.00.077.161 I print_info: n_embd_v_gqa     = 2048
0.00.077.162 I print_info: f_norm_eps       = 1.0e-05
0.00.077.162 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.163 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.163 I print_info: f_logit_scale    = 0.0e+00
0.00.077.164 I print_info: n_ff             = 8192
0.00.077.164 I print_info: n_expert         = 0
0.00.077.164 I print_info: n_expert_used    = 0
0.00.077.164 I print_info: causal attn      = 1
0.00.077.164 I print_info: pooling type     = 0
0.00.077.164 I print_info: rope type        = 2
0.00.077.165 I print_info: rope scaling     = linear
0.00.077.165 I print_info: freq_base_train  = 10000.0
0.00.077.165 I print_info: freq_scale_train = 1
0.00.077.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.166 I print_info: rope_finetuned   = unknown
0.00.077.166 I print_info: ssm_d_conv       = 0
0.00.077.166 I print_info: ssm_d_inner      = 0
0.00.077.166 I print_info: ssm_d_state      = 0
0.00.077.166 I print_info: ssm_dt_rank      = 0
0.00.077.166 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.167 I print_info: model type       = 1.4B
0.00.077.167 I print_info: model params     = 1.41 B
0.00.077.167 I print_info: general.name     = 1.4B
0.00.077.168 I print_info: vocab type       = BPE
0.00.077.168 I print_info: n_vocab          = 50304
0.00.077.168 I print_info: n_merges         = 50009
0.00.077.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.169 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.169 I print_info: LF token         = 187 ''
0.00.077.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.170 I print_info: max token length = 1024
0.00.077.170 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.385.552 I load_tensors: offloading 24 repeating layers to GPU
0.01.385.555 I load_tensors: offloading output layer to GPU
0.01.385.555 I load_tensors: offloaded 25/25 layers to GPU
0.01.385.576 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.385.578 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.385.990 I llama_init_from_model: n_seq_max     = 1
0.01.385.991 I llama_init_from_model: n_ctx         = 128
0.01.385.991 I llama_init_from_model: n_ctx_per_seq = 128
0.01.385.991 I llama_init_from_model: n_batch       = 128
0.01.385.991 I llama_init_from_model: n_ubatch      = 128
0.01.385.991 I llama_init_from_model: flash_attn    = 0
0.01.385.992 I llama_init_from_model: freq_base     = 10000.0
0.01.385.992 I llama_init_from_model: freq_scale    = 1
0.01.385.992 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.385.995 I ggml_metal_init: allocating
0.01.386.030 I ggml_metal_init: found device: Apple M4
0.01.386.034 I ggml_metal_init: picking default device: Apple M4
0.01.386.567 I ggml_metal_init: using embedded metal library
0.01.389.002 I ggml_metal_init: GPU name:   Apple M4
0.01.389.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.389.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.389.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.389.005 I ggml_metal_init: simdgroup reduction   = true
0.01.389.005 I ggml_metal_init: simdgroup matrix mul. = true
0.01.389.006 I ggml_metal_init: has residency sets    = true
0.01.389.006 I ggml_metal_init: has bfloat            = true
0.01.389.006 I ggml_metal_init: use bfloat            = true
0.01.389.006 I ggml_metal_init: hasUnifiedMemory      = true
0.01.389.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.399.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.401.299 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.401.302 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.401.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.402.887 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.402.888 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.402.888 I llama_init_from_model: graph nodes  = 967
0.01.402.889 I llama_init_from_model: graph splits = 2
0.01.402.890 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.402.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.437.659 I 
0.01.437.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.437.712 I perplexity: tokenizing the input ..
0.01.441.851 I perplexity: tokenization took 4.137 ms
0.01.441.856 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.573.580 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.574.920 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.574.952 I llama_perf_context_print:        load time =    1413.44 ms
0.01.574.954 I llama_perf_context_print: prompt eval time =     131.49 ms /   128 tokens (    1.03 ms per token,   973.50 tokens per second)
0.01.574.954 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.574.955 I llama_perf_context_print:       total time =     137.29 ms /   129 tokens
0.01.575.381 I ggml_metal_free: deallocating

real	0m1.786s
user	0m0.096s
sys	0m0.218s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.067 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.105 I main: llama backend init
0.00.000.107 I main: load the model and apply lora adapter, if any
0.00.009.807 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.552 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.554 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.444 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.301 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.303 I llama_model_loader: - type  f32:  194 tensors
0.00.036.303 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.304 I print_info: file format = GGUF V3 (latest)
0.00.036.305 I print_info: file type   = Q8_0
0.00.036.306 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.430 I load: special tokens cache size = 25
0.00.052.916 I load: token to piece cache size = 0.2984 MB
0.00.052.934 I print_info: arch             = gptneox
0.00.052.935 I print_info: vocab_only       = 0
0.00.052.937 I print_info: n_ctx_train      = 2048
0.00.052.937 I print_info: n_embd           = 2048
0.00.052.937 I print_info: n_layer          = 24
0.00.052.942 I print_info: n_head           = 16
0.00.052.943 I print_info: n_head_kv        = 16
0.00.052.943 I print_info: n_rot            = 32
0.00.052.943 I print_info: n_swa            = 0
0.00.052.943 I print_info: n_embd_head_k    = 128
0.00.052.943 I print_info: n_embd_head_v    = 128
0.00.052.946 I print_info: n_gqa            = 1
0.00.052.946 I print_info: n_embd_k_gqa     = 2048
0.00.052.947 I print_info: n_embd_v_gqa     = 2048
0.00.052.948 I print_info: f_norm_eps       = 1.0e-05
0.00.052.948 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.948 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.949 I print_info: f_logit_scale    = 0.0e+00
0.00.052.950 I print_info: n_ff             = 8192
0.00.052.950 I print_info: n_expert         = 0
0.00.052.950 I print_info: n_expert_used    = 0
0.00.052.950 I print_info: causal attn      = 1
0.00.052.950 I print_info: pooling type     = 0
0.00.052.950 I print_info: rope type        = 2
0.00.052.950 I print_info: rope scaling     = linear
0.00.052.951 I print_info: freq_base_train  = 10000.0
0.00.052.951 I print_info: freq_scale_train = 1
0.00.052.951 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.952 I print_info: rope_finetuned   = unknown
0.00.052.952 I print_info: ssm_d_conv       = 0
0.00.052.952 I print_info: ssm_d_inner      = 0
0.00.052.952 I print_info: ssm_d_state      = 0
0.00.052.952 I print_info: ssm_dt_rank      = 0
0.00.052.954 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.954 I print_info: model type       = 1.4B
0.00.052.955 I print_info: model params     = 1.41 B
0.00.052.955 I print_info: general.name     = 1.4B
0.00.052.956 I print_info: vocab type       = BPE
0.00.052.956 I print_info: n_vocab          = 50304
0.00.052.956 I print_info: n_merges         = 50009
0.00.052.956 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.956 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.957 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.958 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.958 I print_info: LF token         = 187 ''
0.00.052.959 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.959 I print_info: max token length = 1024
0.00.052.959 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.191.261 I load_tensors: offloading 24 repeating layers to GPU
0.01.191.266 I load_tensors: offloading output layer to GPU
0.01.191.267 I load_tensors: offloaded 25/25 layers to GPU
0.01.191.289 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.191.290 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.192.422 I llama_init_from_model: n_seq_max     = 1
0.01.192.423 I llama_init_from_model: n_ctx         = 2048
0.01.192.424 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.192.424 I llama_init_from_model: n_batch       = 2048
0.01.192.424 I llama_init_from_model: n_ubatch      = 512
0.01.192.425 I llama_init_from_model: flash_attn    = 0
0.01.192.425 I llama_init_from_model: freq_base     = 10000.0
0.01.192.426 I llama_init_from_model: freq_scale    = 1
0.01.192.427 I ggml_metal_init: allocating
0.01.192.435 I ggml_metal_init: found device: Apple M4
0.01.192.441 I ggml_metal_init: picking default device: Apple M4
0.01.193.519 I ggml_metal_init: using embedded metal library
0.01.199.392 I ggml_metal_init: GPU name:   Apple M4
0.01.199.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.199.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.199.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.199.398 I ggml_metal_init: simdgroup reduction   = true
0.01.199.398 I ggml_metal_init: simdgroup matrix mul. = true
0.01.199.398 I ggml_metal_init: has residency sets    = true
0.01.199.398 I ggml_metal_init: has bfloat            = true
0.01.199.399 I ggml_metal_init: use bfloat            = true
0.01.199.399 I ggml_metal_init: hasUnifiedMemory      = true
0.01.199.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.215.694 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.265.419 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.265.424 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.265.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.270.022 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.270.024 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.270.025 I llama_init_from_model: graph nodes  = 967
0.01.270.025 I llama_init_from_model: graph splits = 2
0.01.270.031 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.270.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.270.158 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.328.245 I main: llama threadpool init, n_threads = 4
0.01.328.294 I 
0.01.328.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.328.315 I 
0.01.328.471 I sampler seed: 1234
0.01.328.475 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.328.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.328.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.328.491 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.444.709 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46926.64 tokens per second)
0.02.444.710 I llama_perf_context_print:        load time =    1317.73 ms
0.02.444.711 I llama_perf_context_print: prompt eval time =      49.12 ms /     7 tokens (    7.02 ms per token,   142.52 tokens per second)
0.02.444.712 I llama_perf_context_print:        eval time =    1064.46 ms /    63 runs   (   16.90 ms per token,    59.18 tokens per second)
0.02.444.712 I llama_perf_context_print:       total time =    1117.17 ms /    70 tokens
0.02.444.985 I ggml_metal_free: deallocating

real	0m2.462s
user	0m0.111s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.394 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.790 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.791 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.792 I llama_model_loader: - type  f32:  194 tensors
0.00.039.792 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.793 I print_info: file format = GGUF V3 (latest)
0.00.039.793 I print_info: file type   = Q8_0
0.00.039.795 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.171 I load: special tokens cache size = 25
0.00.062.271 I load: token to piece cache size = 0.2984 MB
0.00.062.285 I print_info: arch             = gptneox
0.00.062.286 I print_info: vocab_only       = 0
0.00.062.286 I print_info: n_ctx_train      = 2048
0.00.062.287 I print_info: n_embd           = 2048
0.00.062.287 I print_info: n_layer          = 24
0.00.062.290 I print_info: n_head           = 16
0.00.062.291 I print_info: n_head_kv        = 16
0.00.062.291 I print_info: n_rot            = 32
0.00.062.291 I print_info: n_swa            = 0
0.00.062.291 I print_info: n_embd_head_k    = 128
0.00.062.292 I print_info: n_embd_head_v    = 128
0.00.062.292 I print_info: n_gqa            = 1
0.00.062.293 I print_info: n_embd_k_gqa     = 2048
0.00.062.293 I print_info: n_embd_v_gqa     = 2048
0.00.062.294 I print_info: f_norm_eps       = 1.0e-05
0.00.062.294 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.294 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.295 I print_info: f_logit_scale    = 0.0e+00
0.00.062.295 I print_info: n_ff             = 8192
0.00.062.295 I print_info: n_expert         = 0
0.00.062.296 I print_info: n_expert_used    = 0
0.00.062.296 I print_info: causal attn      = 1
0.00.062.296 I print_info: pooling type     = 0
0.00.062.296 I print_info: rope type        = 2
0.00.062.296 I print_info: rope scaling     = linear
0.00.062.297 I print_info: freq_base_train  = 10000.0
0.00.062.297 I print_info: freq_scale_train = 1
0.00.062.297 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.297 I print_info: rope_finetuned   = unknown
0.00.062.297 I print_info: ssm_d_conv       = 0
0.00.062.297 I print_info: ssm_d_inner      = 0
0.00.062.297 I print_info: ssm_d_state      = 0
0.00.062.297 I print_info: ssm_dt_rank      = 0
0.00.062.299 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.299 I print_info: model type       = 1.4B
0.00.062.299 I print_info: model params     = 1.41 B
0.00.062.299 I print_info: general.name     = 1.4B
0.00.062.300 I print_info: vocab type       = BPE
0.00.062.301 I print_info: n_vocab          = 50304
0.00.062.301 I print_info: n_merges         = 50009
0.00.062.301 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.301 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.301 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.302 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.302 I print_info: LF token         = 187 ''
0.00.062.302 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.302 I print_info: max token length = 1024
0.00.062.303 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.995.173 I load_tensors: offloading 24 repeating layers to GPU
0.00.995.180 I load_tensors: offloading output layer to GPU
0.00.995.181 I load_tensors: offloaded 25/25 layers to GPU
0.00.995.203 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.995.205 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.996.532 I llama_init_from_model: n_seq_max     = 1
0.00.996.533 I llama_init_from_model: n_ctx         = 128
0.00.996.534 I llama_init_from_model: n_ctx_per_seq = 128
0.00.996.534 I llama_init_from_model: n_batch       = 128
0.00.996.535 I llama_init_from_model: n_ubatch      = 128
0.00.996.535 I llama_init_from_model: flash_attn    = 0
0.00.996.536 I llama_init_from_model: freq_base     = 10000.0
0.00.996.536 I llama_init_from_model: freq_scale    = 1
0.00.996.537 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.996.538 I ggml_metal_init: allocating
0.00.996.558 I ggml_metal_init: found device: Apple M4
0.00.996.566 I ggml_metal_init: picking default device: Apple M4
0.00.997.581 I ggml_metal_init: using embedded metal library
0.01.002.838 I ggml_metal_init: GPU name:   Apple M4
0.01.002.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.002.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.002.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.002.843 I ggml_metal_init: simdgroup reduction   = true
0.01.002.844 I ggml_metal_init: simdgroup matrix mul. = true
0.01.002.844 I ggml_metal_init: has residency sets    = true
0.01.002.844 I ggml_metal_init: has bfloat            = true
0.01.002.844 I ggml_metal_init: use bfloat            = true
0.01.002.845 I ggml_metal_init: hasUnifiedMemory      = true
0.01.002.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.017.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.020.158 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.020.161 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.020.181 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.022.600 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.022.601 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.022.602 I llama_init_from_model: graph nodes  = 967
0.01.022.602 I llama_init_from_model: graph splits = 2
0.01.022.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.022.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.047.253 I 
0.01.047.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.047.321 I perplexity: tokenizing the input ..
0.01.053.866 I perplexity: tokenization took 6.544 ms
0.01.053.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.189.924 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.191.372 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.191.392 I llama_perf_context_print:        load time =    1029.23 ms
0.01.191.393 I llama_perf_context_print: prompt eval time =     135.50 ms /   128 tokens (    1.06 ms per token,   944.66 tokens per second)
0.01.191.394 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.191.394 I llama_perf_context_print:       total time =     144.14 ms /   129 tokens
0.01.191.776 I ggml_metal_free: deallocating

real	0m1.206s
user	0m0.086s
sys	0m0.173s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.013.673 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.308 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.315 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.934 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.936 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.936 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.936 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.937 I llama_model_loader: - type  f32:  194 tensors
0.00.032.937 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.938 I print_info: file format = GGUF V3 (latest)
0.00.032.938 I print_info: file type   = Q4_0
0.00.032.939 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.041.523 I load: special tokens cache size = 25
0.00.047.786 I load: token to piece cache size = 0.2984 MB
0.00.047.799 I print_info: arch             = gptneox
0.00.047.801 I print_info: vocab_only       = 0
0.00.047.801 I print_info: n_ctx_train      = 2048
0.00.047.801 I print_info: n_embd           = 2048
0.00.047.801 I print_info: n_layer          = 24
0.00.047.810 I print_info: n_head           = 16
0.00.047.811 I print_info: n_head_kv        = 16
0.00.047.811 I print_info: n_rot            = 32
0.00.047.811 I print_info: n_swa            = 0
0.00.047.811 I print_info: n_embd_head_k    = 128
0.00.047.811 I print_info: n_embd_head_v    = 128
0.00.047.812 I print_info: n_gqa            = 1
0.00.047.813 I print_info: n_embd_k_gqa     = 2048
0.00.047.813 I print_info: n_embd_v_gqa     = 2048
0.00.047.814 I print_info: f_norm_eps       = 1.0e-05
0.00.047.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.818 I print_info: f_logit_scale    = 0.0e+00
0.00.047.818 I print_info: n_ff             = 8192
0.00.047.818 I print_info: n_expert         = 0
0.00.047.818 I print_info: n_expert_used    = 0
0.00.047.819 I print_info: causal attn      = 1
0.00.047.819 I print_info: pooling type     = 0
0.00.047.819 I print_info: rope type        = 2
0.00.047.819 I print_info: rope scaling     = linear
0.00.047.819 I print_info: freq_base_train  = 10000.0
0.00.047.819 I print_info: freq_scale_train = 1
0.00.047.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.820 I print_info: rope_finetuned   = unknown
0.00.047.820 I print_info: ssm_d_conv       = 0
0.00.047.820 I print_info: ssm_d_inner      = 0
0.00.047.820 I print_info: ssm_d_state      = 0
0.00.047.820 I print_info: ssm_dt_rank      = 0
0.00.047.820 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.820 I print_info: model type       = 1.4B
0.00.047.821 I print_info: model params     = 1.41 B
0.00.047.821 I print_info: general.name     = 1.4B
0.00.047.821 I print_info: vocab type       = BPE
0.00.047.822 I print_info: n_vocab          = 50304
0.00.047.822 I print_info: n_merges         = 50009
0.00.047.822 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.825 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.826 I print_info: LF token         = 187 ''
0.00.047.826 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.826 I print_info: max token length = 1024
0.00.047.827 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.088.475 I load_tensors: offloading 24 repeating layers to GPU
0.01.088.482 I load_tensors: offloading output layer to GPU
0.01.088.482 I load_tensors: offloaded 25/25 layers to GPU
0.01.088.501 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.01.088.502 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.01.089.377 I llama_init_from_model: n_seq_max     = 1
0.01.089.381 I llama_init_from_model: n_ctx         = 2048
0.01.089.381 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.089.382 I llama_init_from_model: n_batch       = 2048
0.01.089.382 I llama_init_from_model: n_ubatch      = 512
0.01.089.383 I llama_init_from_model: flash_attn    = 0
0.01.089.384 I llama_init_from_model: freq_base     = 10000.0
0.01.089.384 I llama_init_from_model: freq_scale    = 1
0.01.089.386 I ggml_metal_init: allocating
0.01.089.423 I ggml_metal_init: found device: Apple M4
0.01.089.438 I ggml_metal_init: picking default device: Apple M4
0.01.090.361 I ggml_metal_init: using embedded metal library
0.01.094.508 I ggml_metal_init: GPU name:   Apple M4
0.01.094.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.094.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.094.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.094.518 I ggml_metal_init: simdgroup reduction   = true
0.01.094.519 I ggml_metal_init: simdgroup matrix mul. = true
0.01.094.519 I ggml_metal_init: has residency sets    = true
0.01.094.519 I ggml_metal_init: has bfloat            = true
0.01.094.519 I ggml_metal_init: use bfloat            = true
0.01.094.521 I ggml_metal_init: hasUnifiedMemory      = true
0.01.094.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.111.070 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.143.760 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.143.767 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.143.790 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.148.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.148.593 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.148.593 I llama_init_from_model: graph nodes  = 967
0.01.148.593 I llama_init_from_model: graph splits = 2
0.01.148.596 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.148.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.148.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.202.365 I main: llama threadpool init, n_threads = 4
0.01.202.422 I 
0.01.202.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.202.442 I 
0.01.202.587 I sampler seed: 1234
0.01.202.592 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.202.607 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.202.607 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.202.607 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.896.813 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.896.813 I llama_perf_context_print:        load time =    1187.92 ms
0.01.896.815 I llama_perf_context_print: prompt eval time =      49.09 ms /     7 tokens (    7.01 ms per token,   142.58 tokens per second)
0.01.896.816 I llama_perf_context_print:        eval time =     642.19 ms /    63 runs   (   10.19 ms per token,    98.10 tokens per second)
0.01.896.816 I llama_perf_context_print:       total time =     695.22 ms /    70 tokens
0.01.897.043 I ggml_metal_free: deallocating

real	0m1.919s
user	0m0.106s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.674 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.641 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.648 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.649 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.649 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.650 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.651 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.651 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.464 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.149 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.151 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.151 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.152 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.153 I llama_model_loader: - type  f32:  194 tensors
0.00.043.153 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.153 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.154 I print_info: file format = GGUF V3 (latest)
0.00.043.154 I print_info: file type   = Q4_0
0.00.043.155 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.051.324 I load: special tokens cache size = 25
0.00.057.719 I load: token to piece cache size = 0.2984 MB
0.00.057.737 I print_info: arch             = gptneox
0.00.057.738 I print_info: vocab_only       = 0
0.00.057.738 I print_info: n_ctx_train      = 2048
0.00.057.738 I print_info: n_embd           = 2048
0.00.057.739 I print_info: n_layer          = 24
0.00.057.743 I print_info: n_head           = 16
0.00.057.744 I print_info: n_head_kv        = 16
0.00.057.744 I print_info: n_rot            = 32
0.00.057.744 I print_info: n_swa            = 0
0.00.057.744 I print_info: n_embd_head_k    = 128
0.00.057.744 I print_info: n_embd_head_v    = 128
0.00.057.745 I print_info: n_gqa            = 1
0.00.057.745 I print_info: n_embd_k_gqa     = 2048
0.00.057.746 I print_info: n_embd_v_gqa     = 2048
0.00.057.746 I print_info: f_norm_eps       = 1.0e-05
0.00.057.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.747 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.747 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.750 I print_info: f_logit_scale    = 0.0e+00
0.00.057.751 I print_info: n_ff             = 8192
0.00.057.751 I print_info: n_expert         = 0
0.00.057.751 I print_info: n_expert_used    = 0
0.00.057.751 I print_info: causal attn      = 1
0.00.057.751 I print_info: pooling type     = 0
0.00.057.752 I print_info: rope type        = 2
0.00.057.753 I print_info: rope scaling     = linear
0.00.057.753 I print_info: freq_base_train  = 10000.0
0.00.057.753 I print_info: freq_scale_train = 1
0.00.057.753 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.754 I print_info: rope_finetuned   = unknown
0.00.057.754 I print_info: ssm_d_conv       = 0
0.00.057.754 I print_info: ssm_d_inner      = 0
0.00.057.754 I print_info: ssm_d_state      = 0
0.00.057.754 I print_info: ssm_dt_rank      = 0
0.00.057.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.754 I print_info: model type       = 1.4B
0.00.057.755 I print_info: model params     = 1.41 B
0.00.057.755 I print_info: general.name     = 1.4B
0.00.057.755 I print_info: vocab type       = BPE
0.00.057.756 I print_info: n_vocab          = 50304
0.00.057.756 I print_info: n_merges         = 50009
0.00.057.756 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.757 I print_info: LF token         = 187 ''
0.00.057.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.757 I print_info: max token length = 1024
0.00.057.757 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.276 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.285 I load_tensors: offloading output layer to GPU
0.00.686.285 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.320 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.686.321 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.688.066 I llama_init_from_model: n_seq_max     = 1
0.00.688.069 I llama_init_from_model: n_ctx         = 128
0.00.688.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.688.070 I llama_init_from_model: n_batch       = 128
0.00.688.070 I llama_init_from_model: n_ubatch      = 128
0.00.688.070 I llama_init_from_model: flash_attn    = 0
0.00.688.073 I llama_init_from_model: freq_base     = 10000.0
0.00.688.073 I llama_init_from_model: freq_scale    = 1
0.00.688.074 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.688.089 I ggml_metal_init: allocating
0.00.688.176 I ggml_metal_init: found device: Apple M4
0.00.688.189 I ggml_metal_init: picking default device: Apple M4
0.00.690.069 I ggml_metal_init: using embedded metal library
0.00.695.947 I ggml_metal_init: GPU name:   Apple M4
0.00.695.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.965 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.966 I ggml_metal_init: simdgroup reduction   = true
0.00.695.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.966 I ggml_metal_init: has residency sets    = true
0.00.695.966 I ggml_metal_init: has bfloat            = true
0.00.695.967 I ggml_metal_init: use bfloat            = true
0.00.695.969 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.128 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.631 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.720.635 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.720.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.109 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.724.111 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.724.111 I llama_init_from_model: graph nodes  = 967
0.00.724.112 I llama_init_from_model: graph splits = 2
0.00.724.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.710 I 
0.00.752.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.821 I perplexity: tokenizing the input ..
0.00.759.791 I perplexity: tokenization took 6.967 ms
0.00.759.800 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.772 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.898.102 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.898.126 I llama_perf_context_print:        load time =     735.02 ms
0.00.898.129 I llama_perf_context_print: prompt eval time =     136.04 ms /   128 tokens (    1.06 ms per token,   940.90 tokens per second)
0.00.898.132 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.898.132 I llama_perf_context_print:       total time =     145.42 ms /   129 tokens
0.00.898.516 I ggml_metal_free: deallocating

real	0m0.932s
user	0m0.082s
sys	0m0.147s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.355 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.491 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.500 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.959 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.962 I llama_model_loader: - type  f32:  194 tensors
0.00.025.962 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.962 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.963 I print_info: file format = GGUF V3 (latest)
0.00.025.963 I print_info: file type   = Q4_1
0.00.025.965 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.204 I load: special tokens cache size = 25
0.00.040.667 I load: token to piece cache size = 0.2984 MB
0.00.040.681 I print_info: arch             = gptneox
0.00.040.682 I print_info: vocab_only       = 0
0.00.040.683 I print_info: n_ctx_train      = 2048
0.00.040.683 I print_info: n_embd           = 2048
0.00.040.683 I print_info: n_layer          = 24
0.00.040.686 I print_info: n_head           = 16
0.00.040.686 I print_info: n_head_kv        = 16
0.00.040.687 I print_info: n_rot            = 32
0.00.040.687 I print_info: n_swa            = 0
0.00.040.687 I print_info: n_embd_head_k    = 128
0.00.040.687 I print_info: n_embd_head_v    = 128
0.00.040.688 I print_info: n_gqa            = 1
0.00.040.690 I print_info: n_embd_k_gqa     = 2048
0.00.040.690 I print_info: n_embd_v_gqa     = 2048
0.00.040.691 I print_info: f_norm_eps       = 1.0e-05
0.00.040.691 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.691 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.692 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.692 I print_info: f_logit_scale    = 0.0e+00
0.00.040.692 I print_info: n_ff             = 8192
0.00.040.692 I print_info: n_expert         = 0
0.00.040.693 I print_info: n_expert_used    = 0
0.00.040.693 I print_info: causal attn      = 1
0.00.040.693 I print_info: pooling type     = 0
0.00.040.694 I print_info: rope type        = 2
0.00.040.696 I print_info: rope scaling     = linear
0.00.040.696 I print_info: freq_base_train  = 10000.0
0.00.040.696 I print_info: freq_scale_train = 1
0.00.040.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.696 I print_info: rope_finetuned   = unknown
0.00.040.697 I print_info: ssm_d_conv       = 0
0.00.040.698 I print_info: ssm_d_inner      = 0
0.00.040.698 I print_info: ssm_d_state      = 0
0.00.040.698 I print_info: ssm_dt_rank      = 0
0.00.040.698 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.698 I print_info: model type       = 1.4B
0.00.040.698 I print_info: model params     = 1.41 B
0.00.040.698 I print_info: general.name     = 1.4B
0.00.040.700 I print_info: vocab type       = BPE
0.00.040.700 I print_info: n_vocab          = 50304
0.00.040.700 I print_info: n_merges         = 50009
0.00.040.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.701 I print_info: LF token         = 187 ''
0.00.040.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.702 I print_info: max token length = 1024
0.00.040.703 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.522 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.539 I load_tensors: offloading output layer to GPU
0.00.635.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.571 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.635.573 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.637.362 I llama_init_from_model: n_seq_max     = 1
0.00.637.364 I llama_init_from_model: n_ctx         = 2048
0.00.637.365 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.365 I llama_init_from_model: n_batch       = 2048
0.00.637.366 I llama_init_from_model: n_ubatch      = 512
0.00.637.366 I llama_init_from_model: flash_attn    = 0
0.00.637.369 I llama_init_from_model: freq_base     = 10000.0
0.00.637.370 I llama_init_from_model: freq_scale    = 1
0.00.637.372 I ggml_metal_init: allocating
0.00.637.457 I ggml_metal_init: found device: Apple M4
0.00.637.470 I ggml_metal_init: picking default device: Apple M4
0.00.639.099 I ggml_metal_init: using embedded metal library
0.00.645.819 I ggml_metal_init: GPU name:   Apple M4
0.00.645.824 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.825 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.827 I ggml_metal_init: simdgroup reduction   = true
0.00.645.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.827 I ggml_metal_init: has residency sets    = true
0.00.645.828 I ggml_metal_init: has bfloat            = true
0.00.645.828 I ggml_metal_init: use bfloat            = true
0.00.645.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.840 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.880 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.207 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.214 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.881 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.883 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.883 I llama_init_from_model: graph nodes  = 967
0.00.720.884 I llama_init_from_model: graph splits = 2
0.00.720.889 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.721.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.905 I main: llama threadpool init, n_threads = 4
0.00.778.948 I 
0.00.778.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.970 I 
0.00.779.137 I sampler seed: 1234
0.00.779.142 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.198 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.202 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.202 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.506.371 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.506.372 I llama_perf_context_print:        load time =     768.80 ms
0.01.506.373 I llama_perf_context_print: prompt eval time =      49.47 ms /     7 tokens (    7.07 ms per token,   141.51 tokens per second)
0.01.506.375 I llama_perf_context_print:        eval time =     675.05 ms /    63 runs   (   10.72 ms per token,    93.33 tokens per second)
0.01.506.375 I llama_perf_context_print:       total time =     728.21 ms /    70 tokens
0.01.506.665 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.111s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.727 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.729 I llama_model_loader: - type  f32:  194 tensors
0.00.029.729 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.729 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.730 I print_info: file format = GGUF V3 (latest)
0.00.029.730 I print_info: file type   = Q4_1
0.00.029.733 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.038.343 I load: special tokens cache size = 25
0.00.044.674 I load: token to piece cache size = 0.2984 MB
0.00.044.692 I print_info: arch             = gptneox
0.00.044.692 I print_info: vocab_only       = 0
0.00.044.693 I print_info: n_ctx_train      = 2048
0.00.044.693 I print_info: n_embd           = 2048
0.00.044.693 I print_info: n_layer          = 24
0.00.044.697 I print_info: n_head           = 16
0.00.044.697 I print_info: n_head_kv        = 16
0.00.044.698 I print_info: n_rot            = 32
0.00.044.698 I print_info: n_swa            = 0
0.00.044.698 I print_info: n_embd_head_k    = 128
0.00.044.698 I print_info: n_embd_head_v    = 128
0.00.044.699 I print_info: n_gqa            = 1
0.00.044.699 I print_info: n_embd_k_gqa     = 2048
0.00.044.700 I print_info: n_embd_v_gqa     = 2048
0.00.044.700 I print_info: f_norm_eps       = 1.0e-05
0.00.044.701 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.701 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.701 I print_info: f_logit_scale    = 0.0e+00
0.00.044.702 I print_info: n_ff             = 8192
0.00.044.702 I print_info: n_expert         = 0
0.00.044.702 I print_info: n_expert_used    = 0
0.00.044.702 I print_info: causal attn      = 1
0.00.044.702 I print_info: pooling type     = 0
0.00.044.702 I print_info: rope type        = 2
0.00.044.703 I print_info: rope scaling     = linear
0.00.044.703 I print_info: freq_base_train  = 10000.0
0.00.044.703 I print_info: freq_scale_train = 1
0.00.044.703 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.704 I print_info: rope_finetuned   = unknown
0.00.044.704 I print_info: ssm_d_conv       = 0
0.00.044.704 I print_info: ssm_d_inner      = 0
0.00.044.704 I print_info: ssm_d_state      = 0
0.00.044.704 I print_info: ssm_dt_rank      = 0
0.00.044.704 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.704 I print_info: model type       = 1.4B
0.00.044.705 I print_info: model params     = 1.41 B
0.00.044.705 I print_info: general.name     = 1.4B
0.00.044.705 I print_info: vocab type       = BPE
0.00.044.709 I print_info: n_vocab          = 50304
0.00.044.709 I print_info: n_merges         = 50009
0.00.044.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: LF token         = 187 ''
0.00.044.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.711 I print_info: max token length = 1024
0.00.044.711 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.176 I load_tensors: offloading output layer to GPU
0.00.641.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.210 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.641.211 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.642.681 I llama_init_from_model: n_seq_max     = 1
0.00.642.684 I llama_init_from_model: n_ctx         = 128
0.00.642.685 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.685 I llama_init_from_model: n_batch       = 128
0.00.642.685 I llama_init_from_model: n_ubatch      = 128
0.00.642.686 I llama_init_from_model: flash_attn    = 0
0.00.642.688 I llama_init_from_model: freq_base     = 10000.0
0.00.642.688 I llama_init_from_model: freq_scale    = 1
0.00.642.689 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.692 I ggml_metal_init: allocating
0.00.642.796 I ggml_metal_init: found device: Apple M4
0.00.642.810 I ggml_metal_init: picking default device: Apple M4
0.00.644.456 I ggml_metal_init: using embedded metal library
0.00.651.289 I ggml_metal_init: GPU name:   Apple M4
0.00.651.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.299 I ggml_metal_init: simdgroup reduction   = true
0.00.651.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.300 I ggml_metal_init: has residency sets    = true
0.00.651.300 I ggml_metal_init: has bfloat            = true
0.00.651.300 I ggml_metal_init: use bfloat            = true
0.00.651.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.665 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.425 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.451 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.750 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.752 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.753 I llama_init_from_model: graph nodes  = 967
0.00.676.753 I llama_init_from_model: graph splits = 2
0.00.676.757 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.757 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.414 I 
0.00.704.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.512 I perplexity: tokenizing the input ..
0.00.712.073 I perplexity: tokenization took 7.557 ms
0.00.712.080 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.038 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.849.385 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.849.414 I llama_perf_context_print:        load time =     695.52 ms
0.00.849.415 I llama_perf_context_print: prompt eval time =     135.05 ms /   128 tokens (    1.06 ms per token,   947.77 tokens per second)
0.00.849.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.416 I llama_perf_context_print:       total time =     145.00 ms /   129 tokens
0.00.849.802 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.082s
sys	0m0.119s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.715 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.489 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.506 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.232 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.232 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.233 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.233 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.234 I llama_model_loader: - type  f32:  194 tensors
0.00.027.234 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.234 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.235 I print_info: file format = GGUF V3 (latest)
0.00.027.235 I print_info: file type   = Q5_0
0.00.027.236 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.339 I load: special tokens cache size = 25
0.00.041.396 I load: token to piece cache size = 0.2984 MB
0.00.041.410 I print_info: arch             = gptneox
0.00.041.411 I print_info: vocab_only       = 0
0.00.041.412 I print_info: n_ctx_train      = 2048
0.00.041.412 I print_info: n_embd           = 2048
0.00.041.412 I print_info: n_layer          = 24
0.00.041.415 I print_info: n_head           = 16
0.00.041.416 I print_info: n_head_kv        = 16
0.00.041.416 I print_info: n_rot            = 32
0.00.041.416 I print_info: n_swa            = 0
0.00.041.416 I print_info: n_embd_head_k    = 128
0.00.041.416 I print_info: n_embd_head_v    = 128
0.00.041.417 I print_info: n_gqa            = 1
0.00.041.418 I print_info: n_embd_k_gqa     = 2048
0.00.041.418 I print_info: n_embd_v_gqa     = 2048
0.00.041.419 I print_info: f_norm_eps       = 1.0e-05
0.00.041.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.420 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.420 I print_info: f_logit_scale    = 0.0e+00
0.00.041.420 I print_info: n_ff             = 8192
0.00.041.421 I print_info: n_expert         = 0
0.00.041.423 I print_info: n_expert_used    = 0
0.00.041.423 I print_info: causal attn      = 1
0.00.041.423 I print_info: pooling type     = 0
0.00.041.423 I print_info: rope type        = 2
0.00.041.423 I print_info: rope scaling     = linear
0.00.041.424 I print_info: freq_base_train  = 10000.0
0.00.041.424 I print_info: freq_scale_train = 1
0.00.041.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.424 I print_info: rope_finetuned   = unknown
0.00.041.424 I print_info: ssm_d_conv       = 0
0.00.041.424 I print_info: ssm_d_inner      = 0
0.00.041.424 I print_info: ssm_d_state      = 0
0.00.041.425 I print_info: ssm_dt_rank      = 0
0.00.041.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.425 I print_info: model type       = 1.4B
0.00.041.425 I print_info: model params     = 1.41 B
0.00.041.425 I print_info: general.name     = 1.4B
0.00.041.426 I print_info: vocab type       = BPE
0.00.041.426 I print_info: n_vocab          = 50304
0.00.041.426 I print_info: n_merges         = 50009
0.00.041.426 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.426 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.427 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.427 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.427 I print_info: LF token         = 187 ''
0.00.041.427 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.427 I print_info: max token length = 1024
0.00.041.428 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.686.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.386 I load_tensors: offloading output layer to GPU
0.00.686.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.424 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.686.425 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.687.965 I llama_init_from_model: n_seq_max     = 1
0.00.687.968 I llama_init_from_model: n_ctx         = 2048
0.00.687.968 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.687.969 I llama_init_from_model: n_batch       = 2048
0.00.687.969 I llama_init_from_model: n_ubatch      = 512
0.00.687.969 I llama_init_from_model: flash_attn    = 0
0.00.687.972 I llama_init_from_model: freq_base     = 10000.0
0.00.687.972 I llama_init_from_model: freq_scale    = 1
0.00.687.974 I ggml_metal_init: allocating
0.00.688.048 I ggml_metal_init: found device: Apple M4
0.00.688.062 I ggml_metal_init: picking default device: Apple M4
0.00.689.765 I ggml_metal_init: using embedded metal library
0.00.696.525 I ggml_metal_init: GPU name:   Apple M4
0.00.696.528 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.529 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.530 I ggml_metal_init: simdgroup reduction   = true
0.00.696.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.531 I ggml_metal_init: has residency sets    = true
0.00.696.531 I ggml_metal_init: has bfloat            = true
0.00.696.532 I ggml_metal_init: use bfloat            = true
0.00.696.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.722 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.963 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.764.970 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.765.002 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.144 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.769.147 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.769.147 I llama_init_from_model: graph nodes  = 967
0.00.769.147 I llama_init_from_model: graph splits = 2
0.00.769.153 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.769.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.769.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.224 I main: llama threadpool init, n_threads = 4
0.00.828.274 I 
0.00.828.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.297 I 
0.00.828.484 I sampler seed: 1234
0.00.828.488 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.517 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.518 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.518 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.618.378 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.618.380 I llama_perf_context_print:        load time =     816.75 ms
0.01.618.380 I llama_perf_context_print: prompt eval time =      53.25 ms /     7 tokens (    7.61 ms per token,   131.45 tokens per second)
0.01.618.381 I llama_perf_context_print:        eval time =     733.73 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.618.383 I llama_perf_context_print:       total time =     790.92 ms /    70 tokens
0.01.618.608 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.110s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.274 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.275 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.276 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.279 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.069 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.892 I llama_model_loader: - type  f32:  194 tensors
0.00.029.892 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.893 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.893 I print_info: file format = GGUF V3 (latest)
0.00.029.894 I print_info: file type   = Q5_0
0.00.029.895 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.038.179 I load: special tokens cache size = 25
0.00.044.571 I load: token to piece cache size = 0.2984 MB
0.00.044.590 I print_info: arch             = gptneox
0.00.044.590 I print_info: vocab_only       = 0
0.00.044.591 I print_info: n_ctx_train      = 2048
0.00.044.591 I print_info: n_embd           = 2048
0.00.044.591 I print_info: n_layer          = 24
0.00.044.595 I print_info: n_head           = 16
0.00.044.595 I print_info: n_head_kv        = 16
0.00.044.595 I print_info: n_rot            = 32
0.00.044.596 I print_info: n_swa            = 0
0.00.044.596 I print_info: n_embd_head_k    = 128
0.00.044.596 I print_info: n_embd_head_v    = 128
0.00.044.600 I print_info: n_gqa            = 1
0.00.044.601 I print_info: n_embd_k_gqa     = 2048
0.00.044.601 I print_info: n_embd_v_gqa     = 2048
0.00.044.602 I print_info: f_norm_eps       = 1.0e-05
0.00.044.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.603 I print_info: f_logit_scale    = 0.0e+00
0.00.044.603 I print_info: n_ff             = 8192
0.00.044.604 I print_info: n_expert         = 0
0.00.044.604 I print_info: n_expert_used    = 0
0.00.044.604 I print_info: causal attn      = 1
0.00.044.604 I print_info: pooling type     = 0
0.00.044.604 I print_info: rope type        = 2
0.00.044.604 I print_info: rope scaling     = linear
0.00.044.605 I print_info: freq_base_train  = 10000.0
0.00.044.605 I print_info: freq_scale_train = 1
0.00.044.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.605 I print_info: rope_finetuned   = unknown
0.00.044.605 I print_info: ssm_d_conv       = 0
0.00.044.606 I print_info: ssm_d_inner      = 0
0.00.044.606 I print_info: ssm_d_state      = 0
0.00.044.606 I print_info: ssm_dt_rank      = 0
0.00.044.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.606 I print_info: model type       = 1.4B
0.00.044.606 I print_info: model params     = 1.41 B
0.00.044.606 I print_info: general.name     = 1.4B
0.00.044.607 I print_info: vocab type       = BPE
0.00.044.607 I print_info: n_vocab          = 50304
0.00.044.607 I print_info: n_merges         = 50009
0.00.044.608 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.608 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.608 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.608 I print_info: LF token         = 187 ''
0.00.044.609 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.609 I print_info: max token length = 1024
0.00.044.609 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.758.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.758.936 I load_tensors: offloading output layer to GPU
0.00.758.936 I load_tensors: offloaded 25/25 layers to GPU
0.00.758.970 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.758.972 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.760.696 I llama_init_from_model: n_seq_max     = 1
0.00.760.699 I llama_init_from_model: n_ctx         = 128
0.00.760.699 I llama_init_from_model: n_ctx_per_seq = 128
0.00.760.700 I llama_init_from_model: n_batch       = 128
0.00.760.700 I llama_init_from_model: n_ubatch      = 128
0.00.760.701 I llama_init_from_model: flash_attn    = 0
0.00.760.703 I llama_init_from_model: freq_base     = 10000.0
0.00.760.704 I llama_init_from_model: freq_scale    = 1
0.00.760.704 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.760.707 I ggml_metal_init: allocating
0.00.760.782 I ggml_metal_init: found device: Apple M4
0.00.760.796 I ggml_metal_init: picking default device: Apple M4
0.00.762.371 I ggml_metal_init: using embedded metal library
0.00.769.282 I ggml_metal_init: GPU name:   Apple M4
0.00.769.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.769.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.769.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.769.293 I ggml_metal_init: simdgroup reduction   = true
0.00.769.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.769.293 I ggml_metal_init: has residency sets    = true
0.00.769.293 I ggml_metal_init: has bfloat            = true
0.00.769.294 I ggml_metal_init: use bfloat            = true
0.00.769.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.769.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.787.665 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.791.137 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.791.150 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.791.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.794.399 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.794.401 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.794.401 I llama_init_from_model: graph nodes  = 967
0.00.794.402 I llama_init_from_model: graph splits = 2
0.00.794.405 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.794.406 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.735 I 
0.00.825.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.824 I perplexity: tokenizing the input ..
0.00.833.282 I perplexity: tokenization took 7.456 ms
0.00.833.288 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.982.482 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.983.816 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.983.845 I llama_perf_context_print:        load time =     816.69 ms
0.00.983.846 I llama_perf_context_print: prompt eval time =     148.25 ms /   128 tokens (    1.16 ms per token,   863.41 tokens per second)
0.00.983.846 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.983.847 I llama_perf_context_print:       total time =     158.11 ms /   129 tokens
0.00.984.279 I ggml_metal_free: deallocating

real	0m0.998s
user	0m0.081s
sys	0m0.128s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.157 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.718 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.720 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.720 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.720 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.720 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.721 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.721 I llama_model_loader: - type  f32:  194 tensors
0.00.027.722 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.722 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.722 I print_info: file format = GGUF V3 (latest)
0.00.027.723 I print_info: file type   = Q5_1
0.00.027.724 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.823 I load: special tokens cache size = 25
0.00.042.174 I load: token to piece cache size = 0.2984 MB
0.00.042.188 I print_info: arch             = gptneox
0.00.042.189 I print_info: vocab_only       = 0
0.00.042.190 I print_info: n_ctx_train      = 2048
0.00.042.190 I print_info: n_embd           = 2048
0.00.042.190 I print_info: n_layer          = 24
0.00.042.193 I print_info: n_head           = 16
0.00.042.193 I print_info: n_head_kv        = 16
0.00.042.194 I print_info: n_rot            = 32
0.00.042.194 I print_info: n_swa            = 0
0.00.042.194 I print_info: n_embd_head_k    = 128
0.00.042.194 I print_info: n_embd_head_v    = 128
0.00.042.195 I print_info: n_gqa            = 1
0.00.042.196 I print_info: n_embd_k_gqa     = 2048
0.00.042.196 I print_info: n_embd_v_gqa     = 2048
0.00.042.197 I print_info: f_norm_eps       = 1.0e-05
0.00.042.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.199 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.199 I print_info: f_logit_scale    = 0.0e+00
0.00.042.201 I print_info: n_ff             = 8192
0.00.042.201 I print_info: n_expert         = 0
0.00.042.201 I print_info: n_expert_used    = 0
0.00.042.201 I print_info: causal attn      = 1
0.00.042.201 I print_info: pooling type     = 0
0.00.042.203 I print_info: rope type        = 2
0.00.042.204 I print_info: rope scaling     = linear
0.00.042.204 I print_info: freq_base_train  = 10000.0
0.00.042.205 I print_info: freq_scale_train = 1
0.00.042.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.205 I print_info: rope_finetuned   = unknown
0.00.042.205 I print_info: ssm_d_conv       = 0
0.00.042.205 I print_info: ssm_d_inner      = 0
0.00.042.205 I print_info: ssm_d_state      = 0
0.00.042.205 I print_info: ssm_dt_rank      = 0
0.00.042.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.210 I print_info: model type       = 1.4B
0.00.042.210 I print_info: model params     = 1.41 B
0.00.042.210 I print_info: general.name     = 1.4B
0.00.042.210 I print_info: vocab type       = BPE
0.00.042.211 I print_info: n_vocab          = 50304
0.00.042.211 I print_info: n_merges         = 50009
0.00.042.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.211 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.211 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.212 I print_info: LF token         = 187 ''
0.00.042.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.212 I print_info: max token length = 1024
0.00.042.212 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.604.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.582 I load_tensors: offloading output layer to GPU
0.00.604.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.613 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.604.615 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.606.113 I llama_init_from_model: n_seq_max     = 1
0.00.606.117 I llama_init_from_model: n_ctx         = 2048
0.00.606.117 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.606.118 I llama_init_from_model: n_batch       = 2048
0.00.606.118 I llama_init_from_model: n_ubatch      = 512
0.00.606.118 I llama_init_from_model: flash_attn    = 0
0.00.606.121 I llama_init_from_model: freq_base     = 10000.0
0.00.606.121 I llama_init_from_model: freq_scale    = 1
0.00.606.135 I ggml_metal_init: allocating
0.00.606.207 I ggml_metal_init: found device: Apple M4
0.00.606.235 I ggml_metal_init: picking default device: Apple M4
0.00.607.703 I ggml_metal_init: using embedded metal library
0.00.614.276 I ggml_metal_init: GPU name:   Apple M4
0.00.614.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.281 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.282 I ggml_metal_init: simdgroup reduction   = true
0.00.614.282 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.282 I ggml_metal_init: has residency sets    = true
0.00.614.283 I ggml_metal_init: has bfloat            = true
0.00.614.283 I ggml_metal_init: use bfloat            = true
0.00.614.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.589 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.063 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.069 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.099 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.687.334 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.687.336 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.687.336 I llama_init_from_model: graph nodes  = 967
0.00.687.336 I llama_init_from_model: graph splits = 2
0.00.687.342 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.687.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.472 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.792 I main: llama threadpool init, n_threads = 4
0.00.743.839 I 
0.00.743.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.861 I 
0.00.744.037 I sampler seed: 1234
0.00.744.043 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.075 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.075 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.582.059 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.01.582.060 I llama_perf_context_print:        load time =     731.32 ms
0.01.582.060 I llama_perf_context_print: prompt eval time =      42.18 ms /     7 tokens (    6.03 ms per token,   165.96 tokens per second)
0.01.582.061 I llama_perf_context_print:        eval time =     792.78 ms /    63 runs   (   12.58 ms per token,    79.47 tokens per second)
0.01.582.061 I llama_perf_context_print:       total time =     838.99 ms /    70 tokens
0.01.582.292 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.030.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.575 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.039.179 I llama_model_loader: - type  f32:  194 tensors
0.00.039.180 I llama_model_loader: - type q5_1:   97 tensors
0.00.039.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.181 I print_info: file format = GGUF V3 (latest)
0.00.039.181 I print_info: file type   = Q5_1
0.00.039.182 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.047.368 I load: special tokens cache size = 25
0.00.053.889 I load: token to piece cache size = 0.2984 MB
0.00.053.906 I print_info: arch             = gptneox
0.00.053.907 I print_info: vocab_only       = 0
0.00.053.907 I print_info: n_ctx_train      = 2048
0.00.053.908 I print_info: n_embd           = 2048
0.00.053.908 I print_info: n_layer          = 24
0.00.053.911 I print_info: n_head           = 16
0.00.053.912 I print_info: n_head_kv        = 16
0.00.053.912 I print_info: n_rot            = 32
0.00.053.916 I print_info: n_swa            = 0
0.00.053.916 I print_info: n_embd_head_k    = 128
0.00.053.916 I print_info: n_embd_head_v    = 128
0.00.053.916 I print_info: n_gqa            = 1
0.00.053.917 I print_info: n_embd_k_gqa     = 2048
0.00.053.918 I print_info: n_embd_v_gqa     = 2048
0.00.053.918 I print_info: f_norm_eps       = 1.0e-05
0.00.053.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.920 I print_info: f_logit_scale    = 0.0e+00
0.00.053.921 I print_info: n_ff             = 8192
0.00.053.921 I print_info: n_expert         = 0
0.00.053.921 I print_info: n_expert_used    = 0
0.00.053.921 I print_info: causal attn      = 1
0.00.053.921 I print_info: pooling type     = 0
0.00.053.921 I print_info: rope type        = 2
0.00.053.922 I print_info: rope scaling     = linear
0.00.053.922 I print_info: freq_base_train  = 10000.0
0.00.053.922 I print_info: freq_scale_train = 1
0.00.053.924 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.924 I print_info: rope_finetuned   = unknown
0.00.053.924 I print_info: ssm_d_conv       = 0
0.00.053.924 I print_info: ssm_d_inner      = 0
0.00.053.924 I print_info: ssm_d_state      = 0
0.00.053.924 I print_info: ssm_dt_rank      = 0
0.00.053.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.925 I print_info: model type       = 1.4B
0.00.053.925 I print_info: model params     = 1.41 B
0.00.053.925 I print_info: general.name     = 1.4B
0.00.053.926 I print_info: vocab type       = BPE
0.00.053.926 I print_info: n_vocab          = 50304
0.00.053.926 I print_info: n_merges         = 50009
0.00.053.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: LF token         = 187 ''
0.00.053.927 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.927 I print_info: max token length = 1024
0.00.053.928 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.735.313 I load_tensors: offloading 24 repeating layers to GPU
0.00.735.327 I load_tensors: offloading output layer to GPU
0.00.735.328 I load_tensors: offloaded 25/25 layers to GPU
0.00.735.362 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.735.363 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.737.070 I llama_init_from_model: n_seq_max     = 1
0.00.737.074 I llama_init_from_model: n_ctx         = 128
0.00.737.074 I llama_init_from_model: n_ctx_per_seq = 128
0.00.737.075 I llama_init_from_model: n_batch       = 128
0.00.737.075 I llama_init_from_model: n_ubatch      = 128
0.00.737.076 I llama_init_from_model: flash_attn    = 0
0.00.737.078 I llama_init_from_model: freq_base     = 10000.0
0.00.737.079 I llama_init_from_model: freq_scale    = 1
0.00.737.079 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.737.082 I ggml_metal_init: allocating
0.00.737.161 I ggml_metal_init: found device: Apple M4
0.00.737.175 I ggml_metal_init: picking default device: Apple M4
0.00.738.797 I ggml_metal_init: using embedded metal library
0.00.745.410 I ggml_metal_init: GPU name:   Apple M4
0.00.745.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.745.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.745.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.745.417 I ggml_metal_init: simdgroup reduction   = true
0.00.745.417 I ggml_metal_init: simdgroup matrix mul. = true
0.00.745.418 I ggml_metal_init: has residency sets    = true
0.00.745.418 I ggml_metal_init: has bfloat            = true
0.00.745.418 I ggml_metal_init: use bfloat            = true
0.00.745.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.745.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.955 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.766.351 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.766.358 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.766.393 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.607 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.769.609 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.769.609 I llama_init_from_model: graph nodes  = 967
0.00.769.610 I llama_init_from_model: graph splits = 2
0.00.769.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.769.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.058 I 
0.00.801.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.150 I perplexity: tokenizing the input ..
0.00.808.828 I perplexity: tokenization took 7.675 ms
0.00.808.834 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.957.433 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.958.789 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.958.816 I llama_perf_context_print:        load time =     777.98 ms
0.00.958.818 I llama_perf_context_print: prompt eval time =     147.69 ms /   128 tokens (    1.15 ms per token,   866.69 tokens per second)
0.00.958.820 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.958.820 I llama_perf_context_print:       total time =     157.76 ms /   129 tokens
0.00.959.233 I ggml_metal_free: deallocating

real	0m0.985s
user	0m0.081s
sys	0m0.163s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.762 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.061 I llama_model_loader: - type  f32:  194 tensors
0.00.024.062 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.062 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.063 I print_info: file format = GGUF V3 (latest)
0.00.024.063 I print_info: file type   = Q2_K - Medium
0.00.024.064 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.949 I load: special tokens cache size = 25
0.00.038.298 I load: token to piece cache size = 0.2984 MB
0.00.038.312 I print_info: arch             = gptneox
0.00.038.313 I print_info: vocab_only       = 0
0.00.038.313 I print_info: n_ctx_train      = 2048
0.00.038.313 I print_info: n_embd           = 2048
0.00.038.313 I print_info: n_layer          = 24
0.00.038.317 I print_info: n_head           = 16
0.00.038.317 I print_info: n_head_kv        = 16
0.00.038.318 I print_info: n_rot            = 32
0.00.038.318 I print_info: n_swa            = 0
0.00.038.318 I print_info: n_embd_head_k    = 128
0.00.038.318 I print_info: n_embd_head_v    = 128
0.00.038.319 I print_info: n_gqa            = 1
0.00.038.320 I print_info: n_embd_k_gqa     = 2048
0.00.038.320 I print_info: n_embd_v_gqa     = 2048
0.00.038.321 I print_info: f_norm_eps       = 1.0e-05
0.00.038.321 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.321 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.322 I print_info: f_logit_scale    = 0.0e+00
0.00.038.325 I print_info: n_ff             = 8192
0.00.038.325 I print_info: n_expert         = 0
0.00.038.325 I print_info: n_expert_used    = 0
0.00.038.325 I print_info: causal attn      = 1
0.00.038.325 I print_info: pooling type     = 0
0.00.038.325 I print_info: rope type        = 2
0.00.038.327 I print_info: rope scaling     = linear
0.00.038.327 I print_info: freq_base_train  = 10000.0
0.00.038.327 I print_info: freq_scale_train = 1
0.00.038.328 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.329 I print_info: rope_finetuned   = unknown
0.00.038.329 I print_info: ssm_d_conv       = 0
0.00.038.329 I print_info: ssm_d_inner      = 0
0.00.038.329 I print_info: ssm_d_state      = 0
0.00.038.329 I print_info: ssm_dt_rank      = 0
0.00.038.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.330 I print_info: model type       = 1.4B
0.00.038.330 I print_info: model params     = 1.41 B
0.00.038.330 I print_info: general.name     = 1.4B
0.00.038.331 I print_info: vocab type       = BPE
0.00.038.331 I print_info: n_vocab          = 50304
0.00.038.331 I print_info: n_merges         = 50009
0.00.038.338 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: LF token         = 187 ''
0.00.038.340 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.340 I print_info: max token length = 1024
0.00.038.340 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.386 I load_tensors: offloading output layer to GPU
0.00.338.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.421 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.422 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.185 I llama_init_from_model: n_seq_max     = 1
0.00.340.191 I llama_init_from_model: n_ctx         = 2048
0.00.340.192 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.192 I llama_init_from_model: n_batch       = 2048
0.00.340.193 I llama_init_from_model: n_ubatch      = 512
0.00.340.193 I llama_init_from_model: flash_attn    = 0
0.00.340.195 I llama_init_from_model: freq_base     = 10000.0
0.00.340.196 I llama_init_from_model: freq_scale    = 1
0.00.340.198 I ggml_metal_init: allocating
0.00.340.291 I ggml_metal_init: found device: Apple M4
0.00.340.305 I ggml_metal_init: picking default device: Apple M4
0.00.341.908 I ggml_metal_init: using embedded metal library
0.00.347.541 I ggml_metal_init: GPU name:   Apple M4
0.00.347.557 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.559 I ggml_metal_init: simdgroup reduction   = true
0.00.347.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.560 I ggml_metal_init: has residency sets    = true
0.00.347.560 I ggml_metal_init: has bfloat            = true
0.00.347.560 I ggml_metal_init: use bfloat            = true
0.00.347.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.369.346 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.422.644 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.422.653 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.422.687 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.427.630 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.427.633 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.427.633 I llama_init_from_model: graph nodes  = 967
0.00.427.633 I llama_init_from_model: graph splits = 2
0.00.427.638 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.427.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.427.767 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.166 I main: llama threadpool init, n_threads = 4
0.00.489.217 I 
0.00.489.237 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.239 I 
0.00.489.411 I sampler seed: 1234
0.00.489.416 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.489.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.489.434 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.489.434 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.171.266 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.171.267 I llama_perf_context_print:        load time =     479.66 ms
0.01.171.268 I llama_perf_context_print: prompt eval time =      43.22 ms /     7 tokens (    6.17 ms per token,   161.95 tokens per second)
0.01.171.269 I llama_perf_context_print:        eval time =     635.77 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.171.270 I llama_perf_context_print:       total time =     682.84 ms /    70 tokens
0.01.171.511 I ggml_metal_free: deallocating

real	0m1.188s
user	0m0.112s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.024 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.043 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.058 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.739 I llama_model_loader: - type  f32:  194 tensors
0.00.024.739 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.740 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.741 I print_info: file format = GGUF V3 (latest)
0.00.024.741 I print_info: file type   = Q2_K - Medium
0.00.024.743 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.363 I load: special tokens cache size = 25
0.00.039.489 I load: token to piece cache size = 0.2984 MB
0.00.039.506 I print_info: arch             = gptneox
0.00.039.507 I print_info: vocab_only       = 0
0.00.039.507 I print_info: n_ctx_train      = 2048
0.00.039.507 I print_info: n_embd           = 2048
0.00.039.507 I print_info: n_layer          = 24
0.00.039.511 I print_info: n_head           = 16
0.00.039.512 I print_info: n_head_kv        = 16
0.00.039.512 I print_info: n_rot            = 32
0.00.039.512 I print_info: n_swa            = 0
0.00.039.512 I print_info: n_embd_head_k    = 128
0.00.039.512 I print_info: n_embd_head_v    = 128
0.00.039.513 I print_info: n_gqa            = 1
0.00.039.514 I print_info: n_embd_k_gqa     = 2048
0.00.039.514 I print_info: n_embd_v_gqa     = 2048
0.00.039.515 I print_info: f_norm_eps       = 1.0e-05
0.00.039.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.515 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.515 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.516 I print_info: f_logit_scale    = 0.0e+00
0.00.039.516 I print_info: n_ff             = 8192
0.00.039.517 I print_info: n_expert         = 0
0.00.039.517 I print_info: n_expert_used    = 0
0.00.039.517 I print_info: causal attn      = 1
0.00.039.517 I print_info: pooling type     = 0
0.00.039.517 I print_info: rope type        = 2
0.00.039.517 I print_info: rope scaling     = linear
0.00.039.518 I print_info: freq_base_train  = 10000.0
0.00.039.518 I print_info: freq_scale_train = 1
0.00.039.519 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.519 I print_info: rope_finetuned   = unknown
0.00.039.519 I print_info: ssm_d_conv       = 0
0.00.039.520 I print_info: ssm_d_inner      = 0
0.00.039.520 I print_info: ssm_d_state      = 0
0.00.039.520 I print_info: ssm_dt_rank      = 0
0.00.039.520 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.520 I print_info: model type       = 1.4B
0.00.039.520 I print_info: model params     = 1.41 B
0.00.039.521 I print_info: general.name     = 1.4B
0.00.039.521 I print_info: vocab type       = BPE
0.00.039.521 I print_info: n_vocab          = 50304
0.00.039.521 I print_info: n_merges         = 50009
0.00.039.522 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: LF token         = 187 ''
0.00.039.523 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.523 I print_info: max token length = 1024
0.00.039.523 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.334.986 I load_tensors: offloading 24 repeating layers to GPU
0.00.334.999 I load_tensors: offloading output layer to GPU
0.00.335.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.335.032 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.335.034 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.336.612 I llama_init_from_model: n_seq_max     = 1
0.00.336.614 I llama_init_from_model: n_ctx         = 128
0.00.336.615 I llama_init_from_model: n_ctx_per_seq = 128
0.00.336.615 I llama_init_from_model: n_batch       = 128
0.00.336.616 I llama_init_from_model: n_ubatch      = 128
0.00.336.616 I llama_init_from_model: flash_attn    = 0
0.00.336.618 I llama_init_from_model: freq_base     = 10000.0
0.00.336.619 I llama_init_from_model: freq_scale    = 1
0.00.336.619 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.336.622 I ggml_metal_init: allocating
0.00.336.708 I ggml_metal_init: found device: Apple M4
0.00.336.727 I ggml_metal_init: picking default device: Apple M4
0.00.338.244 I ggml_metal_init: using embedded metal library
0.00.343.781 I ggml_metal_init: GPU name:   Apple M4
0.00.343.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.343.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.343.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.343.795 I ggml_metal_init: simdgroup reduction   = true
0.00.343.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.343.795 I ggml_metal_init: has residency sets    = true
0.00.343.795 I ggml_metal_init: has bfloat            = true
0.00.343.796 I ggml_metal_init: use bfloat            = true
0.00.343.797 I ggml_metal_init: hasUnifiedMemory      = true
0.00.343.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.365.382 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.369.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.369.133 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.369.176 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.372.521 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.372.523 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.372.524 I llama_init_from_model: graph nodes  = 967
0.00.372.524 I llama_init_from_model: graph splits = 2
0.00.372.527 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.372.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.406.029 I 
0.00.406.131 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.406.140 I perplexity: tokenizing the input ..
0.00.412.591 I perplexity: tokenization took 6.449 ms
0.00.412.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.919 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.264 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.284 I llama_perf_context_print:        load time =     397.00 ms
0.00.553.285 I llama_perf_context_print: prompt eval time =     138.77 ms /   128 tokens (    1.08 ms per token,   922.37 tokens per second)
0.00.553.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.286 I llama_perf_context_print:       total time =     147.26 ms /   129 tokens
0.00.553.681 I ggml_metal_free: deallocating

real	0m0.568s
user	0m0.081s
sys	0m0.085s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.788 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.516 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.390 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.392 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.393 I llama_model_loader: - type  f32:  194 tensors
0.00.025.393 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.393 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.394 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.394 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.395 I print_info: file format = GGUF V3 (latest)
0.00.025.395 I print_info: file type   = Q3_K - Medium
0.00.025.396 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.339 I load: special tokens cache size = 25
0.00.039.449 I load: token to piece cache size = 0.2984 MB
0.00.039.465 I print_info: arch             = gptneox
0.00.039.466 I print_info: vocab_only       = 0
0.00.039.466 I print_info: n_ctx_train      = 2048
0.00.039.466 I print_info: n_embd           = 2048
0.00.039.467 I print_info: n_layer          = 24
0.00.039.469 I print_info: n_head           = 16
0.00.039.470 I print_info: n_head_kv        = 16
0.00.039.470 I print_info: n_rot            = 32
0.00.039.470 I print_info: n_swa            = 0
0.00.039.470 I print_info: n_embd_head_k    = 128
0.00.039.471 I print_info: n_embd_head_v    = 128
0.00.039.471 I print_info: n_gqa            = 1
0.00.039.474 I print_info: n_embd_k_gqa     = 2048
0.00.039.474 I print_info: n_embd_v_gqa     = 2048
0.00.039.475 I print_info: f_norm_eps       = 1.0e-05
0.00.039.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.485 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.485 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.485 I print_info: f_logit_scale    = 0.0e+00
0.00.039.492 I print_info: n_ff             = 8192
0.00.039.492 I print_info: n_expert         = 0
0.00.039.493 I print_info: n_expert_used    = 0
0.00.039.494 I print_info: causal attn      = 1
0.00.039.495 I print_info: pooling type     = 0
0.00.039.495 I print_info: rope type        = 2
0.00.039.495 I print_info: rope scaling     = linear
0.00.039.495 I print_info: freq_base_train  = 10000.0
0.00.039.495 I print_info: freq_scale_train = 1
0.00.039.496 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.496 I print_info: rope_finetuned   = unknown
0.00.039.496 I print_info: ssm_d_conv       = 0
0.00.039.496 I print_info: ssm_d_inner      = 0
0.00.039.496 I print_info: ssm_d_state      = 0
0.00.039.496 I print_info: ssm_dt_rank      = 0
0.00.039.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.498 I print_info: model type       = 1.4B
0.00.039.498 I print_info: model params     = 1.41 B
0.00.039.498 I print_info: general.name     = 1.4B
0.00.039.499 I print_info: vocab type       = BPE
0.00.039.499 I print_info: n_vocab          = 50304
0.00.039.499 I print_info: n_merges         = 50009
0.00.039.499 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.500 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.500 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.500 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.500 I print_info: LF token         = 187 ''
0.00.039.501 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.501 I print_info: max token length = 1024
0.00.039.501 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.091 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.104 I load_tensors: offloading output layer to GPU
0.00.444.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.138 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.140 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.224 I llama_init_from_model: n_seq_max     = 1
0.00.445.226 I llama_init_from_model: n_ctx         = 2048
0.00.445.227 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.445.227 I llama_init_from_model: n_batch       = 2048
0.00.445.228 I llama_init_from_model: n_ubatch      = 512
0.00.445.228 I llama_init_from_model: flash_attn    = 0
0.00.445.230 I llama_init_from_model: freq_base     = 10000.0
0.00.445.231 I llama_init_from_model: freq_scale    = 1
0.00.445.240 I ggml_metal_init: allocating
0.00.445.321 I ggml_metal_init: found device: Apple M4
0.00.445.336 I ggml_metal_init: picking default device: Apple M4
0.00.446.906 I ggml_metal_init: using embedded metal library
0.00.452.621 I ggml_metal_init: GPU name:   Apple M4
0.00.452.635 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.635 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.636 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.636 I ggml_metal_init: simdgroup reduction   = true
0.00.452.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.637 I ggml_metal_init: has residency sets    = true
0.00.452.637 I ggml_metal_init: has bfloat            = true
0.00.452.638 I ggml_metal_init: use bfloat            = true
0.00.452.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.644 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.060 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.974 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.527.980 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.528.014 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.533.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.533.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.533.244 I llama_init_from_model: graph nodes  = 967
0.00.533.244 I llama_init_from_model: graph splits = 2
0.00.533.247 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.533.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.533.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.843 I main: llama threadpool init, n_threads = 4
0.00.588.888 I 
0.00.588.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.909 I 
0.00.589.065 I sampler seed: 1234
0.00.589.070 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.589.097 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.589.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.589.099 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.322.688 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.322.688 I llama_perf_context_print:        load time =     579.06 ms
0.01.322.689 I llama_perf_context_print: prompt eval time =      40.42 ms /     7 tokens (    5.77 ms per token,   173.19 tokens per second)
0.01.322.690 I llama_perf_context_print:        eval time =     690.26 ms /    63 runs   (   10.96 ms per token,    91.27 tokens per second)
0.01.322.690 I llama_perf_context_print:       total time =     734.59 ms /    70 tokens
0.01.322.886 I ggml_metal_free: deallocating

real	0m1.337s
user	0m0.110s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.180 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.961 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.962 I llama_model_loader: - type  f32:  194 tensors
0.00.024.962 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.962 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.962 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.963 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.963 I print_info: file format = GGUF V3 (latest)
0.00.024.964 I print_info: file type   = Q3_K - Medium
0.00.024.965 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.707 I load: special tokens cache size = 25
0.00.040.175 I load: token to piece cache size = 0.2984 MB
0.00.040.193 I print_info: arch             = gptneox
0.00.040.194 I print_info: vocab_only       = 0
0.00.040.194 I print_info: n_ctx_train      = 2048
0.00.040.194 I print_info: n_embd           = 2048
0.00.040.195 I print_info: n_layer          = 24
0.00.040.205 I print_info: n_head           = 16
0.00.040.206 I print_info: n_head_kv        = 16
0.00.040.206 I print_info: n_rot            = 32
0.00.040.206 I print_info: n_swa            = 0
0.00.040.207 I print_info: n_embd_head_k    = 128
0.00.040.207 I print_info: n_embd_head_v    = 128
0.00.040.207 I print_info: n_gqa            = 1
0.00.040.208 I print_info: n_embd_k_gqa     = 2048
0.00.040.208 I print_info: n_embd_v_gqa     = 2048
0.00.040.209 I print_info: f_norm_eps       = 1.0e-05
0.00.040.209 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.209 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.210 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.210 I print_info: f_logit_scale    = 0.0e+00
0.00.040.210 I print_info: n_ff             = 8192
0.00.040.211 I print_info: n_expert         = 0
0.00.040.211 I print_info: n_expert_used    = 0
0.00.040.211 I print_info: causal attn      = 1
0.00.040.211 I print_info: pooling type     = 0
0.00.040.211 I print_info: rope type        = 2
0.00.040.211 I print_info: rope scaling     = linear
0.00.040.212 I print_info: freq_base_train  = 10000.0
0.00.040.212 I print_info: freq_scale_train = 1
0.00.040.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.212 I print_info: rope_finetuned   = unknown
0.00.040.212 I print_info: ssm_d_conv       = 0
0.00.040.214 I print_info: ssm_d_inner      = 0
0.00.040.214 I print_info: ssm_d_state      = 0
0.00.040.214 I print_info: ssm_dt_rank      = 0
0.00.040.214 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.215 I print_info: model type       = 1.4B
0.00.040.215 I print_info: model params     = 1.41 B
0.00.040.215 I print_info: general.name     = 1.4B
0.00.040.215 I print_info: vocab type       = BPE
0.00.040.215 I print_info: n_vocab          = 50304
0.00.040.216 I print_info: n_merges         = 50009
0.00.040.216 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.216 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.217 I print_info: LF token         = 187 ''
0.00.040.217 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.217 I print_info: max token length = 1024
0.00.040.217 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.187 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.202 I load_tensors: offloading output layer to GPU
0.00.440.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.242 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.243 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.004 I llama_init_from_model: n_seq_max     = 1
0.00.442.006 I llama_init_from_model: n_ctx         = 128
0.00.442.007 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.007 I llama_init_from_model: n_batch       = 128
0.00.442.008 I llama_init_from_model: n_ubatch      = 128
0.00.442.008 I llama_init_from_model: flash_attn    = 0
0.00.442.010 I llama_init_from_model: freq_base     = 10000.0
0.00.442.011 I llama_init_from_model: freq_scale    = 1
0.00.442.011 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.014 I ggml_metal_init: allocating
0.00.442.144 I ggml_metal_init: found device: Apple M4
0.00.442.159 I ggml_metal_init: picking default device: Apple M4
0.00.443.787 I ggml_metal_init: using embedded metal library
0.00.450.376 I ggml_metal_init: GPU name:   Apple M4
0.00.450.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.385 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.387 I ggml_metal_init: simdgroup reduction   = true
0.00.450.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.387 I ggml_metal_init: has residency sets    = true
0.00.450.388 I ggml_metal_init: has bfloat            = true
0.00.450.388 I ggml_metal_init: use bfloat            = true
0.00.450.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.469.308 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.472.852 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.472.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.476.070 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.476.072 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.476.073 I llama_init_from_model: graph nodes  = 967
0.00.476.073 I llama_init_from_model: graph splits = 2
0.00.476.076 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.476.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.109 I 
0.00.505.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.221 I perplexity: tokenizing the input ..
0.00.512.413 I perplexity: tokenization took 7.189 ms
0.00.512.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.531 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.889 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.913 I llama_perf_context_print:        load time =     496.20 ms
0.00.655.914 I llama_perf_context_print: prompt eval time =     141.35 ms /   128 tokens (    1.10 ms per token,   905.54 tokens per second)
0.00.655.914 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.914 I llama_perf_context_print:       total time =     150.81 ms /   129 tokens
0.00.656.342 I ggml_metal_free: deallocating

real	0m0.670s
user	0m0.081s
sys	0m0.111s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.144 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.744 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.753 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.510 I llama_model_loader: - type  f32:  194 tensors
0.00.026.510 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.510 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.510 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.511 I print_info: file format = GGUF V3 (latest)
0.00.026.511 I print_info: file type   = Q4_K - Medium
0.00.026.512 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.715 I load: special tokens cache size = 25
0.00.041.034 I load: token to piece cache size = 0.2984 MB
0.00.041.048 I print_info: arch             = gptneox
0.00.041.049 I print_info: vocab_only       = 0
0.00.041.050 I print_info: n_ctx_train      = 2048
0.00.041.050 I print_info: n_embd           = 2048
0.00.041.050 I print_info: n_layer          = 24
0.00.041.053 I print_info: n_head           = 16
0.00.041.053 I print_info: n_head_kv        = 16
0.00.041.054 I print_info: n_rot            = 32
0.00.041.054 I print_info: n_swa            = 0
0.00.041.054 I print_info: n_embd_head_k    = 128
0.00.041.054 I print_info: n_embd_head_v    = 128
0.00.041.055 I print_info: n_gqa            = 1
0.00.041.056 I print_info: n_embd_k_gqa     = 2048
0.00.041.057 I print_info: n_embd_v_gqa     = 2048
0.00.041.057 I print_info: f_norm_eps       = 1.0e-05
0.00.041.058 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.059 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.059 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.061 I print_info: f_logit_scale    = 0.0e+00
0.00.041.061 I print_info: n_ff             = 8192
0.00.041.061 I print_info: n_expert         = 0
0.00.041.061 I print_info: n_expert_used    = 0
0.00.041.062 I print_info: causal attn      = 1
0.00.041.063 I print_info: pooling type     = 0
0.00.041.064 I print_info: rope type        = 2
0.00.041.064 I print_info: rope scaling     = linear
0.00.041.066 I print_info: freq_base_train  = 10000.0
0.00.041.066 I print_info: freq_scale_train = 1
0.00.041.066 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.067 I print_info: rope_finetuned   = unknown
0.00.041.067 I print_info: ssm_d_conv       = 0
0.00.041.067 I print_info: ssm_d_inner      = 0
0.00.041.067 I print_info: ssm_d_state      = 0
0.00.041.067 I print_info: ssm_dt_rank      = 0
0.00.041.067 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.067 I print_info: model type       = 1.4B
0.00.041.069 I print_info: model params     = 1.41 B
0.00.041.069 I print_info: general.name     = 1.4B
0.00.041.069 I print_info: vocab type       = BPE
0.00.041.069 I print_info: n_vocab          = 50304
0.00.041.070 I print_info: n_merges         = 50009
0.00.041.070 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: LF token         = 187 ''
0.00.041.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.072 I print_info: max token length = 1024
0.00.041.073 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.551.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.551.121 I load_tensors: offloading output layer to GPU
0.00.551.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.551.157 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.551.167 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.552.955 I llama_init_from_model: n_seq_max     = 1
0.00.552.958 I llama_init_from_model: n_ctx         = 2048
0.00.552.958 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.552.959 I llama_init_from_model: n_batch       = 2048
0.00.552.959 I llama_init_from_model: n_ubatch      = 512
0.00.552.960 I llama_init_from_model: flash_attn    = 0
0.00.552.962 I llama_init_from_model: freq_base     = 10000.0
0.00.552.963 I llama_init_from_model: freq_scale    = 1
0.00.552.966 I ggml_metal_init: allocating
0.00.553.043 I ggml_metal_init: found device: Apple M4
0.00.553.057 I ggml_metal_init: picking default device: Apple M4
0.00.554.686 I ggml_metal_init: using embedded metal library
0.00.561.470 I ggml_metal_init: GPU name:   Apple M4
0.00.561.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.561.475 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.561.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.561.477 I ggml_metal_init: simdgroup reduction   = true
0.00.561.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.561.477 I ggml_metal_init: has residency sets    = true
0.00.561.478 I ggml_metal_init: has bfloat            = true
0.00.561.478 I ggml_metal_init: use bfloat            = true
0.00.561.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.561.480 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.579.240 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.481 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.631.488 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.631.516 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.229 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.636.231 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.636.231 I llama_init_from_model: graph nodes  = 967
0.00.636.231 I llama_init_from_model: graph splits = 2
0.00.636.236 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.636.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.636.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.454 I main: llama threadpool init, n_threads = 4
0.00.694.501 I 
0.00.694.522 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.523 I 
0.00.694.674 I sampler seed: 1234
0.00.694.679 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.703 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.704 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.457.361 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.457.362 I llama_perf_context_print:        load time =     683.59 ms
0.01.457.363 I llama_perf_context_print: prompt eval time =      58.04 ms /     7 tokens (    8.29 ms per token,   120.61 tokens per second)
0.01.457.363 I llama_perf_context_print:        eval time =     701.75 ms /    63 runs   (   11.14 ms per token,    89.78 tokens per second)
0.01.457.364 I llama_perf_context_print:       total time =     763.62 ms /    70 tokens
0.01.457.565 I ggml_metal_free: deallocating

real	0m1.477s
user	0m0.110s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.239 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.087 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.089 I llama_model_loader: - type  f32:  194 tensors
0.00.025.089 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.089 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.090 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.090 I print_info: file format = GGUF V3 (latest)
0.00.025.096 I print_info: file type   = Q4_K - Medium
0.00.025.097 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.700 I load: special tokens cache size = 25
0.00.040.289 I load: token to piece cache size = 0.2984 MB
0.00.040.306 I print_info: arch             = gptneox
0.00.040.307 I print_info: vocab_only       = 0
0.00.040.307 I print_info: n_ctx_train      = 2048
0.00.040.307 I print_info: n_embd           = 2048
0.00.040.308 I print_info: n_layer          = 24
0.00.040.312 I print_info: n_head           = 16
0.00.040.312 I print_info: n_head_kv        = 16
0.00.040.313 I print_info: n_rot            = 32
0.00.040.313 I print_info: n_swa            = 0
0.00.040.313 I print_info: n_embd_head_k    = 128
0.00.040.313 I print_info: n_embd_head_v    = 128
0.00.040.314 I print_info: n_gqa            = 1
0.00.040.314 I print_info: n_embd_k_gqa     = 2048
0.00.040.315 I print_info: n_embd_v_gqa     = 2048
0.00.040.316 I print_info: f_norm_eps       = 1.0e-05
0.00.040.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.316 I print_info: f_logit_scale    = 0.0e+00
0.00.040.317 I print_info: n_ff             = 8192
0.00.040.320 I print_info: n_expert         = 0
0.00.040.320 I print_info: n_expert_used    = 0
0.00.040.320 I print_info: causal attn      = 1
0.00.040.320 I print_info: pooling type     = 0
0.00.040.320 I print_info: rope type        = 2
0.00.040.321 I print_info: rope scaling     = linear
0.00.040.321 I print_info: freq_base_train  = 10000.0
0.00.040.321 I print_info: freq_scale_train = 1
0.00.040.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.322 I print_info: rope_finetuned   = unknown
0.00.040.322 I print_info: ssm_d_conv       = 0
0.00.040.322 I print_info: ssm_d_inner      = 0
0.00.040.323 I print_info: ssm_d_state      = 0
0.00.040.323 I print_info: ssm_dt_rank      = 0
0.00.040.324 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.324 I print_info: model type       = 1.4B
0.00.040.324 I print_info: model params     = 1.41 B
0.00.040.324 I print_info: general.name     = 1.4B
0.00.040.325 I print_info: vocab type       = BPE
0.00.040.325 I print_info: n_vocab          = 50304
0.00.040.325 I print_info: n_merges         = 50009
0.00.040.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.326 I print_info: LF token         = 187 ''
0.00.040.326 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.326 I print_info: max token length = 1024
0.00.040.327 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.531.926 I load_tensors: offloading 24 repeating layers to GPU
0.00.531.940 I load_tensors: offloading output layer to GPU
0.00.531.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.531.975 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.531.976 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.533.609 I llama_init_from_model: n_seq_max     = 1
0.00.533.617 I llama_init_from_model: n_ctx         = 128
0.00.533.617 I llama_init_from_model: n_ctx_per_seq = 128
0.00.533.618 I llama_init_from_model: n_batch       = 128
0.00.533.618 I llama_init_from_model: n_ubatch      = 128
0.00.533.619 I llama_init_from_model: flash_attn    = 0
0.00.533.621 I llama_init_from_model: freq_base     = 10000.0
0.00.533.621 I llama_init_from_model: freq_scale    = 1
0.00.533.622 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.533.624 I ggml_metal_init: allocating
0.00.533.690 I ggml_metal_init: found device: Apple M4
0.00.533.706 I ggml_metal_init: picking default device: Apple M4
0.00.535.687 I ggml_metal_init: using embedded metal library
0.00.542.661 I ggml_metal_init: GPU name:   Apple M4
0.00.542.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.670 I ggml_metal_init: simdgroup reduction   = true
0.00.542.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.671 I ggml_metal_init: has residency sets    = true
0.00.542.671 I ggml_metal_init: has bfloat            = true
0.00.542.671 I ggml_metal_init: use bfloat            = true
0.00.542.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.029 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.564.698 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.564.703 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.564.735 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.567.971 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.567.973 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.567.974 I llama_init_from_model: graph nodes  = 967
0.00.567.974 I llama_init_from_model: graph splits = 2
0.00.567.978 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.567.978 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.133 I 
0.00.598.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.224 I perplexity: tokenizing the input ..
0.00.605.316 I perplexity: tokenization took 7.088 ms
0.00.605.323 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.357 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.749.695 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.749.714 I llama_perf_context_print:        load time =     589.13 ms
0.00.749.715 I llama_perf_context_print: prompt eval time =     142.06 ms /   128 tokens (    1.11 ms per token,   901.00 tokens per second)
0.00.749.718 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.718 I llama_perf_context_print:       total time =     151.59 ms /   129 tokens
0.00.750.187 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.082s
sys	0m0.129s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.750 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.751 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.244 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.245 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.245 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.247 I llama_model_loader: - type  f32:  194 tensors
0.00.025.247 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.247 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.248 I print_info: file format = GGUF V3 (latest)
0.00.025.248 I print_info: file type   = Q5_K - Medium
0.00.025.249 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.445 I load: special tokens cache size = 25
0.00.039.682 I load: token to piece cache size = 0.2984 MB
0.00.039.696 I print_info: arch             = gptneox
0.00.039.697 I print_info: vocab_only       = 0
0.00.039.698 I print_info: n_ctx_train      = 2048
0.00.039.698 I print_info: n_embd           = 2048
0.00.039.698 I print_info: n_layer          = 24
0.00.039.701 I print_info: n_head           = 16
0.00.039.702 I print_info: n_head_kv        = 16
0.00.039.702 I print_info: n_rot            = 32
0.00.039.702 I print_info: n_swa            = 0
0.00.039.702 I print_info: n_embd_head_k    = 128
0.00.039.702 I print_info: n_embd_head_v    = 128
0.00.039.703 I print_info: n_gqa            = 1
0.00.039.704 I print_info: n_embd_k_gqa     = 2048
0.00.039.705 I print_info: n_embd_v_gqa     = 2048
0.00.039.705 I print_info: f_norm_eps       = 1.0e-05
0.00.039.706 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.706 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.706 I print_info: f_logit_scale    = 0.0e+00
0.00.039.707 I print_info: n_ff             = 8192
0.00.039.707 I print_info: n_expert         = 0
0.00.039.707 I print_info: n_expert_used    = 0
0.00.039.708 I print_info: causal attn      = 1
0.00.039.708 I print_info: pooling type     = 0
0.00.039.709 I print_info: rope type        = 2
0.00.039.711 I print_info: rope scaling     = linear
0.00.039.712 I print_info: freq_base_train  = 10000.0
0.00.039.712 I print_info: freq_scale_train = 1
0.00.039.712 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.712 I print_info: rope_finetuned   = unknown
0.00.039.712 I print_info: ssm_d_conv       = 0
0.00.039.712 I print_info: ssm_d_inner      = 0
0.00.039.712 I print_info: ssm_d_state      = 0
0.00.039.713 I print_info: ssm_dt_rank      = 0
0.00.039.713 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.714 I print_info: model type       = 1.4B
0.00.039.715 I print_info: model params     = 1.41 B
0.00.039.715 I print_info: general.name     = 1.4B
0.00.039.715 I print_info: vocab type       = BPE
0.00.039.716 I print_info: n_vocab          = 50304
0.00.039.716 I print_info: n_merges         = 50009
0.00.039.716 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: LF token         = 187 ''
0.00.039.717 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: max token length = 1024
0.00.039.717 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.595.490 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.505 I load_tensors: offloading output layer to GPU
0.00.595.506 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.538 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.595.539 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.597.144 I llama_init_from_model: n_seq_max     = 1
0.00.597.146 I llama_init_from_model: n_ctx         = 2048
0.00.597.147 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.597.147 I llama_init_from_model: n_batch       = 2048
0.00.597.148 I llama_init_from_model: n_ubatch      = 512
0.00.597.148 I llama_init_from_model: flash_attn    = 0
0.00.597.149 I llama_init_from_model: freq_base     = 10000.0
0.00.597.150 I llama_init_from_model: freq_scale    = 1
0.00.597.151 I ggml_metal_init: allocating
0.00.597.174 I ggml_metal_init: found device: Apple M4
0.00.597.188 I ggml_metal_init: picking default device: Apple M4
0.00.598.458 I ggml_metal_init: using embedded metal library
0.00.604.901 I ggml_metal_init: GPU name:   Apple M4
0.00.604.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.906 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.907 I ggml_metal_init: simdgroup reduction   = true
0.00.604.907 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.908 I ggml_metal_init: has residency sets    = true
0.00.604.908 I ggml_metal_init: has bfloat            = true
0.00.604.908 I ggml_metal_init: use bfloat            = true
0.00.604.909 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.911 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.461 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.674.667 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.674.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.212 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.214 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.214 I llama_init_from_model: graph nodes  = 967
0.00.679.215 I llama_init_from_model: graph splits = 2
0.00.679.221 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.828 I main: llama threadpool init, n_threads = 4
0.00.734.870 I 
0.00.734.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.890 I 
0.00.735.039 I sampler seed: 1234
0.00.735.044 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.088 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.088 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.586.433 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.586.434 I llama_perf_context_print:        load time =     725.22 ms
0.01.586.435 I llama_perf_context_print: prompt eval time =      52.93 ms /     7 tokens (    7.56 ms per token,   132.26 tokens per second)
0.01.586.436 I llama_perf_context_print:        eval time =     795.50 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.586.437 I llama_perf_context_print:       total time =     852.35 ms /    70 tokens
0.01.586.661 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.293 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.612 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.628 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.205 I llama_model_loader: - type  f32:  194 tensors
0.00.027.206 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.206 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.207 I print_info: file format = GGUF V3 (latest)
0.00.027.207 I print_info: file type   = Q5_K - Medium
0.00.027.208 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.359 I load: special tokens cache size = 25
0.00.041.621 I load: token to piece cache size = 0.2984 MB
0.00.041.638 I print_info: arch             = gptneox
0.00.041.639 I print_info: vocab_only       = 0
0.00.041.639 I print_info: n_ctx_train      = 2048
0.00.041.640 I print_info: n_embd           = 2048
0.00.041.640 I print_info: n_layer          = 24
0.00.041.644 I print_info: n_head           = 16
0.00.041.644 I print_info: n_head_kv        = 16
0.00.041.645 I print_info: n_rot            = 32
0.00.041.645 I print_info: n_swa            = 0
0.00.041.645 I print_info: n_embd_head_k    = 128
0.00.041.645 I print_info: n_embd_head_v    = 128
0.00.041.646 I print_info: n_gqa            = 1
0.00.041.646 I print_info: n_embd_k_gqa     = 2048
0.00.041.647 I print_info: n_embd_v_gqa     = 2048
0.00.041.647 I print_info: f_norm_eps       = 1.0e-05
0.00.041.653 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.653 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.653 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.653 I print_info: f_logit_scale    = 0.0e+00
0.00.041.654 I print_info: n_ff             = 8192
0.00.041.654 I print_info: n_expert         = 0
0.00.041.654 I print_info: n_expert_used    = 0
0.00.041.654 I print_info: causal attn      = 1
0.00.041.654 I print_info: pooling type     = 0
0.00.041.655 I print_info: rope type        = 2
0.00.041.655 I print_info: rope scaling     = linear
0.00.041.655 I print_info: freq_base_train  = 10000.0
0.00.041.655 I print_info: freq_scale_train = 1
0.00.041.657 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.657 I print_info: rope_finetuned   = unknown
0.00.041.657 I print_info: ssm_d_conv       = 0
0.00.041.657 I print_info: ssm_d_inner      = 0
0.00.041.657 I print_info: ssm_d_state      = 0
0.00.041.657 I print_info: ssm_dt_rank      = 0
0.00.041.658 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.658 I print_info: model type       = 1.4B
0.00.041.658 I print_info: model params     = 1.41 B
0.00.041.660 I print_info: general.name     = 1.4B
0.00.041.660 I print_info: vocab type       = BPE
0.00.041.660 I print_info: n_vocab          = 50304
0.00.041.660 I print_info: n_merges         = 50009
0.00.041.661 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.661 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.663 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.663 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.663 I print_info: LF token         = 187 ''
0.00.041.663 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.663 I print_info: max token length = 1024
0.00.041.664 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.300 I load_tensors: offloading output layer to GPU
0.00.594.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.333 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.335 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.758 I llama_init_from_model: n_seq_max     = 1
0.00.595.766 I llama_init_from_model: n_ctx         = 128
0.00.595.766 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.767 I llama_init_from_model: n_batch       = 128
0.00.595.767 I llama_init_from_model: n_ubatch      = 128
0.00.595.768 I llama_init_from_model: flash_attn    = 0
0.00.595.770 I llama_init_from_model: freq_base     = 10000.0
0.00.595.770 I llama_init_from_model: freq_scale    = 1
0.00.595.771 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.773 I ggml_metal_init: allocating
0.00.595.858 I ggml_metal_init: found device: Apple M4
0.00.595.873 I ggml_metal_init: picking default device: Apple M4
0.00.597.324 I ggml_metal_init: using embedded metal library
0.00.602.290 I ggml_metal_init: GPU name:   Apple M4
0.00.602.298 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.300 I ggml_metal_init: simdgroup reduction   = true
0.00.602.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.300 I ggml_metal_init: has residency sets    = true
0.00.602.300 I ggml_metal_init: has bfloat            = true
0.00.602.300 I ggml_metal_init: use bfloat            = true
0.00.602.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.334 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.080 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.095 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.617.860 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.617.862 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.617.862 I llama_init_from_model: graph nodes  = 967
0.00.617.862 I llama_init_from_model: graph splits = 2
0.00.617.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.707 I 
0.00.650.751 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.754 I perplexity: tokenizing the input ..
0.00.654.849 I perplexity: tokenization took 4.094 ms
0.00.654.853 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.897 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.802.238 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.802.263 I llama_perf_context_print:        load time =     640.41 ms
0.00.802.263 I llama_perf_context_print: prompt eval time =     145.81 ms /   128 tokens (    1.14 ms per token,   877.85 tokens per second)
0.00.802.264 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.264 I llama_perf_context_print:       total time =     151.56 ms /   129 tokens
0.00.802.621 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.068s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.732 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.739 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.121 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.122 I llama_model_loader: - type  f32:  194 tensors
0.00.024.123 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.123 I print_info: file format = GGUF V3 (latest)
0.00.024.124 I print_info: file type   = Q6_K
0.00.024.125 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.256 I load: special tokens cache size = 25
0.00.038.828 I load: token to piece cache size = 0.2984 MB
0.00.038.842 I print_info: arch             = gptneox
0.00.038.843 I print_info: vocab_only       = 0
0.00.038.843 I print_info: n_ctx_train      = 2048
0.00.038.843 I print_info: n_embd           = 2048
0.00.038.844 I print_info: n_layer          = 24
0.00.038.846 I print_info: n_head           = 16
0.00.038.847 I print_info: n_head_kv        = 16
0.00.038.847 I print_info: n_rot            = 32
0.00.038.848 I print_info: n_swa            = 0
0.00.038.848 I print_info: n_embd_head_k    = 128
0.00.038.850 I print_info: n_embd_head_v    = 128
0.00.038.850 I print_info: n_gqa            = 1
0.00.038.851 I print_info: n_embd_k_gqa     = 2048
0.00.038.852 I print_info: n_embd_v_gqa     = 2048
0.00.038.853 I print_info: f_norm_eps       = 1.0e-05
0.00.038.853 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.853 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.853 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.855 I print_info: f_logit_scale    = 0.0e+00
0.00.038.855 I print_info: n_ff             = 8192
0.00.038.855 I print_info: n_expert         = 0
0.00.038.856 I print_info: n_expert_used    = 0
0.00.038.856 I print_info: causal attn      = 1
0.00.038.856 I print_info: pooling type     = 0
0.00.038.856 I print_info: rope type        = 2
0.00.038.856 I print_info: rope scaling     = linear
0.00.038.856 I print_info: freq_base_train  = 10000.0
0.00.038.857 I print_info: freq_scale_train = 1
0.00.038.857 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.857 I print_info: rope_finetuned   = unknown
0.00.038.857 I print_info: ssm_d_conv       = 0
0.00.038.858 I print_info: ssm_d_inner      = 0
0.00.038.858 I print_info: ssm_d_state      = 0
0.00.038.858 I print_info: ssm_dt_rank      = 0
0.00.038.859 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.859 I print_info: model type       = 1.4B
0.00.038.860 I print_info: model params     = 1.41 B
0.00.038.860 I print_info: general.name     = 1.4B
0.00.038.861 I print_info: vocab type       = BPE
0.00.038.862 I print_info: n_vocab          = 50304
0.00.038.862 I print_info: n_merges         = 50009
0.00.038.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.863 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.863 I print_info: LF token         = 187 ''
0.00.038.863 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.865 I print_info: max token length = 1024
0.00.038.865 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.452 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.466 I load_tensors: offloading output layer to GPU
0.00.637.467 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.499 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.500 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.036 I llama_init_from_model: n_seq_max     = 1
0.00.639.038 I llama_init_from_model: n_ctx         = 2048
0.00.639.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.039 I llama_init_from_model: n_batch       = 2048
0.00.639.039 I llama_init_from_model: n_ubatch      = 512
0.00.639.040 I llama_init_from_model: flash_attn    = 0
0.00.639.041 I llama_init_from_model: freq_base     = 10000.0
0.00.639.042 I llama_init_from_model: freq_scale    = 1
0.00.639.043 I ggml_metal_init: allocating
0.00.639.078 I ggml_metal_init: found device: Apple M4
0.00.639.090 I ggml_metal_init: picking default device: Apple M4
0.00.640.384 I ggml_metal_init: using embedded metal library
0.00.646.651 I ggml_metal_init: GPU name:   Apple M4
0.00.646.655 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.657 I ggml_metal_init: simdgroup reduction   = true
0.00.646.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.658 I ggml_metal_init: has residency sets    = true
0.00.646.658 I ggml_metal_init: has bfloat            = true
0.00.646.658 I ggml_metal_init: use bfloat            = true
0.00.646.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.230 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.715.137 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.715.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.732 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.734 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.734 I llama_init_from_model: graph nodes  = 967
0.00.719.734 I llama_init_from_model: graph splits = 2
0.00.719.739 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.882 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.341 I main: llama threadpool init, n_threads = 4
0.00.788.393 I 
0.00.788.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.413 I 
0.00.788.598 I sampler seed: 1234
0.00.788.602 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.788.618 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.788.619 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.788.619 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.684.283 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.01.684.285 I llama_perf_context_print:        load time =     778.67 ms
0.01.684.286 I llama_perf_context_print: prompt eval time =      57.52 ms /     7 tokens (    8.22 ms per token,   121.69 tokens per second)
0.01.684.286 I llama_perf_context_print:        eval time =     835.39 ms /    63 runs   (   13.26 ms per token,    75.41 tokens per second)
0.01.684.286 I llama_perf_context_print:       total time =     896.75 ms /    70 tokens
0.01.684.588 I ggml_metal_free: deallocating

real	0m1.701s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4865 (e128a1bf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.810 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.812 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.814 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.816 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.819 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.819 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.467 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.468 I llama_model_loader: - type  f32:  194 tensors
0.00.024.469 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.469 I print_info: file format = GGUF V3 (latest)
0.00.024.470 I print_info: file type   = Q6_K
0.00.024.471 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.638 I load: special tokens cache size = 25
0.00.038.872 I load: token to piece cache size = 0.2984 MB
0.00.038.889 I print_info: arch             = gptneox
0.00.038.891 I print_info: vocab_only       = 0
0.00.038.891 I print_info: n_ctx_train      = 2048
0.00.038.891 I print_info: n_embd           = 2048
0.00.038.891 I print_info: n_layer          = 24
0.00.038.896 I print_info: n_head           = 16
0.00.038.896 I print_info: n_head_kv        = 16
0.00.038.897 I print_info: n_rot            = 32
0.00.038.897 I print_info: n_swa            = 0
0.00.038.897 I print_info: n_embd_head_k    = 128
0.00.038.897 I print_info: n_embd_head_v    = 128
0.00.038.898 I print_info: n_gqa            = 1
0.00.038.899 I print_info: n_embd_k_gqa     = 2048
0.00.038.899 I print_info: n_embd_v_gqa     = 2048
0.00.038.900 I print_info: f_norm_eps       = 1.0e-05
0.00.038.900 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.901 I print_info: f_logit_scale    = 0.0e+00
0.00.038.901 I print_info: n_ff             = 8192
0.00.038.901 I print_info: n_expert         = 0
0.00.038.902 I print_info: n_expert_used    = 0
0.00.038.902 I print_info: causal attn      = 1
0.00.038.902 I print_info: pooling type     = 0
0.00.038.902 I print_info: rope type        = 2
0.00.038.902 I print_info: rope scaling     = linear
0.00.038.902 I print_info: freq_base_train  = 10000.0
0.00.038.903 I print_info: freq_scale_train = 1
0.00.038.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.903 I print_info: rope_finetuned   = unknown
0.00.038.903 I print_info: ssm_d_conv       = 0
0.00.038.903 I print_info: ssm_d_inner      = 0
0.00.038.903 I print_info: ssm_d_state      = 0
0.00.038.904 I print_info: ssm_dt_rank      = 0
0.00.038.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.904 I print_info: model type       = 1.4B
0.00.038.904 I print_info: model params     = 1.41 B
0.00.038.904 I print_info: general.name     = 1.4B
0.00.038.905 I print_info: vocab type       = BPE
0.00.038.905 I print_info: n_vocab          = 50304
0.00.038.905 I print_info: n_merges         = 50009
0.00.038.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.906 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.906 I print_info: LF token         = 187 ''
0.00.038.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.907 I print_info: max token length = 1024
0.00.038.907 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.579.439 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.442 I load_tensors: offloading output layer to GPU
0.00.579.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.467 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.579.470 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.580.845 I llama_init_from_model: n_seq_max     = 1
0.00.580.847 I llama_init_from_model: n_ctx         = 128
0.00.580.848 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.848 I llama_init_from_model: n_batch       = 128
0.00.580.848 I llama_init_from_model: n_ubatch      = 128
0.00.580.849 I llama_init_from_model: flash_attn    = 0
0.00.580.850 I llama_init_from_model: freq_base     = 10000.0
0.00.580.850 I llama_init_from_model: freq_scale    = 1
0.00.580.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.852 I ggml_metal_init: allocating
0.00.580.877 I ggml_metal_init: found device: Apple M4
0.00.580.886 I ggml_metal_init: picking default device: Apple M4
0.00.582.190 I ggml_metal_init: using embedded metal library
0.00.588.306 I ggml_metal_init: GPU name:   Apple M4
0.00.588.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.588.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.588.311 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.588.312 I ggml_metal_init: simdgroup reduction   = true
0.00.588.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.588.312 I ggml_metal_init: has residency sets    = true
0.00.588.312 I ggml_metal_init: has bfloat            = true
0.00.588.313 I ggml_metal_init: use bfloat            = true
0.00.588.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.588.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.715 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.094 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.609.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.609.122 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.447 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.612.448 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.612.449 I llama_init_from_model: graph nodes  = 967
0.00.612.450 I llama_init_from_model: graph splits = 2
0.00.612.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.612.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.681 I 
0.00.643.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.785 I perplexity: tokenizing the input ..
0.00.650.901 I perplexity: tokenization took 7.111 ms
0.00.650.909 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.435 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.784.771 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.784.793 I llama_perf_context_print:        load time =     634.76 ms
0.00.784.794 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.48 tokens per second)
0.00.784.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.795 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.785.160 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.079s
sys	0m0.132s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4865 (e128a1bf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12af04a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12af05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12af05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12af05cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12af06270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12af06820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12af06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12af07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12af07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12af07e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12af08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12af08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12af09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12af09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12af0a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12af0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12af0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12af0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12af0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12af0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12af0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12af0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12af0dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12af0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12af0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12af0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12af0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12af0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12af10100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12af105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12af10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12af10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12af11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12af116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12af11b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12af11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12af12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12af12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12af12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12af13270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12af13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12af13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12af14050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12af144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12af147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12af14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12af151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12af15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12af16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12af16510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12af169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12af16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12af172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12af17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12af17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12af180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12af18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12af18a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12af18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12af19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12af196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12af19b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12af1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12af1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12af1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12af1ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12af1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12af1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12af1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12af1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12af1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12af1c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12af1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12af1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12af1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12af1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12af1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12af1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12af1ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12af1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12af1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12af1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12af20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12af208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12af20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12af21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12af218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12af21df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12af22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12af22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12af22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12af23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12af23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12af23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12af24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12af24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12af24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12af156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12af25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12af259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12af25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12af26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12af269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12af26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12af27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12af279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12af27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12af28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12af289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12af28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12af29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12af299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12af29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12af2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12af2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12af2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12af2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12af2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12af2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12af2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12af2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12af2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12af2cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12af2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12af2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12af2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12af2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12af2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12af2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12af2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12af2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12af2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12af2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12af30010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12af304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12af30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12af30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12af31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12af31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12af31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12af32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12af32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12af329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12af32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12af332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12af33790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12af33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12af340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12af34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12af34a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12af34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12af35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ae04db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ae05220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ae078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ae08070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ae08510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ae089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ae08e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ae092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ae09790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ae09c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ae0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ae0a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ae0aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ae0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ae0b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ae0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ae0bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ae0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ae0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ae0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ae0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ae0d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ae0d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ae0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ae0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ae0e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ae0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ae0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ae0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ae0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ae0fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ae101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ae10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ae10b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ae11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ae11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ae11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ae12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ae126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ae12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ae130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ae13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ae13b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ae140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ae14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ae14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ae14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ae15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ae157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ae15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ae161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ae16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ae16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ae16fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ae17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ae17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ae17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ae185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ae18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ae18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ae19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ae199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ae19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ae1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ae1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ae1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ae1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ae1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ae1c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ae1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ae1cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ae1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ae1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ae1de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ae1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ae1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ae1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ae1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ae1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ae20030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ae205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ae20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ae21140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ae216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ae21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ae22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ae22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ae22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ae23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ae23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ae23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ae24470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ae24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ae24fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ae25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ae25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ae260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ae26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ae26c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ae271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ae277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ae27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ae28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ae288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ae28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ae29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ae299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ae29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ae2a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ae2aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ae2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ae2b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ae2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ae2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ae2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ae2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ae2d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ae2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ae2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ae2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ae2e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ae2ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ae2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ae2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ae2f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ae2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ae30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ae30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ae30d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ae31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ae31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ae31c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ae32140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ae32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ae32b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ae33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ae33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ae33a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ae33f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ae34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ae34e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ae35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ae35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ae363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ae36670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ae36e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ae372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ae37740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.721.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae05540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae05e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae06290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae06700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae06b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae06fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae07450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae07d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae08890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae09b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae0aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae0b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae0bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae0c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae0ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae0dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae0e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae10350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae12fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae13440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae14b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae15de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae16720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae16bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae18100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae19580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae19e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae1a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae1a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae1b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae1c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae1d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae1f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae1fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae20470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae22c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae23540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae37620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae3a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae3b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae3c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae3d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae3f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae3ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ae414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ae41bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ae42070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ae42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ae429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ae43200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ae434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ae43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ae44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ae445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ae44b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ae45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ae456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ae45c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ae46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ae467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ae46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ae47350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ae47900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ae47eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ae48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ae48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ae48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ae49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ae49b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ae4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ae4a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ae4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ae4b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ae4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ae4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ae4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ae4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ae4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ae4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ae4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ae4df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ae4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ae4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ae4f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ae4f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ae4fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ae50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ae50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ae50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ae51290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ae51840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ae51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ae523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ae52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ae52f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ae534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ae53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ae54010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ae545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ae54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ae55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ae556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ae55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ae56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ae567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ae56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ae57340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ae57840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ae57d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ae58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ae58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ae58c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ae59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ae59640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ae59b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ae5a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ae5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ae5aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ae5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ae5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ae5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12ae5be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12ae5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12ae5c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12ae5cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12ae5d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12ae5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12ae5dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12ae5e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12ae5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12ae5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ae5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ae5fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ae60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ae60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ae60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ae61270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ae61a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ae61ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ae62340 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae4c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae55990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae43d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae4ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae5f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae4aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae61530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae40270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae62600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae628c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae62e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae63910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae63bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae64150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae64990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae64c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae64f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae65490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae65750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae65cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae65f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae66510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae66a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae66d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae67010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae67590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae67850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae67b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae67dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae68090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae68350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae68610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae688d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae68b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae68e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae69110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae69690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae69950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae69c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae69ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae6a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae6a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae6a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae6a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae6ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae6b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae6b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae6b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae6ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae6bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae6bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae6c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae6c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae6c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae6cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae6cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae6d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae6d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae6d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae6d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae6db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae6de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae6e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae6e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae6e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae6e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae6ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae6ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae6f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae6f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae6f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae6f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae6fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae6ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae701d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae70490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae70750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae70a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae70cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae70f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae71250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae71510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae717d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae71a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae71d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae72010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae72590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae72850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae72b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae72dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae73090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae73350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae73610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae738d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae73b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae73e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae74110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae743d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae74690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae74950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae74c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae74ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae75190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae75450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae75710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae759d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae75c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae75f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae76210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae764d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae76790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae76a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae76d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae76fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae77290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae77550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae77810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae77ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae77d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae78050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae78310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae78890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae78b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae78e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae79390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae79650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae79910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae79bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae79e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae7a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae7a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae7a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae7a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae7ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae7af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae7b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae7b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae7b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae7ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae7bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae7bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae7c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae7c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae7c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae7ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12af05420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12af0e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12af08af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12af0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12af04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12af10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12af35610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12af358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12af35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12af35e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12af36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12af363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12af36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12af36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12af36c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12af36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12af37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12af37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12af37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12af379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12af37c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12af37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12af38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12af384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12af38790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12af38a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12af38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12af38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12af39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12af39550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12af39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12af39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12af39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12af3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12af3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12af3a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12af3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12af3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12af3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12af3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12af3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12af3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12af3b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12af3bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12af3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12af3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12af3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12af3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12af3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12af3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12af3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12af3d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12af3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12af3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12af3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12af3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12af3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12af3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12af3e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12af3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12af3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12af3ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12af3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12af3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12af3f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12af3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12af3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12af3fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12af40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12af40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12af40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12af408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12af40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12af40e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12af41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12af413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12af41690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12af41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12af41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12af41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12af42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12af42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12af42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12af429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12af42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12af42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12af43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12af434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12af43790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12af43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12af43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12af43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12af44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12af44550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12af44810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12af44ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.793s
user	0m0.278s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4865 (e128a1bf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15600a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15600ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15600b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15600b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15600be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15600c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15600c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15600cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15600d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15600da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15600df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15600e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15600ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15600f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15600ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156010650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156011490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156011bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156012380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156012aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1560131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1560138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156014180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1560148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156014d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1560151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156015880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156015d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1560161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156016b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156016e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1560172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156017770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156017c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1560180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156018550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1560189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156018e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156019330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1560197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156019c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15601a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15601a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15601a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15601adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15601b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15601bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15601c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15601c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15601ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15601cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15601d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15601d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15601dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15601e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15601e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15601eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15601f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15601f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15601f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15601fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1560200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156020560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156020a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156020ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156021340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1560217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156021c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156022120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1560225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156022a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156022fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156023500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156023fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1560244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156024a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156024f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1560254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156025a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1560264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156026a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156026f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1560274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156027a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156027f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1560284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156028a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156028f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1560294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1560299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15602a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15602a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15601b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15602ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15602b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15602bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15602c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15602c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15602cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15602d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15602d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15602db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15602e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15602e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15602eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15602f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15602f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15602fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15602ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156030450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1560308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156030d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156031230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1560316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156032010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1560324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156032950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156032df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156033290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156033730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156033bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156034070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156034510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1560349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156034e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1560352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156035790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156035c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1560360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156036570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156036a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156036eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156037350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1560377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156037c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156038130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1560385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156038a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156038f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1560393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156039850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156039cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15603a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15603a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15603aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15603af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15603b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15603b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15603bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15603c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15603c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15603cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15603cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15603d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15603d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15603ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15603e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15603e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15603eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15603f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15603f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15603f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15603fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1560402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156040750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156040bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156041090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156041530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1560419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156041e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156042310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1560427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156042c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1560430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156043590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156043a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156043ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156044370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156044810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156044cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1560455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156045a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156045f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1560463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156046870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156046d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156047260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1560477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156047d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156048250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1560486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156048b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156049030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1560494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156049970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156049e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15604a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15604a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15604aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15604b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15604b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15604ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15604bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15604c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15604ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15604cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15604d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15604dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15604e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15604e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15604ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15604f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15604f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15604fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156050280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156050830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156050de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156051390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156051940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156051ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1560524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156052a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156053000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1560535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156053b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156054110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1560546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156054c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156055220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1560557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156055d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156056330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1560568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156056e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156057440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1560579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156057fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156058550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156058b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1560590b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156059660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156059c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15605a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15605a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15605ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15605b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15605b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15605be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15605c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15605c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15605cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15605d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15605daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15605e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15605e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15605ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15605f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15605f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15605fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156060270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156060820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156060d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156061220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156061720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156061c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156062120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156062620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156062b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156063020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156063520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156063a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156063f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156064420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156064920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156064e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x156065320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x156065820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x156065d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x156066220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x156066720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x156066c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x156067120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x156067620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x156067b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x156068020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156068520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156068f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156069650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156069d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15606a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15606a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15606aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15606b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15606b820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.102.344 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.348 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154e08130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154e083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x154e08860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x154e08cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x154e09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x154e095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x154e09a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x154e09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x154e0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x154e0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x154e0abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x154e0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154e0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154e0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x154e0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154e0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154e0dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154e0e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154e0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154e0f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154e0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154e0ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154e106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154e11530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154e117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154e11ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154e11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154e12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154e12800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154e12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154e139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154e13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154e142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154e14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154e14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154e150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154e15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154e15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154e15eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154e16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154e167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154e16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154e17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154e175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154e17f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154e183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154e18850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154e18cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x154e19630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x154e19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154e19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154e1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154e1ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154e1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x154e1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154e1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x154e1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154e1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154e1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x154e1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154e1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154e1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154e1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154e1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154e1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154e1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154e1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x154e1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x154e1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x154e1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x154e1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x154e20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x154e206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x154e20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x154e20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x154e21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x154e218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x154e21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x154e22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x154e22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x154e22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x154e22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x154e23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x154e237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x154e23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x154e240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x154e24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x154e24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x154e24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x154e25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x154e256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x154e25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x154e25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x154e26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x154e26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x154e26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x154e27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x154e275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x154e27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x154e27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x154e28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x154e287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x154e28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x154e29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x154e294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x154e29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x154e29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154e2a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154e2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154e2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154e2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154e2b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154e2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154e2c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154e2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154e2ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154e2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154e2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154e2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154e2dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154e2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x154e2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154e2e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154e2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154e2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154e2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154e2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154e2ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154e303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154e30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154e30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154e31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154e315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154e31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154e31e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154e322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154e32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x154e32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154e33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154e334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x154e33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154e33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154e34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154e34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154e34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154e34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154e353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154e35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154e35ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154e36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154e36580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154e369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154e36e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154e372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154e37740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154e37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x154e38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154e38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154e38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154e39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154e39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x154e397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154e39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154e3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154e3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154e3a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154e3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154e3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154e3b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154e3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154e3bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154e3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154e3c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154e3ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154e3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154e3d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154e3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154e3deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154e3e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154e3e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154e3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154e3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154e3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x154e3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154e3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154e40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154e406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154e40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x154e40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154e413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x154e41860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154e41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154e42140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154e425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x154e42a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x154e431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154e434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154e43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x154e43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154e44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x154e44ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154e44f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154e45420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154e45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154e45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154e464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154e46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154e47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154e475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154e47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154e48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154e48700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154e49260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154e49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154e49dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154e4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154e4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154e4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154e4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154e4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154e4bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154e4c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154e4cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154e4d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154e4d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154e4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154e4e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154e4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154e4ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154e4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154e4f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154e4fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154e50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154e50f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154e51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154e51ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154e52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154e52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154e52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154e531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154e53750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x154e53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154e542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154e54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154e54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154e553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x154e55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154e55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154e564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154e56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154e57030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154e575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154e57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154e58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154e586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154e58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154e59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154e59800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x154e59db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x154e5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154e5a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154e5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154e5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154e5b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154e5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154e5c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154e5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154e5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154e5cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154e5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154e5d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154e5deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154e5e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x154e5e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x154e5edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x154e5f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x154e5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x154e5fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x154e601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x154e606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x154e60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x154e610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x154e615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154e61ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154e624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154e62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154e63300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154e63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154e63ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x154e64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x154e64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154e64db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15604d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1560565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1560554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1560521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15604f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15605ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15605c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15605a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156058260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156050540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15604dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156052d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156053e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156059370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156056040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15605dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156050af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156051c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156058dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15605afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156053870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156054980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156059ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156056ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156057150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156052760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15605f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15605cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15604e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156057cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15604d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15604f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15605f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156054f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1560687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15605d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1560532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156055a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156059920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1560510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15605b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15604ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15605e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15605bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156057700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156060530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15604ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15605ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15604e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15605e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156058810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15605aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15605d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15605c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1560543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156016740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15606bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15606bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15606c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15606c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15606c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15606c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15606cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15606ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15606d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15606d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15606d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15606d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15606dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15606dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15606e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15606e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15606e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15606e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15606ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15606ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15606f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15606f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15606f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15606fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15606fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15606ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156070260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156070520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1560707e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156070aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156070d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156071020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1560712e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1560715a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156071860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156071b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156071de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1560720a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156072360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156072620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1560728e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156072ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156072e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156073120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1560733e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1560736a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156073960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156073c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156073ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1560741a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156074460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156074720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1560749e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156074ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156074f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156075220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1560754e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1560757a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156075a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156075d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156075fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1560762a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156076560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156076820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156076ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156076da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156077060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156077320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1560775e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1560778a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156077b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156077e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1560780e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1560783a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156078660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156078920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156078be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156078ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156079160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156079420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1560796e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1560799a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156079c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156079f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15607a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15607a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15607a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15607aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15607ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15607afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15607b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15607b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15607b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15607baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15607bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15607c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15607c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15607c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15607c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15607cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15607cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15607d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15607d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15607d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15607d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15607dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15607de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15607e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15607e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15607e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15607e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15607ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15607eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15607f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15607f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15607f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15607f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15607fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15607ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156080220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1560804e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1560807a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156080a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156080d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156080fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1560812a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156081560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156081820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156081ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156081f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156082420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1560828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156082d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156083200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1560836a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156083b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156084090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1560845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156084b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156085080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156085340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156085600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156085b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156086000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156086500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156086a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156086f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1560874d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1560879d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156087ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1560885d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156088890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156088da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156089840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156089b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15608a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15608a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15608ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15608b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15608b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15608bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15608c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15608c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15608cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15608d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15608da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15608e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15608e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15608eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15608f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15608f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15608fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156090280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156090840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156090e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1560913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156091980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156091f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156092500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156092ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156093080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156093640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156093c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1560941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156094780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156094d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156095300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1560958c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156095e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156096440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156096a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156096fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156097580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156097b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156098100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1560986c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156098c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156099240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156099800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156099dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15609a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15609a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15609af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15609b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15609ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15609c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15609c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15609cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15609d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15609d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15609dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15609e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15609e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15609ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15609f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15609f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15609fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1560a0000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1560a0500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1560a0a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1560a0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1560a1400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1560a1900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1560a1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1560a2300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1560a2800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1560a2d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1560a3200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1560a3700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1560a3c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1560a4100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1560a4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1560a4b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1560a5000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1560a5500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1560a5a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1560a6410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1560a6b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1560a7250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1560a7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1560a7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1560a83c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1560a8680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1560a8b90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.232s
sys	0m0.189s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.09 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.52 sec*proc (2 tests)

Total Test time (real) =   1.54 sec
        1.56 real         0.51 user         0.19 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.12 user         0.08 sys
```
